[
  {
    "objectID": "research/hypotheses.html",
    "href": "research/hypotheses.html",
    "title": "Research questions",
    "section": "",
    "text": "In general, a research question’s nature depends on the goal and type of the pursued analysis. Table 1 provides a useful framework (Source: fig 3.9 in Francom 2024).\n\n\nTable 1: Overview of analysis types\n\n\n\n\n\n\n\n\n\nType\nAims\nApproach\nEvaluation\n\n\n\nExploratory\nExplore: gain insight\nInductive, data-driven, and iterative\nAssociative\n\n\nPredictive\nPredict: validate associations\nSemi-deductive, data-/ theory-driven, and iterative\nModel performance, feature importance, and associative\n\n\nInferential\nExplain: test hypotheses\nDeductive, theory-driven, and non-iterative\nCausal\n\n\n\n\n\n\n\n\nIs there a pattern in the WB project document corpus1 that reveals non-random variation in the frequency of certain words, phrases, or policy concepts over time?\n1 WB project documents refer here to Project Development Objectives (PDOs), which are brief descriptive texts.\nThe hypothesis is that the WB project document corpus exhibits non-random variation in the frequency of specific policy concepts2 over time. This question is approached through a data-driven analysis, where patterns observed in the text data inspire the data interrogation rather than starting with a predetermined assumption. \n2 Concepts include “policy focus,” “sector,” “strategy,” or “emerging priority” within the development funding landscape.\nIs there any external input (whether captured in other official documents or not) that could help “explain” or correlate with any observed non-random patterns in the text data?\nFor instance, a sudden rise in popularity of a particular policy goal or catchphrase might influence the choice of Project Development Objectives (PDO) in operations over a specific period.\n\nFraming this question for empirical investigation, I explored the possible correlation between the World Development Reports (WDR)3 and the frequency trends of specific sector and theme words in the PDO text data. In this analysis, the “alternative” hypothesis being tested is that the WDR has a “traction effect” on the PDOs of subsequent fiscal years.\n\n\n\n\n\n3 WDRs (World Development Reports) are the flagship reports that the World Bank Group has been publishing annually since 1978.\n\n\nGiven the incomplete feature tagging in the WB project document corpus, can predictive classification techniques help address such data limitations?\n\nThe hypothesis is that certain machine learning (ML) techniques can serve to enhance the quality of the source data. Some illustrative analysis has been conducted to predict the missing sector or theme tags, based on the text of the PDO description, plus other available metadata variables, testing various ML algorithms.\n\n\n\n\n\n\n\n\n\n\nFor now, the primary aim of the study is to EXPLORE (e.g., trends over time in phrase occurrence) and, to a lesser extent, to PREDICT (e.g., using ML to improve the quality of metadata variables). Potential follow-up questions will be shaped by the findings of this initial exploratory phase."
  },
  {
    "objectID": "research/hypotheses.html#exploratory-research-questions",
    "href": "research/hypotheses.html#exploratory-research-questions",
    "title": "Research questions",
    "section": "",
    "text": "Is there a pattern in the WB project document corpus1 that reveals non-random variation in the frequency of certain words, phrases, or policy concepts over time?\n1 WB project documents refer here to Project Development Objectives (PDOs), which are brief descriptive texts.\nThe hypothesis is that the WB project document corpus exhibits non-random variation in the frequency of specific policy concepts2 over time. This question is approached through a data-driven analysis, where patterns observed in the text data inspire the data interrogation rather than starting with a predetermined assumption. \n2 Concepts include “policy focus,” “sector,” “strategy,” or “emerging priority” within the development funding landscape.\nIs there any external input (whether captured in other official documents or not) that could help “explain” or correlate with any observed non-random patterns in the text data?\nFor instance, a sudden rise in popularity of a particular policy goal or catchphrase might influence the choice of Project Development Objectives (PDO) in operations over a specific period.\n\nFraming this question for empirical investigation, I explored the possible correlation between the World Development Reports (WDR)3 and the frequency trends of specific sector and theme words in the PDO text data. In this analysis, the “alternative” hypothesis being tested is that the WDR has a “traction effect” on the PDOs of subsequent fiscal years.\n\n\n\n\n\n3 WDRs (World Development Reports) are the flagship reports that the World Bank Group has been publishing annually since 1978."
  },
  {
    "objectID": "research/hypotheses.html#predictive-research-questions",
    "href": "research/hypotheses.html#predictive-research-questions",
    "title": "Research questions",
    "section": "",
    "text": "Given the incomplete feature tagging in the WB project document corpus, can predictive classification techniques help address such data limitations?\n\nThe hypothesis is that certain machine learning (ML) techniques can serve to enhance the quality of the source data. Some illustrative analysis has been conducted to predict the missing sector or theme tags, based on the text of the PDO description, plus other available metadata variables, testing various ML algorithms.\n\n\n\n\n\n\n\n\n\n\nFor now, the primary aim of the study is to EXPLORE (e.g., trends over time in phrase occurrence) and, to a lesser extent, to PREDICT (e.g., using ML to improve the quality of metadata variables). Potential follow-up questions will be shaped by the findings of this initial exploratory phase."
  },
  {
    "objectID": "posts/PDO_eda.html",
    "href": "posts/PDO_eda.html",
    "title": "Tracing policy signals in text",
    "section": "",
    "text": "I have always been fascinated by the idea of analyzing language as data and I finally found some time to study Natural Language Processing (NLP) and Text Analytics techniques.\nFor this learning project, I explore a dataset of World Bank Projects & Operations, with a focus on the text data contained in the Project Development Objective (PDO) section of World Bank’s projects (loans, grants, technical assistance). A PDO outlines, in synthetic form, the proposed objectives of operations, as defined in the early stages of the World Bank project cycle. \nNormally, a few objectives are listed in paragraphs that are a couple sentences long. Table 1 shows two examples.\n\n\n\nTable 1: Illustrative PDOs text in Projects’ documents\n\n\n\n\nProject_ID\nProject_Name\nProject_Development_Objective\n\n\n\nP127665\nSecond Economic Recovery Development Policy Loan\nThis development policy loan supports the Government of Croatia's reform efforts with the aim to: (i) enhance fiscal sustainability through expenditure-based consolidation; and (ii) strengthen investment climate.\n\n\nP179010\nTunisia Emergency Food Security Response Project\nTo (a) ensure, in the short-term, the supply of (i) agricultural inputs for farmers to secure the next cropping seasons and for continued dairy production, and (ii) wheat for uninterrupted access to bread and other grain products for poor and vulnerable households; and (b) strengthen Tunisia’s resilience to food crises by laying the ground for reforms of the grain value chain.\n\n\n\n\n\n\n\n\nThe dataset also includes some relevant metadata about the projects, including: country, fiscal year of approval, project status, main sector, main theme, environmental risk category, or lending instrument.s\n\n\n\n\n\n\nNote\n\n\n\n\n\nI retrieved the data on this page WBG Projects. Such data is classified by the World Bank as “public” and accessible under a Creative Commons Attribution 4.0 International License.",
    "crumbs": [
      "Blog",
      "Tracing policy signals in text"
    ]
  },
  {
    "objectID": "posts/PDO_eda.html#preprocessing-the-pdo-text-data",
    "href": "posts/PDO_eda.html#preprocessing-the-pdo-text-data",
    "title": "Tracing policy signals in text",
    "section": "Preprocessing the PDO text data",
    "text": "Preprocessing the PDO text data\nCleaning text data entails extra steps compared to numerical data. A key process is tokenization, which breaks text into smaller units like words, bigrams, n-grams, or sentences. After that, a common cleaning task is normalization, where text is standardized (e.g., converting to lowercase). Similarly, data reduction techniques like stemming and lemmatization simplify words to their root form (e.g., “running,” “ran,” and “runs” become “run”). This can help to reduce dimensionality, especially with very large datasets, when the word form is not relevant.\nUpon tokenization, it is very common to remove irrelevant elements like punctuation or stop words (unimportant words like “the”, “ii)”, “at”, or repeated ones in context like “PDO”) which add noise to the data.\nIn contrast, data enhancement techniques like part-of-speech (POS) tagging add value by identifying grammatical components, allowing focus on meaningful elements like nouns, verbs, or adjectives.",
    "crumbs": [
      "Blog",
      "Tracing policy signals in text"
    ]
  },
  {
    "objectID": "posts/PDO_eda.html#words-and-stems",
    "href": "posts/PDO_eda.html#words-and-stems",
    "title": "Tracing policy signals in text",
    "section": "Words and stems",
    "text": "Words and stems\nEvidently, after stemming, more words (or stems) reach the threshold frequency count of 800 (as they have been combined by root). Despite the pre-processing of PDOs’ text data, these aren’t particularly informative words.\n\n\n\n\n\nFigure 1",
    "crumbs": [
      "Blog",
      "Tracing policy signals in text"
    ]
  },
  {
    "objectID": "posts/PDO_eda.html#bigrams",
    "href": "posts/PDO_eda.html#bigrams",
    "title": "Tracing policy signals in text",
    "section": "Bigrams",
    "text": "Bigrams\nFigure 2 shows the most frequent bigrams in the PDO text data. The top-ranking bigrams align with expectations, featuring phrases like “increase access”, “service delivery”, “institutional capacity”, “poverty reduction” at the top. Notably, while “health” appears in several bigrams (e.g., “health services”, “public health”, “health care”), “education” is absent from the top 25. Another noteworthy observation is the frequent mention (over 100 instances) of “eligible crisis”, which was somewhat unexpected.\n\n\n\n\nFigure 2",
    "crumbs": [
      "Blog",
      "Tracing policy signals in text"
    ]
  },
  {
    "objectID": "posts/PDO_eda.html#trigrams",
    "href": "posts/PDO_eda.html#trigrams",
    "title": "Tracing policy signals in text",
    "section": "Trigrams",
    "text": "Trigrams\nFigure 3 shows the most frequent trigrams in the PDO text data. Here, the recurrence of phrases involving “health” is reiterated, along with a few phrases revolving around “environmental” goals, as well as terms that inherently belong together: like “water resource management”, “social safety net”, etc..\n\n\n\n\nFigure 3",
    "crumbs": [
      "Blog",
      "Tracing policy signals in text"
    ]
  },
  {
    "objectID": "posts/PDO_eda.html#sectors-in-the-pdo-text",
    "href": "posts/PDO_eda.html#sectors-in-the-pdo-text",
    "title": "Tracing policy signals in text",
    "section": "Sectors in the PDO text",
    "text": "Sectors in the PDO text\nTo focus on a meaningful set of tokens, I examined the frequency of sector-related terms within the PDO text data. To capture the broader concept of “sector,” I created a comprehensive SECTOR variable that encompasses all relevant words within an expanded definition.\n\n\n\n\n\n\nCustom Sector Definitions\n\n\n\n\n\nThe “sector” term discussed here is not the sector variable available in the data, but it is an artificial construct reflecting the occurrence of terms referred to the same sector semantic field. Besides conceptual association, these definitions are rooted in the World Bank’s own classification of sector and sub-sector.\nBelow are the “broad SECTOR” definitions used in this analysis:\n\n\nWAT_SAN = water|wastewater|sanitat|sewer|sewage|irrigat|drainag|river basin|groundwater\n\nTRANSPORT = transport|railway|road|airport|waterway|bus|metropolitan|inter-urban|aviation|highway|transit|bridge|port\n\nURBAN = urban|housing|inter-urban|peri-urban|waste manag|slum|city|megacity|intercity|inter-city|town\n\nENERGY = energ|electri|hydroele|hydropow|renewable|transmis|grid|transmission|electric power|geothermal|solar|wind|thermal|nuclear power|energy generation\n\nHEALTH = health|hospital|medicine|drugs|epidem|pandem|covid-19|vaccin|immuniz|diseas|malaria|hiv|aids|tb|maternal|clinic|nutrition\n\nEDUCATION = educat|school|vocat|teach|univers|student|literacy|training|curricul|pedagog\n\nAGR_FOR_FISH = agricultural|agro|fish|forest|crop|livestock|fishery|land|soil\n\nMINING_OIL_GAS = minin|oil|gas|mineral|quarry|extract|coal|natural gas|mine|petroleum|hydrocarbon\n\nSOCIAL_PROT = social protec|social risk|social assistance|living standard|informality|insurance|social cohesion|gig economy|human capital|employment|unemploy|productivity|wage lev|intergeneration|lifelong learn|vulnerab|empowerment|sociobehav\n\nFINANCIAL = bank|finan|investment|credit|microfinan|loan|financial stability|banking|financial intermed|fintech\n\nICT = information|communication|ict|internet|telecom|cyber|data|ai|artificial intelligence|blockchain|e-learn|e-commerce|platform|software|hardware|digital\n\nIND_TRADE_SERV = industry|trade|service|manufactur|tourism|trade and services|market|export|import|supply chain|logistic|distribut|e-commerce|retail|wholesale|trade facilitation|trade policy|trade agreement|trade barrier|trade finance|trade promotion|trade integration|trade liberalization|trade balance|trade deficit|trade surplus|trade war|trade dispute|trade negotiation|trade cooperation|trade relation|trade partner|trade route|trade corridor\n\nINSTIT_SUPP = government|public admin|institution|central agenc|sub-national gov|law|justice|governance|policy|regulation|public expenditure|public investment|public procurement\n\nGENDER_EQUAL = gender|women|girl|woman|femal|gender equal|gender-base|gender inclus|gender mainstream|gender sensit|gender respons|gender gap|gender-based|gender-sensitive|gender-responsive|gender-transform|gender-equit|gender-balance\n\nCLIMATE = climate chang|environment|sustain|resilience|adaptation|mitigation|green|eco|eco-|carbon|carbon cycle|carbon dioxide|climate change|ecosystem|emission|energy effic|greenhouse|greenhouse gas|temperature anomalies|zero net|green growth|low carbon|climate resilient|climate smart|climate tech|climate variab\n\n\n\n\nThe occurrence trends over time for key sector terms are shown in Figure 4.\nInterestingly, all the broadly defined “sector term” in the PDO present one or more peaks at some point in time. For the (broadly defined) HEALTH sector, it is likely that Covid-19 triggered the peak in 2020. What about the other sectors? What could be the driving reason?\n\n\n\n\nFigure 4\n\n\n\n\n\n\n\nA possible explanation is that the PDOs may echo themes from the World Development Reports (WDR), the World Bank’s flagship annual publication that analyzes a key development issue each year. Far from being speculative research, each WDR is grounded in the Bank’s field-based insights and, in turn, it informs the Bank’s policy and operational priorities. This would suggest a likely alignment between WDR themes and project objectives in the PDOs.\nTo some extent, visual exploration (see examples below) seems to support this hypothesis: thematically relevant WDRs consistently appear in close proximity to peaks in sector-related term frequencies. However, further validation is necessary. Additionally, preparing each WDR typically takes 2-3 years, so a temporal alignment with project documents may include some lag.\n\n\n\nExamples of sectors-term trend\nFigure 5 shows a “combined sector” that is quite broadly defined (AGRICULTURE, FORESTRY, FISHING) with the highest peak in 2010, two years after the publication of the WDR on “Agriculture for Development”. Perhaps the “alignment” hypothesis is not very meaningful with such a broadly defined sector.\n\n\n\n\nFigure 5\n\n\n\n\n\n\n\nFigure 6, tracking frequency of CLIMATE-related terms, shows how the highest peak coincided with the publication of the WDR on “Development and Climate Change” in 2010.\n\n\n\n\nFigure 6\n\n\n\n\n\n\n\nFigure 7 reports two WDR publications relevant to EDUCATION, which seemingly preceded two peaks in the sector-related terms in the PDOs:\n\nin 2007, on “Development and the Next Generation”\n\nin 2018, on “Learning to Realize Education’s Promise”\n\n\n\n\n\n\nFigure 7\n\n\n\n\n\n\n\nFigure 8 shows that the highest frequency of terms related to GENDER EQUALITY was instead recorded a couple of years before the publication of the WDR on “Gender Equality and Development” in 2012.\n\n\n\n\nFigure 8",
    "crumbs": [
      "Blog",
      "Tracing policy signals in text"
    ]
  },
  {
    "objectID": "posts/PDO_eda.html#comparing-pdo-text-against-variable-sector",
    "href": "posts/PDO_eda.html#comparing-pdo-text-against-variable-sector",
    "title": "Tracing policy signals in text",
    "section": "Comparing PDO text against variable sector\n",
    "text": "Comparing PDO text against variable sector\n\nThe available data includes not only text but also relevant metadata, such as the sector1 variable, which captures the project’s primary sector. Do the terms in the PDO text align with this sector label? To examine this, I applied the two-sample Kolmogorov-Smirnov test to compare the distribution of sector-related terms in the PDO text with the distribution of sector1.\n\n\n\n\n\n\nNote\n\n\n\n\n\nThe Kolmogorov-Smirnov test is non-parametric and makes no assumptions about the underlying distributions, making it a versatile tool for comparing distributions. The null hypothesis is that the two samples are drawn from the same distribution. Hence, if the p-value is less than the significance level (0.05), the null hypothesis is rejected, suggesting the observed distributions are in fact different. The test statistic is the maximum difference between the cumulative distribution functions (CDF) of the two samples.\n\n\nKS statistic: The vectors of observed distributions have been rescaled (bringing n_pdo and n_tag to a [0, 1] range before applying the Kolmogorov-Smirnov (KS) test). This is useful when distributions differ substantially in scale or units, as it makes them directly comparable in relative terms.\n\n\n\n\nAs shown in Table 2, the results indicate similar distributions across most sectors. This is promising, as it suggests that in cases where metadata is lacking, sector assignments can be reasonably inferred from the PDO text.\n\n\n\nTable 2: Comparing the freqeuncy distributions of SECTOR in text and metadata\n\n\n\n\nSECTORS\nKS statistic\nKS p-value\nDistributions\n\n\n\nMINING_OIL_GAS\n0.5882\n0.0030\nDissimilar\n\n\nENERGY\n0.4091\n0.0452\nDissimilar\n\n\nEDUCATION\n0.3182\n0.1836\nSimilar\n\n\nTRANSPORT\n0.3182\n0.1976\nSimilar\n\n\nHEALTH\n0.2273\n0.6009\nSimilar\n\n\nICT\n0.2000\n0.7909\nSimilar\n\n\nWAT_SAN\n0.1818\n0.8479\nSimilar\n\n\n\n\n\n\n\n\nBelow is a graphical representation of two illustrative sectors, showing the most similar and the most dissimilar distributions of the sector as deducted form text data, versus the proper metadata sector labeling.\nFigure 9 shows the distributions of the TRANSPORT sector in the PDOs’ text and in the metadata. The two distributions are the most similar, as confirmed by the Kolmogorov-Smirnov test with a p-value of 0.641.\n\n\n\n\nFigure 9\n\n\n\n\n\n\n\nFigure 10 compares visually the distributions of the ENERGY sector in the PDOs’ text data and the metadata. The two distributions are the most dissimilar, as the Kolmogorov-Smirnov test confirms with a p-value of 0.0001.\n\n\n\n\nFigure 10",
    "crumbs": [
      "Blog",
      "Tracing policy signals in text"
    ]
  },
  {
    "objectID": "posts/PDO_eda.html#comparing-pdo-text-against-variable-amount-committed",
    "href": "posts/PDO_eda.html#comparing-pdo-text-against-variable-amount-committed",
    "title": "Tracing policy signals in text",
    "section": "Comparing PDO text against variable amount committed\n",
    "text": "Comparing PDO text against variable amount committed\n\nA similar question is: do word trends observed in PDOs also reflect the allocation of funds by sector? I explored this question with the same approach as before, but this time I compared the distribution of sector-related terms in the PDOs’ text against the distribution of the sum of the amount committed in corresponding projects (i.e. filtered by sector1 category). Given the very different ranges, I compared rescaled values (using the Kolmogorov-Smirnov two-sample test) to evaluate the independence of these two distributions.\nAs shown in Table 3, the results indicate less homogeneity of the distributions across key sectors, somthing that could be further investigated.\n\n\n\nTable 3: Comparing the distributions of SECTOR in text and in corresponding $$ committed\n\n\n\n\nSECTORS\nKS statistic\nKS p-value\nDistributions\n\n\n\nICT\n0.6818\n0.0000\nDissimilar\n\n\nMINING_OIL_GAS\n0.5909\n0.0007\nDissimilar\n\n\nEDUCATION\n0.5000\n0.0069\nDissimilar\n\n\nENERGY\n0.2727\n0.3867\nSimilar\n\n\nHEALTH\n0.2727\n0.3937\nSimilar\n\n\nWAT_SAN\n0.2273\n0.6276\nSimilar\n\n\nTRANSPORT\n0.2273\n0.6324\nSimilar\n\n\n\n\n\n\n\n\nLet us pick a couple of examples of specific sectors to check visually.\nWATER & SANITATION sector: words v. funding\nThe distributions in the “WATER & SANITATION” sector are among the most similar pairs (K-S test p-value is = 0.4218).\n\n\n\n\nFigure 11\n\n\n\n\n\n\n\nICT sector: words v. funding\nThe distributions in the ICT sector are among the least similar (K-S test p-value is = 0.0001).\n\n\n\n\nFigure 12",
    "crumbs": [
      "Blog",
      "Tracing policy signals in text"
    ]
  },
  {
    "objectID": "posts/PDO_eda.html#concordances-a.k.a.-keywords-in-context",
    "href": "posts/PDO_eda.html#concordances-a.k.a.-keywords-in-context",
    "title": "Tracing policy signals in text",
    "section": "Concordances: a.k.a. keywords in context",
    "text": "Concordances: a.k.a. keywords in context\nAnother useful analysis that can be done exploring text data refers to concordance, which enables a closer look at the context surrounding a word (or combination of words). This approach can help clarify the word’s specific meaning or reveal underlying patterns in the data.\nThe bigram “eligible crisis” in the PDOs\nFor instance, among the most frequent bigrams (two-word combinations) in the PDO text (illustrated in Figure 2), the phrase “eligible crisis” stands out. Besides appearing in the PDOs of 112 projects, this phrase is often used in a similar context. Specifically, in 32% of these cases, it is paired with phrases like “respond promptly and effectively” or “immediate and effective response”. As shown in Table 4, this suggests a sort of recurring standard phrasing.\n\n\n\nTable 4: Context of the bigram “eligible crisis” in the PDOs\n\n\n\n\nWB Project ID\nExcerpt of PDO Sentences with 'Eligible Crisis'\n\n\n\nP179636\n(...) and (iii) respond effectively in case of an eligible crisis or emergency.\n\n\nP176982\n(...) borrower’s territory; and (iii) in case of an eligible crisis or emergency, respond promptly and effectively to it.\n\n\nP147827\n(...) of associated institutions, and in case of an eligible crisis or emergency, respond promptly andeffectively to it.\n\n\nP177816\n(...) in project areas, and, in case of an eligible crisis or emergency, to respond promptly and effectively to\n\n\nP125961\n(...) an earlyemergency response in the event of an eligible crisis or emergency.\n\n\nP156012\n(...) health services, and, in the event of an eligible crisis or emergency, to provide immediate and effective response\n\n\n\nP171093\n(...) communities and to provide immediate response to an eligible crisis or emergency as needed.\n\n\nP158231\n(...) communities and to provide immediate response to an eligible crisis or emergency as needed.\n\n\nP147280\n(...) and to respond effectively in case of an eligible crisis or emergency\n\n\nP167512\n(...) provide an immediate and effective response to an eligible crisis or emergency.\n\n\n\n\n\n\n\n\nThe bigram “climate change” in the PDOs\nAnother frequently occurring bigram is “climate change”, found in 92 PDOs. Table 5 displays words that commonly appear near this bigram. Notably, the word “mitigation” (which I associate with a more aspirational, long-term response) appears more frequently than “adaptation” (which I view as a more practical, short-term response). However, the ratio would flip considering that “resilience” may convey a similar practical intent as “adaptation”. Another interesting insight worth exploring further in the future.\n\n\n\nTable 5: Frequent words near “climate change”\n\n\n\n\nNear 'climate change'\nCount\nPercentage\n\n\n\nvulnerability\n23\n33.8%\n\n\nresilience\n16\n23.5%\n\n\nmitigate\n14\n20.6%\n\n\nadapt\n9\n13.2%\n\n\nhazard\n6\n8.8%\n\n\n\n\n\n\n\n\nTable 6 shows a few examples for each of the words most frequently found in the vicinity of the bigram “climate change”.\n\n\n\nTable 6: Context of the bigram “climate change” in the PDOs\n\n\n\n\nNear word (root)\nWB Project ID\nClosest Text\n\n\n\nadapt\n\n\nadapt\nP128137\n(...) phase i of the disaster risk management and climate change adaptation project are to strengthen the ca pacity\n\n\nadapt\nP091979\n(...) arid and semi-arid lands to plan and implement climate change adaptation measures\n\n\nadapt\nP120134\n(...) support the gom's efforts to foster adaptation to climate change in the water sector, contributing to long-term sustainable\n\n\nhazard\n\n\nhazard\nP177124\n(...) islands to the impacts of natural hazards and climate change\n\n\n\nhazard\nP146768\n(...) buildings and infrastructure due to natural hazards or climate change impacts; and (b) increased capacity of oecs governments\n\n\nhazard\nP123896\n(...) agencies to financial protection from losses caused by climate change and geological hazards.\n\n\nmitig\n\n\nmitig\nP077763\n(...) goal of the fund is to mitigate the climate change and demonstrate the possibilities of public -private partnerships\n\n\nmitig\nP081743\n(...) to help mitigate global climate change through certified carbon emission reductions (cers) of 178,000\n\n\nmitig\nP111940\n(...) developing actions to mitigate the effects of global climate change in the atlantic rain forest, ensuring the conservation\n\n\nresil\n\n\nresil\nP114294\n(...) implement measures to enhance biodiversity resilie nce to climate change and protect forest carbon assets.\n\n\nresil\nP170052\n(...) iii) strengthening financial resilience to natural disasters and climate change\n\n\n\nresil\nP178141\n(...) in the city, strengthen the city’s resilience to climate change and enhance access to basic services in the\n\n\nvulnerab\n\n\nvulnerab\nP117871\n(...) at measurably reducing vulnerability to natural hazards and climate change impacts in the eastern caribbean sub-region.\n\n\nvulnerab\nP146768\n(...) at measurably reducing vulnerability to natural hazards and climate change impacts in the eastern caribbean sub-region.\n\n\nvulnerab\nP149259\n(...) at measurably reducing vulnerability to natural hazards and climate change impacts in the eastern caribbean sub-region.",
    "crumbs": [
      "Blog",
      "Tracing policy signals in text"
    ]
  },
  {
    "objectID": "posts/PDO_eda.html#using-ml-models-to-predict-a-missing-feature",
    "href": "posts/PDO_eda.html#using-ml-models-to-predict-a-missing-feature",
    "title": "Tracing policy signals in text",
    "section": "Using ML models to predict a missing feature",
    "text": "Using ML models to predict a missing feature\nThe goal at hand has to do with text classification, that is assigning categories to some observations. To predict a missing feature based on a mix of text data and other available predictors, several machine learning (ML) algorithms can be applied. I tested a few suitable algorithms.\nThe sample splitting (necessary in ML to save testing dataset for model evaluation) was done based on the availability of the env_cat variable. The sample was actually split into three groups:\n\n\nTraining set (with env_cat available) 2,264 observations\n\nTesting set (with env_cat available) 972 observations\n\nValidation set (with env_cat missing) 1,167 observations",
    "crumbs": [
      "Blog",
      "Tracing policy signals in text"
    ]
  },
  {
    "objectID": "posts/PDO_eda.html#choosing-the-ml-algorithm",
    "href": "posts/PDO_eda.html#choosing-the-ml-algorithm",
    "title": "Tracing policy signals in text",
    "section": "Choosing the ML algorithm",
    "text": "Choosing the ML algorithm\nTo predict the missing binary categorical outcome env_cat_f2, I tried several models, including: Lasso logistic regression (with different specifications including only text or a mix of text and other predictors) and Naive Bayes classification (Here I only report the results, but details can be found on this webpage). Since text data is sparse and high-dimensional, it is critical to perform some pre-treatment of the features (i.e. the explanatory variables) before modeling.\n\n\n\n\n\n\nModels syntethic description\n\n\n\n\n\n\nLASSO models (for logistic regression) is an approach that basically defines how much of a penalty to put on some features in order to select only the most useful out of all the original possible variables (tokens). It is a good choice when dealing with a high-dimensional dataset, like text data.\nNaïve Bayes classification is a simple and efficient algorithm for text classification. It assumes feature independence, which may not always hold, but it’s often a good baseline, particularly with short texts.\n\nOther supervised ML algorithms could be used in this case, such as Random Forest, Support-Vector Machines, K-Nearest Neighbors, but they were not tested here.\nThe steps to predict the missing feature\n\nOutcome label engineering: Define what to predict (outcome variable, \\(y\\)), and its functional form (binary or multiclass, log form or not if numeric)./ \nSample design: Select the observations to use. In ML this is typically done by splitting the sample into training and testing sets. \n\nFeature Engineering: Define the input data (predictors, \\(X\\)) and their format. Here, text data was combined with other predictors (e.g. sector, region, FY approved, etc.) to create a feature matrix.\n\n\nText preprocessing: The text data was preprocessed by tokenization, filtering of tokens by frequency, removal of stopwords, weighting via TF-IDF (Term Frequency-Inverse Document Frequency), to make it suitable for ML algorithms.\n\n   \n\n\nModel selection and fitting: The models were trained on the training set.\n\nDifferent algorithms will have different parameters that can be adjusted which can affect the performance of the model (hyperparameters tuning, typically done while training the model).\n\n\n\n  \n\nPrediction: The best model was used to predict the missing env_cat_f2 and evaluate the model’s performance on the hold-out sample (testing set).\n\nEvaluation: The predictions were evaluated on the testing set based on performance metrics:\n\n\naccuracy, which is the proportion of correct predictions, and\n\nROC-AUC (Receiver Operating Characteristic - Area Under the Curve), which summarizes how well the model can distinguish between classes.\n\n   \n\nInterpretation: The model was interpreted to understand which features were most important in predicting the outcome.\n\n\nML is an iterative process, so it is common to revise (some of) the above steps multiple times to refine the model.",
    "crumbs": [
      "Blog",
      "Tracing policy signals in text"
    ]
  },
  {
    "objectID": "posts/PDO_eda.html#models-and-results",
    "href": "posts/PDO_eda.html#models-and-results",
    "title": "Tracing policy signals in text",
    "section": "Models and Results",
    "text": "Models and Results\nTable 9 reports the specifications of the models and their performance.\n\n\n\nTable 9: Comparison of models and results for binary outcome\n\n\n\n\nAlgorithm\nFeatures\nSpecification\nAccuracy\nROC_auc\n\n\n\nLASSO logistic regression\nText only\nenv_cat_f2 ~ pdo\n0.750\n0.777\n\n\nLASSO logistic regression (more preprocessing)\nText only\nenv_cat_f2 ~ pdo\n0.762\n0.807\n\n\nLASSO logistic regression (more preprocessing)\nText + other predictors\nenv_cat_f2 ~ pdo + sector_f + regionname + FYapprov\n0.790\n0.850\n\n\nNaïve Bayes classification\nText + other predictors\nenv_cat_f2 ~ pdo + sector_f + regionname + FYapprov\n0.691\n0.784\n\n\n\n\n\n\n\n\nThe best model performance was achieved by the LASSO logistic regression model that combined both PDOs’ text and some available metadata information to predict the missing env_cat_f2 in the testing set. The model achieved an accuracy of 0.79 and an ROC-AUC of 0.85, whereas:\n\n\naccuracy is the proportion of correct predictions made by the model out of all predictions or, in other words, how often the model is correct overall.\n\nROC-AUC (Receiver Operating Characteristic - Area Under the Curve) goes further by evaluating the model’s ability to distinguish between classes across various thresholds. It summarizes how well the model can separates the classes, providing a more nuanced view of its performance, especially useful when the class distribution is uneven.",
    "crumbs": [
      "Blog",
      "Tracing policy signals in text"
    ]
  },
  {
    "objectID": "posts/PDO_eda.html#performance-of-the-preferred-ml-model",
    "href": "posts/PDO_eda.html#performance-of-the-preferred-ml-model",
    "title": "Tracing policy signals in text",
    "section": "Performance of the preferred ML model",
    "text": "Performance of the preferred ML model\nFigure 13 presents the confusion matrix for the preferred ML model used to predict the missing environment risk category assigned to World Bank projects. This matrix shows the distribution of true and predicted classifications. Ideally, a high-performing model would have most observations (or darker shading) along the diagonal, indicating correct classifications—specifically, true positives in the top-left quadrant and true negatives in the bottom-right quadrant.\nIn this case, the model performs well in predicting the environment risk category for the High-Med group but struggles with the Low & Other group. Many of these cases are incorrectly classified as High-Med Risk (false positives). This result is understandable, as the Low & Other category is more loosely defined and even includes Missing observations (which, in hindsight, could have been excluded from the prediction).\n\n\n\n\nFigure 13",
    "crumbs": [
      "Blog",
      "Tracing policy signals in text"
    ]
  },
  {
    "objectID": "posts/PDO_eda.html#most-important-features-for-prediction",
    "href": "posts/PDO_eda.html#most-important-features-for-prediction",
    "title": "Tracing policy signals in text",
    "section": "Most important features for prediction",
    "text": "Most important features for prediction\nIt’s also insightful to examine which coefficients are most influential in the model. This can be done visually through the feature importance plot (see Figure 14).\nThe feature importance plot displays the top 50 predictors of the environmental risk (binary) category, ranked by their impact in a LASSO logistic regression model. For clarity, predictors are divided according to the risk level they predict. As expected, given the structure of the data, words from the PDO text (those variables starting with pdo_*) are among the most important predictors. However, other predictors also play a significant role, such as sector_f_TRANSPORT (left panel), regionname, and sector_f_FINANCIAL (right panel).\n\n\n\n\nFigure 14: Top 50 most important features in the preferred ML model",
    "crumbs": [
      "Blog",
      "Tracing policy signals in text"
    ]
  },
  {
    "objectID": "posts/PDO_eda.html#prediction-and-interpretation",
    "href": "posts/PDO_eda.html#prediction-and-interpretation",
    "title": "Tracing policy signals in text",
    "section": "Prediction and Interpretation",
    "text": "Prediction and Interpretation\nWhile the model’s prediction performance is not particularly remarkable, it is sufficient to illustrate the potential of this analysis to enhance the quality of incomplete datasets. With further improvements in preprocessing, feature engineering, algorithm selection, and hyperparameter tuning, there is significant potential to optimize a similar ML model.\nAlthough not reported here, I also explored predicting a multiclass outcome (sector, grouped into 7 levels). However, the results were less favorable compared to the binary classification. This outcome is expected, as multiclass classification is inherently more challenging, particularly with imbalanced data or limited sample sizes.",
    "crumbs": [
      "Blog",
      "Tracing policy signals in text"
    ]
  },
  {
    "objectID": "analysis/02a_WB_project_pdo_feat_class_envcat.html",
    "href": "analysis/02a_WB_project_pdo_feat_class_envcat.html",
    "title": "WB Project PDO features classification",
    "section": "",
    "text": "Warning\n\n\n\n\n\nWORK IN PROGRESS! (Please expect unfinished sections, and unpolished code. Feedback is welcome!)"
  },
  {
    "objectID": "analysis/02a_WB_project_pdo_feat_class_envcat.html#what-ml-models-work-with-text",
    "href": "analysis/02a_WB_project_pdo_feat_class_envcat.html#what-ml-models-work-with-text",
    "title": "WB Project PDO features classification",
    "section": "What ML models work with text?",
    "text": "What ML models work with text?\n\nRemember that text data is SPARSE!\n\nTo predict a missing feature (e.g., sector) based on available features from text data, several supervised machine learning algorithms can be applied. Given that you have a mixture of text and structured data, here are some suitable algorithms:\n\n\nLogistic Regression / Multinomial Logistic Regression: If you’re predicting a categorical variable like “sector”, logistic regression can work well, especially with appropriate feature engineering for text (e.g., converting text data into numeric features using TF-IDF or word embeddings).\n\nLasso regression/classification learns how much of a penalty to put on some features (sometimes penalizing all the way down to zero) so that we can select only some features out of the high-dimensional space of original possible variables (tokens) for the final model.\n\nk-Nearest Neighbors (k-NN): k-NN can be useful for text data, especially when you have a mix of structured and unstructured data. It’s a simple algorithm that can work well with text data, but it can be computationally expensive.\n\nDecision Trees / Random Forests: These algorithms handle both numeric and categorical data efficiently and can manage missing values quite well. You can input text-based features as well, though you might need to preprocess the text into numeric form (e.g., using embeddings).\n\nNaive Bayes: Naive Bayes is a simple and efficient algorithm for text classification. It assumes feature independence, which may not always hold, but it’s often a good baseline, particularly with short texts.\n\nSupport Vector Machines (SVMs): SVMs are useful when you have high-dimensional data, which is common with text after feature extraction (like TF-IDF). They can perform well with a mix of structured and unstructured data.  \n\n\nSome model parameters can be learned from data during fitting/training. Some CANNOT 😱. These are hyperparameters, and we estimate them by training lots of models with different hyperparameters and comparing them\nCheck missing feature\n\nnames (projs_train2)\n\ntot &lt;- sum(!is.na(projs_train2$pdo)) # 4425\nsum(!is.na(projs_train2$regionname)) / tot  # 100%\nsum(!is.na(projs_train2$countryname)) / tot  # 100%\nsum(!is.na(projs_train2$status)) / tot  # 100%\nsum(!is.na(projs_train2$lendinginstr)) / tot  # 98% \nsum(!is.na(projs_train2$curr_total_commitment)) / tot  # 100% \n\nsum(!is.na(projs_train2$ESrisk)) / tot  # 0.092  !!!!!\nprojs_train2 |&gt; count(ESrisk) # 4 levels+ NA\n\nsum(!is.na(projs_train2$env_cat)) / tot  # 72%\ntable(projs_train2$env_cat, useNA = \"ifany\") # 5 levels+ NA\nprojs_train2 |&gt; count(env_cat) # 5 levels+ NA\n\nsum(!is.na(projs_train2$sector1)) /tot# 99%\nprojs_train2 |&gt; count(sector1) # 76levels\n\nsum(!is.na(projs_train2$theme1)) /tot # 71%  --&gt; 99%\nprojs_train2 |&gt; count(theme1, useNA = \"ifany\") # 81 levels\n\n\n# source function\nsource(here(\"R\",\"f_recap_values.R\")) \n\n# check candidate lables for classification \nf_recap_values(projs_train2, c(\"sector1\", \"theme1\",\"env_cat\",\"ESrisk\")) %&gt;% \n   kable()\n\n\n\nskim_variable\ntotal_rows\nn_distinct\nn_missing\nmissing_perc\n\n\n\nsector1\n4425\n76\n5\n0.1%\n\n\ntheme1\n4425\n81\n7\n0.2%\n\n\nenv_cat\n4425\n6\n1195\n27%\n\n\nESrisk\n4425\n5\n4014\n90.7%\n\n\n\n\n\nIdentify features for classification\nThese could be:\n\n\nfeatures derived from raw text (e.g. characters, words, ngrams, etc.),\n\nfeature vectors (e.g. word embeddings), or\n\nmeta-linguistic features (e.g. part-of-speech tags, syntactic parses, or semantic features)\n\nHow do we use them?\n\nDo we use raw token counts?\n\nDo we use normalized frequencies?\nDo we use some type of weighting scheme? ✅\n\nyes, we use tf-idf (a weighting scheme, which will downweight words that are common across all documents and upweight words that are unique to a document)\n\n\nDo we use some type of dimensionality reduction? ✅ # TEXT CLASSIFICATION for Environmental Assessment Category\n\nFrancom text Classification (in R) Stanford slide"
  },
  {
    "objectID": "analysis/02a_WB_project_pdo_feat_class_envcat.html#prep-for-data-split-based-on-outcome-projs_train2",
    "href": "analysis/02a_WB_project_pdo_feat_class_envcat.html#prep-for-data-split-based-on-outcome-projs_train2",
    "title": "WB Project PDO features classification",
    "section": "0) Prep for data split based on outcome [projs_train2]",
    "text": "0) Prep for data split based on outcome [projs_train2]\nRecode env_cat variable\n\nprojs_train2 &lt;- projs_train2 %&gt;% \n   # useful for later \n   # rename(., proj_id = \"id\") %&gt;%\n   # recode as factors \n   dplyr::mutate (across (c(status, FY_appr, regionname, countryname, sector1, \n                            theme1, lendinginstr), as.factor))  %&gt;% \n   # env risk category 7 levels\n   dplyr::mutate(env_cat_f = forcats::fct_na_value_to_level(\n      env_cat, level = \"Missing\")) %&gt;% \n   dplyr::mutate(env_cat_f = forcats::fct_recode(\n      env_cat_f, \n      \"A_high risk\" = \"A\", \n      \"B_med risk\" = \"B\", \n      \"C_low risk\" = \"C\", \n      \"F_fin expos\" = \"F\", \n      # \"Other\" = \"H\", \n      # \"Other\" = \"M\", \n      \"Other\" = \"U\", \n      \"Missing\" = \"Missing\" )) %&gt;% \n   dplyr::relocate(env_cat_f , .after = env_cat) %&gt;% \n   # collaapse env_cat_f into 2 levels\n   dplyr::mutate(env_cat_f2 = forcats::fct_collapse(\n      env_cat_f, \n      \"High-Med-risk\" = c(\"A_high risk\", \"B_med risk\"),\n      \"Low-risk_Othr\" = c(\"C_low risk\", \"F_fin expos\", \"Other\", \"Missing\")\n   )) %&gt;% \n   dplyr::relocate(env_cat_f2, .after = env_cat_f)\n\n# Recap\ntabyl(projs_train2, env_cat, show_na = TRUE) # 7 levels\ntabyl(projs_train2, env_cat_f, show_na = TRUE) # 2 levels\ntabyl(projs_train2, env_cat_f2, show_na = TRUE) # 7levels\n\n\n# Show as contingency table env_cat_f and env_cat_f2\nenv_cat_f_f2_k &lt;- table(projs_train2$env_cat_f, projs_train2$env_cat_f2) %&gt;% \n   kable() %&gt;% \n   kable_styling(\"striped\", full_width = F)\n\nenv_cat_f_f2_k\nsaveRDS(env_cat_f_f2_k, here(\"analysis\", \"output\", \"tables\", \"env_cat_f_f2_k.rds\"))\n\n\nTable 1: Recoded Environmental Assessment Category\n\n\n\n\n\nHigh-Med-risk\nLow-risk_Othr\n\n\n\nA_high risk\n315\n0\n\n\nB_med risk\n1837\n0\n\n\nC_low risk\n0\n905\n\n\nF_fin expos\n0\n122\n\n\nOther\n0\n51\n\n\nMissing\n0\n1195"
  },
  {
    "objectID": "analysis/02a_WB_project_pdo_feat_class_envcat.html#recode-sector1-variable",
    "href": "analysis/02a_WB_project_pdo_feat_class_envcat.html#recode-sector1-variable",
    "title": "WB Project PDO features classification",
    "section": "Recode sector1 variable",
    "text": "Recode sector1 variable\n(this I use later as MULTI-CLASS outcome)\n\n# !!!!! `sector_f` e' diverso da `tok_sector_broad` XCHE si basa su `sector1` !!!! \nprojs_train2 &lt;- projs_train2 %&gt;% # sector1 99 levels\n   mutate(sector_f = case_when(\n      str_detect(sector1, regex(\"water|wastewater|sanitat|Sewer|Irrigat|Drainag\", \n                                ignore_case = T)) ~ \"WAT & SAN\",\n      str_detect(sector1, regex(\"transport|railway|road|airport|waterway|bus|metropolitan|inter-urban|aviation\",\n                                ignore_case = T)) ~ \"TRANSPORT\",\n      sector1 == \"port\" ~ \"TRANSPORT\",\n      str_detect(sector1, regex(\"urban|housing|inter-urban|peri-urban|waste\",\n                                ignore_case = T)) ~ \"URBAN\",\n      str_detect(sector1, regex(\"energ|electri|hydroele|hydropow|renewable|transmis\",\n                                ignore_case = T)) ~ \"ENERGY\",  # Matches either \"energy\" or \"power\"\n      str_detect(sector1, regex(\"health|hospital|medicine|drugs|epidem|pandem|covid-19|vaccin\",\n                                ignore_case = T)) ~ \"HEALTH\",\n      str_detect(sector1, regex(\"educat|school|vocat|teach|univers|student|literacy|training|curricul\",\n                                ignore_case = T)) ~ \"EDUCATION\",\n      \n      str_detect(sector1, regex(\"Agricultural|Agro|Fish|Forest|Crop|livestock|agri-business\",\n                                ignore_case = T)) ~ \"AGR FOR FISH\",\n      str_detect(sector1, regex(\"Minin|oil|gas|mineral|Extract\",\n                                ignore_case = T)) ~ \"MINING OIL&GAS\",\n      str_detect(sector1, regex(\"Social Protec\",\n                                ignore_case = T)) ~ \"SOCIAL PROT.\",\n      \n      str_detect(sector1, regex(\"Bank|finan|Investment\",\n                                ignore_case = T)) ~ \"FINANCIAL\",\n      str_detect(sector1, regex(\"Information|Communication|ICT|Internet|Technologies\",\n                                ignore_case = T)) ~ \"ICT\",\n      str_detect(sector1, regex(\"Tourism|Trade and Services|Manuf|Other Industry|Trade and Services\",\n                                ignore_case = T)) ~ \"IND TRADE SERV\",\n      str_detect(sector1, regex(\"Government|Public Admin|Institution|Central Agenc|Sub-national Gov|law|justice|governance\",\n                                ignore_case = T)) ~ \"INSTIT. SUPP.\",\n      TRUE ~ \"Missing\")) %&gt;% \n   relocate(sector_f, .after = sector1)  \n\n# Recap\nprojs_train2 |&gt; count(sector1) # 76 levels\nprojs_train2 |&gt; count(sector_f) # 14 levels"
  },
  {
    "objectID": "analysis/02a_WB_project_pdo_feat_class_envcat.html#split-data-in-traintest-based-on-env_cat",
    "href": "analysis/02a_WB_project_pdo_feat_class_envcat.html#split-data-in-traintest-based-on-env_cat",
    "title": "WB Project PDO features classification",
    "section": "1) Split data in train/test [based on env_cat]",
    "text": "1) Split data in train/test [based on env_cat]\n\n\nSampling strategy\n\n\n🟧 Proportional sub-set rsample::initial_split\n\nBased on availability of env_cat_f.\nWe will use the strata argument to stratify the data by the outcome variable (env_cat_f). This will ensure that the training and validation sets have the same proportion.\n\n# Create a stratified split based on missing vs non-missing env_cat\nprojs_train2 %&gt;% tabyl(env_cat_f) # 7 levels\n\n# Split BUT only \"Not Missing\" `env_cat_f` \n## --- 0) THIS WILL BE 4 TRAINING & VALIDATION \nenv_cat_use &lt;- projs_train2 %&gt;% \n   filter(env_cat_f != \"Missing\") # 3236 proj \n\n# SPLIT INTO TRAINING, VALIDATION \nset.seed(123)  # Ensure reproducibility\nenv_split &lt;- initial_split(env_cat_use, prop = 0.7, # 70% training, 30% testing\n                       strata = env_cat_f) # stratify by OUTCOME \n\n## -- 1) for training (labelled `env_cat_f`)\nenv_cat_train &lt;- training(env_split)   # 2265 proj\n    \n## -- 2) for validation (labelled `env_cat_f`)\nenv_cat_test &lt;- testing(env_split)  # 971 proj\n   \n# # UNLABELLED PORTION \n## -- 3) for actual test (UNlabelled `env_cat_f`)\nenv_cat_missing &lt;- projs_train2 %&gt;% \n  filter(env_cat_f == \"Missing\") # 1167 proj \n\n# check ditribution of `env_cat_f` in training and validation\ntabyl(env_cat_train, env_cat_f) |&gt; adorn_totals(\"row\") |&gt; adorn_pct_formatting(digits = 1)# \ntabyl(env_cat_test, env_cat_f)|&gt; adorn_totals(\"row\") |&gt; adorn_pct_formatting(digits = 1)# \n\n\nrm( env_cat_use, env_cat_missing)\n# env_cat_train |&gt; count(env_cat_f)"
  },
  {
    "objectID": "analysis/02a_WB_project_pdo_feat_class_envcat.html#pre-processing-and-featurization-textrecipes",
    "href": "analysis/02a_WB_project_pdo_feat_class_envcat.html#pre-processing-and-featurization-textrecipes",
    "title": "WB Project PDO features classification",
    "section": "2) Pre-processing and featurization (textrecipes)",
    "text": "2) Pre-processing and featurization (textrecipes)\n\n\n\n\n\n\nNote\n\n\n\n\n\n{textrecipes} provides a number of step functions for pre-processing text data. These include functions to tokenize (e.g. step_tokenize()), remove stop words (e.g. step_stopwords()), and to derive meta-features (e.g. step_lemma(), step_stem(), etc.) Furthermore, there are functions to engineer features in ways that are particularly relevant to text data, such as feature frequencies and weights (e.g. step_tf(), step_tfidf(), etc.) and token filtering (e.g. step_tokenfilter()).\n\nstep_tokenize()\nstep_tfidf()\n\nsmooth_idf = FALSE (terms that appear in many (or all) documents will not be down weighted as much as they would be if the smoothing term was not added)\n\nsee here\n\n\n\nNull ecipe [env_recipe_zero]\n\n# Create a recipe that only uses the `pdo` text variable\nenv_recipe_zero &lt;- recipe (formula = env_cat_f2 ~ pdo,\n                            data = env_cat_train) %&gt;%\n   # tokenize\n   step_tokenize(pdo) %&gt;% \n   # tf-idf matrix of term frequencies weighted by inverse document frequency \n   step_tfidf(pdo, smooth_idf = FALSE) \n\n# Review the recipe   \nenv_recipe_zero\n\nrecipes::bake() takes a trained recipe and applies its operations to a data set to create a design matrix.\n\n# Run the recipe \nenv_recipe_zero_desmatrix &lt;-  env_recipe_zero %&gt;% \n   # chooses the parameters for the recipe based on the data\n   prep(training = NULL) %&gt;% \n   # applies the recipe to the data already specified in the recipe\n   bake(new_data = NULL)\n\n# preview the DESIGN Matrix baked recipe -- TOO SPARSE \ndim(env_recipe_zero_desmatrix)\n#[1] 2260 7763\n\nThe resulting engineered features data frame has 2260 observations and 7763 variables!!! I.e. for each writing sample, only a small subset of them will actually appear, most of our cells will be filled with zeros. This is what is known as a sparse matrix. Furthermore, the more features we have, the more chance these features will capture the nuances of these particular writing samples increasing the likelihood we overfit the model.\nImproved recipe [env_recipe]\nBasically, we added step_tokenfilter(pdo, max_tokens = 100) to use just raw the 100 most common words (and reduce the number of features).\n\n# -- Rebuild recipe with tokenfilter step\nenv_recipe &lt;- recipe (formula = env_cat_f2 ~ pdo,\n                      data = env_cat_train) %&gt;%\n   # tokenize\n   step_tokenize(pdo) %&gt;%   \n   # !!! filter by frequency of occurrence !!!\n   step_tokenfilter(pdo, max_tokens = 100) %&gt;%  \n   # tf-idf  creates matrix of weighted term frequencies  \n   step_tfidf(pdo, smooth_idf = FALSE)\n\nRe-check the design matrix\n\n# -- Run the recipe \nenv_recipe_bake &lt;-  env_recipe %&gt;% \n   # chooses the parameters for the recipe based on the data\n   prep(training = NULL) %&gt;% \n   # applies the recipe to the data\n   bake(new_data = NULL)\n\n# -- preview the baked recipe\ndim(env_recipe_bake)\n#[1] 2260 7763--&gt; #[1] 2260  101\n\n# subset check\nenv_recipe_bake[1:5, 1:10 ]"
  },
  {
    "objectID": "analysis/02a_WB_project_pdo_feat_class_envcat.html#model-specification",
    "href": "analysis/02a_WB_project_pdo_feat_class_envcat.html#model-specification",
    "title": "WB Project PDO features classification",
    "section": "3) Model specification",
    "text": "3) Model specification\nNow that the data is ready, the model can be specified. The parsnip package is used for this:\n\nmodel specification,\nmodel type mode,\nthe engine, i.e. the software that fits the model (given the type)\n\nLet’s start with a simple logistic regression model to see how well we can classify the texts in the training set with the features we have engineered. We will use the parsnip::logistic_reg() function to specify the logistic regression model. We then select the implementation engine (glmnet General Linear Model) and the mode of the model (classification).\n\n\ntune() is a placeholder for a range of values for the penalty hyperparameter.\n\n\n# Create a model specification\nenv_spec &lt;-\n   # lasso regularized model\n   parsnip::logistic_reg(\n      # non-negative number ~ the total amount of regularization\n      penalty = tune(),  # 0 = no penalty, 1 = max\n      # number between zero and one (inclusive) \n      mixture = 1 # specifies a pure lasso model,\n   ) %&gt;%\n   # set the mode of the model\n   parsnip::set_mode(\"classification\") %&gt;%\n   # set the engine of the model\n   parsnip::set_engine(\"glmnet\")\n\n# Preview \nenv_spec"
  },
  {
    "objectID": "analysis/02a_WB_project_pdo_feat_class_envcat.html#model-training-workflows",
    "href": "analysis/02a_WB_project_pdo_feat_class_envcat.html#model-training-workflows",
    "title": "WB Project PDO features classification",
    "section": "4) Model training (workflows)",
    "text": "4) Model training (workflows)\nThe resulting workflow (a container object that aggregates information required to fit and predict from a model) by adding the recipe and model to it.\n\n# Create a workflow\nenv_wf &lt;- workflows::workflow() %&gt;%\n   # preprocessing recipe\n   add_recipe(env_recipe) %&gt;%\n   # model specifications\n   add_model(env_spec)\n\n\nI haven’t chosen the penaly yet in env_spec!\n\nIt can then be fit using fit(), but I need to specify hyperparameters (e.g. pennalty first).\n\n#env_wf %&gt;% workflows::fit(data = env_cat_train)"
  },
  {
    "objectID": "analysis/02a_WB_project_pdo_feat_class_envcat.html#model-evaluation-tweaking",
    "href": "analysis/02a_WB_project_pdo_feat_class_envcat.html#model-evaluation-tweaking",
    "title": "WB Project PDO features classification",
    "section": "5) Model evaluation / tweaking",
    "text": "5) Model evaluation / tweaking\nDifferent algorithms will have different parameters that can be adjusted which can affect the performance of the model (hyperparameters) ≠ parameters (features) –&gt; hyperparameters tuning which is tipically done during fitting the model to the training set and evaluating its performance\n— Penalty tuning [env_grid]\nIn logistic regression, the penalty hyperparameter is like a control that helps prevent the model from becoming too complex and overfitting to the training data. There are two common types of penalties:\n\nL1 (Lasso): Encourages the model to use fewer features by making some of the coefficients exactly zero. This can simplify the model.\nL2 (Ridge): Tries to keep all the coefficients small but not exactly zero, which can help stabilize the model and avoid overfitting.\n\nthe logistic regression model using glmnet can be tuned to prevent overfitting by adjusting the penalty and mixture (combination of L1 and L2) hyperparameters\n\nIn our env_spec model, tune() was a placeholder for a range of values for the penalty hyperparameter.\nTo tune the penalty hyperparameter, we use thegrid_regular() function from {dials} to specify a grid of values to try.\n\n\nThe package dials contains infrastructure to create and manage values of tuning parameters for the tidymodels packages.\n\n\n# Create a grid of values for the penalty hyperparameter (random set of 10 values)\nenv_grid &lt;- dials::grid_regular(\n   penalty(), \n   levels = 10)\n\n# Preview\nenv_grid \n# 0 no penalty\n# ... \n# 1 max penalty\n\n— K-fold cross-validation (for penalty optimal) [ env_fold, env_tune]\nNow to perform the tuning and choose an optimal value for penalty we need to create a tuning workflow. We use the strategy of resampling (splitting env_cat_train in multiple training/testing sets) called k-fold cross-validation to arrive at the optimal value for the penalty hyperparameter.\n\nset.seed(123)\n\n# Create a resampling object\nenv_vfold &lt;- rsample::vfold_cv(env_cat_train, v = 10)\n\n# Create a tuning workflow\nenv_tune &lt;- tune::tune_grid(\n  object = env_wf,\n  resamples = env_vfold,\n  grid = env_grid,\n  control = control_grid(save_pred = TRUE),\n  metrics = metric_set(roc_auc, accuracy, sensitivity, specificity)\n  \n)\n\n# preview\nenv_tune\n\nThe env_tune object contains the results of the tuning for each fold. We can see the results of the tuning for each fold by calling the collect_metrics() function on the env_tune object\n\n# Collect the results of the tuning\nenv_tune_metrics &lt;- tune::collect_metrics(env_tune)\nenv_tune_pred  &lt;- tune::collect_predictions(env_tune)\n\n# visualize the results\ntune::autoplot(env_tune) +\n  labs(\n    title = \"Lasso model performance across regularization penalties\",\n    subtitle = \"Performance metrics can be used to identify the best penalty\"\n  )\n# in roc_auc: many of the penalty values performed similarly, with a drop-off in performance at the higher values\n\nThe most common metrics for model performance in classification are:\n\n\naccuracy (the proportion of correct predictions)\n\nhere it drops sharply at the higher values of penalty (1e-02), meaning dropping too many features will hurt predictive performance\n\n\n\nROC-AUC (area under the receiver operating characteristic area under the curve, i.e. a single score for how well the model can distinguish between classes) The closer to 1 the more discriminating power the model has.\n\nsimilar, i.e song penalties will damage the model’s ability to distinguish between classes.\n\n\n\nsensitivity (the proportion of TP that are correctly identified)\n\nhere it peaks at 1.0 beyond 1e-02, meaning the model is very good at identifying the positive class (High-Med-risk) BUT…\n\n\n\nspecificity (the proportion of TN that are correctly identified)\n\n… at the cost of specificity). In fact here, performance is very poor, suggesting no true negatives are correctly identified.\n\n\n\nConveniently, the show_best() function from {tune} takes a tune_grid object and returns the best performing hyperparameter values.\n\n# Show the best hyperparameter values\ntune::show_best(env_tune, metric = \"roc_auc\") # 0.00599\n\n# Make selection of penalty programmatically\nenv_best &lt;- select_best(env_tune, metric =\"roc_auc\")           # 0.00599\nenv_best_lambda &lt;- env_best$penalty                            # 1e-10\n\nenv_best_acc &lt;- select_best(env_tune, metric =\"accuracy\")      # 0.00599\nenv_best_sens &lt;- select_best(env_tune, metric =\"sensitivity\")  # 0.0774"
  },
  {
    "objectID": "analysis/02a_WB_project_pdo_feat_class_envcat.html#hyperparameter-tuning",
    "href": "analysis/02a_WB_project_pdo_feat_class_envcat.html#hyperparameter-tuning",
    "title": "WB Project PDO features classification",
    "section": "6) Hyperparameter tuning",
    "text": "6) Hyperparameter tuning\nUpdate workflow [env_wf_lasso]\nNow we can update the model specification and workflow with the best performing hyperparameter value using the previous cls_wf_tune workflow and the finalize_workflow() function.\n\nInstead of penalty = tune() like before, now our workflow has finalized values for all arguments.\n\n\n# Update the workflow/model specification with the best penalty value\nenv_wf_lasso &lt;- env_wf %&gt;% \n   tune::finalize_workflow(env_best)\n\n# Preview updated workflow object (with defined penalty paramv  0.00599)\nenv_wf_lasso"
  },
  {
    "objectID": "analysis/02a_WB_project_pdo_feat_class_envcat.html#final-fit-on-train-set",
    "href": "analysis/02a_WB_project_pdo_feat_class_envcat.html#final-fit-on-train-set",
    "title": "WB Project PDO features classification",
    "section": "7) Final fit on train set",
    "text": "7) Final fit on train set\nFit the final model\n\n# Fit the model to the training data\nenv_lasso_fit &lt;- env_wf_lasso %&gt;% \n   # fit the model to the training data\n   workflows::fit(data = env_cat_train)\n\nHere I see:\n\n\nDf = Number of non-zero coefficients (i.e., selected features) at a given lambda.\n\n%Dev = Percentage of null deviance explained (i.e., a measure of model fit – how well the model explains the variation in the response).\n\nLambda = The regularization parameter – higher values imply more shrinkage (simpler models).\n\n(here) lambda = 1e-10 means no penalty (i.e., no regularization), i.e. the model is almost unpenalized, so it is similar to a standard logistic regression model.\n\n\n\nThis table shows the model performance across different levels of regularization (lambda). As lambda decreases:\n\nThe number of selected features (Df) increases.\nModel fit improves (%Dev goes up), but complexity increases.\nAt the smallest lambda shown (0.001950), 93 features are included, explaining 23.73% of deviance.\n\n\nYou’ll typically select a lambda using cross-validation (e.g., tune_grid() or select_best() in tidymodels). The goal is to balance simplicity and predictive performance – too small a lambda overfits; too large underfits.\n\nCoefficients for the lambda with hte best performance\n\nusing the workflows::extract_fit_parsnip() function, I see only the model coefficients for the best performing lambda value. This function extracts the fitted model object from the workflow, and\nthen I can use the tidy() function to get the coefficients.\n\n\n# Get the coefficients of the model\nenv_lasso_fit %&gt;% \n   workflows::extract_fit_parsnip() %&gt;% \n   broom::tidy() %&gt;% \n   dplyr::filter(term != \"(Intercept)\") %&gt;% \n   dplyr::arrange(desc(abs(estimate))) %&gt;% \n   dplyr::slice(1:20) # top 10\n\nI can see stopwords like among the most important features!\n— Get predicted class and probabilities (train)\nHere I got the predicted probabilities from env_lasso_fit logistic regression model on the dataset env_cat_train + with type = \"prob\" argument in the predict() function, the model returns class probabilities for each of the 2260 observations 1. .pred_High-Med-risk = probability of being in the High-Med-risk category 2. .pred_Low-risk_Othr = probability of being in the Low-risk_Othr category\n\n# Prep data frame containing actual and predicted values\npred_train  &lt;-  stats::predict(env_lasso_fit, \n                               new_data = env_cat_train, \n                               type = \"prob\") |&gt; # prob **high** and prob **low** for each observ based on model \n   # combine with the original training data (labeled)\n   bind_cols(env_cat_train) %&gt;% \n   select(env_cat_f2,  # ACTUAL CLASS of the 1obs\n          pred_high_risk = '.pred_High-Med-risk', # PROB 1obs is PREDICTED HIGH-MED-RISK \n          pred_low_risk = '.pred_Low-risk_Othr'   # PROB 1obs is PREDICTED LOW-RISK\n   )  # 2260 rows\n\n\n# convert to long format for plotting\npred_train_long &lt;- pred_train |&gt;\n   pivot_longer(cols = c(pred_high_risk, pred_low_risk),\n                names_to = \"pred_risk_type\", values_to = \"pred_risk_value\") # 4,520 rows\n\n— [FIG] Assessing performance on training set\n\n# Plot the predictions with boxplots and jittered points without duplicate legends\nggplot(pred_train_long, aes(x = env_cat_f2, y = pred_risk_value, fill = pred_risk_type)) +\n  geom_boxplot(alpha = 0.4, position = position_dodge(width = 0.8)) +\n  #geom_jitter(alpha = 0.6, position = position_dodge(width = 0.8)) +  # Remove width and use position_dodge\n   labs(title = \"PREDICTED class distribution (y axis) v. ACTUAL class (x axis)\",\n        subtitle = \"Model: Lasso Regression fitted on training data\",\n       x = \"ACTUAL env. class\",\n       y = \"Probabiity of PREDICTED env. class\",\n       fill = \"Risk Type\") +  # Set label for fill legend\n  theme_minimal() +\n  guides(color = \"none\")  # Suppress the color legend\n\n\n\n\n\n\n\nCalculate performance metrics\n\ntrain_pred_probs &lt;- predict(env_lasso_fit, new_data = env_cat_train, type = \"prob\")\ntrain_pred_class &lt;- predict(env_lasso_fit, new_data = env_cat_train, type = \"class\")\n\ntrain_results &lt;- bind_cols(env_cat_train, train_pred_probs, train_pred_class) |&gt; \n   select(env_cat_f2,  # ACTUAL CLASS of the 1obs\n          pred_high_risk = '.pred_High-Med-risk', # PROB 1obs is PREDICTED HIGH-MED-RISK \n          pred_low_risk = '.pred_Low-risk_Othr',   # PROB 1obs is PREDICTED LOW-RISK\n          pred_class = '.pred_class')\n\n# Confusion matrix\na_train_acc   &lt;- accuracy(train_results, truth = env_cat_f2, estimate = pred_class)\n# ROC AUC (needs probabilities + positive class specified)\na_train_roc_auc   &lt;- roc_auc(train_results, truth = env_cat_f2, pred_high_risk)\n# Sensitivity and Specificity\na_train_sens   &lt;- sens(train_results, truth = env_cat_f2, estimate = pred_class)\na_train_spec   &lt;- spec(train_results, truth = env_cat_f2, estimate = pred_class)\n\n# confusion matrix\nconf_mat(train_results, truth = env_cat_f2, estimate = pred_class) |&gt; \n   autoplot(type = \"heatmap\") +\n   labs(title = \"Confusion matrix for Lasso model\",\n        subtitle = \"Training set\")\n\n\n\n\n\n\n# roc curve\nroc_curve(train_results, truth = env_cat_f2, pred_high_risk) |&gt; \n   autoplot() +\n   labs(title = \"ROC curve for Lasso model (only x = pdo)\",\n        subtitle = \"Training set\")"
  },
  {
    "objectID": "analysis/02a_WB_project_pdo_feat_class_envcat.html#evaluation-on-test-set",
    "href": "analysis/02a_WB_project_pdo_feat_class_envcat.html#evaluation-on-test-set",
    "title": "WB Project PDO features classification",
    "section": "8) Evaluation on test set",
    "text": "8) Evaluation on test set\n— Get predicted class and probabilities (test)\n\n# Predict on the test set\npred_test  &lt;-  stats::predict(env_lasso_fit, \n                      new_data = env_cat_test, \n                      type = \"prob\") |&gt; # prob **high** and prob **low** for each observ based on model \n   # combine with the original training data (labeled)\n   bind_cols(env_cat_test) %&gt;% \n   select(env_cat_f2,  # ACTUAL CLASS of the 1obs\n          pred_high_risk = '.pred_High-Med-risk', # PROB 1obs is PREDICTED HIGH-MED-RISK \n          pred_low_risk = '.pred_Low-risk_Othr'   # PROB 1obs is PREDICTED LOW-RISK\n          )  # 970 rows\n\n# convert to long format for plotting\npred_test_long &lt;- pred_test |&gt; \n   pivot_longer(cols = c(pred_high_risk, pred_low_risk),\n                names_to = \"pred_risk_type\", values_to = \"pred_risk_value\") # 1,940 rows\n\n— [FIG] Assessing performance on test set\n\n# Plot the predictions with boxplots and jittered points without duplicate legends\nggplot(pred_test_long, aes(x = env_cat_f2, y = pred_risk_value, fill = pred_risk_type)) +\n  geom_boxplot(alpha = 0.4, position = position_dodge(width = 0.8)) +\n  #geom_jitter(alpha = 0.6, position = position_dodge(width = 0.8)) +  # Remove width and use position_dodge\n   labs(title = \"PREDICTED class distribution (y axis) v. ACTUAL class (x axis)\",\n        subtitle = \"Model: Lasso Regression fitted on test data\",\n       x = \"ACTUAL env. class\",\n       y = \"Probabiity of PREDICTED env. class\",\n       fill = \"Risk Type\") +  # Set label for fill legend\n  theme_minimal() +\n  guides(color = \"none\")  # Suppress the color legend\n\n\n\n\n\n\n\nCalculate performance metrics\nUsing yardstick package, I can calculate the performance metrics for the model on the test set. The roc_auc() function calculates the area under the ROC curve, which is a measure of how well the model can distinguish between the two classes.\n\ntest_pred_probs &lt;- predict(env_lasso_fit, new_data = env_cat_test, type = \"prob\")\ntest_pred_class &lt;- predict(env_lasso_fit, new_data = env_cat_test, type = \"class\")\n\ntest_results &lt;- bind_cols(env_cat_test, test_pred_probs, test_pred_class) |&gt; \n   select(env_cat_f2,  # ACTUAL CLASS of the 1obs\n          pred_high_risk = '.pred_High-Med-risk', # PROB 1obs is PREDICTED HIGH-MED-RISK \n          pred_low_risk = '.pred_Low-risk_Othr',   # PROB 1obs is PREDICTED LOW-RISK\n          pred_class = '.pred_class')\n\n# Confusion matrix\na_test_acc &lt;- accuracy(test_results, truth = env_cat_f2, estimate = pred_class)\n# ROC AUC (needs probabilities + positive class specified)\na_test_roc_auc &lt;- roc_auc(test_results, truth = env_cat_f2, pred_high_risk)\n# Sensitivity and Specificity\na_test_sens &lt;- sens(test_results, truth = env_cat_f2, estimate = pred_class)\na_test_spec &lt;- spec(test_results, truth = env_cat_f2, estimate = pred_class)\n\n# confusion matrix\nconf_mat(test_results, truth = env_cat_f2, estimate = pred_class) |&gt; \n   autoplot(type = \"heatmap\") +\n   labs(title = \"Confusion matrix for Lasso model\",\n        subtitle = \"testing set\")\n\n\n\n\n\n\n# roc curve\nroc_curve(test_results, truth = env_cat_f2, pred_high_risk) |&gt; \n   autoplot() +\n   labs(title = \"ROC curve for Lasso model (only x = pdo)\",\n        subtitle = \"testing set\")\n\n\n\n\n\n\n\nRecap\nSo, the results of the ‘best model’ in the training and testing set are:\n\n#| tbl-cap: Model performance metrics\n\n#| tbl-label: tbl-model-metrics\n#| output: true\n\n# table with accuracy, roc_auc, sensitivity, specificity\nmodel_metrics_recap &lt;- tibble(\n   metric = c(\"Accuracy\", \"ROC AUC\", \"Sensitivity\", \"Specificity\"),\n   train = c(a_train_acc$.estimate, \n             a_train_roc_auc$.estimate, \n             a_train_sens$.estimate, \n             a_train_spec$.estimate      )  ,\n   test =  c(a_test_acc$.estimate, \n             a_test_roc_auc$.estimate, \n             a_test_sens$.estimate, \n             a_test_spec$.estimate      )  \n   ) |&gt; \n   mutate(across(c(train, test), ~ round(.x, 3))) |&gt;\n   kable() |&gt; \n   kable_styling(\"striped\", full_width = F) |&gt; \n   # header merged column \n   kableExtra::add_header_above(c(\"A) Lasso Logistic model\", \"datasets\" = 2), \n                                bold = TRUE, color = \"black\", background = \"#D9EAD3\")  \n\nmodel_metrics_recap\n\n\n\n\n\n\n\n\n\n\nA) Lasso Logistic model\n\n\ndatasets\n\n\n\nmetric\ntrain\ntest\n\n\n\n\nAccuracy\n0.759\n0.759\n\n\nROC AUC\n0.812\n0.774\n\n\nSensitivity\n0.910\n0.905\n\n\nSpecificity\n0.456\n0.473"
  },
  {
    "objectID": "analysis/02a_WB_project_pdo_feat_class_envcat.html#prep-for-split-based-on-outcome-using-projs_train2",
    "href": "analysis/02a_WB_project_pdo_feat_class_envcat.html#prep-for-split-based-on-outcome-using-projs_train2",
    "title": "WB Project PDO features classification",
    "section": "0) Prep for split based on outcome [using projs_train2]",
    "text": "0) Prep for split based on outcome [using projs_train2]\n(same)"
  },
  {
    "objectID": "analysis/02a_WB_project_pdo_feat_class_envcat.html#split-data-in-traintest-based-on-env_cat-1",
    "href": "analysis/02a_WB_project_pdo_feat_class_envcat.html#split-data-in-traintest-based-on-env_cat-1",
    "title": "WB Project PDO features classification",
    "section": "1) Split data in train/test [based on env_cat]",
    "text": "1) Split data in train/test [based on env_cat]\n(same)"
  },
  {
    "objectID": "analysis/02a_WB_project_pdo_feat_class_envcat.html#pre-processing-and-featurization-recipes",
    "href": "analysis/02a_WB_project_pdo_feat_class_envcat.html#pre-processing-and-featurization-recipes",
    "title": "WB Project PDO features classification",
    "section": "2) Pre-processing and featurization (recipes)",
    "text": "2) Pre-processing and featurization (recipes)\nTo improve supervised learning models, consider:\n\n\nEngineering the features differently\n\nwe set a token filter to limit the number of features to 100, which we could adjust (max_tokens)\n\n\nSelecting different (or additional) features\nChanging the algorithm\nTuning the hyperparameters differently\n\n\n# Create a custom stopword list\nstop_vector &lt;- custom_stop_words_df %&gt;%  pull(word)\n\n— Improve recipe [env_stop_recipe]\n\n# ---  Create a recipe with a token filter step that excludes stopwords\n# Rebuild recipe with tokenfilter step\nenv_stop_recipe &lt;- recipes::recipe (\n   formula = env_cat_f2 ~ pdo,\n   data = env_cat_train) %&gt;%\n   # tokenize\n   step_tokenize(pdo) %&gt;%   \n   # remove CUSTOM stopwords (NEW!)\n   step_stopwords(pdo, custom_stopword_source = stop_vector) %&gt;%  \n   # filter by frequency of occurrence\n   step_tokenfilter(pdo, max_tokens = 100) %&gt;%  \n   # tf-idf  creates matrix of weighted term frequencies \n   step_tfidf(pdo, smooth_idf = FALSE)       \n\n# prep and bake the recipe\nenv_stop_recipe_bake &lt;-  env_stop_recipe %&gt;% \n  prep() %&gt;% \n   bake(new_data = NULL)\n\n# preview the baked recipe\ndim(env_stop_recipe_bake)\n#[1] 2264 101\nenv_recipe_bake[1:5, 1:10]\nenv_stop_recipe_bake[1:5, 1:10]"
  },
  {
    "objectID": "analysis/02a_WB_project_pdo_feat_class_envcat.html#model-specification-1",
    "href": "analysis/02a_WB_project_pdo_feat_class_envcat.html#model-specification-1",
    "title": "WB Project PDO features classification",
    "section": "3) Model specification",
    "text": "3) Model specification\nThe model specification is the same as before (env_spec), but now we will use the new recipe with stopwords removed.\n— Model specification [env_spec]\n\n# Create a model specification\nenv_spec &lt;-\n   # generalized linear model for binary outcomes\n   parsnip::logistic_reg(\n      # A non-negative number representing the total amount of regularization\n      penalty = tune(),  # 0 = no penalty, 1 = max\n      #A number between zero and one (inclusive)\n      mixture = 1 # pecifies a pure lasso model,\n   ) %&gt;%\n   set_engine(\"glmnet\")\n                           ##### tune() IS A PLACEHOLDER\n# Preview\nenv_spec"
  },
  {
    "objectID": "analysis/02a_WB_project_pdo_feat_class_envcat.html#model-training-workflows-1",
    "href": "analysis/02a_WB_project_pdo_feat_class_envcat.html#model-training-workflows-1",
    "title": "WB Project PDO features classification",
    "section": "4) Model training (workflows)",
    "text": "4) Model training (workflows)\nWe create a new workflow [env_stop_wf], with env_stop_recipe, the part that changed in this workflow adding step_stopwords().\n\n# Create a workflow\nenv_stop_wf &lt;- workflows::workflow() %&gt;%\n   add_recipe(env_stop_recipe) %&gt;%  # NEW RECIPE\n   add_model(env_spec)\n# Preview\nenv_stop_wf"
  },
  {
    "objectID": "analysis/02a_WB_project_pdo_feat_class_envcat.html#model-evaluation-tweaking-1",
    "href": "analysis/02a_WB_project_pdo_feat_class_envcat.html#model-evaluation-tweaking-1",
    "title": "WB Project PDO features classification",
    "section": "5) Model evaluation / tweaking",
    "text": "5) Model evaluation / tweaking\n(same)\n— Penalty tuning [env_grid] (same)\n\n# Create a grid of values for the penalty hyperparameter (random set of 10 values)\nenv_grid &lt;- dials::grid_regular(\n  penalty(), levels = 10\n  )\n# Preview\nenv_grid \n\n— K-fold cross-val (same, but NEW wf)\nNow to perform the tuning and choose an optimal value for penalty we need to create a tuning workflow. We use the strategy of resampling (splitting env_cat_train in multiple training/testing sets) called k-fold cross-validation to arrive at the optimal value for the penalty hyperparameter.\n\nset.seed(123)\n\n# Create a resampling object\nenv_vfold &lt;- rsample::vfold_cv(env_cat_train, v = 10)\n\n# Create a tuning workflow\nenv_stop_tune &lt;- tune::tune_grid(\n  object = env_stop_wf, # changed ! \n  resamples = env_vfold,\n  grid = env_grid,\n  control = control_grid(save_pred = TRUE),\n  metrics = metric_set(roc_auc, accuracy, sensitivity, specificity)\n)\n# preview\nenv_stop_tune\n\nThe env_stop_tune object contains the results of the tuning for each fold. We can see the results of the tuning for each fold by calling the collect_metrics() function on the env_stop_tune object\n\n# Collect the results of the tuning\nenv_stop_tune_metrics &lt;- tune::collect_metrics(env_stop_tune)\n\n# visualize the results\ntune::autoplot(env_stop_tune) +\n  labs(\n    title = \"Lasso model performance across regularization penalties\",\n    subtitle = \"Performance metrics can be used to identify the best penalty\"\n  )\n# in roc_auc: many many of the penalty values performed similarly, with a drop-off in performance at the higher val- ues\n\nOnce again, as the penalty increases, the model performance reach a peak for accuracy, but with a tradeoff between sensitivity and specificity. The roc_auc metric is similar to the previous model, but the accuracy metric is slightly lower.\nConveniently, the tune::show_best() function takes a tune_grid object and returns the best performing hyperparameter values (i.e. PENALTY in lasso logistic).\n\n# Show the best hyperparameter values (BASED ON A CHOSEN METRIC)\nshow_best(env_stop_tune, metric = \"roc_auc\")  # 0.00599\n\n# Make selection of penalty programmatically\nenv_stop_best &lt;- select_best(env_stop_tune, metric =\"roc_auc\")           # 0.00599\nenv_stop_best_lambda &lt;- env_stop_best$penalty                            # 1e-10\n\nenv_stop_best_acc &lt;- select_best(env_stop_tune, metric =\"accuracy\")      # 0.00599\nenv_stop_best_sens &lt;- select_best(env_stop_tune, metric =\"sensitivity\")  # 0.0774"
  },
  {
    "objectID": "analysis/02a_WB_project_pdo_feat_class_envcat.html#hyperparameter-tuning-1",
    "href": "analysis/02a_WB_project_pdo_feat_class_envcat.html#hyperparameter-tuning-1",
    "title": "WB Project PDO features classification",
    "section": "6) Hyperparameter tuning",
    "text": "6) Hyperparameter tuning\nUpdate workflow [env_stop_wf_lasso]\nNow we can update the model specification and workflow with the best performing hyperparameter value using the previous cls_wf_tune workflow and the finalize_workflow() function.\n\n# Update the model specification\nenv_stop_wf_lasso &lt;- env_stop_wf %&gt;% \n   tune::finalize_workflow(env_stop_best)\n\n# Preview updated workflow object (with defined penalty paramv  0.00599)\nenv_stop_wf_lasso"
  },
  {
    "objectID": "analysis/02a_WB_project_pdo_feat_class_envcat.html#final-fit-on-training-set",
    "href": "analysis/02a_WB_project_pdo_feat_class_envcat.html#final-fit-on-training-set",
    "title": "WB Project PDO features classification",
    "section": "7) Final fit on training set",
    "text": "7) Final fit on training set\n— Fit the final model\n\n# Fit the model to the training data\nenv_stop_lasso_fit &lt;- fit (env_stop_wf_lasso, data = env_cat_train)\n\nHere I see:\n\n\nDf = Number of non-zero coefficients (i.e., selected features) at a given lambda.\n\n%Dev = Percentage of null deviance explained (i.e., a measure of model fit – how well the model explains the variation in the response).\n\nLambda = The regularization parameter – higher values imply more shrinkage (simpler models).\n\n(here) lambda = 1e-10 means no penalty (i.e., no regularization), i.e. the model is almost unpenalized, so it is similar to a standard logistic regression model.\n\n\n\nThis table shows the model performance across different levels of regularization (lambda). As lambda decreases:\n\nThe number of selected features (Df) increases.\nModel fit improves (%Dev goes up), but complexity increases.\nAt the smallest lambda shown (0.001950), 93 features are included, explaining 23.73% of deviance.\n\n\nYou’ll typically select a lambda using cross-validation (e.g., tune_grid() or select_best() in tidymodels). The goal is to balance simplicity and predictive performance – too small a lambda overfits; too large underfits.\n\n— Coefficients for the lambda with the best performance\n\nusing the workflows::extract_fit_parsnip() function, I see only the model coefficients for the best performing lambda value. This function extracts the fitted model object from the workflow\nThen for the penalty we chose, we see what terms contribute the most to env cat NOT being high risk .\n\n\nenv_stop_lasso_fit |&gt; \n   workflows::extract_fit_parsnip() %&gt;% \n   broom::tidy() %&gt;% \n   dplyr::filter(term != \"(Intercept)\") %&gt;% \n   dplyr::arrange(desc(abs(estimate))) %&gt;% \n   dplyr::slice(1:20) # top 10\n\n\n“and” is not top coefficient anymore!!!\n\n— Get predicted class and probabilities (train)\n\n# Example of a data frame containing actual and predicted values\npred_stop_train  &lt;- stats::predict(env_stop_lasso_fit, \n                           new_data = env_cat_train , \n                           type = \"prob\")|&gt;# prob **high** and prob **low** for each observ based on model \n   bind_cols(env_cat_train) %&gt;% \n   select(env_cat_f2,  pred_high_risk = '.pred_High-Med-risk', pred_low_risk = '.pred_Low-risk_Othr')   # 2260 rows \n\n\npred_stop_train_long &lt;- pred_stop_train |&gt;\n   pivot_longer(cols = c(pred_high_risk, pred_low_risk),\n                names_to = \"risk_type\", values_to = \"risk_value\") # 4,520 rows\n\n— [FIG] Assessing performance [env_stop_lasso_fit] on training set\n\n# Plot the predictions with boxplots and jittered points without duplicate legends\nggplot(pred_stop_train_long, aes(x = env_cat_f2, y = risk_value, fill = risk_type)) +\n   geom_boxplot(alpha = 0.4, position = position_dodge(width = 0.8)) +\n   #geom_jitter(alpha = 0.6, position = position_dodge(width = 0.8)) +  # Remove width and use position_dodge\n   labs(title = \"PREDICTED class distribution (y axis) v. ACTUAL class (x axis)\",\n        subtitle = \"Model: Lasso Regression fitted on training data\",\n        x = \"ACTUAL env. class\",\n        y = \"Probabiity of PREDICTED env. class\",\n        fill = \"Risk Type\") +  # Set label for fill legend\n   theme_minimal() +\n   guides(color = \"none\")  # Suppress the color legend\n\n\n\n\n\n\n\nThis model did not improve much (especially on the LOW-risk-Other level prediction!\nCalculate performance metrics\n\ntrain_stop_pred_probs &lt;- predict(env_stop_lasso_fit, new_data = env_cat_train, type = \"prob\")\ntrain_stop_pred_class &lt;- predict(env_stop_lasso_fit, new_data = env_cat_train, type = \"class\")\n\ntrain_stop_results &lt;- bind_cols(env_cat_train, train_stop_pred_probs, train_stop_pred_class) |&gt; \n   select(env_cat_f2,  # ACTUAL CLASS of the 1obs\n          pred_high_risk = '.pred_High-Med-risk', # PROB 1obs is PREDICTED HIGH-MED-RISK \n          pred_low_risk = '.pred_Low-risk_Othr',   # PROB 1obs is PREDICTED LOW-RISK\n          pred_class = '.pred_class')\n\n# Confusion matrix\nb_train_acc &lt;- accuracy(train_stop_results, truth = env_cat_f2, estimate = pred_class)\n# ROC AUC (needs probabilities + positive class specified)\nb_train_roc_auc &lt;- roc_auc(train_stop_results, truth = env_cat_f2, pred_high_risk)\n# Sensitivity and Specificity\nb_train_sens &lt;- sens(train_stop_results, truth = env_cat_f2, estimate = pred_class)\nb_train_spec &lt;- spec(train_stop_results, truth = env_cat_f2, estimate = pred_class)\n\n# confusion matrix\nconf_mat(train_stop_results, truth = env_cat_f2, estimate = pred_class) |&gt; \n   autoplot(type = \"heatmap\") +\n   labs(title = \"Confusion matrix for Lasso model\",\n        subtitle = \"Training set\")\n\n\n\n\n\n\n# roc curve\nroc_curve(train_stop_results, truth = env_cat_f2, pred_high_risk) |&gt; \n   autoplot() +\n   labs(title = \"ROC curve for Lasso model B (only x = pdo)\",\n        subtitle = \"Training set\")\n\n\n\n\n\n\n\nThere are more false positives (low risk predicted to be high risk) than false negatives. (This is a common issue in imbalanced datasets and can be addressed by adjusting the decision threshold of the model.)"
  },
  {
    "objectID": "analysis/02a_WB_project_pdo_feat_class_envcat.html#evaluation-on-test-set-1",
    "href": "analysis/02a_WB_project_pdo_feat_class_envcat.html#evaluation-on-test-set-1",
    "title": "WB Project PDO features classification",
    "section": "8) Evaluation on test set",
    "text": "8) Evaluation on test set\n— Get predicted class and probabilities (test)\n\n# Predict on the test set\npred_stop_test  &lt;- stats::predict(env_stop_lasso_fit, \n                           new_data = env_cat_test , \n                           type = \"prob\")|&gt;# prob **high** and prob **low** for each observ based on model \n   bind_cols(env_cat_test) %&gt;% \n   select(env_cat_f2,  pred_high_risk = '.pred_High-Med-risk', pred_low_risk = '.pred_Low-risk_Othr')   # 970 rows\n\npred_stop_test_long &lt;- pred_stop_test |&gt;\n   pivot_longer(cols = c(pred_high_risk, pred_low_risk),\n                names_to = \"risk_type\", values_to = \"risk_value\") # 1,940 rows\n\n— [FIG] Assessing performance on test set\n\n# Plot the predictions with boxplots and jittered points without duplicate legends\nggplot(pred_stop_test_long, aes(x = env_cat_f2, y = risk_value, fill = risk_type)) +\n   geom_boxplot(alpha = 0.4, position = position_dodge(width = 0.8)) +\n   #geom_jitter(alpha = 0.6, position = position_dodge(width = 0.8)) +  # Remove width and use position_dodge\n   labs(title = \"PREDICTED class distribution (y axis) v. ACTUAL class (x axis)\",\n        subtitle = \"Model: Lasso Regression fitted on test data\",\n        x = \"ACTUAL env. class\",\n        y = \"Probabiity of PREDICTED env. class\",\n        fill = \"Risk Type\") +  # Set label for fill legend\n   theme_minimal() +\n   guides(color = \"none\")  # Suppress the color legend\n\n\n\n\n\n\n\nCalculate performance metrics\n\ntest_stop_pred_probs &lt;- predict(env_stop_lasso_fit, new_data = env_cat_test, type = \"prob\")\ntest_stop_pred_class &lt;- predict(env_stop_lasso_fit, new_data = env_cat_test, type = \"class\")\ntest_stop_results &lt;- bind_cols(env_cat_test, test_stop_pred_probs, test_stop_pred_class) |&gt; \n   select(env_cat_f2,  # ACTUAL CLASS of the 1obs\n          pred_high_risk = '.pred_High-Med-risk', # PROB 1obs is PREDICTED HIGH-MED-RISK \n          pred_low_risk = '.pred_Low-risk_Othr',   # PROB 1obs is PREDICTED LOW-RISK\n          pred_class = '.pred_class')\n\n# accuracy\nb_test_acc &lt;- accuracy(test_stop_results, truth = env_cat_f2, estimate = pred_class)\n# ROC AUC (needs probabilities + positive class specified)\nb_test_roc_auc &lt;- roc_auc(test_stop_results, truth = env_cat_f2, pred_high_risk)\n# Sensitivity and Specificity\nb_test_sens &lt;- sens(test_stop_results, truth = env_cat_f2, estimate = pred_class)\nb_test_spec &lt;- spec(test_stop_results, truth = env_cat_f2, estimate = pred_class)\n\n# confusion matrix\nconf_mat(test_stop_results, truth = env_cat_f2, estimate = pred_class) |&gt; \n   autoplot(type = \"heatmap\") +\n   labs(title = \"Confusion matrix for Lasso model\",\n        subtitle = \"testing set\")\n\n\n\n\n\n\n# roc curve\nroc_curve(test_stop_results, truth = env_cat_f2, pred_high_risk) |&gt; \n   autoplot() +\n   labs(title = \"ROC curve for Lasso model B (only x = pdo)\",\n        subtitle = \"testing set\")\n\n\n\n\n\n\n\nRecap\nSo, the results of the ‘best model’ in the training and testing set are:\n\n# table with accuracy, roc_auc, sensitivity, specificity\nmodel_stop_metrics_recap &lt;- tibble(\n   metric = c(\"Accuracy\", \"ROC AUC\", \"Sensitivity\", \"Specificity\"),\n   train = c(b_train_acc$.estimate, \n             b_train_roc_auc$.estimate, \n             b_train_sens$.estimate, \n             b_train_spec$.estimate      )  ,\n   test =  c(b_test_acc$.estimate, \n             b_test_roc_auc$.estimate, \n             b_test_sens$.estimate, \n             b_test_spec$.estimate      )  \n   ) |&gt; \n   mutate(across(c(train, test), ~ round(.x, 3))) |&gt;\n   kable() |&gt; \n   kable_styling(\"striped\", full_width = F) |&gt; \n   # header merged column \n   kableExtra::add_header_above(c(\"B) Lasso Logistic model\", \"datasets\" = 2), \n                                bold = TRUE, color = \"black\", background = \"#D9EAD3\")\n#model_metrics_recap\nmodel_stop_metrics_recap\n\n\nModel performance metrics\n\n\n\n\n\n\n\n\nB) Lasso Logistic model\n\n\ndatasets\n\n\n\nmetric\ntrain\ntest\n\n\n\n\nAccuracy\n0.766\n0.755\n\n\nROC AUC\n0.816\n0.785\n\n\nSensitivity\n0.913\n0.907\n\n\nSpecificity\n0.471\n0.457\n\n\n\n\n\n🟨 Alternative (7) & (8) together\n\nTo do this we need to fit the tuned workflow to the training set, which is the actual training phase. We will use the last_fit() function from {workflows}\nLe’s use the updated workflow env_stop_wf_lasso\n— Fit the best model (to train) and evaluate on the test set\n(same as I got in split steps)\n\n# fit the model to the training set and evaluate on the validation set\nenv_stop_lasso_final_fit &lt;- last_fit(\n   env_stop_wf_lasso, \n   split = env_split)\n\n# Evaluate the model on the validation set (in my case)\ncollect_metrics(env_stop_lasso_final_fit) |&gt; \n   dplyr::filter(.metric %in% c(\"accuracy\", \"roc_auc\", \n                                # not available \n                                \"sens\", \"spec\"))\n\n# 1 accuracy    binary         0.762 Preprocessor1_Model1\n# 2 roc_auc     binary         0.807 Preprocessor1_Model1\n# 3 brier_class binary         0.163 Preprocessor1_Model1\n\nThe performance metrics are very close to those we achieved on the training set –&gt; good sign that the model is robust as it performs well on both training and test (validation) sets."
  },
  {
    "objectID": "analysis/02a_WB_project_pdo_feat_class_envcat.html#prep-for-split-based-on-outcome-using-projs_train2-1",
    "href": "analysis/02a_WB_project_pdo_feat_class_envcat.html#prep-for-split-based-on-outcome-using-projs_train2-1",
    "title": "WB Project PDO features classification",
    "section": "0) Prep for split based on outcome [using projs_train2]",
    "text": "0) Prep for split based on outcome [using projs_train2]\n(same)"
  },
  {
    "objectID": "analysis/02a_WB_project_pdo_feat_class_envcat.html#split-data-in-traintest-based-on-env_cat-2",
    "href": "analysis/02a_WB_project_pdo_feat_class_envcat.html#split-data-in-traintest-based-on-env_cat-2",
    "title": "WB Project PDO features classification",
    "section": "1) Split data in train/test [based on env_cat]",
    "text": "1) Split data in train/test [based on env_cat]\n(same)"
  },
  {
    "objectID": "analysis/02a_WB_project_pdo_feat_class_envcat.html#pre-processing-and-featurization-recipes-1",
    "href": "analysis/02a_WB_project_pdo_feat_class_envcat.html#pre-processing-and-featurization-recipes-1",
    "title": "WB Project PDO features classification",
    "section": "2) Pre-processing and featurization (recipes)",
    "text": "2) Pre-processing and featurization (recipes)\n\n— Improve recipe [env_FEAT_recipe] (NEW!)\n\nusing sector_f to include the sector tag but with less dimensions\n\nstep_dummy because logistic regression, especially when using certain tuning functions in tidymodels, requires numeric or dummy variables.\n\n\n# ---  Create a recipe with a token filter step that excludes stopwords\n# Rebuild recipe with tokenfilter step\nenv_FEAT_recipe &lt;- recipe (env_cat_f2 ~ pdo + sector_f + regionname + FY_appr,\n                           data = training(env_split)) %&gt;%\n   # tokenize the text\n   step_tokenize(pdo) %&gt;%  \n   # remove CUSTOM stopwords\n   step_stopwords(pdo, custom_stopword_source = stop_vector) %&gt;%  \n   # filter by frequency of occurrence\n   step_tokenfilter(pdo, max_tokens = 100) %&gt;%  \n   # creates tf-idf matrix of weighted term frequencies\n   step_tfidf(pdo, smooth_idf = FALSE) %&gt;%\n   # add NA as special factor level\n   step_unknown(sector_f ,new_level = \"Unknown sect\" ) %&gt;%\n   step_unknown(regionname ,new_level = \"Unknown reg\" ) %&gt;%\n   step_unknown(FY_appr ,new_level = \"Unknown FY\" ) %&gt;%\n   # convert to dummy variables\n   step_dummy(sector_f, regionname, FY_appr, one_hot = TRUE) \n\ncheck what changed…\n\n# prep and bake the recipe\nenv_FEAT_recipe_bake &lt;-  env_FEAT_recipe %&gt;% \n  prep() %&gt;% \n   bake(new_data = NULL)\n\n# preview the baked recipe\ndim(env_FEAT_recipe_bake)\n#[1] 2264 101 --&gt; 2260  149\nenv_FEAT_recipe_bake[1:5, 1:10]"
  },
  {
    "objectID": "analysis/02a_WB_project_pdo_feat_class_envcat.html#model-specification-2",
    "href": "analysis/02a_WB_project_pdo_feat_class_envcat.html#model-specification-2",
    "title": "WB Project PDO features classification",
    "section": "3) Model specification",
    "text": "3) Model specification\n(same)\n— Model specification [env_spec]\n\n# Create a model specification\nenv_spec &lt;-\n   # generalized linear model for binary outcomes\n   parsnip::logistic_reg(\n      mode = \"classification\",\n      # A non-negative number representing the total amount of regularization\n      penalty = tune(),  # 0 = no penalty, 1 = max\n      #A number between zero and one (inclusive)\n      mixture = 1 # pecifies a pure lasso model,\n   ) %&gt;%\n   set_engine(\"glmnet\")\n                           ##### tune() IS A PLACEHOLDER\n# Preview\nenv_spec"
  },
  {
    "objectID": "analysis/02a_WB_project_pdo_feat_class_envcat.html#model-training-workflows-2",
    "href": "analysis/02a_WB_project_pdo_feat_class_envcat.html#model-training-workflows-2",
    "title": "WB Project PDO features classification",
    "section": "4) Model training (workflows)",
    "text": "4) Model training (workflows)\n— Create workflow [env_FEAT_wf] (NEW!)\nenv_FEAT_recipe is actually the part that changed in this workflow adding step_stopwords().\n\n# Create a workflow\nenv_FEAT_wf &lt;- workflows::workflow() %&gt;%\n   add_recipe(env_FEAT_recipe) %&gt;%  # NEW RECIPE\n   add_model(env_spec) # same model\n# Preview\nenv_FEAT_wf"
  },
  {
    "objectID": "analysis/02a_WB_project_pdo_feat_class_envcat.html#model-evaluation-tweaking-2",
    "href": "analysis/02a_WB_project_pdo_feat_class_envcat.html#model-evaluation-tweaking-2",
    "title": "WB Project PDO features classification",
    "section": "5) Model evaluation / tweaking",
    "text": "5) Model evaluation / tweaking\n— Penalty tuning + folds [env_grid, env_fold] (same)\n\n# Create a grid of values for the penalty hyperparameter (random set of 10 values)\nenv_grid &lt;- dials::grid_regular(\n  penalty(),\n  levels = 10)\n\nenv_grid\n\n— K-fold cross-val tuning [env_FEAT_tune] (NEW)\n[…] To choose an optimal value for penalty we need to create a tuning workflow, that resamples env_cat_train in multiple training/testing sets\n\nset.seed(123)\n\n# Create a resampling object\nenv_vfold &lt;- rsample::vfold_cv(env_cat_train, v = 10)\n\n# Create a tuning workflow\nenv_FEAT_tune &lt;- tune::tune_grid(\n  object = env_FEAT_wf, # changed ! \n  resamples = env_vfold,\n  grid = env_grid,\n  control = control_grid(save_pred = TRUE),\n  # metrics \n  metrics = metric_set(roc_auc, accuracy, sensitivity, specificity)\n)\n# preview\nenv_FEAT_tune\n\nThe env_FEAT_tune object contains the results of the tuning for each fold. We can see the results of the tuning for each fold by calling the collect_metrics() function on the env_FEAT_tune object\n\n# Collect the results of the tuning\nenv_FEAT_tune_metrics &lt;- tune::collect_metrics(env_FEAT_tune)\n\n# visualize the results\ntune::autoplot(env_FEAT_tune) +\n  labs(\n    title = \"Lasso model performance across regularization penalties\",\n    subtitle = \"Performance metrics can be used to identify the best penalty\"\n  )\n# in roc_auc: many many of the penalty values performed similarly, with a drop-off in performance at the higher val- ues\n\nConveniently, the tune::show_best() function takes a tune_grid object and returns the best performing hyperparameter values.\n\n# Show the best hyperparameter values\ntune::show_best(env_FEAT_tune, metric = \"roc_auc\")                  # 0.00599 (same )\n\n# Make selection of penalty programmatically\nenv_FEAT_best &lt;- select_best(env_FEAT_tune, metric =\"roc_auc\")      # 0.00599 (same)\nenv_FEAT_best_lambda &lt;- env_best$penalty                            # 0.00599 (same)\n\nenv_FEAT_best_acc &lt;- select_best(env_FEAT_tune, metric =\"accuracy\") # 0.00599 (same)\nenv_FEAT_best_sens &lt;- select_best(env_FEAT_tune, metric =\"sensitivity\")  # 0.0774"
  },
  {
    "objectID": "analysis/02a_WB_project_pdo_feat_class_envcat.html#hyperparameter-tuning-2",
    "href": "analysis/02a_WB_project_pdo_feat_class_envcat.html#hyperparameter-tuning-2",
    "title": "WB Project PDO features classification",
    "section": "6) Hyperparameter tuning",
    "text": "6) Hyperparameter tuning\nUpdate workflow [env_FEAT_wf2]\nNow we can update the model specification and workflow with the best performing hyperparameter value using the previous env_FEAT_tune workflow and the finalize_workflow() function.\n\n# Update the model specification\nenv_FEAT_wf2 &lt;- env_FEAT_wf %&gt;% \n   tune::finalize_workflow(env_FEAT_best)\n\n# Preview updated workflow object (with defined penalty paramv  0.00599)\nenv_FEAT_wf2"
  },
  {
    "objectID": "analysis/02a_WB_project_pdo_feat_class_envcat.html#final-fit-on-training-set-1",
    "href": "analysis/02a_WB_project_pdo_feat_class_envcat.html#final-fit-on-training-set-1",
    "title": "WB Project PDO features classification",
    "section": "7) Final fit on training set",
    "text": "7) Final fit on training set\nFit the final model\n\n“Take my model workflow (env_FEAT_wf2), train it on the training data (env_cat_train), and save the result as env_FEAT_fit so I can use it to make predictions or evaluate performance.”\n\n\n# Fit the model to the training data\nenv_FEAT_fit &lt;- env_FEAT_wf2 |&gt; \n   workflows::fit (data = env_cat_train)\n\nFrom this object I can extract many objects about processing and results of the fitted model:\n— Extract …\n\n# Extract the fitted model\nenv_FEAT_fit |&gt; \n   workflows::extract_fit_parsnip()  \n\n# Extract Preprocessor\nenv_FEAT_fit |&gt; \n   workflows::extract_preprocessor()\n \n# Extract names of the features used in the model\nextract_fit_parsnip(env_FEAT_fit)$fit$beta@Dimnames[[1]] # names of the features used in the model\n\n# Extract names of penalty values\nextract_fit_parsnip(env_FEAT_fit)$fit$beta@Dimnames[[2]] # Lambda values used in the model\n# etc\n\n— Extract Coefficients [enf_fitted_coeff]\nHere we access the model coefficients to see which features are most important in the model + We see here, for the penalty we chose, what terms contribute the most to a en cat NOT being high risk .\n\nenv_FEAT_fit %&gt;% \n   #workflows::extract_fit_parsnip() %&gt;% \n   broom::tidy() %&gt;% \n   dplyr::filter(term != \"(Intercept)\") %&gt;% \n   dplyr::arrange(desc(abs(estimate))) %&gt;% \n   dplyr::slice(1:20) # top 10\n\n\n“sector_f…” appear among the top coefficients!!!\n— Get predicted class and probabilities (train)\nHere I get the predicted probabilities from env_FEAT_fit logistic regression model on the dataset env_cat_train + with type = \"prob\" argument in the predict() function, the model returns class probabilities for each of the 2260 observations 1. .pred_High-Med-risk = probability of being in the High-Med-risk category 2. .pred_Low-risk_Othr = probability of being in the Low-risk_Othr category\n\n# Fit the model on the training set\nenv_FEAT_fit &lt;- env_FEAT_wf2 %&gt;% # NEW\n   fit(data = training(env_split))\n\n# Example of a data frame containing actual and predicted values\npred_FEAT  &lt;- predict(env_FEAT_fit, \n                           new_data = training(env_split), \n                           type = \"prob\")|&gt;\n   # combine with the original training data (labeled)\n   bind_cols(training(env_split)) %&gt;% \n   select(env_cat_f2,  # ACTUAL CLASS of the 1obs\n          pred_high_risk = '.pred_High-Med-risk', # PROB 1obs is PREDICTED HIGH-MED-RISK \n          pred_low_risk = '.pred_Low-risk_Othr'   # PROB 1obs is PREDICTED LOW-RISK\n   )  # 2260 rows\n\npred_FEAT_long &lt;- pred_FEAT |&gt; \n   pivot_longer(cols = c(pred_high_risk, pred_low_risk),\n                names_to = \"risk_type\", values_to = \"risk_value\") # 4,520 rows\n\n— [FIG] Assessing performance [env_FEAT_fit] on training set\nNow is env_split_train\n\n# Plot the predictions with boxplots and jittered points without duplicate legends\nggplot(pred_FEAT_long, aes(x = env_cat_f2, y = risk_value, fill = risk_type)) +\n  geom_boxplot(alpha = 0.4, position = position_dodge(width = 0.8)) +\n  #geom_jitter(alpha = 0.6, position = position_dodge(width = 0.8)) +  # Remove width and use position_dodge\n  labs(title = \"PREDICTED class distribution (y axis) v. ACTUAL class (x axis)\",\n        subtitle = \"Model: Lasso Regression fitted on training data\",\n       x = \"ACTUAL env. class\",\n       y = \"Probabiity of PREDICTED env. class\",\n       fill = \"Risk Type\") +  # Set label for fill legend\n  theme_minimal() +\n  guides(color = \"none\")  # Suppress the color legend\n\n\n\n\n\n\n\nSeems improved also LOW risk prediction (at least o average although more dispersion)\nCalculate performance metrics\n\ntrain_FEAT_pred_probs &lt;- predict(env_FEAT_fit, new_data = env_cat_train, type = \"prob\")\ntrain_FEAT_pred_class &lt;- predict(env_FEAT_fit, new_data = env_cat_train, type = \"class\")\n\ntrain_FEAT_results &lt;- bind_cols(env_cat_train, train_FEAT_pred_probs, train_FEAT_pred_class) |&gt; \n   select(env_cat_f2,  # ACTUAL CLASS of the 1obs\n          pred_high_risk = '.pred_High-Med-risk', # PROB 1obs is PREDICTED HIGH-MED-RISK \n          pred_low_risk = '.pred_Low-risk_Othr',   # PROB 1obs is PREDICTED LOW-RISK\n          pred_class = '.pred_class')\n\n# Confusion matrix\nc_train_acc &lt;- accuracy(train_FEAT_results, truth = env_cat_f2, estimate = pred_class)\n# ROC AUC (needs probabilities + positive class specified)\nc_train_roc_auc &lt;- roc_auc(train_FEAT_results, truth = env_cat_f2, pred_high_risk)\n# Sensitivity and Specificity\nc_train_sens &lt;- sens(train_FEAT_results, truth = env_cat_f2, estimate = pred_class)\nc_train_spec &lt;- spec(train_FEAT_results, truth = env_cat_f2, estimate = pred_class)\n\n# confusion matrix\nconf_mat(train_FEAT_results, truth = env_cat_f2, estimate = pred_class) |&gt; \n   autoplot(type = \"heatmap\") +\n   labs(title = \"Confusion matrix for Lasso model\",\n        subtitle = \"Training set\")\n\n\n\n\n\n\n# roc curve\nroc_curve(train_FEAT_results, truth = env_cat_f2, pred_high_risk) |&gt; \n   autoplot() +\n   labs(title = \"ROC curve for Lasso model (only x = pdo)\",\n        subtitle = \"Training set\")"
  },
  {
    "objectID": "analysis/02a_WB_project_pdo_feat_class_envcat.html#evaluation-on-test-set-2",
    "href": "analysis/02a_WB_project_pdo_feat_class_envcat.html#evaluation-on-test-set-2",
    "title": "WB Project PDO features classification",
    "section": "8) Evaluation on test set",
    "text": "8) Evaluation on test set\n— Get predicted class and probabilities (test)\n\n# Predict on the test set\npred_FEAT_test  &lt;- stats::predict(env_FEAT_fit, \n                           new_data = testing(env_split), \n                           type = \"prob\")|&gt;# prob **high** and prob **low** for each observ based on model \n   bind_cols(testing(env_split)) %&gt;% \n   select(env_cat_f2,  pred_high_risk = '.pred_High-Med-risk', pred_low_risk = '.pred_Low-risk_Othr')   # 970 rows\n\npred_FEAT_long_test &lt;- pred_FEAT_test |&gt;\n   pivot_longer(cols = c(pred_high_risk, pred_low_risk),\n                names_to = \"risk_type\", values_to = \"risk_value\") # 1,940 rows\n\n— [FIG] Assessing performance on test set\n\n# Plot the predictions with boxplots and jittered points without duplicate legends\nggplot(pred_FEAT_long_test, aes(x = env_cat_f2, y = risk_value, fill = risk_type)) +\n  geom_boxplot(alpha = 0.4, position = position_dodge(width = 0.8)) +\n  #geom_jitter(alpha = 0.6, position = position_dodge(width = 0.8)) +  # Remove width and use position_dodge\n   labs(title = \"PREDICTED class distribution (y axis) v. ACTUAL class (x axis)\",\n        subtitle = \"Model: Lasso Regression fitted on test data\",\n       x = \"ACTUAL env. class\",\n       y = \"Probabiity of PREDICTED env. class\",\n       fill = \"Risk Type\") +  # Set label for fill legend\n  theme_minimal() +\n  guides(color = \"none\")  # Suppress the color legend\n\n\n\n\n\n\n\nCalculate performance metrics\n\ntest_FEAT_pred_probs &lt;- predict(env_FEAT_fit, new_data = env_cat_test, type = \"prob\")\ntest_FEAT_pred_class &lt;- predict(env_FEAT_fit, new_data = env_cat_test, type = \"class\")\n\ntest_FEAT_results &lt;- bind_cols(env_cat_test, test_FEAT_pred_probs, test_FEAT_pred_class) |&gt; \n   select(env_cat_f2,  # ACTUAL CLASS of the 1obs\n          pred_high_risk = '.pred_High-Med-risk', # PROB 1obs is PREDICTED HIGH-MED-RISK \n          pred_low_risk = '.pred_Low-risk_Othr',   # PROB 1obs is PREDICTED LOW-RISK\n          pred_class = '.pred_class')\n\n# Confusion matrix\nc_test_acc &lt;- accuracy(test_FEAT_results, truth = env_cat_f2, estimate = pred_class)\n# ROC AUC (needs probabilities + positive class specified)\nc_test_roc_auc &lt;- roc_auc(test_FEAT_results, truth = env_cat_f2, pred_high_risk)\n# Sensitivity and Specificity\nc_test_sens &lt;- sens(test_FEAT_results, truth = env_cat_f2, estimate = pred_class)\nc_test_spec &lt;- spec(test_FEAT_results, truth = env_cat_f2, estimate = pred_class)\n\n# confusion matrix\nconf_mat(test_FEAT_results, truth = env_cat_f2, estimate = pred_class) |&gt; \n   autoplot(type = \"heatmap\") +\n   labs(title = \"Confusion matrix for Lasso model\",\n        subtitle = \"testing set\")\n\n\n\n\n\n\n# roc curve\nroc_curve(test_FEAT_results, truth = env_cat_f2, pred_high_risk) |&gt; \n   autoplot() +\n   labs(title = \"ROC curve for Lasso model (only x = pdo)\",\n        subtitle = \"testing set\")\n\n\n\n\n\n\n\nRecap\n\n# table with accuracy, roc_auc, sensitivity, specificity\nmodel_FEAT_metrics_recap &lt;- tibble(\n   metric = c(\"Accuracy\", \"ROC AUC\", \"Sensitivity\", \"Specificity\"),\n   train = c(c_train_acc$.estimate, \n             c_train_roc_auc$.estimate, \n             c_train_sens$.estimate, \n             c_train_spec$.estimate      )  ,\n   test =  c(c_test_acc$.estimate, \n             c_test_roc_auc$.estimate, \n             c_test_sens$.estimate, \n             c_test_spec$.estimate      )  \n   ) |&gt; \n   mutate(across(c(train, test), ~ round(.x, 3))) |&gt;\n   kable() |&gt; \n   kable_styling(\"striped\", full_width = F) |&gt; \n   # header merged column \n   kableExtra::add_header_above(c(\"C) Lasso Logistic model\", \"datasets\" = 2), \n                                bold = TRUE, color = \"black\", background = \"#D9EAD3\")\n\nmodel_FEAT_metrics_recap\n\n\nModel performance metrics\n\n\n\n\n\n\n\n\nC) Lasso Logistic model\n\n\ndatasets\n\n\n\nmetric\ntrain\ntest\n\n\n\n\nAccuracy\n0.792\n0.787\n\n\nROC AUC\n0.859\n0.823\n\n\nSensitivity\n0.902\n0.903\n\n\nSpecificity\n0.571\n0.558\n\n\n\n\n\nCompared to previous model, there is just a little improvement in false positive (low risk predicted to be high risk) as they are less than before. This model did improve (especially in the low risk category): in fact the probability of being classified as HIGH RISK is less than 50% and of being classified LOW RISK above 50%.\n8) CONFUSION MATRIX on test set\nTo do this we need to fit the tuned workflow to the training set, which is the actual training phase. We will use the tune::last_fit() function\nLe’s use the updated workflow env_FEAT_wf2 (After determining the best model, the final fit on the entire training set is needed and is then evaluated on the test set).\n— [data prep]\n\n# After fitting the best model (to the training set) --&gt; evaluate it on the validation set\nenv_FEAT_final_fit &lt;- tune::last_fit(\n   env_FEAT_wf2, \n   split = env_split)\n\nThe performance metrics are very close to those we achieved on the training set (actually better!!) –&gt; good sign that the model is robust as it performs well on both training and test (validation) sets.\n— [FIG] Visualize the confusion matrix\n\n# Create a table for the confusion matrix counts\nML_final_fit_cm_p &lt;- env_FEAT_final_fit %&gt;%\n  collect_predictions() %&gt;%\n  conf_mat(truth = env_cat_f2, estimate = .pred_class) %&gt;%\n  autoplot(type = \"heatmap\") +\n  labs(\n    title = \"Confusion Matrix for Lasso Logistic Regression Model\",\n    x = \"Predicted Class\",\n    y = \"True Class\",\n    fill = \"Count\"\n  ) +\n  scale_fill_gradient(low = \"#f2e8ea\", high = \"#964957\") +  # Adjust color gradient for better contrast\n  theme_minimal(base_size = 14) +                              # Set a clean theme with larger base text size\n  theme(\n    plot.title = element_text(hjust = 0.5, face = \"bold\"),     # Center and bold the title\n    #axis.text.x = element_text(angle = 45, hjust = 1),         # Angle x-axis text for readability\n    legend.position = \"right\"                                  # Place the legend on the right\n  )\n\nML_final_fit_cm_p\n\nf_save_plot_obj &lt;- function(plot_object, plot_obj_name) {\n   # Save the plot object\n   saveRDS(plot_object, here(\"analysis\", \"output\", \"figures\", paste0(plot_obj_name, \".rds\")))\n}\n\nf_save_plot_obj(ML_final_fit_cm_p, \"ML_final_fit_cm_p\")\n\n\n\nFigure 1: Confusion matrix for the final model on the validation set\n\n\n\n\n\n\n\nStill (on the validation set) imbalanced false positives (138) and false negatives (63).\n— [FIG] Visualize the ROC AUC curve\nTake the output of last_fit() (env_FEAT_final_fit) and use it to plot the ROC curve.\n\ncolnames(env_FEAT_final_fit)\n# Extract predictions from the final fit object\n# Extract the tibble from the list\nenv_FEAT_final_fit_pred &lt;-  env_FEAT_final_fit$.predictions[[1]]\nstr(env_FEAT_final_fit_pred)\n\n# Visualize the ROC curve \nenv_FEAT_final_fit_pred %&gt;% \n   roc_curve(truth = env_cat_f2, '.pred_High-Med-risk') %&gt;% \n   autoplot() +\n   labs(\n      title = \"ROC Curve for High-Med Risk Prediction\",\n      x = \"1 - Specificity (False Positive Rate)\",\n      y = \"Sensitivity (True Positive Rate)\",\n      caption = \"logistic regression model on text (stopwords) + features\"\n   )\n\n9) Interpret the model\n\n— Inspecting what levels of the outcome are most difficult to estimate\n\n# collect the predictions from the final model\nenv_FEAT_final_fit_feat &lt;- env_FEAT_final_fit %&gt;%\n   collect_predictions() %&gt;%\n   bind_cols(env_cat_test)  %&gt;%\n   rename(env_cat_f2 = 'env_cat_f2...6') %&gt;% \n   select ( -'env_cat_f2...32')\n\n#preview the predictions\nglimpse(env_FEAT_final_fit_feat)\n\nI will then select the columns with the actual outcome (env_cat_f2), the predicted outcome, the env_cat_f level, and the pdo text and separate the predicted outcome to inspect them separately\n\nenv_FEAT_final_fit_feat %&gt;%\n   filter(env_cat_f2 != .pred_class ) %&gt;%  \n   select(env_cat_f2, .pred_class,  env_cat_f, pdo, proj_id) \n\nInspect to see in which actual category (env_cat_f) are proj when they are actually env_cat_f2 == 'High-Med-risk' but falsely predicted to be .pred_class == 'Low-risk_Othr': not surprisingly most of them are med risk level. (this makes sense)\n\nenv_FEAT_final_fit_feat %&gt;%\n   filter(env_cat_f2 == 'High-Med-risk' & .pred_class == 'Low-risk_Othr') %&gt;%  \n   select(env_cat_f2, .pred_class,  env_cat_f, pdo, proj_id) %&gt;% \n   count(env_cat_f )\n\nlevels(env_FEAT_final_fit_feat$env_cat_f2)\n#[1] \"High-Med-risk\" \"Low-risk_Othr\"\n\n— Inspecting the most important features for predicting the outcome\n\nusing the extract_fit_parsnip() function from the workflows package to extract the model object from the workflow.\nestimates are the log odds of the outcome for each feature (i.e. the probability of the outcome (High risk) divided by the probability of the opposite outcome (low risk)).\n\n\nPositive coefficient: A positive coefficient indicate an increased likelihood of being in the “Low-risk_Othr” category compared to the “High-Med-risk” category.\n\nNegative coefficient: A negative coefficient indicate an increased likelihood of being in the “High-Med-risk” category compared to the “Low-risk_Othr” category.\n\n\n\nodds ratio (exponentiated coeff) means that the feature is associated with a lower probability of the outcome, while positive odds means that the feature is associated with a higher probability of the outcome.\n\n\nOdds ratio &gt; 1: Indicates that the predictor increases the likelihood of the outcome (e.g., “Low-risk_Othr”).\n\nOdds ratio &lt; 1: Indicates that the predictor decreases the likelihood of the outcome “High-Med-risk”.\n\n\n\n\n# Extract the estimate (log-odds)\nenv_FEAT_final_fit_features &lt;- extract_fit_parsnip(env_FEAT_final_fit) %&gt;% \n   tidy() %&gt;% \n   # Calculate the exponentiated estimate\n   mutate(odds = exp(estimate),\n          probability = odds / (1 + odds))  \n\n#  tibble: 206 × 5\n# term                   estimate penalty  odds probability\n# &lt;chr&gt;                     &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;       &lt;dbl&gt;\n# (Intercept)            -1.26    0.00599 0.284      0.221 \n# tfidf_pdo_access       -0.00452 0.00599 0.995      0.499 \n# tfidf_pdo_activities    0.786   0.00599 2.19       0.687 \n\n\n\ntfidf_pdo_activities est 0.786 | odds 2.19 | prob 0.687 (associated with a LOW risk)\n\ntfidf_pdo_access est -0.004 | odds 0.995 | prob 0.499 (associated with a HIGH risk)\n— Extract the most important features\nA quick way to extract the most important features for predicting each outcome is to use the vi() function from {vip}.\n\nThe vi() function calculates the permutation importance of each feature in the model.\n\n\nlibrary(vip)\n\n# Extract the most important features\nenv_FEAT_var_importance &lt;-  extract_fit_parsnip(env_FEAT_final_fit) %&gt;% \n   vip::vi() %&gt;%\n   # it is kinda counterintuitive  \n   mutate(note = case_when(\n       Sign  ==  \"POS\" ~ \"More likely to be in Low-risk_Othr\",\n       Sign  ==  \"NEG\" ~ \"More likely to be in High-Med-risk\",    \n      TRUE ~ \"NEU\"\n   )) \n\n— [FIG] Plot the most important features\n\n# Recode variable and sign \nvar_importance_tbl &lt;- env_FEAT_var_importance %&gt;% \n   mutate(Feature =  str_remove(Variable, \"tfidf_\"),\n          EnvRiskOutcome = case_when(\n             Sign == \"NEG\" ~ \"High-Med-risk\",\n             Sign == \"POS\" ~ \"Low-risk_Othr\") ) %&gt;% \n   select(Feature, Importance,  EnvRiskOutcome)  \n\nsummary(var_importance_tbl$EnvRiskOutcome)\n\n\n# Plot the most important features\nML_feature_importance_p &lt;- var_importance_tbl %&gt;%\n   slice_max(Importance, n = 50) %&gt;%\n   ggplot(aes(x = reorder(Feature, Importance), y = Importance, color = EnvRiskOutcome)) +\n   geom_point() +\n   coord_flip() +\n   facet_wrap(~EnvRiskOutcome, ncol = 2, scales = \"free_y\") +\n   labs(\n      title = glue(\"Most influential features for predicting Environmental Risk\"),\n      subtitle = \"LASSO Logistic Regression model on text + metadata tags \\n(Importance = absolute value of the logistic regression coefficients)\",\n      #caption = \"(Importance of each feature calculated as the absolute value of the logistic regression coefficients)\",\n      x = \"\",\n      y = \"\",\n      fill = \"\"\n   ) +\n   lulas_theme + \n   theme(\n     plot.title = element_text(hjust = 0.5, face = \"bold\")\n   ) +\n   guides(color = \"none\")\n\nML_feature_importance_p\n\n\n\n\n\n\n\nThe feature importance plot highlights the top 50 predictors of environmental risk (binary) category, ranked by their influence in a LASSO logistic regression model. For better readability the predictors are split according to the level of risk predicted. It should be no surprise that words in the PDO text (those variables starting with pdo_*) are the most important predictors given the data. Still some of the other predictors are also important, such as sector_f_URBAN (left panel) or regionname and sector_f_FINANCIAL (right panel).\nEach facet groups features by environmental risk outcome, allowing a comparison of which factors contribute most to each category. Features with higher importance values, like [mention a few key features], play a significant role in predicting environmental risk, offering insights for targeted risk assessment and decision-making.\n\n# show plot\nML_feature_importance_p\n# save as rds\nf_save_plot_obj(ML_feature_importance_p, \"ML_feature_importance_p\")"
  },
  {
    "objectID": "analysis/02a_WB_project_pdo_feat_class_envcat.html#prep-for-split-based-on-outcome-using-projs_train2-2",
    "href": "analysis/02a_WB_project_pdo_feat_class_envcat.html#prep-for-split-based-on-outcome-using-projs_train2-2",
    "title": "WB Project PDO features classification",
    "section": "0) Prep for split based on outcome [using projs_train2]",
    "text": "0) Prep for split based on outcome [using projs_train2]\n(same)"
  },
  {
    "objectID": "analysis/02a_WB_project_pdo_feat_class_envcat.html#split-data-in-traintest-based-on-env_cat-3",
    "href": "analysis/02a_WB_project_pdo_feat_class_envcat.html#split-data-in-traintest-based-on-env_cat-3",
    "title": "WB Project PDO features classification",
    "section": "1) Split data in train/test [based on env_cat]",
    "text": "1) Split data in train/test [based on env_cat]\n(same)"
  },
  {
    "objectID": "analysis/02a_WB_project_pdo_feat_class_envcat.html#pre-processing-and-featurization-recipes-2",
    "href": "analysis/02a_WB_project_pdo_feat_class_envcat.html#pre-processing-and-featurization-recipes-2",
    "title": "WB Project PDO features classification",
    "section": "2) Pre-processing and featurization (recipes)",
    "text": "2) Pre-processing and featurization (recipes)"
  },
  {
    "objectID": "analysis/02a_WB_project_pdo_feat_class_envcat.html#model-specification-3",
    "href": "analysis/02a_WB_project_pdo_feat_class_envcat.html#model-specification-3",
    "title": "WB Project PDO features classification",
    "section": "3) Model specification",
    "text": "3) Model specification\n— Model specification [new!]\n\nLet’s use a naive Bayes model, which is available in the tidymodels package discrim. One of the main advantages is its ability to handle a large number of features, such as those we deal with when using word count methods. Here we have only kept the 1000 most frequent tokens, but we could have kept more tokens and a naive Bayes model would still be able to handle such predictors well. For now, we will limit the model to a moderate number of tokens.\n\n# needed for naive Bayes\nlibrary(discrim)\n\n# Create a model specification\nenv_NB_spec &lt;-\n   # generalized linear model for binary outcomes\n   parsnip::naive_Bayes(\n      # optional tunable parameter \n      #smoothness = tune(), # 0 = no penalty, 1 = max\n   ) %&gt;%\n   # Specify the mode of the model\n   set_mode(\"classification\") %&gt;%\n   # Specify the engine\n   set_engine(\"naivebayes\")\n \n# Preview\nenv_NB_spec"
  },
  {
    "objectID": "analysis/02a_WB_project_pdo_feat_class_envcat.html#model-training-workflows-3",
    "href": "analysis/02a_WB_project_pdo_feat_class_envcat.html#model-training-workflows-3",
    "title": "WB Project PDO features classification",
    "section": "4) Model training (workflows)",
    "text": "4) Model training (workflows)\n— Create workflow (NEW!)\nNew workflow with\n\nsame preprocessing steps as Model C (env_FEAT_recipe)\nNEW model specification (env_NB_spec): Naive Bayes model instead of the logistic regression model.\n\n\n# Create a workflow\nenv_NB_wf &lt;- workflows::workflow() %&gt;%\n   add_recipe(env_FEAT_recipe) %&gt;%  # same RECIPE\n   add_model(env_NB_spec) # NEW MODEL!\n# Preview\nenv_NB_wf"
  },
  {
    "objectID": "analysis/02a_WB_project_pdo_feat_class_envcat.html#model-evaluation-tweaking-3",
    "href": "analysis/02a_WB_project_pdo_feat_class_envcat.html#model-evaluation-tweaking-3",
    "title": "WB Project PDO features classification",
    "section": "5) Model evaluation / tweaking",
    "text": "5) Model evaluation / tweaking\n— Fit the classificatoin model to the training set [env_NB_fit] (NEW!)\n\n# Fit the model to the training set\nenv_NB_fit &lt;- env_NB_wf %&gt;%\n   #add_model(env_NB_spec) %&gt;%\n   fit(data = env_cat_train)\n\n— (NO tuning!) RESAMPLES\nLet’s use resampling to estimate the performance of the naive Bayes classification model we just fit. We can do this using resampled data sets built from the training set. Let’s create 10-fold cross-validation sets, and use these resampled sets for performance estimates.\n— K-fold cross-validation sets\n\n[env_grid, env_fold, env_NB_tune] (same, same, new)\n\nThe env_FEAT_tune object contains the results of the tuning for each fold. We can see the results of the tuning for each fold by calling the collect_metrics() function on the env_FEAT_tune object\n\nset.seed(123)\n\nenv_vfold &lt;- rsample::vfold_cv(env_cat_train, v = 10) \n\n# Fit the model to the resampled folds\nenv_NB_tune &lt;- fit_resamples(\n   object = env_NB_wf, # changed ! \n   resamples = env_vfold,\n   control = control_resamples(save_pred = TRUE),\n  # metrics \n  metrics = metric_set(roc_auc, accuracy, sensitivity, specificity)\n)\n\n# preview\nenv_NB_tune\n\nWe can extract the relevant information using collect_metrics() and collect_predictions().\n\n# Get the avarage performance metrics across all resampled folds\nenv_NB_tune_metrics &lt;- collect_metrics(env_NB_tune)\n\nenv_NB_tune_metrics\n\n— Visualize the resamples’ performance metrics\nNote: autoplot() is designed to work with tuning results, so here it doesn’t work anymore because NB model doesn’t have a penalty parameter to tune.\n\n# !!!!!!!!!!!!! NOT ANYMORE !!!!!!!!!!!!! \n# visualize the results\n#autoplot(env_NB_tune)\n\n\nenv_NB_tune_metrics %&gt;%\n  ggplot(aes(x = .metric, y = mean, fill = .metric)) +\n  geom_col(show.legend = FALSE) +\n  #facet_wrap(~.metric, scales = \"free_y\") +\n  labs(\n    title = \"Naive Bayes cross-validated metrics\",\n    subtitle = \"Obtained as mean from 10-fold cross-validation\",\n    y = \"Mean performance across folds\",\n    x = NULL\n  )\n\n\nAccuracy (overall correct predictions)\nROC AUC (discrimination ability)\nSensitivity (true positive rate)\nSpecificity (true negative rate)\n— Visualize the ROC AUC curve by fold\nROC curve is a way to visualize the trade-off between sensitivity and specificity at different thresholds.\n\neach point on the curve represents a different threshold for classifying an observation as positive (\"High-Med-risk\") or negative.\n\nx-axis: 1 – specificity (false positive rate)\ny-axis: sensitivity (true positive rate)\n\n\n\nThe area under the curve (AUC) is a single number that summarizes the performance of the model across all thresholds. AUC values range from 0 to 1, with higher values indicating better model performance.\n\nenv_NB_tune_predictions &lt;- collect_predictions(env_NB_tune)\n\n# Visualize the ROC curve \nenv_NB_tune_predictions %&gt;% \n   # predictions grouped by fold\n   group_by(id) %&gt;% \n   # roc_curve(truth = env_cat_f2, .pred_class) %&gt;% \n   # Not work with factor, use \"positive\" class \n   yardstick::roc_curve(\n      truth = env_cat_f2,    # ACTUAL class label\n      '.pred_High-Med-risk', # PREDICTED class label\n      ) %&gt;% \n   autoplot(color = id) +\n   labs(\n      title = \"ROC Curve for Naive Bayes model\",\n      x = \"1 - Specificity \\n(FP Rate)\",\n      y = \"Sensitivity \\n(TP Rate)\",\n      caption = \"Naive Bayes model on text + features\"\n   )\n\nThe area under each of these curves is the roc_auc metric we have computed. (If the curve was close to the diagonal line, then the model’s predictions would be no better than random guessing.)\n— [FIG] Visualize the confusion matrix\n\n# Plot the confusion matrix\nconf_mat_resampled(env_NB_tune, tidy = FALSE) %&gt;% \n   autoplot(type = \"heatmap\")\n\n\n\n\n\n\n\n\ndifferent from logistic\nVery well with true positive (high risk), but very bad with true negative (low risk)."
  },
  {
    "objectID": "analysis/02a_WB_project_pdo_feat_class_envcat.html#hyperparameter-tuning-3",
    "href": "analysis/02a_WB_project_pdo_feat_class_envcat.html#hyperparameter-tuning-3",
    "title": "WB Project PDO features classification",
    "section": "6) Hyperparameter tuning\n",
    "text": "6) Hyperparameter tuning\n\nNot done bc NB doesn’t have any hyperparameters to tune."
  },
  {
    "objectID": "analysis/02a_WB_project_pdo_feat_class_envcat.html#final-fit-on-training-set-2",
    "href": "analysis/02a_WB_project_pdo_feat_class_envcat.html#final-fit-on-training-set-2",
    "title": "WB Project PDO features classification",
    "section": "7) Final fit on training set",
    "text": "7) Final fit on training set\nFit the final model\n\n# Fit the model to the training set\nenv_NB_fit &lt;- env_NB_wf %&gt;% \n   workflows::fit(data = env_cat_train)\n\n— Extract…\n\n# Extract the fitted model as a parsnip model object\nnb_model &lt;- env_NB_fit |&gt; \n   workflows::extract_fit_parsnip()\n# model\nclass(nb_model$fit)\n\n# structure of nodel object\nstr(nb_model$fit, max.level = 2)\n\n# Extract Preprocessor\nenv_NB_fit |&gt; \n   workflows::extract_preprocessor()\n\n# Extract stuff on the model\n#extract_fit_parsnip(env_NB_fit)$fit$....   \n\n— Extract coefficients\nNOTA Naive Bayes is a probabilistic model, and the {naivebayes} package doesn’t expose the internal details (e.g., class-conditional probabilities) in a tidy-friendly format out of the box.\n\n# # Extract the coefficients\n# env_NB_fit %&gt;% \n#    #workflows::extract_fit_parsnip() %&gt;% \n#    broom::tidy() %&gt;% \n#    dplyr::filter(term != \"(Intercept)\") %&gt;% \n#    dplyr::arrange(desc(abs(estimate))) %&gt;% \n#    dplyr::slice(1:20) # top 10\n\n\n# a named list of 148 predictors, each storing class-conditional densities for \"High-Med-risk\" and \"Low-risk_Othr\".\nnb_model$fit$tables\n\n— Get predicted class and probabilities (train)\nUsing env_NB_fit   \n\n\n\n\n\n\n\n\n\n\n\n\n\n# where \n# env_NB_fit &lt;- env_NB_wf %&gt;% \n#    workflows::fit(data = env_cat_train)\n\n# Predicted values \npred_NB_train &lt;- predict(env_NB_fit, \n                           new_data = training(env_split), \n                           type = \"class\")|&gt;\n   # combine with the original training data (labeled)\n   bind_cols(training(env_split)) %&gt;% \n   select(proj_id,\n          env_cat_f2,  # ACTUAL CLASS of the 1obs\n          pred_class = '.pred_class',  \n   )  # 2260 rows\n\n# Convert to long \npred_NB_train_long &lt;- pred_NB_train |&gt; \n   pivot_longer( cols = c(env_cat_f2, pred_class),\n                names_to = \"actual_OR_predicted\", values_to = \"class\") # 4,520 rows\n  # 4,520 rows\n\n— 🟨 [FIG] Assessing performance [env_NB_fit] on training set\n\n# plot actual v predicted class \nggplot(pred_NB_train_long, aes(x = actual_OR_predicted, y = class)) +\n  geom_boxplot(aes(fill = actual_OR_predicted), alpha = 0.4) +\n  geom_jitter(aes(color = actual_OR_predicted), alpha = 0.6) +\n  labs(title = \"PREDICTED class distribution (y axis) v. ACTUAL class (x axis)\",\n       subtitle = \"Model: Naive Bayes fitted on training data\",\n       x = \"ACTUAL env. class\",\n       y = \"PREDICTED env. class\",\n       fill = \"Risk Type\") +  # Set label for fill legend\n  theme_minimal() +\n  guides(color = \"none\")  # Suppress the color legend\n\n\nPredicted class distribution (y axis) v. Actual class (x axis)\n\n\n\n\nCalculate performance metrics\n\ntrain_NB_pred_probs &lt;- predict(env_NB_fit, new_data = env_cat_train, type = \"prob\")\ntrain_NB_pred_class &lt;- predict(env_NB_fit, new_data = env_cat_train, type = \"class\")\n\ntrain_NB_results &lt;- bind_cols(env_cat_train, train_NB_pred_probs, train_NB_pred_class) |&gt; \n   select(proj_id,\n          env_cat_f2,  # ACTUAL CLASS of the 1obs\n          pred_high_risk = '.pred_High-Med-risk', # PROB 1obs is PREDICTED HIGH-MED-RISK \n          pred_low_risk = '.pred_Low-risk_Othr',   # PROB 1obs is PREDICTED LOW-RISK\n          pred_class = '.pred_class')\n\n# Accuracy\nd_train_acc &lt;- accuracy(train_NB_results, truth = env_cat_f2, estimate = pred_class)\n# ROC AUC (needs probabilities + positive class specified)\nd_train_roc_auc &lt;- roc_auc(train_NB_results, truth = env_cat_f2, pred_high_risk)\n# Sensitivity  \nd_train_sens &lt;- sens(train_NB_results, truth = env_cat_f2, estimate = pred_class)\n# Specificity\nd_train_spec &lt;- spec(train_NB_results, truth = env_cat_f2, estimate = pred_class)\n\n# confusion matrix\nconf_mat(train_NB_results, truth = env_cat_f2, estimate = pred_class) |&gt; \n   autoplot(type = \"heatmap\") +\n   labs(title = \"Confusion matrix for Naive Bayes model\",\n        subtitle = \"Training set\")\n\n\n\n\n\n\n# roc curve\nroc_curve(train_NB_results, truth = env_cat_f2, pred_high_risk) |&gt; \n   autoplot() +\n   labs(title = \"ROC curve for Naive Bayes model (only x = pdo)\",\n        subtitle = \"Training set\")"
  },
  {
    "objectID": "analysis/02a_WB_project_pdo_feat_class_envcat.html#evaluation-on-test-set-3",
    "href": "analysis/02a_WB_project_pdo_feat_class_envcat.html#evaluation-on-test-set-3",
    "title": "WB Project PDO features classification",
    "section": "8) Evaluation on test set",
    "text": "8) Evaluation on test set\n— Get predicted class and probabilities (test)\n\n# Predict on the test set\npred_NB_test  &lt;- stats::predict(env_NB_fit, \n                           new_data = testing(env_split), \n                           type = \"class\")|&gt;\n   bind_cols(testing(env_split)) %&gt;% \n   select(proj_id,\n          env_cat_f2,  # ACTUAL CLASS of the 1obs\n          pred_class = '.pred_class',  \n   )  # 970 rows\n\n# Convert to long\npred_NB_test_long &lt;- pred_NB_test |&gt; \n   pivot_longer( cols = c(env_cat_f2, pred_class),\n                names_to = \"actual_OR_predicted\", values_to = \"class\") # 1,940 rows\n\n— [FIG] Assessing performance on test set\n\n# plot actual v predicted class\nggplot(pred_NB_test_long, aes(x = actual_OR_predicted, y = class)) +\n  geom_boxplot(aes(fill = actual_OR_predicted), alpha = 0.4) +\n  geom_jitter(aes(color = actual_OR_predicted), alpha = 0.6) +\n  labs(title = \"PREDICTED class distribution (y axis) v. ACTUAL class (x axis)\",\n       subtitle = \"Model: Naive Bayes fitted on test data\",\n       x = \"ACTUAL env. class\",\n       y = \"PREDICTED env. class\",\n       fill = \"Risk Type\") +  # Set label for fill legend\n  theme_minimal() +\n  guides(color = \"none\")  # Suppress the color legend\n\n\nPredicted class distribution (y axis) v. Actual class (x axis)\n\n\n\n\nCalculate performance metrics\n\ntest_NB_pred_probs &lt;- predict(env_NB_fit, new_data = env_cat_test, type = \"prob\")\ntest_NB_pred_class &lt;- predict(env_NB_fit, new_data = env_cat_test, type = \"class\")\n\ntest_NB_results &lt;- bind_cols(env_cat_test, test_NB_pred_probs, test_NB_pred_class) |&gt; \n   select(proj_id,\n          env_cat_f2,  # ACTUAL CLASS of the 1obs\n          pred_high_risk = '.pred_High-Med-risk', # PROB 1obs is PREDICTED HIGH-MED-RISK \n          pred_low_risk = '.pred_Low-risk_Othr',   # PROB 1obs is PREDICTED LOW-RISK\n          pred_class = '.pred_class')\n\n# Accuracy\nd_test_acc &lt;- accuracy(test_NB_results, truth = env_cat_f2, estimate = pred_class)\n# ROC AUC (needs probabilities + positive class specified)\nd_test_roc_auc &lt;- roc_auc(test_NB_results, truth = env_cat_f2, pred_high_risk)\n# Sensitivity\nd_test_sens &lt;- sens(test_NB_results, truth = env_cat_f2, estimate = pred_class)\n# Specificity\nd_test_spec &lt;- spec(test_NB_results, truth = env_cat_f2, estimate = pred_class)\n\n# confusion matrix\nconf_mat(test_NB_results, truth = env_cat_f2, estimate = pred_class) |&gt; \n   autoplot(type = \"heatmap\") +\n   labs(title = \"Confusion matrix for Naive Bayes model\",\n        subtitle = \"testing set\")\n\n\n\n\n\n\n# roc curve\nroc_curve(test_NB_results, truth = env_cat_f2, pred_high_risk) |&gt; \n   autoplot() +\n   labs(title = \"ROC curve for Naive Bayes model (only x = pdo)\",\n        subtitle = \"testing set\")\n\n\n\n\n\n\n\nRecap\n\n# table with accuracy, roc_auc, sensitivity, specificity\nmodel_NB_metrics_recap &lt;- tibble(\n   metric = c(\"Accuracy\", \"ROC AUC\", \"Sensitivity\", \"Specificity\"),\n   train = c(d_train_acc$.estimate, \n             d_train_roc_auc$.estimate, \n             d_train_sens$.estimate, \n             d_train_spec$.estimate      )  ,\n   test =  c(d_test_acc$.estimate, \n             d_test_roc_auc$.estimate, \n             d_test_sens$.estimate, \n             d_test_spec$.estimate      )  \n   ) |&gt; \n   mutate(across(c(train, test), ~ round(.x, 3))) |&gt;\n   kable() |&gt; \n   kable_styling(\"striped\", full_width = F) |&gt; \n   # header merged column \n   kableExtra::add_header_above(c(\"D) Naive Bayes model\", \"datasets\" = 2), \n                                bold = TRUE, color = \"black\", background = \"#D9EAD3\")\n\nmodel_NB_metrics_recap\n\n\nModel performance metrics\n\n\n\n\n\n\n\n\nD) Naive Bayes model\n\n\ndatasets\n\n\n\nmetric\ntrain\ntest\n\n\n\n\nAccuracy\n0.688\n0.671\n\n\nROC AUC\n0.900\n0.735\n\n\nSensitivity\n0.998\n0.988\n\n\nSpecificity\n0.065\n0.052\n\n\n\n\n\n\nmodel_metrics_recap\nmodel_stop_metrics_recap\nmodel_FEAT_metrics_recap\nmodel_NB_metrics_recap"
  },
  {
    "objectID": "analysis/01a_WB_project_pdo_prep.html",
    "href": "analysis/01a_WB_project_pdo_prep.html",
    "title": "WB Project PDOs Data Preprocessing",
    "section": "",
    "text": "Warning\n\n\n\n\n\nWORK IN PROGRESS! (Please expect unfinished sections, and unpolished code. Feedback is welcome!)"
  },
  {
    "objectID": "analysis/01a_WB_project_pdo_prep.html#ingest-projects-data-manually-from-.xlsx-file",
    "href": "analysis/01a_WB_project_pdo_prep.html#ingest-projects-data-manually-from-.xlsx-file",
    "title": "WB Project PDOs Data Preprocessing",
    "section": "✅ Ingest Projects data (manually from *.xlsx file)",
    "text": "✅ Ingest Projects data (manually from *.xlsx file)\n\n# Load the data\nall_projects_as_of29ago2024 &lt;- readxl::read_excel(here::here (\n   \"data\", \"raw_data\", \"project2\",\"all_projects_as_of29ago2024.xls\"), \n   col_names = FALSE,\n   skip = 1) \n\n# Column names only\ncnames &lt;- read_excel(here::here(\"data\", \"raw_data\", \"project2\", \n                                \"all_projects_as_of29ago2024.xls\"), \n                     col_names = FALSE,\n                     skip = 1,\n                     n_max = 2) \n# Complete file\nall_proj &lt;- read_excel(here::here(\"data\", \"raw_data\", \"project2\", \n                                  \"all_projects_as_of29ago2024.xls\"), \n                         col_names = TRUE,\n                         skip = 2) \n# Save as .RDS \nsave(all_proj, file = here::here(\"data\", \"raw_data\", \"project2\", \n                                 \"all_projects_as_of29ago2024.Rdata\") ) \nrm(all_projects_as_of29ago2024)"
  },
  {
    "objectID": "analysis/01a_WB_project_pdo_prep.html#projects-as-of-31032025",
    "href": "analysis/01a_WB_project_pdo_prep.html#projects-as-of-31032025",
    "title": "WB Project PDOs Data Preprocessing",
    "section": "🟠 projects as of 31/03/2025",
    "text": "🟠 projects as of 31/03/2025\n\nall_proj_25_temp &lt;- readxl::read_excel(here::here (\n   \"data\", \"raw_data\", \"project3\",\"all_projects_as_of31mar2025.xlsx\"), \n      sheet = \"World Bank Projects\", skip = 1) %&gt;% \n   janitor::clean_names() %&gt;% \n   # keep only project_id with match in all_proj\n   filter(project_id %in% all_proj$id)\n   \nnames(all_proj_temp)\nnames(all_proj_25_temp)\n\n\nall_proj_25 &lt;- all_proj_25_temp %&gt;% \n   # new = old\n   dplyr::rename(\"id\"  = \"project_id\" ,\n          \"project_name\"  =     \"project_name\"                      ,\n          \"pdo_2025\"    =     \"project_development_objective\"     ,\n          \"impagency\"  =     \"implementing_agency\"               ,\n          \"cons_serv_reqd_ind\"  =     \"consultant_services_required\"      ,\n         # NO url\n          \"regionname\"  =     \"region\"                            ,\n          \"countryname\" =     \"country\"                           ,\n          \n         \"projectstatusdisplay\"  =     \"project_status\"                    ,\n         \"last_stage_reached_name\" =     \"last_stage_reached_name\"           ,\n         # \"public_disclosure_date\"           ,\n         \"boardapprovaldate\"  =     \"board_approval_date\"               ,\n         #\"loan_effective_date\"              ,\n         \"closingdate\"  =     \"project_closing_date\"              ,\n           \n         \"projectfinancialtype\" = \"financing_type\" ,\n         \n         \"curr_project_cost\" =     \"current_project_cost\"              ,\n         \"curr_ibrd_commitment\" =     \"ibrd_commitment\"                   ,\n         \"curr_ida_commitment\"  =     \"ida_commitment\"                    ,\n          \"curr_total_commitment\"    = \"total_ibrd_ida_and_grant_commitment\",\n         \"grantamt\"  =     \"grant_amount\"                      ,\n\n          \"borrower\"  =     \"borrower\"                          ,\n          \"lendinginstr\" =     \"lending_instrument\"                ,\n          \"envassesmentcategorycode\" =     \"environmental_assessment_category\" ,\n          \"esrc_ovrl_risk_rate\" =     \"environmental_and_social_risk\"     \n          # \"associated_project\"               ,\n          # NO \"sector1\",\n          # NO \"sector2\",\n          # NO \"sector3\",\n          # NO \"theme1\",\n          # NO \"theme2\"\n) |&gt; \n   dplyr::select(\n      id, project_name, pdo_2025, impagency, cons_serv_reqd_ind,\n      regionname, countryname, projectstatusdisplay, last_stage_reached_name,\n      boardapprovaldate, \n      closingdate,\n      projectfinancialtype, curr_project_cost, curr_ibrd_commitment,\n      curr_ida_commitment, curr_total_commitment, grantamt,\n      borrower, lendinginstr, \n      envassesmentcategorycode, esrc_ovrl_risk_rate,\n   )\n\n\n# keep only row with level_1 !=na\nall_proj_25_t &lt;- all_proj_25 %&gt;% \n   # keep only project_id with match in all_proj\n   filter(id %in% all_proj$id)"
  },
  {
    "objectID": "analysis/01a_WB_project_pdo_prep.html#themes",
    "href": "analysis/01a_WB_project_pdo_prep.html#themes",
    "title": "WB Project PDOs Data Preprocessing",
    "section": "🟠 themes",
    "text": "🟠 themes\n[No clear why there are so many rows per Project ID in the Themes sheet –&gt; not mooving until understood]\n\nall_proj_themes_25_l &lt;- readxl::read_excel(here::here (\n   \"data\", \"raw_data\", \"project3\",\"all_projects_as_of31mar2025.xlsx\"), \n      sheet = \"Themes\", skip = 1) %&gt;% \n   janitor::clean_names() %&gt;% \n   # keep only project_id with match in all_proj\n   filter(project_id %in% all_proj$id)\n   \n# # save next to the original file \n# save(all_proj_themes_25, file = here::here(\"data\", \"raw_data\", \"project2\", \n#                                            \"all_proj_themes_2025.Rdata\") )\n\n# convert to wide format with one row per project_id\n# all_proj_themes_25 &lt;- all_proj_themes_25_l %&gt;%\n#   tidyr::pivot_wider(\n#     id_cols = project_id,\n#     names_from = theme_no,\n#     values_from = c(level, percentage)\n#   )               \n\n✅ Theme 1 from 2025 data\nExtract matching project IDs with the largest percentage of level_1 (theme) for each project.\n\n# keep only row with level_1 !=na\n\nall_proj_theme_1_25 &lt;- all_proj_themes_25_l %&gt;% \n   filter(is.na(level_2) & is.na(level_3)) %&gt;% \n   # keep the largest value of percentage_1 among the rows with the same project_id\n   group_by(project_id) %&gt;%\n   filter(percentage_1 == max(percentage_1)) %&gt;% \n   # keep just one row for each project_id\n   slice(1) %&gt;% \n   # drop empty cols\n   select(project_id, level_1, percentage_1) %&gt;% \n   rename(\n      theme_1_25 =  level_1,\n      theme1_perc_25 = percentage_1\n   )"
  },
  {
    "objectID": "analysis/01a_WB_project_pdo_prep.html#sectors",
    "href": "analysis/01a_WB_project_pdo_prep.html#sectors",
    "title": "WB Project PDOs Data Preprocessing",
    "section": "🟠 sectors",
    "text": "🟠 sectors\n[Here too, it seems in long fomr but it is not very clear why there are so many rows per Project ID in the Sectors sheet –&gt; not mooving until understandood]\n\nall_proj_sectors_25_l &lt;- readxl::read_excel(here::here (\n   \"data\", \"raw_data\", \"project3\",\"all_projects_as_of31mar2025.xlsx\"), \n      sheet = \"Sectors\", skip = 1) %&gt;% \n   janitor::clean_names() %&gt;% \n   # keep only project_id with match in all_proj\n   filter(project_id %in% all_proj$id)\n\n# convert to wide format with one row per project_id\nall_proj_sectors_25 &lt;- all_proj_sectors_25_l %&gt;%\n  tidyr::pivot_wider(\n    id_cols = project_id,\n    names_from = major_sector,\n    values_from = c(sector, sector_percent)\n  )\n\n# # save next to the original file\n# save(all_proj_sectors_25, file = here::here(\"data\", \"raw_data\", \"project2\", \n#                                            \"all_proj_sectors_2025.Rdata\") )\n\n✅ Sector 1 from 2025 data\nExtract matching project IDs with the largest percentage of sector_percent (sector) for each project.\n\n# keep only row with sector_percent !=na\nall_proj_sector_1_25 &lt;- all_proj_sectors_25_l %&gt;% \n   filter(!is.na(sector_percent)) %&gt;% \n   # keep the largest value of sector_percent among the rows with the same project_id\n   group_by(project_id) %&gt;%\n   filter(sector_percent == max(sector_percent)) %&gt;% \n   # keep just one row for each project_id\n   slice(1) %&gt;% \n   rename(\n      major_sector1_25 = major_sector  ,\n      sector1_25 = sector ,\n      sector1_perc_25 = sector_percent\n   )"
  },
  {
    "objectID": "analysis/01a_WB_project_pdo_prep.html#geo-locations",
    "href": "analysis/01a_WB_project_pdo_prep.html#geo-locations",
    "title": "WB Project PDOs Data Preprocessing",
    "section": "🟠 GEO locations",
    "text": "🟠 GEO locations\nIn long form?\n\n# multiple loc per ID \nall_proj_geo_25 &lt;- readxl::read_excel(here::here (\n   \"data\", \"raw_data\", \"project3\",\"all_projects_as_of31mar2025.xlsx\"), \n      sheet = \"GEO Locations\", skip = 1) %&gt;% \n   janitor::clean_names() %&gt;% \n   # keep only project_id with match in all_proj\n   filter(project_id %in% all_proj$id)\n\n# # save next to the original file\n# save(all_proj_geo_25, file = here::here(\"data\", \"raw_data\", \"project3\", \n#                                            \"all_proj_geo_2025.Rdata\") )"
  },
  {
    "objectID": "analysis/01a_WB_project_pdo_prep.html#financiers",
    "href": "analysis/01a_WB_project_pdo_prep.html#financiers",
    "title": "WB Project PDOs Data Preprocessing",
    "section": "🟠 Financiers",
    "text": "🟠 Financiers\nIn long form?\n\nall_proj_financiers_25_l &lt;- readxl::read_excel(here::here (\n   \"data\", \"raw_data\", \"project3\",\"all_projects_as_of31mar2025.xlsx\"), \n      sheet = \"Financers\", skip = 1) %&gt;% \n   janitor::clean_names() %&gt;% \n   # keep only project_id with match in all_proj\n   filter(project %in% all_proj$id) %&gt;% \n   rename (project_id = project)\n\n# # save next to the original file\n# save(all_proj_financiers_25, file = here::here(\"data\", \"raw_data\", \"project2\", \n#                                            \"all_proj_financiers_2025.Rdata\") )"
  },
  {
    "objectID": "analysis/01a_WB_project_pdo_prep.html#join-pdo-sector1-and-theme1-to-all_proj_t",
    "href": "analysis/01a_WB_project_pdo_prep.html#join-pdo-sector1-and-theme1-to-all_proj_t",
    "title": "WB Project PDOs Data Preprocessing",
    "section": "✅ Join pdo, sector1 and theme1 to all_proj_t\n",
    "text": "✅ Join pdo, sector1 and theme1 to all_proj_t\n\nThis info may help so I will attach to the existin gfile (but not change ooriginal project list)\n\n# select only the columns of interest\nall_proj_theme_1_25 &lt;- all_proj_theme_1_25 %&gt;% \n   select(project_id, theme_1_25, theme1_perc_25)\n\nall_proj_sector_1_25 &lt;- all_proj_sector_1_25 %&gt;%\n   select(project_id, major_sector1_25, sector1_25, sector1_perc_25)\n\nall_proj_25_t &lt;- all_proj_25_t %&gt;% \n   select(id, pdo_2025)\n\n# join the data to the original file\nall_proj_t &lt;- left_join(\n   # add theme1 rom 2025\n   all_proj_temp, all_proj_theme_1_25, by = c(\"id\" = \"project_id\")) %&gt;% \n   relocate(theme_1_25, theme1_perc_25,  .after = theme1) %&gt;% \n   \n   # add sector1 from 2025\n   left_join(all_proj_sector_1_25, by = c(\"id\" = \"project_id\")) %&gt;% \n   relocate(major_sector1_25, sector1_25, sector1_perc_25, .after = sector1) %&gt;% \n   \n   # add pdo from 2025\n   left_join(all_proj_25_t, by = c(\"id\" = \"id\"))  %&gt;% \n   relocate(pdo_2025, .after = pdo)\n\nWith the updated data from 2025, the tagging of sector1_25 (4.4% missing instead of 16.6%) and theme1_25 (30.9% missing instead of 51.9%) is more complete!"
  },
  {
    "objectID": "analysis/01a_WB_project_pdo_prep.html#recap-content",
    "href": "analysis/01a_WB_project_pdo_prep.html#recap-content",
    "title": "WB Project PDOs Data Preprocessing",
    "section": "Recap content",
    "text": "Recap content\n\nA lot of PDOs are missing, especially from earlier FYs\n\n\n\n22,569 Projects’ IDs, of which…\n\n9,774 (non missing) PDOs … 49,7% are missing\n22,569-4981 = 17,588 (non missing) board approval FY … 22.1% are missing\n\nranging from 1947 - 2026\n\n\n\n[FUN] Count missing/distinct values all_proj_t\n\n\n# # Function to count missing values and distinct values in a subset of columns\n# f_recap_values &lt;- function(data, columns) {\n#    # Select the subset of columns\n#    df_subset &lt;- data %&gt;% select(all_of(columns))\n# \n#    # 1) Use skimr to skim the data\n#    skimmed &lt;- skim(df_subset)\n#    \n#    # 2) Get the number of rows in the dataset\n#    total_rows &lt;- nrow(df_subset)\n# \n#    \n#    # 3) Calculate the number of distinct values for each column\n#    distinct_counts &lt;- df_subset %&gt;%\n#       summarise(across(everything(), n_distinct)) %&gt;%\n#       pivot_longer(everything(), names_to = \"skim_variable\", values_to = \"n_distinct\")\n#    \n#    # Extract the relevant columns for column names, missing values, and distinct counts\n#    missing_table &lt;- skimmed %&gt;%  # 1) \n#       select(skim_variable, n_missing) %&gt;%\n#       # Add the total number of rows\n#       mutate(total_rows = total_rows) %&gt;%  # 2) \n#       # Join with distinct counts\n#       left_join(distinct_counts, by = \"skim_variable\") %&gt;%  # 3) \n#       relocate(n_distinct, n_missing, .after = total_rows) %&gt;%\n#       mutate(missing_perc = round((n_missing/total_rows)*100, 1), \n#              missing_perc = glue::glue(\"{missing_perc}%\")) %&gt;%\n#       arrange(desc(n_distinct))\n#    \n#    # Return the table\n#    return(missing_table) \n# }\n# \n# # exe use \n# #f_recap_values(df, c(\"col1\",\"col2\"))\n\n\n# CALL  the function on a subset of columns\nf_recap_values(all_proj_t, c(\"id\",\"pdo\",\"pdo_2025\",\n                             \"projectstatusdisplay\", \n                             \"boardapprovalFY\", \"regionname\",\n                             \"sector1\",\"sector1_25\",\n                             \"theme1\", \"theme_1_25\",   \n                             \"theme1\", \"projectfinancialtype\"))"
  },
  {
    "objectID": "analysis/01a_WB_project_pdo_prep.html#combine-pdo-from-2024-and-2025",
    "href": "analysis/01a_WB_project_pdo_prep.html#combine-pdo-from-2024-and-2025",
    "title": "WB Project PDOs Data Preprocessing",
    "section": "Combine PDO from 2024 and 2025",
    "text": "Combine PDO from 2024 and 2025\n\n# Combine the PDOs from 2024 and 2025\nall_proj_t &lt;- all_proj_t %&gt;%\n   dplyr::mutate(pdo = coalesce(pdo, pdo_2025)) %&gt;%\n   # sector \n   mutate(sector1 = coalesce(sector1, major_sector1_25, sector1_25))  %&gt;% \n   # theme\n   mutate(theme1 = coalesce(theme1, theme_1_25)) %&gt;% \n   # drop the 2025 columns\n   select(-c(pdo_2025, major_sector1_25, sector1_25, theme_1_25))\n\n\n# CALL  the function on a subset of columns\nf_recap_values(all_proj_t, c(\"id\",\n                             \"project_name\",\n                             \"pdo\", \n                             \"projectstatusdisplay\", \n                             \"boardapprovalFY\", \n                             \"regionname\",\n                             \"sector1\", \n                             \"theme1\",   \n                             \"envassesmentcategorycode\",\n                             \"esrc_ovrl_risk_rate\",                             \"projectfinancialtype\"))\n\n# A tibble: 9 × 5   \n#   skim_variable             missing_perc\n# id                            0%          \n# project_name                  7.9%        \n# pdo                          31.1%       \n# projectstatusdisplay          7.9%        \n# regionname                    7.9%        \n# sector1                       4.2%        \n# theme1                       28.5%       \n# envassesmentcategorycode     45.9%       \n# esrc_ovrl_risk_rate          92.1%       \n# projectfinancialtype         45.2%       \n# boardapprovalFY              22.1%"
  },
  {
    "objectID": "analysis/01a_WB_project_pdo_prep.html#which-pdos-are-missing",
    "href": "analysis/01a_WB_project_pdo_prep.html#which-pdos-are-missing",
    "title": "WB Project PDOs Data Preprocessing",
    "section": "Which PDOs are missing?",
    "text": "Which PDOs are missing?\nIdeally, the PDOs are missing in a non systematic way. To check it, I compare the distributions across some key features of ALL PROJs v. NON-missing-PDO-PROJs\n\nmissing_pdo &lt;- all_proj_t %&gt;% \n   #select(id, pdo, countryname, projectstatusdisplay, lendinginstr, boardapprovalFY, projectfinancialtype) %&gt;% \n   filter(is.na(pdo))\n\n# Now I compare to get a sense of distribution in all_proj_t v. missing_pdo... \ntabyl(all_proj_t$projectstatusdisplay) %&gt;%  adorn_pct_formatting()\ntabyl(missing_pdo$projectstatusdisplay) %&gt;%  adorn_pct_formatting()\n# Region  (seem same)\ntabyl(all_proj_t$regionname)  %&gt;% adorn_pct_formatting() \ntabyl(missing_pdo$regionname)  %&gt;% adorn_pct_formatting() \n# FY (seem same)\ntabyl(all_proj_t$boardapprovalFY) %&gt;%  adorn_pct_formatting()\ntabyl(missing_pdo$boardapprovalFY) %&gt;%  adorn_pct_formatting()\n# too many sectors... (seem same)\ntabyl(all_proj_t$sector1)  %&gt;% adorn_pct_formatting() \ntabyl(missing_pdo$sector1)  %&gt;% adorn_pct_formatting() \n\n# too little filled themes\ntabyl(all_proj_t$theme1)  %&gt;% adorn_pct_formatting() # 51.9% NA\ntabyl(missing_pdo$theme1)  %&gt;% adorn_pct_formatting() # 67.4%  NA\n# Environmental Assessment Category\ntabyl(all_proj_t$envassesmentcategorycode)  %&gt;% adorn_pct_formatting() # 92.1%   NA\ntabyl(missing_pdo$envassesmentcategorycode)  %&gt;% adorn_pct_formatting() \n# Environmental and Social Risk\ntabyl(all_proj_t$esrc_ovrl_risk_rate)  %&gt;% adorn_pct_formatting() # 98.4%  NA\ntabyl(missing_pdo$esrc_ovrl_risk_rate)  %&gt;% adorn_pct_formatting() \n# by fin instrum \ntabyl(all_proj_t$projectfinancialtype)  %&gt;% adorn_pct_formatting() \ntabyl(missing_pdo$projectfinancialtype)  %&gt;% adorn_pct_formatting() \n# by lending instrum\ntabyl(all_proj_t$lendinginstr)  %&gt;% adorn_pct_formatting()  # Spec Inv Loan 6567 |29.1%\ntabyl(missing_pdo$lendinginstr)  %&gt;% adorn_pct_formatting() # Spec Inv Loan 4928 |43.9%\n\n[Chi-Squared Test in the case of projectfinancialtype]\n\nGoodness of fit test to see if the distribution of projectfinancialtype in the sample is significantly different from the population.\n\n# Chi-Square Test: if a discrete distribution of a sample reflects the population, you can use a chi-square goodness-of-fit test\n\n# by financialtype \n# Population distribution (excluding NAs)\npop &lt;- all_proj_t %&gt;%\n  filter(!is.na(projectfinancialtype)) %&gt;%\n  count(projectfinancialtype, name = \"n\") %&gt;%\n  mutate(percent = n / sum(n))\n\n# Observed distribution (excluding NAs)\nobs &lt;- missing_pdo %&gt;%\n  filter(!is.na(projectfinancialtype)) %&gt;%\n  count(projectfinancialtype, name = \"n\") %&gt;% \n  mutate(percent = n / sum(n))\n\n# Ensure both pop and obs are aligned by category\ncombined &lt;- obs %&gt;%\n  inner_join(pop, by = \"projectfinancialtype\", suffix = c(\"_obs\", \"_pop\"))\n\n\n# Extract observed counts and expected proportions (from the population)\nobserved &lt;- combined$n_obs\nexpected_prop &lt;- combined$percent_pop\nobserved\nexpected_prop\n\n# Perform the chi-square test\nchisq_test &lt;- chisq.test(observed, p = expected_prop)\n\n# Display the results\nchisq_test\n# data:  observed\n# X-squared = 3421.4, df = 4, p-value &lt; 0.00000000000000022\n\n# low p-value (typically &lt; 0.05) suggests that the sample distribution is significantly different from the population!!!!\n\n[Chi-Squared Test in the case of lendinginstr]\nGoodness of fit test to see if the distribution of lendinginstr in the sample is significantly different from the population.\n\n# Chi-Square Test: if a discrete distribution of a sample reflects the population, you can use a chi-square goodness-of-fit test\n\n# by fin lendinginstr \n# Population distribution (excluding NAs)\npop2 &lt;- all_proj_t %&gt;%\n  filter(!is.na(lendinginstr)) %&gt;%\n  count(lendinginstr, name = \"n\") %&gt;%\n  mutate(percent = n / sum(n))\n\n# Observed distribution (excluding NAs)\nobs2 &lt;- missing_pdo %&gt;%\n  filter(!is.na(lendinginstr)) %&gt;%\n  count(lendinginstr, name = \"n\") %&gt;% \n  mutate(percent = n / sum(n))\n\n# Ensure both pop and obs are aligned by category\ncombined2 &lt;- obs2 %&gt;%\n  inner_join(pop2, by = \"lendinginstr\", suffix = c(\"_obs\", \"_pop\"))\n\ncombined2\n\n# Extract observed counts and expected proportions (from the population)\nobserved2 &lt;- combined2$n_obs\n# Normalize expected_prop2 so they sum to 1\nexpected_prop2 &lt;- combined2$percent_pop / sum(combined2$percent_pop)\n\n# Perform the chi-square test\nchisq_test2 &lt;- chisq.test(observed2, p = expected_prop2)\n\n# Display the results\nchisq_test2\n# data:  observed\n#X-squared = 5659.6, df = 15, p-value &lt; 0.00000000000000022\n\n# low p-value (typically &lt; 0.05) suggests that the sample distribution is significantly different from the population!!!!\n\nBased on some available project features, I would say that even if many projects have missing feature value, PDO texts’ missingness seems to happen at random, except maybe for:\n\napproval FY\n\nprojectfinancialtype (Grants, IBRD, etc) but there are many missing type (many more in with-PDO sample)…\n\nlendinginstr specific Investment Loan are missing PDO in 4928 pr (43.9%).\n\n\nWhy? + Chi-square tests are sensitive to sample size. Even small percentage differences can become statistically significant when the sample size is large (which is the case here!)\n\n\n# Prep data for plotting\ncombined_long &lt;- combined %&gt;%\n  select(projectfinancialtype, percent_obs, percent_pop) %&gt;%\n  pivot_longer(\n    cols = starts_with(\"percent\"),\n    names_to = \"source\",\n    values_to = \"percent\"\n  ) %&gt;%\n  mutate(\n    source = recode(source,\n      \"percent_obs\" = \"Sample (missing PDO)\",\n      \"percent_pop\" = \"Population\"\n    )\n  )\n\n# Plot\n# Plot\nggplot(combined_long, aes(x = projectfinancialtype, y = percent, fill = source)) +\n  geom_col(position = \"dodge\") +\n  scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +\n  labs(\n    title = \"Distribution of Project Financial Type\",\n    x = \"Financial Type\",\n    y = \"Percentage\",\n    fill = \"Source\"\n  ) +\n  theme_minimal(base_size = 13)\n\n\n\n\n\n\n\n\n# Prep data for plotting\ncombined_long2 &lt;- combined2 %&gt;%\n  select(lendinginstr, percent_obs, percent_pop) %&gt;%\n  pivot_longer(\n    cols = starts_with(\"percent\"),\n    names_to = \"source\",\n    values_to = \"percent\"\n  ) %&gt;%\n  mutate(\n    source = recode(source,\n      \"percent_obs\" = \"Sample (missing PDO)\",\n      \"percent_pop\" = \"Population\"\n    )\n  )\n\n# plot\nggplot(combined_long2, aes(x = lendinginstr, y = percent, fill = source)) +\n  geom_col(position = \"dodge\") +\n  scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +\n  labs(\n    title = \"Distribution of Lending Instrument\",\n    x = \"Lending Instrument\",\n    y = \"Percentage\",\n    fill = \"Source\"\n  ) +\n  theme_minimal(base_size = 13) +\n   # Rotate x-axis labels\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))"
  },
  {
    "objectID": "analysis/01a_WB_project_pdo_prep.html#save-intermediate-data",
    "href": "analysis/01a_WB_project_pdo_prep.html#save-intermediate-data",
    "title": "WB Project PDOs Data Preprocessing",
    "section": "Save intermediate data",
    "text": "Save intermediate data\n\n# origin\nsaveRDS(all_proj_t, here(\"data\" , \"derived_data\", \"all_proj_t.rds\"))#  11,279 obs"
  },
  {
    "objectID": "analysis/01a_WB_project_pdo_prep.html#duplicated-pdos",
    "href": "analysis/01a_WB_project_pdo_prep.html#duplicated-pdos",
    "title": "WB Project PDOs Data Preprocessing",
    "section": "Duplicated PDOs 🤯",
    "text": "Duplicated PDOs 🤯\n\nstr(projs)\nskim(projs$id) # 11,278\nskim(projs$pdo) # 9914\n\nn_distinct(projs$id) # 11,278\nndist_pdo &lt;- n_distinct(projs$pdo) # 9914 !!!!!!!! \n\n \n# Detect duplicated PDOs and the corresponding IDs\nduplicates &lt;- projs %&gt;%\n  group_by(pdo) %&gt;%\n  filter(n() &gt; 1) %&gt;%   # Filter PDOs that appear more than once\n  select(id, pdo)       # Select only id and pdo columns\n\n# View the duplicated PDOs and their corresponding IDs\nnrow(duplicates) # 2461\n\n# Count duplicates per each unique ID based on duplicated PDOs\ncount_dp &lt;- duplicates %&gt;%          # Filter PDOs that appear more than once\n  group_by(pdo) %&gt;%             # Group by id\n  summarise(dup_count = n())   # Count occurrences of duplicated PDOs per id\n\nmin(count_dp$dup_count)    # 2\nmax(count_dp$dup_count)    # 13\nmean(count_dp$dup_count)   # 2.243391\nn_distinct(duplicates$id)  # 2461 projectes with duplicated PDOs\nn_distinct(duplicates$pdo) # 1097 unique PDOs are duplicated in the dataset\n\nThere are 2461 projects with NON-UNIQUE PDO text in the dataset. In some cases, evidently, the same PDO is used for multiple projects (from a minimum of 2 to a maximum of 13 time!!!), most likely when there is a parent project or subsequent phases of the same."
  },
  {
    "objectID": "analysis/01a_WB_project_pdo_prep.html#pos-tagging-with-cleannlp",
    "href": "analysis/01a_WB_project_pdo_prep.html#pos-tagging-with-cleannlp",
    "title": "WB Project PDOs Data Preprocessing",
    "section": "PoS tagging with cleanNLP\n",
    "text": "PoS tagging with cleanNLP\n\n\nHere’s the general process for tagging (or “annotating”) text with the cleanNLP package:\n\nMake a dataset where one column is the id (line number, chapter number, book+chapter, etc.), and another column is the text itself.\nInitialize the NLP tagger. You can use any of these:\n\n️✅ cnlp_init_udpipe(): Use an R-only tagger that should work without installing anything extra (a little slower than the others, but requires no extra steps!)\n\ncnlp_init_spacy(): Use spaCy (if you’ve installed it on your computer with Python)\n\ncnlp_init_corenlp(): Use Stanford’s NLP library (if you’ve installed it on your computer with Java)\n\n\nFeed the data frame from step 1 into the cnlp_annotate() function and wait.\nSave the tagged data on your computer so you don’t have to re-tag it every time.\n\nPrep HYPHENATED words\n\n# Replace hyphens with a placeholder before annotation\npdo_train_to_tag &lt;- projs_train %&gt;%\n   select(id, pdo) %&gt;%\n   mutate(pdo_2 = case_when(\n      str_detect(pdo, \"scale-up|scaled-up\") ~ str_replace_all(pdo, \"scale-up|scaled-up\", \"SCALEUPPLACEHOLDER\"), # 17 \n      str_detect(pdo, \"follow-up\") ~ str_replace_all(pdo, \"follow-up\", \"FOLLOWUPPLACEHOLDER\"), # 6\n      str_detect(pdo, \"know-how\") ~ str_replace_all(pdo, \"know-how\", \"KNOWHOWPLACEHOLDER\"), # 5 \n      str_detect(pdo, \"Covid-19|COVID-19|covid-19\") ~ str_replace_all(pdo, \"Covid-19|COVID-19|covid-19\", \"COVIDPLACEHOLDER\"), # ???\n      # last !!!!!!!!\n      str_detect(pdo, \"(?&lt;=\\\\p{L})-(?=\\\\p{L})\") ~ str_replace_all(pdo, \"-\", \"HYPHENWORD\"), # 1396 \n      TRUE ~ pdo\n   ))"
  },
  {
    "objectID": "analysis/01a_WB_project_pdo_prep.html#looong-annotate-with-cleannlpcnlp_annotate",
    "href": "analysis/01a_WB_project_pdo_prep.html#looong-annotate-with-cleannlpcnlp_annotate",
    "title": "WB Project PDOs Data Preprocessing",
    "section": "☣️☣️⚠️ [LOOONG] Annotate with cleanNLP::cnlp_annotate()\n",
    "text": "☣️☣️⚠️ [LOOONG] Annotate with cleanNLP::cnlp_annotate()\n\n[Only the 1st time]\n\n# ---- Initialize the tagger\ncleanNLP::cnlp_init_udpipe()\n\n# ---- Use the built-in R-based tagger\npdo_train_tagged &lt;- cleanNLP::cnlp_annotate(pdo_train_to_tag,\n                                  text_name = \"pdo_2\",\n                                  doc_name = \"id\")\n\n# ---- save the input data as .rds\nsaveRDS(pdo_train_tagged, here(\"data\",\"derived_data\",\"pdo_train_tagged.rds\"))\n\n[Conditionally re-annotate]\n\n# save the input data as .rds\nsaveRDS(pdo_train_to_tag, here(\"data\",\"derived_data\",\"pdo_train_to_tag.rds\"))\n# define the path to save the tagged data\ninput_path &lt;- here(\"data\",\"derived_data\",\"pdo_train_to_tag.rds\")\noutput_path &lt;- here(\"data\",\"derived_data\",\"pdo_train_tagged.rds\")\n\n\n# ---- Initialize the tagger\ncleanNLP::cnlp_init_udpipe()\n\n# ---- Conditionally annotate the data only if it has changed since the last time\n# Check if previous input data exists; if not, save it for the first time\nif (!file.exists(input_path)) {\n  # Save pdo_train_to_tag to initialize the tracking\n  saveRDS(pdo_train_to_tag, input_path)\n  message(\"Initial input saved.\")\n}\n\n# Load the previous input if it exists\nprevious_input &lt;- readRDS(input_path)\n\n# Only re-run if the input data has changed\nif (!identical(pdo_train_to_tag, previous_input)) {\n  # Run the annotation\n  pdo_train_tagged &lt;- cleanNLP::cnlp_annotate(pdo_train_to_tag, \n                                              text_name = \"pdo_2\", \n                                              doc_name = \"id\")\n  \n  # Save the updated input and annotated result\n  saveRDS(pdo_train_to_tag, input_path)\n  saveRDS(pdo_train_tagged, output_path)\n  \n  message(\"Annotation updated.\")\n} else {\n  # Load the previously saved output if data is unchanged\n  if (file.exists(output_path)) {\n    pdo_train_tagged &lt;- readRDS(output_path)\n    message(\"Loaded previous annotation.\")\n  } else {\n    message(\"No previous annotation found. Please run the annotation first.\")\n  }\n}\n\nManipulate the tagged data\n\n# ---- Extract $token & Convert annotations to a tibble (data frame format)\npdo_train_tag_t &lt;-  pdo_train_tagged$token %&gt;% \n   # ... convert annotations to a tibble (data frame format)\n   as_tibble()    # 225,360\n\n# ---- Replace the placeholder back with a hyphen\npdo_train_tag_t &lt;- pdo_train_tag_t %&gt;% \n   mutate(word = token) %&gt;%\n   \n   # Replace placeholders in the 'token' column\n   mutate(token = case_when(\n      str_detect(word, \"HYPHENWORD\") ~ str_replace_all(word, \"HYPHENWORD\", \"-\"),\n      str_detect(word, \"SCALEUPPLACEHOLDER\") ~ str_replace_all(word, \"SCALEUPPLACEHOLDER\", \"scale-up\"),\n      str_detect(word, \"FOLLOWUPPLACEHOLDER\") ~ str_replace_all(word, \"FOLLOWUPPLACEHOLDER\", \"follow-up\"),\n      str_detect(word, \"KNOWHOWPLACEHOLDER\") ~ str_replace_all(word, \"KNOWHOWPLACEHOLDER\", \"know-how\"),\n      str_detect(word, \"COVIDPLACEHOLDER\") ~ str_replace_all(word, \"COVIDPLACEHOLDER\", \"covid-19\"),\n      TRUE ~ word\n   )) %&gt;%\n   \n   # Replace placeholders in the 'token_ws' column\n   mutate(token_with_ws = case_when(\n      str_detect(token_with_ws, \"HYPHENWORD\") ~ str_replace_all(token_with_ws, \"HYPHENWORD\", \"-\"),\n      str_detect(token_with_ws, \"SCALEUPPLACEHOLDER\") ~ str_replace_all(token_with_ws, \"SCALEUPPLACEHOLDER\", \"scale-up\"),\n      str_detect(token_with_ws, \"FOLLOWUPPLACEHOLDER\") ~ str_replace_all(token_with_ws, \"FOLLOWUPPLACEHOLDER\", \"follow-up\"),\n      str_detect(token_with_ws, \"KNOWHOWPLACEHOLDER\") ~ str_replace_all(token_with_ws, \"KNOWHOWPLACEHOLDER\", \"know-how\"),\n      str_detect(token_with_ws, \"COVIDPLACEHOLDER\") ~ str_replace_all(token_with_ws, \"COVIDPLACEHOLDER\", \"covid-19\"),   \n      TRUE ~ token_with_ws\n   )) %&gt;%\n   \n   # Replace placeholders in the 'lemma' column\n   mutate(lemma = case_when(\n      str_detect(lemma, \"HYPHENWORD\") ~ str_replace_all(lemma, \"HYPHENWORD\", \"-\"),\n      str_detect(lemma, \"Scaleupplaceholder\") ~ str_replace_all(lemma, \"Scaleupplaceholder\", \"scale-up\"),\n      str_detect(lemma, \"Followupplaceholder\") ~ str_replace_all(lemma, \"Followupplaceholder\", \"follow-up\"),\n      str_detect(lemma, \"Knowhowplaceholder\") ~ str_replace_all(lemma, \"Knowhowplaceholder\", \"know-how\"),\n      str_detect(lemma, \"Covidplaceholder\") ~ str_replace_all(lemma, \"Covidplaceholder\", \"covid-19\"),\n      TRUE ~ lemma\n   )) %&gt;%\n \n   relocate(word, .after = doc_id) # 225,360\n\nCheck for duplicates\n\n# check  (old =4403)\nprojs_train$id %&gt;% n_distinct() #  5637\npdo_train_tagged$token$doc_id %&gt;% n_distinct() #  5637 OK!\npdo_train_tag_t$doc_id %&gt;% n_distinct() #  5637 OK! \n\nRe-add cols from projs_train to pdo_train_t\n\n\n# ---- RE-add cols from `projs_train_token ` `\npdo_train_t &lt;-  pdo_train_tag_t %&gt;% \n   left_join(projs_train, by = c(\"doc_id\" = \"id\"))  \n\n# ---- Select cols \npdo_train_t &lt;- pdo_train_t %&gt;% \n   select(\n      proj_id=  doc_id                ,\n      pdo,                                               # [FROM projs_train]\n      sid , # sentence ID\n      tid , # token ID within sentence\n      word,\n      token, # Tokenized form of the token.\n      token_with_ws, # Token with trailing whitespace.\n      lemma, # Lemmatized form of the token.\n      upos, # Universal part-of-speech tag.\n      xpos, # Language-specific part-of-speech tag\n      feats, # Morphological features of the token.\n      tid_source, # Token ID in the source document.\n      relation, # Dependency relation to the head of the token.\n      project_name,  # Name of the parent token.        # [FROM projs_train]\n      regionname            ,\n      countryname           ,\n      projectstatusdisplay,                             # [FROM projs_train] \n      boardapprovaldate,                   # [FROM projs_train]\n      boardapprovalFY,                    # [FROM projs_train] \n      closingdate,                         # [FROM projs_train]\n      closingdateFY,                      # [FROM projs_train]\n      sector1               ,       # [FROM projs_train]\n      theme1                ,       # [FROM projs_train]\n      borrower,                    # [FROM projs_train]\n      lendinginstr          ,       # [FROM projs_train]\n      envassesmentcategorycode ,       # [FROM projs_train]\n      esrc_ovrl_risk_rate ,       # [FROM projs_train]\n      \n      curr_total_commitment        # [FROM projs_train]\n   )\n\nEnsure tid is numeric\n\n# Ensure token_id is numeric\npdo_train_t &lt;- pdo_train_t %&gt;% \n  mutate(tid = as.numeric(tid))  # Convert token_id to numeric\n\nAdd low case token\n\npdo_train_t &lt;- pdo_train_t %&gt;% \n  mutate(token_l = tolower(token)) %&gt;% \n   relocate(token_l, .after = token) %&gt;% \n   select(-token_with_ws) \n\nRestore variations of “hyphenword” with “-”\n\n#Replace variations of \"hyphenword\" with \"-\"\npdo_train_t &lt;- pdo_train_t %&gt;% \n  mutate(\n    lemma = str_replace_all(lemma, regex(\"hyphenword|hyphenwor\", \n                                         ignore_case = TRUE), \"-\")\n  )"
  },
  {
    "objectID": "analysis/00_data.html",
    "href": "analysis/00_data.html",
    "title": "Data",
    "section": "",
    "text": "The raw and derived data files are accessible from the Github Repo  for this project.\n\nWorld Bank Projects & Operations were obtained from:\n\n\nWorld Bank Projects & Operations Data Catalog - links + some metadata\n\nWorld Bank Projects & Operations Advanced Search - search engine with filters.\n\nThe Accessibility Classification is public under Creative Commons Attribution 4.0\n\n\nRetrieve ALL WB projects (22,571) listed (approval obtained or requested between FY 1947 and 2026 as of 31/08/2024) using the Excel button on this page: WBG Projects\n\nSplit the dataset and keep only projs_train (~50% of projects with PDO text, i.e. 5,637 PDOs)\nClean the projs_train dataset\nFurther processing of the column pdo…\n\n\nThese files in the folder data/raw_data/ are downloaded from the World Bank website.\n\n\n\nList of Source Files and Retrieval Dates\n\nSource File Name\nDetails\nRetrieved\n\n\n\nproject2/all_projects_as_of29ago2024.xls\n22,571 obs (projects)\n29 of August 2024\n\n\nproject3/all_projects_as_of31mar2025.xlsx (Sheet Projects)\n22,210 obs (projects)\n31 of March 2025\n\n\nproject3/all_projects_as_of31mar2025.xlsx (Sheet Themes)\n22,210 obs (projects)\n31 of March 2025\n\n\nproject3/all_projects_as_of31mar2025.xlsx (Sheet Sectors)\n22,210 obs (projects)\n31 of March 2025\n\n\nproject3/all_projects_as_of31mar2025.xlsx (Sheet GEOLocations)\n22,210 obs (projects)\n31 of March 2025\n\n\nproject3/all_projects_as_of31mar2025.xlsx (Sheet Financers)\n22,210 obs (projects)\n31 of March 2025\n\n\nwdr.rds\n45 obs (WDRs)\nfrom 2022, then completed manually\n\n\n\n\n\n\nThese files in the folder data/derived_data/ are created in different scripts and saved here to be reused in other scripts.\n\n\n\nKey `.rds` Files, Their Sources, and Contents\n\n\n\n\n\n\nFile *.rds name\nSource File Name\nDetails\n\n\n\nwdr.rds\n[from OLD repo ~/Github/slogan_old/]\n- ...OLD/_my_stuff/WDR-data-ingestion.Rmd\n- result as ...OLD/data/raw_data/WDR.rds\n- text processing on WDR abstracts\n- ...OLD/01b_WDR_data-exploration_abstracts.Rmd\n- result as ...OLD/data/raw_data/wdr.rds\n\nas df (44)problem, API changed — not reproducible\n~ like text processing on PDOs\n\n\nall_proj_t.rds\nanalysis/01a_WB_project_pdo_prep.qmd\n11,279 obs (projects)\n\n\nprojs_train.rds\nanalysis/01a_WB_project_pdo_prep.qmd\n5,637 obs (projects)\n4,425 if &lt; 2001 FY\n\n\nprojs_test.rds\nanalysis/01a_WB_project_pdo_prep.qmd\n2,821 obs (projects)\n\n\nprojs_val.rds\nanalysis/01a_WB_project_pdo_prep.qmd\n2,820 obs (projects)\n\n\npdo_train_to_tag.rds\nanalysis/01a_WB_project_pdo_prep.qmd\n5,637 obs (input)\nPost split\n\n\npdo_train_tagged.rds\nanalysis/01a_WB_project_pdo_prep.qmd\nLARGE `cnlp` object\nintermediate step (output)\n\n\npdo_train_t.rds\nanalysis/01a_WB_project_pdo_prep.qmd\n314,821 obs (tokens)\n248,256 if &lt; 2001 FY\n\n\nprojs_train2.rds\nanalysis/01b_WB_project_pdo_EDA.qmd\n4,425 obs (projects)changed\n\n\n\npdo_train2_t.rds\nanalysis/01b_WB_project_pdo_EDA.qmd\n252,705 obs (tokens)changed\n\n\n\ncustom_stop_words.rds\nanalysis/01b_WB_project_pdo_EDA.qmd\nas vector\n\n\ncustom_stop_words_df.rds\nanalysis/01b_WB_project_pdo_EDA.qmd\nas df\n\n\nwdr2.rds\nanalysis/01b_WB_project_pdo_EDA.qmd\nas df (46)\n[added WDR 2023/2024 manually]"
  },
  {
    "objectID": "analysis/00_data.html#wb-projects-operations",
    "href": "analysis/00_data.html#wb-projects-operations",
    "title": "Data",
    "section": "",
    "text": "World Bank Projects & Operations were obtained from:\n\n\nWorld Bank Projects & Operations Data Catalog - links + some metadata\n\nWorld Bank Projects & Operations Advanced Search - search engine with filters.\n\nThe Accessibility Classification is public under Creative Commons Attribution 4.0"
  },
  {
    "objectID": "analysis/00_data.html#process-to-ingest-preprocess-raw-pdo-text-data",
    "href": "analysis/00_data.html#process-to-ingest-preprocess-raw-pdo-text-data",
    "title": "Data",
    "section": "",
    "text": "Retrieve ALL WB projects (22,571) listed (approval obtained or requested between FY 1947 and 2026 as of 31/08/2024) using the Excel button on this page: WBG Projects\n\nSplit the dataset and keep only projs_train (~50% of projects with PDO text, i.e. 5,637 PDOs)\nClean the projs_train dataset\nFurther processing of the column pdo…"
  },
  {
    "objectID": "analysis/00_data.html#input-data-files",
    "href": "analysis/00_data.html#input-data-files",
    "title": "Data",
    "section": "",
    "text": "These files in the folder data/raw_data/ are downloaded from the World Bank website.\n\n\n\nList of Source Files and Retrieval Dates\n\nSource File Name\nDetails\nRetrieved\n\n\n\nproject2/all_projects_as_of29ago2024.xls\n22,571 obs (projects)\n29 of August 2024\n\n\nproject3/all_projects_as_of31mar2025.xlsx (Sheet Projects)\n22,210 obs (projects)\n31 of March 2025\n\n\nproject3/all_projects_as_of31mar2025.xlsx (Sheet Themes)\n22,210 obs (projects)\n31 of March 2025\n\n\nproject3/all_projects_as_of31mar2025.xlsx (Sheet Sectors)\n22,210 obs (projects)\n31 of March 2025\n\n\nproject3/all_projects_as_of31mar2025.xlsx (Sheet GEOLocations)\n22,210 obs (projects)\n31 of March 2025\n\n\nproject3/all_projects_as_of31mar2025.xlsx (Sheet Financers)\n22,210 obs (projects)\n31 of March 2025\n\n\nwdr.rds\n45 obs (WDRs)\nfrom 2022, then completed manually"
  },
  {
    "objectID": "analysis/00_data.html#output-data-files",
    "href": "analysis/00_data.html#output-data-files",
    "title": "Data",
    "section": "",
    "text": "These files in the folder data/derived_data/ are created in different scripts and saved here to be reused in other scripts.\n\n\n\nKey `.rds` Files, Their Sources, and Contents\n\n\n\n\n\n\nFile *.rds name\nSource File Name\nDetails\n\n\n\nwdr.rds\n[from OLD repo ~/Github/slogan_old/]\n- ...OLD/_my_stuff/WDR-data-ingestion.Rmd\n- result as ...OLD/data/raw_data/WDR.rds\n- text processing on WDR abstracts\n- ...OLD/01b_WDR_data-exploration_abstracts.Rmd\n- result as ...OLD/data/raw_data/wdr.rds\n\nas df (44)problem, API changed — not reproducible\n~ like text processing on PDOs\n\n\nall_proj_t.rds\nanalysis/01a_WB_project_pdo_prep.qmd\n11,279 obs (projects)\n\n\nprojs_train.rds\nanalysis/01a_WB_project_pdo_prep.qmd\n5,637 obs (projects)\n4,425 if &lt; 2001 FY\n\n\nprojs_test.rds\nanalysis/01a_WB_project_pdo_prep.qmd\n2,821 obs (projects)\n\n\nprojs_val.rds\nanalysis/01a_WB_project_pdo_prep.qmd\n2,820 obs (projects)\n\n\npdo_train_to_tag.rds\nanalysis/01a_WB_project_pdo_prep.qmd\n5,637 obs (input)\nPost split\n\n\npdo_train_tagged.rds\nanalysis/01a_WB_project_pdo_prep.qmd\nLARGE `cnlp` object\nintermediate step (output)\n\n\npdo_train_t.rds\nanalysis/01a_WB_project_pdo_prep.qmd\n314,821 obs (tokens)\n248,256 if &lt; 2001 FY\n\n\nprojs_train2.rds\nanalysis/01b_WB_project_pdo_EDA.qmd\n4,425 obs (projects)changed\n\n\n\npdo_train2_t.rds\nanalysis/01b_WB_project_pdo_EDA.qmd\n252,705 obs (tokens)changed\n\n\n\ncustom_stop_words.rds\nanalysis/01b_WB_project_pdo_EDA.qmd\nas vector\n\n\ncustom_stop_words_df.rds\nanalysis/01b_WB_project_pdo_EDA.qmd\nas df\n\n\nwdr2.rds\nanalysis/01b_WB_project_pdo_EDA.qmd\nas df (46)\n[added WDR 2023/2024 manually]"
  },
  {
    "objectID": "analysis/01b_WB_project_pdo_EDA.html",
    "href": "analysis/01b_WB_project_pdo_EDA.html",
    "title": "WB Project PDO text EDA",
    "section": "",
    "text": "Warning\n\n\n\n\n\nWORK IN PROGRESS! (Please expect unfinished sections, and unpolished code. Feedback is welcome!)"
  },
  {
    "objectID": "analysis/01b_WB_project_pdo_EDA.html#saved-file-projs_train_t-pdo_train_t",
    "href": "analysis/01b_WB_project_pdo_EDA.html#saved-file-projs_train_t-pdo_train_t",
    "title": "WB Project PDO text EDA",
    "section": "[Saved file projs_train_t & pdo_train_t]",
    "text": "[Saved file projs_train_t & pdo_train_t]\n\n#projs_train_old &lt;- readRDS(here::here(\"data\",  \"projs_train_old.rds\")) # to check 4403\n\n\n# Load Proj train dataset `projs_train_t`\nprojs_train_bef2001 &lt;- readRDS(here::here(\"data\", \"derived_data\", \"projs_train.rds\")) # 5637\n\n# names(projs_train_bef2001)\n\nprojs_train &lt;- projs_train_bef2001  |&gt; \n      # 2001 was to be sure there are pdo  (2024 is the last year with complete data)\n   dplyr::filter(boardapprovalFY &gt; 2001) |&gt;    # 4425\n   dplyr::rename(\n      # new = old\n      proj_id = id,\n      status = projectstatusdisplay ,\n      pr_name = project_name,\n      env_cat = envassesmentcategorycode ,\n      ESrisk = esrc_ovrl_risk_rate  ,\n      FY_appr = boardapprovalFY,\n      FY_clos = closingdateFY\n   )\n#names(projs_train) \n\n\n# Load clean tokenized-PDO dataset `pdo_train_t`\npdo_train_t &lt;- readRDS(here::here(\"data\", \"derived_data\", \"pdo_train_t.rds\"))\n#  names(pdo_train_t)\n\npdo_train2_t &lt;-  pdo_train_t |&gt;         # 314 k \n   # 2001 was to be shure there are pdo  (2024 is the last year with complete data)\n   dplyr::filter(boardapprovalFY &gt; 2001 ) |&gt;           # 252 k  \n   dplyr::rename(\n      # new = old\n      status = projectstatusdisplay ,\n      pr_name = project_name,\n      env_cat = envassesmentcategorycode ,\n      ESrisk = esrc_ovrl_risk_rate  ,\n      FY_appr = boardapprovalFY,\n      FY_clos = closingdateFY\n   )\n\n#names(pdo_train2_t)"
  },
  {
    "objectID": "analysis/01b_WB_project_pdo_EDA.html#tbl-illustrative-pdos-text-in-projects-documents",
    "href": "analysis/01b_WB_project_pdo_EDA.html#tbl-illustrative-pdos-text-in-projects-documents",
    "title": "WB Project PDO text EDA",
    "section": "[TBL] Illustrative PDOs text in Projects’ documents",
    "text": "[TBL] Illustrative PDOs text in Projects’ documents\n\n\n\n\n\n\n\n\n\nProject_ID\nProject_Name\nProject_Development_Objective\n\n\n\nP127665\nSecond Economic Recovery Development Policy Loan\nThis development policy loan supports the Government of Croatia’s reform efforts with the aim to: (i) enhance fiscal sustainability through expenditure-based consolidation; and (ii) strengthen investment climate.\n\n\nP069934\nPERNAMBUCO INTEGRATED DEVELOPMENT: EDUCATION QUALITY IMPROVEMENT PROJECT\nThe development objectives of the Pernambuco Integrated Development: Education Quality Improvement Project are to (a) improve the quality, efficiency, and inclusiveness of the public education system; (b) modernize and strengthen the managerial, financial, and administrative capacity of the Secretariat of Education to set policies and guidelines for the sector and deliver public education efficiently; and (c) support the overall state modernization effort through interventions to be carried out in the Secretariat of Education and to be replicated in other state institutions."
  },
  {
    "objectID": "analysis/01b_WB_project_pdo_EDA.html#previous-tokenization-and-pos-tagging",
    "href": "analysis/01b_WB_project_pdo_EDA.html#previous-tokenization-and-pos-tagging",
    "title": "WB Project PDO text EDA",
    "section": "[Previous] Tokenization and PoS Tagging",
    "text": "[Previous] Tokenization and PoS Tagging\n[In analysis/01a_WB_project_pdo_prep.qmd I ran cleanNLP::cnlp_init_udpipe() and cleanNLP::cnlp_annotate ot obtain pdo_train_tagged && projs_train–&gt; pdo_train2_t with each token ant its annexed features.\n[1] “proj_id”“pdo”“sid”“tid”“word”\n[6] “token”“token_l”“lemma”“upos”“xpos” [11] “feats”“tid_source”“relation”“pr_name”“regionname”\n[16] “countryname”“status”“boardapprovaldate”“FY_appr”“closingdate”\n[21] “FY_clos”“sector1”“theme1”“borrower”“lendinginstr”\n[26] “env_cat”“ESrisk”“curr_total_commitment”]\nTypically, one of the first steps in this transformation from natural language to feature, or any of kind of text analysis, is tokenization.\ni) Explain Tokenization\nBreaking units of language into components relevant for the research question is called “tokenization”. Components can be words, n-grams, sentences, etc. or combining smaller units into larger units.\n\nTokenization is a row-wise operation: it changes the number of rows in the dataset.\nThe choices of tokenization\n\nShould words be lower cased?\nShould punctuation be removed?\n\nShould numbers be replaced by some placeholder?\nShould words be stemmed (also called lemmatization)? ☑️\nShould bigrams/multi-word phrase be used instead of single word phrases? ☑️\nShould stopwords (the most common words) be removed? ☑️\nShould rare words be removed? ❌\nShould hyphenated words be split into two words? ❌\n\n\nfor the moment I keep all as conservatively as possible\n\nii) Explain Pos Tagging\nLinguistic annotation is a common for of enriching text data, i.e. adding information about the text that is not directly present in the text itself.\nUpon this, e.g. classifying noun, verb, adjective, etc., one can discover intent or action in a sentence, or scanning “verb-noun” patterns.\nHere I have a training dataset file with:\n\n\n\n\n\n\n\n\n\n\n\nVariable\nType\nProvenance\nDescription\nExample\n\n\n\nproj_id\nchr\noriginal PDO data\n\n\n\n\npdo\nchr\noriginal PDO data\n\n\n\n\nword\nchr\noriginal PDO data\n\nGovernments\n\n\nsid\nint\noutput cleanNLP\nsentence ID\n\n\n\ntid\nchr\noutput cleanNLP\ntoken ID within sentence\n\n\n\ntoken\nchr\noutput cleanNLP\nTokenized form of the token.\ngovernment\n\n\ntoken_with_ws\nchr\noutput cleanNLP\nToken with trailing whitespace\ngovernment\n\n\nlemma\nchr\noutput cleanNLP\nThe base form of the token\ngovernment\n\n\nstem\nchr\noutput SnowballC\nThe base form of the token\ngovern\n\n\nupos\nchr\noutput cleanNLP\nUniversal part-of-speech tag (e.g., NOUN, VERB, ADJ).\n\n\n\nxpos\nchr\noutput cleanNLP\nLanguage-specific part-of-speech tags.\n\n\n\nfeats\nchr\noutput cleanNLP\nMorphological features of the token\n\n\n\ntid_source\nchr\noutput cleanNLP\nToken ID in the source document\n\n\n\nrelation\nchr\noutput cleanNLP\nDependency relation between the token and its head token\n\n\n\npr_name\nchr\noutput cleanNLP\nName of the parent token\n\n\n\nFY_appr\ndbl\noriginal PDO data\n\n\n\n\nFY_clos\ndbl\noriginal PDO data\n\n\n\n\nstatus\nchr\noriginal PDO data\n\n\n\n\nregionname\nchr\noriginal PDO data\n\n\n\n\ncountryname\nchr\noriginal PDO data\n\n\n\n\nsector1\nchr\noriginal PDO data\n\n\n\n\ntheme1\nchr\noriginal PDO data\n\n\n\n\nlendinginstr\nchr\noriginal PDO data\n\n\n\n\nenv_cat\nchr\noriginal PDO data\n\n\n\n\nESrisk\nchr\noriginal PDO data\n\n\n\n\ncurr_total_commitment\ndbl\noriginal PDO data\n\n\n\n\n\n\n\n— PoS Tagging: upos (Universal Part-of-Speech)\n\n\n\n\nupos\nn\npercent\nexplan\n\n\n\nADJ\n21764\n0.0861241\nAdjective\n\n\nADP\n27533\n0.1089531\nAdposition\n\n\nADV\n2990\n0.0118320\nAdverb\n\n\nAUX\n3803\n0.0150492\nAuxiliary\n\n\nCCONJ\n14249\n0.0563859\nCoordinating conjunction\n\n\nDET\n21948\n0.0868523\nDeterminer\n\n\nINTJ\n64\n0.0002533\nInterjection\n\n\nNOUN\n71606\n0.2833581\nNoun\n\n\nNUM\n2239\n0.0088601\nNumeral\n\n\nPART\n8737\n0.0345739\nParticle\n\n\nPRON\n2378\n0.0094102\nPronoun\n\n\nPROPN\n14813\n0.0586178\nProper noun\n\n\nPUNCT\n28734\n0.1137057\nPunctuation\n\n\nSCONJ\n2238\n0.0088562\nSubordinating conjunction\n\n\nSYM\n284\n0.0011238\nSymbol\n\n\nVERB\n26093\n0.1032548\nVerb\n\n\nX\n3232\n0.0127896\nOther\n\n\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\n\n\nOn random visual check, these are not always correct, but they are a good starting point for now.\n\n\n\niii) Custom Stopwords\nRemove stop words, which are the most common words in a language.\n\nbut I don’t want to remove any meaningful word for now\n\n\n# Custom list of articles, prepositions, and pronouns\ncustom_stop_words &lt;- c(\n  # Articles\n  \"the\", \"a\", \"an\",\n  \"and\", \"but\", \"or\", \"yet\", \"so\", \"for\", \"nor\", \"as\", \"at\", \"by\", \"per\",\n  # Prepositions\n  \"of\", \"in\", \"on\", \"at\", \"by\", \"with\", \"about\", \"against\", \"between\", \"into\", \"through\",\n  \"during\", \"before\", \"after\", \"above\", \"below\", \"to\", \"from\", \"up\", \"down\", \"under\",\n  \"over\", \"again\", \"further\", \"then\", \"once\",\n  # Pronouns\n  \"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \"you\", \"your\",\n  \"yours\", \"yourself\", \"yourselves\", \"he\", \"him\", \"his\", \"himself\", \"she\", \"her\",\n  \"hers\", \"herself\", \"it\", \"its\", \"itself\", \"they\", \"them\", \"their\", \"theirs\", \"themselves\",\n  \"this\", \"that\", \"these\", \"those\", \"which\", \"who\", \"whom\", \"whose\", \"what\", \"where\",\n  \"when\", \"why\", \"how\", \"all\", \"any\", \"both\", \"each\", \"few\", \"more\", \"most\", \"other\",\n  # \"some\", \"such\", \"no\",  \"not\",\n  # \"too\", \"very\",\n  # verbs\n  \"is\", \"are\", \"would\", \"could\", \"will\", \"be\", \"e.g\", \"e.g.\", \"i.e.\",\n  \"i\", \"ii\", \"iii\", \"iv\", \"v\",\n  # because tautology\n  \"pdo\"\n)\n\n# Convert to a data frame if needed for consistency with tidytext\ncustom_stop_words_df &lt;- tibble(word = custom_stop_words)\n\n\nsaveRDS(custom_stop_words, here(\"data\", \"derived_data\", \"custom_stop_words.rds\"))\nsaveRDS(custom_stop_words_df, here(\"data\", \"derived_data\", \"custom_stop_words_df.rds\"))\n\niv) Stemming\nOften documents contain different versions of one base word, often called a stem. Stemming is the process of reducing words to their base or root form.\nSnowball is one framework released in 1980 with an open-source license that can be found in R package SnowballC.\n\n# Using `SnowballC::wordStem` to stem the words. e.g.\npdo_train2_t &lt;- pdo_train2_t %&gt;%\n  mutate(stem = SnowballC::wordStem(token_l)) %&gt;%\n  relocate(stem, .after = lemma)\n\nWhy Stemming?: For example, in topic modeling, stemming reduces noise by making it easier for the model to identify core topics without being distracted by grammatical variations. (Lemmatization is more computationally intensive as it requires linguistic context and dictionaries, making it slower, especially on large datasets)\n\n\nToken\nLemma\nStem\n\n\n\ndevelopment\ndevelopment\ndevelop\n\n\nquality\nquality\nqualiti\n\n\nhigh-quality\nhigh-quality\nhigh-qual\n\n\ninclude\ninclude\ninclud\n\n\nlogistics\nlogistic\nlogist\n\n\ngovernment/governance\nGovernemnt/government/governance\ngovern\n\n\n\n\nNOTE: Among word / stems encountered in PDOs, there are a lot of acronyms which may refer to World Bank lingo, or local agencies, etc… Especially when looked at in low case form they don’t make much sense…\n\nNotes on sparsity\nSparsity in the context of a document-term matrix refers to the proportion of cells in the matrix that contain zeros. High sparsity means that most terms do not appear in most documents.\n\nremoving stopwords before stemming can reduce sparsity\n\ntidytext::cast_tdm turns a “tidy” one-term-per-document-per-row data frame into a Document-Term Matrix (DTM) from the tm package.\n\nthis dataset contains 4403 documents (each of them a PDO) and 11029 terms (distinct words). Notice that this DTM is 100% sparse (100% of document-word pairings are zero, bc most pairings of document and term do not occur (they have the value zero).\n\n\n\n\n# create document-word matrix\nDTM &lt;- pdo_train2_t %&gt;%\n  anti_join(custom_stop_words_df, by = c(\"token_l\" = \"word\")) %&gt;%\n  count(proj_id, token_l) %&gt;%\n  tidytext::cast_dtm(proj_id, token_l, n) # HIGH!!!\n\nDTM\n# &lt;&lt;DocumentTermMatrix (documents: 4403, terms: 11029)&gt;&gt;\n# Non-/sparse entries: 129940/48430747\n# Sparsity           : 100%\n# Maximal term length: 34\n# Weighting          : term frequency (tf)\n\nv) Document-term matrix or TF-IDF\n\nThe tf-idf is the product of the term frequency and the inverse document frequency::\n\n\\[\n\\begin{aligned}\ntf(\\text{term}) &= \\frac{n_{\\text{term}}}{n_{\\text{terms in document}}} \\\\\nidf(\\text{term}) &= \\ln{\\left(\\frac{n_{\\text{documents}}}{n_{\\text{documents containing term}}}\\right)} \\\\\ntf\\text{-}idf(\\text{term}) &= tf(\\text{term}) \\times idf(\\text{term})\n\\end{aligned}\n\\]\n— TF-IDF matrix on train pdo\n\n# reduce size\n\npdo_train_4_tf_idf &lt;- pdo_train2_t %&gt;% # 255964\n  # Keep only content words [very restrictive for now]\n  # normally c(\"NOUN\", \"VERB\", \"ADJ\", \"ADV\")\n  filter(upos %in% c(\"NOUN\")) %&gt;% #    72,668\n  filter(!token_l %in% c(\"development\", \"objective\", \"project\")) %&gt;% #  66,741\n  # get rid of stop words (from default list)\n  filter(!token_l %in% custom_stop_words_df$word) %&gt;% #  66,704\n  # Optional: Remove lemmas of length 1 or shorter\n  filter(nchar(lemma) &gt; 1) #  66,350\n\nNow, count the occurrences of each lemma for each document. (This is the term frequency or tf)\n\n# This is the term frequency or `tf`\n\n# Count lemmas per document\nlemma_counts &lt;- pdo_train_4_tf_idf %&gt;%\n  count(proj_id, lemma, sort = TRUE)\n# Preview the result\nhead(lemma_counts)\n\nWith the lemma counts prepared, the bind_tf_idf() function from the tidytext package computes the TF-IDF scores.\n\n# Compute the TF-IDF scores\nlemma_tf_idf &lt;- lemma_counts %&gt;%\n  bind_tf_idf(lemma, proj_id, n) %&gt;%\n  arrange(desc(tf_idf))\n\nhead(lemma_tf_idf)\n\n\n\n\n\n\n\nNote\n\n\n\n\n\nWhat to use: token, lemma, or stem?\nGeneral Preference in Real-World NLP:\n\n\nTokens for analyses where word forms matter or for sentiment analysis.\n\nLemmas (*) for most general-purpose NLP tasks where you want to reduce dimensionality while maintaining accuracy and clarity of meaning.\n\nStems for very large datasets, search engines, and applications where speed and simplicity are more important than linguistic precision.\n\n(*) I use lemma, after “aggressively” reducing the number of words to consider, and removing stop words (at least for now)."
  },
  {
    "objectID": "analysis/01b_WB_project_pdo_EDA.html#term-frequency",
    "href": "analysis/01b_WB_project_pdo_EDA.html#term-frequency",
    "title": "WB Project PDO text EDA",
    "section": "Term frequency",
    "text": "Term frequency\nNote: normally, the most frequent words are function words (e.g. determiners, prepositions, pronouns, and auxiliary verbs), which are not very informative. Moreover, even content words (e.g. nouns, verbs, adjectives, and adverbs) can often be quite generic semantically speaking (e.g. “good” may be used for many different things).\nHowever, in this analysis, I do not use the STOPWORD approach, but use the POS tags to reduce – in a more controlled way – the dataset, filtering the content words such as nouns, verbs, adjectives, and adverbs.\n[FUNC] save plot Ouptput\nPointless bc does not render in the HTML output.\n[FUNC] save plot Object\n[FIG] Overall token freq ggplot\n\nExcluding “project” “develop”,“objective”\n\nIncluding only “content words” (NOUN, VERB, ADJ, ADV)\n\n\n# Evaluate the title with glue first\ntitle_text &lt;- glue::glue(\"Most frequent TOKEN in {n_distinct(pdo_train2_t$proj_id)} PDOs from projects approved between FY {min(pdo_train2_t$FY_appr)}-{max(pdo_train2_t$FY_appr)}\")\n\npdo_wrd_freq &lt;- pdo_train2_t %&gt;% # 123,927\n  # include only content words\n  filter(upos %in% c(\"NOUN\", \"VERB\", \"ADJ\", \"ADV\")) %&gt;%\n  # filter (!(upos %in% c(\"AUX\",\"CCONJ\", \"INTJ\", \"DET\", \"PART\",\"ADP\", \"SCONJ\", \"SYM\", \"PART\", \"PUNCT\"))) %&gt;%\n  filter(!(relation %in% c(\"nummod\"))) %&gt;% # 173,686\n  filter(!(token_l %in% c(\n    \"pdo\", \"project\", \"development\", \"objective\", \"objectives\", \"i\", \"ii\", \"iii\",\n    \"is\"\n  ))) %&gt;% # whne it is VERB\n  count(token_l) %&gt;%\n  filter(n &gt; 800) %&gt;%\n  mutate(token_l = reorder(token_l, n)) # reorder values by frequency\n\n# plot\npdo_wrd_freq_p &lt;- pdo_wrd_freq %&gt;%\n  ggplot(aes(token_l, n)) +\n  geom_col(fill = \"#d7b77b\") +\n  scale_y_continuous(breaks = seq(0, max(pdo_wrd_freq$n), by = 400)) + # directly use 'n' instead of .data$n\n  coord_flip() + # flip x and y coordinates so we can read the words better\n  labs( # title = title_text,\n    subtitle = \"[TOKEN with count &gt; 800]\", y = \"\", x = \"\"\n  ) +\n  geom_hline(yintercept = 800, linetype = \"dashed\", color = \"#873c4a\") +\n  lulas_theme +\n  theme( # Adjust angle and alignment of x labels\n    axis.text.x = element_text(angle = 45, hjust = 1)\n  )\n\n[FIG] Overall stem freq ggplot\n\nWithout “project” “develop”,“objective”\n\nIncluding only “content words” (NOUN, VERB, ADJ, ADV)\n\n\n# Evaluate the title with glue first\ntitle_text &lt;- glue::glue(\"Most frequent STEM in {n_distinct(pdo_train2_t$proj_id)} PDOs from projects approved between FY {min(pdo_train2_t$FY_appr)}-{max(pdo_train2_t$FY_appr)}\")\n\n# Plot\npdo_stem_freq &lt;- pdo_train2_t %&gt;% # 256,632\n  # include only content words\n  filter(upos %in% c(\"NOUN\", \"VERB\", \"ADJ\", \"ADV\")) %&gt;%\n  filter(!(relation %in% c(\"nummod\"))) %&gt;% # 173,686\n  filter(!(stem %in% c(\"pdo\", \"project\", \"develop\", \"object\", \"i\", \"ii\", \"iii\"))) %&gt;%\n  count(stem) %&gt;%\n  filter(n &gt; 800) %&gt;%\n  mutate(stem = reorder(stem, n)) # reorder values by frequency\n\n# plot\npdo_stem_freq_p &lt;- pdo_stem_freq %&gt;%\n  ggplot(aes(stem, n)) +\n  geom_col(fill = \"#d7b77b\") +\n  scale_y_continuous(breaks = seq(0, max(pdo_stem_freq$n), by = 400)) + # directly use 'n' instead of .data$n\n  coord_flip() + # flip x and y coordinates so we can read the words better\n  labs( # title = title_text,\n    subtitle = \"[STEM with count &gt; 800]\", y = \"\", x = \"\"\n  ) +\n  geom_hline(yintercept = 800, linetype = \"dashed\", color = \"#873c4a\") +\n  lulas_theme +\n  theme( # Adjust angle and alignment of x labels\n    axis.text.x = element_text(angle = 45, hjust = 1)\n  )\n\n\nEvidently, after stemming, more words (or stems) reach the threshold frequency count of 800 (they have been combined by root).\n\n[FIG] token + stem freq ggplot\n\ntitle2_text &lt;- glue::glue(\"Most frequent TOKEN & STEM in {n_distinct(pdo_train2_t$proj_id)} PDOs\")\n\nsubtitle2_text &lt;- glue::glue(\"From projects approved between FY {min(pdo_train2_t$FY_appr)}-{max(pdo_train2_t$FY_appr)}\")\n\ncombo_freq_p &lt;- pdo_wrd_freq_p + pdo_stem_freq_p +\n  plot_annotation(\n    title = title2_text,\n    subtitle = subtitle2_text,\n    # caption = \"Source: World Bank Project Documents\",\n    theme = theme(\n      plot.title = element_text(size = 12, face = \"bold\"),\n      plot.subtitle = element_text(size = 10, face = \"italic\"),\n      plot.caption = element_text(size = 10, face = \"italic\")\n    )\n  )\n\ncombo_freq_p\n\n\n\n\n\n\n\n\n# f_save_plot(\"combo_freq_p\", combo_freq_p)\nf_save_plot_obj(combo_freq_p, \"combo_freq_p\")"
  },
  {
    "objectID": "analysis/01b_WB_project_pdo_EDA.html#sector-related-term-frequency",
    "href": "analysis/01b_WB_project_pdo_EDA.html#sector-related-term-frequency",
    "title": "WB Project PDO text EDA",
    "section": "SECTOR-related term frequency",
    "text": "SECTOR-related term frequency\nIsolate SECTOR words and see frequency over years\nTo try and make it a bit more meaningful, let’s focus on the frequency of the most common words related to SECTORS.\nFrom token_l, I created a “broad SECTOR” variable to group the sectors in broader definitions:\n\n\nWAT_SAN = water|wastewater|sanitat|Sewer|sewage|Irrigat|Drainag|river basin|groundwater\n\nTRANSPORT = transport|railway|road|airport|waterway|bus|metropolitan|inter-urban|aviation|highway|transit|bridge|port\n\nURBAN = urban|housing|inter-urban|peri-urban|waste manag|slum|city|megacity|intercity|inter-city|town\n\nENERGY = energ|electri|hydroele|hydropow|renewable|transmis|grid|transmission|electric power|geothermal|solar|wind|thermal|nuclear power|energy generation\n\nHEALTH = health|hospital|medicine|drugs|epidem|pandem|covid-19|vaccin|immuniz|diseas|malaria|HIV|AIDS|TB|maternal|clinic|nutrition\n\nEDUCATION = educat|school|vocat|teach|univers|student|literacy|training|curricul|pedagog\n\nAGR_FOR_FISH (Agriculture, Forestry, Fishing) = Agricultural|Agro|Fish|Forest|Crop|livestock|fishery|land|soil\n** MINING_OIL_GAS** = Minin|oil|gas|mineral|quarry|extract|coal|natural gas|mine|petroleum|hydrocarbon\n\nSOCIAL_PROT = Social Protec|social risk|social assistance|living standard|informality|insurance|social choesion|gig economy|human capital|employment|unemploy|productivity|wage lev|intergeneration|lifelong learn|vulnerab|empowerment|sociobehav\n\nFINANCIAL = Bank|finan|Investment|credit|microfinan|loan|financial stability|banking|financial intermed|fintech\n\nICT = Information|Communication|ICT|Internet|telecom|cyber|data|AI|artificial intelligence|blockchain|e-learn|e-commerce|platform|software|hardware|digital\n\nIND_TRADE_SERV = Industry|Trade|Service|manufactur|Tourism|Trade and Services|market|export|import|supply chain|logistic|distribut|e-commerce|retail|wholesale|trade facilitation|trade policy|trade agreement|trade barrier|trade finance|trade promotion|trade integration|trade liberalization|trade balance|trade deficit|trade surplus|trade war|trade dispute|trade negotiation|trade cooperation|trade relation|trade partner|trade route|trade corridor\n\n“INSTIT_SUPP” = Government|Public Admin|Institution|Central Agenc|Sub-national Gov|law|justice|governance|Policy|Regulation|Public Expenditure|Public Investment|Public Procurement\n\n“GENDER_EQUAL” = Gender|Women|Girl|Woman|femal|Gender Equal|gender-base|gender inclus|gender mainstream|gender sensit|gender respons|gender gap|gender-based|gender-sensitive|gender-responsive|gender-transform|gender-equit|gender-balance\n\n“CLIMATE” = Climate|Environment|Sustain|Resilience|Adaptation|Mitigation|Green|Eco|Eco-|carbon|carbon cycle|carbon dioxide|climate change|ecosystem|emission|energy effic|greenhouse|greenhouse gas|temperature anomalies|zero net|green growth|low carbon|climate resilient|climate smart|climate tech|climate variab\n\n\npdo_train2_t &lt;- pdo_train2_t %&gt;%\n  # dealing with water/watershed/waterway\n  mutate(tok_sector_broad = case_when(\n    stringr::str_detect(token_l, regex(\"water|wastewater|sanitat|Sewer|sewage|Irrigat|Drainag|river basin|groundwater\", ignore_case = T)) ~ \"WAT_SAN\",\n    stringr::str_detect(token_l, regex(\"transport|railway|road|airport|waterway|bus|metropolitan|inter-urban|aviation|highway|transit|bridge|port\", ignore_case = T)) ~ \"TRANSPORT\",\n    stringr::str_detect(token_l, regex(\"urban|housing|inter-urban|peri-urban|waste manag|slum|city|megacity|intercity|inter-city|town\", ignore_case = T)) ~ \"URBAN\",\n    stringr::str_detect(token_l, regex(\"energ|electri|hydroele|hydropow|renewable|transmis|grid|transmission|electric power|geothermal|solar|wind|thermal|nuclear power|energy generation\", ignore_case = T)) ~ \"ENERGY\",\n    stringr::str_detect(token_l, regex(\"health|hospital|medicine|drugs|epidem|pandem|covid-19|vaccin|immuniz|diseas|malaria|HIV|AIDS|TB|maternal|clinic|nutrition\", ignore_case = T)) ~ \"HEALTH\",\n    stringr::str_detect(token_l, regex(\"educat|school|vocat|teach|univers|student|literacy|training|curricul|pedagog\", ignore_case = T)) ~ \"EDUCATION\",\n    # not infra\n    stringr::str_detect(token_l, regex(\"Agricultural|Agro|Fish|Forest|Crop|livestock|fishery|land|soil\", ignore_case = T)) ~ \"AGR_FOR_FISH\",\n    stringr::str_detect(token_l, regex(\"Minin|oil|gas|mineral|quarry|extract|coal|natural gas|mine|petroleum|hydrocarbon\", ignore_case = T)) ~ \"MINING_OIL_GAS\",\n    stringr::str_detect(token_l, regex(\"Social Protec|social risk|social assistance|living standard|informality|insurance|social choes|gig economy|human capital|employment|unemploy|productivity|wage lev|intergeneration|lifelong learn|vulnerab|empowerment|sociobehav\", ignore_case = T)) ~ \"SOCIAL_PROT\",\n    stringr::str_detect(token_l, regex(\"Bank|finan|Investment|credit|microfinan|loan|financial stability|banking|financial intermed|fintech\", ignore_case = T)) ~ \"FINANCIAL\",\n    stringr::str_detect(token_l, regex(\"Information|Communication|ICT|Internet|telecom|cyber|data|AI|artificial intelligence|blockchain|e-learn|platform|software|hardware|digital\", ignore_case = T)) ~ \"ICT\",\n    stringr::str_detect(token_l, regex(\"Industry|Trade|Service|manufactur|Tourism|Trade and Services|market|export|import|supply chain|logistic|distribut|e-commerce|retail|wholesale|trade facilitation|trade policy|trade agreement|trade barrier|trade finance|trade promotion|trade integration|trade liberalization|trade balance|trade deficit|trade surplus|trade war|trade dispute|trade negotiation|trade cooperation|trade relation|trade partner|trade route|trade corridor\", ignore_case = T)) ~ \"IND_TRADE_SERV\",\n    stringr::str_detect(token_l, regex(\"Government|Public Admin|Institution|Central Agenc|Sub-national Gov|law|justice|governance|Policy|Regulation|Public Expenditure|Public Investment|Public Procurement\", ignore_case = T)) ~ \"INSTIT_SUPP\",\n    stringr::str_detect(token_l, regex(\"Gender|Women|Girl|Woman|femal|Gender Equal|gender-base|gender inclus|gender mainstream|gender sensit|gender respons|gender gap|gender-based|gender-sensitive|gender-responsive|gender-transform|gender-equit|gender-balance\", ignore_case = T)) ~ \"GENDER_EQUAL\",\n    stringr::str_detect(token_l, regex(\"Climate chan|Environment|Sustain|Resilience|Adaptation|Mitigation|Green|Eco|Eco-|carbon|carbon cycle|carbon dioxide|climate change|ecosystem|emission|energy effic|greenhouse|greenhouse gas|temperature anomalies|zero net|green growth|low carbon|climate resilient|climate smart|climate tech|climate variab\", ignore_case = T)) ~ \"CLIMATE\",\n    TRUE ~ NA_character_\n  )) %&gt;%\n  relocate(tok_sector_broad, .after = token_l) # move the new column to the right of token_l\n\nData prep for sector plots\n\ntabyl(pdo_train2_t$tok_sector_broad)\n\n# Create a custom color list for each sector\nsector_colors &lt;- c(\n  \"WAT_SAN\" = \"#26BDE2\", # SDG 6\n  \"ENERGY\" = \"#FCC30B\", # 7SDG\n  \"MINING_OIL_GAS\" = \"#23399b\", # no SDG!\n  \"URBAN\" = \"#FD9D24\", # SDG 11\n  \"ICT\" = \"#0f7184\", # no SDG!\n  \"HEALTH\" = \"#4C9F38\", # SDG 3\n  \"EDUCATION\" = \"#C5192D\", # SDG 4\n  # SDGS\n  \"POVERTY\" = \"#E5243B\", # SDG 1\n  \"ZERO_HUNGER\" = \"#DDA63A\", # SDG 2\n  \"GENDER_EQUAL\" = \"#FF3A21\", # SDG 5\n  \"WORK\" = \"#A21942\", # SDG 8\n  \"INDUSTRY\" = \"#FD9D24\", # SDG 9\n  \"INEQUALITY\" = \"#DD1367\", # SDG 10\n  \"CONSUMPTION\" = \"#BF8B2E\", # SDG 12\n  \"CLIMATE\" = \"#3F7E44\", # SDG 13\n  \"OCEANS\" = \"#0A97D9\", # SDG 14\n  \"BIODIVERSITY\" = \"#56C02B\", # SDG 15\n  \"PEACE\" = \"#00689D\", # SDG 16\n  \"PARTNERSHIP\" = \"#19486A\", # SDG 17\n  # MINE\n  \"TRANSPORT\" = \"#A6A6A6\",\n  \"AGR_FOR_FISH\" = \"#56C02B\",\n  \"SOCIAL_PROT\" = \"#e28293\",\n  \"FINANCIAL\" = \"#e60066\",\n  \"IND_TRADE_SERV\" = \"#85239b\",\n  \"INSTIT_SUPP\" = \"#49239b\"\n)\n\n# prepare data for plotting (count)\nsector_broad_pdo &lt;- pdo_train2_t %&gt;%\n  filter(!is.na(tok_sector_broad)) %&gt;%\n  filter(tok_sector_broad %in% c(\n    \"WAT_SAN\", \"ENERGY\", \"TRANSPORT\", \"URBAN\", \"MINING_OIL_GAS\", \"ICT\", \"HEALTH\", \"EDUCATION\",\n    # not infrastructure\n    \"AGR_FOR_FISH\", \"GENDER_EQUAL\", \"CLIMATE\", \"SOCIAL_PROT\", \"FINANCIAL\", \"IND_TRADE_SERV\", \"INSTIT_SUPP\"\n  )) %&gt;%\n  count(tok_sector_broad, FY_appr) %&gt;%\n  # filter(n &gt; 0) %&gt;%\n  mutate(tok_sector_broad = factor(tok_sector_broad, levels = c(\n    \"WAT_SAN\", \"ENERGY\", \"TRANSPORT\", \"URBAN\", \"MINING_OIL_GAS\", \"ICT\", \"HEALTH\", \"EDUCATION\",\n    # not infrastructure\n    \"AGR_FOR_FISH\", \"GENDER_EQUAL\", \"CLIMATE\", \"SOCIAL_PROT\", \"FINANCIAL\", \"IND_TRADE_SERV\", \"INSTIT_SUPP\"\n  ))) # reorder values by frequency\n# df$FY\n\n[FIG] faceted sector (tok_sector_broad) freq ggplot\n\n# Evaluate the title with glue first\ntitle_text &lt;- glue::glue(\"Sector words frequency in PDO over FY {min(pdo_train2_t$FY_appr)}-{max(pdo_train2_t$FY_appr)}\")\n\n# Plot\npdo_sect_broad_freq &lt;- sector_broad_pdo %&gt;%\n  # only the \"original group\n  filter(tok_sector_broad %in% c(\"WAT_SAN\", \"ENERGY\", \"TRANSPORT\", \"URBAN\", \"MINING_OIL_GAS\", \"ICT\", \"HEALTH\", \"EDUCATION\")) %&gt;%\n  ggplot(\n    .,\n    aes(\n      x = FY_appr, y = n,\n      group = tok_sector_broad, color = tok_sector_broad\n    )\n  ) +\n  geom_line(linetype = \"dotted\", alpha = 0.5, linewidth = 1) +\n  geom_point(size = 2) +\n  scale_x_continuous(breaks = seq(2001, 2023, by = 1)) +\n  scale_y_continuous(breaks = seq(0, 300, by = 25)) +\n  # ~ SDG colors\n  # scale_color_viridis_d(option = \"magma\", end = 0.9) +\n  scale_color_manual(values = sector_colors) +\n  facet_wrap(~tok_sector_broad, ncol = 3, scales = \"free\") +\n  guides(color = \"none\") +\n  lulas_theme +\n  theme( # Adjust angle and alignment of x labels\n    axis.text.x = element_text(angle = 45, hjust = 1)\n  ) +\n  labs(\n    title = \"Sector words frequency in PDOs by fiscal years of approval\",\n    subtitle = \"[Using \\\"custom\\\" broad sector definition]\",\n    x = \"\", # \"Board approval FY\",\n    y = \"\" # \"Counts of 'sector' word (tok_sector_broad)\"\n  ) +\n  # Add the reference line at y = 50, red, dashed, and transparent (50% opacity)\n  geom_hline(yintercept = 50, linetype = \"longdash\", color = \"#d02e4c\", alpha = 0.40)\n# geom_vline(data = subset(sector_broad_pdo, tok_sector_broad == \"HEALTH\"),\n#            aes(xintercept = 2020),\n#            linetype = \"solid\", color = \"#9b6723\",alpha = 0.35) +\n# geom_text(data = subset(sector_broad_pdo, tok_sector_broad == \"HEALTH\"),\n#           aes(x = 2020, y = max(sector_broad_pdo$n)*0.65, label = \"Covid\"),\n#           angle = 90, vjust = -0.5, color = \"#9b6723\")\n\npdo_sect_broad_freq\n\n\n\n\n\n\n\n[FUNC] Figure split sector (tok_sector_broad) freq ggplot\n\n# --- Get a LIST of unique sectors (facets) and split the data\nPDOsector_list &lt;- base::split(x = sector_broad_pdo, f = sector_broad_pdo$tok_sector_broad)\n\n# --- Create a function to plot for each sector with custom colors\nf_plot_sector &lt;- function(data) {\n  # Get the sector name\n  sector &lt;- unique(data$tok_sector_broad)\n  # Create the plot\n  p &lt;- ggplot(\n    data = data,\n    aes(\n      x = FY_appr, y = n,\n      group = tok_sector_broad, color = tok_sector_broad\n    )\n  ) +\n    # By sector ...\n    geom_line(color = sector_colors[sector], linetype = \"dotted\", alpha = 0.5, size = 1) +\n    geom_point(color = sector_colors[sector], size = 3) +\n    scale_x_continuous(breaks = seq(2001, 2023, by = 1)) +\n    scale_y_continuous(breaks = seq(0, max(data$n), by = 25)) +\n    # custom\n    lulas_theme +\n    theme(axis.text.x = element_text(angle = 45, hjust = 1)) +\n    labs(\n      title = paste(\"\\\"\", sector, \"\\\" in PDOs by fiscal years of approval\"), # Use facet-specific title\n      subtitle = \"[Using a \\\"custom\\\" broad sector definition &\\nshowing relevant WDR publication(s)]\",\n      x = \"\",\n      y = \"\" # Remove y-axis label\n    ) +\n    # Ensure y-axis limit includes 50\n    expand_limits(y = 50) +\n    # Add the reference line at y = 50, red, dashed, and transparent (50% opacity)\n    geom_hline(yintercept = 50, linetype = \"longdash\", color = \"#d02e4c\", alpha = 0.75)\n  # # Add vline and text annotation only for the HEALTH sector\n  if (sector == \"HEALTH\") {\n    p &lt;- p +\n      geom_vline(aes(xintercept = 2020), linetype = \"solid\", color = \"#9b6723\", alpha = 0.35) +\n      geom_text(aes(x = 2020, y = max(n) * 0.75, label = \"Covid\"),\n        angle = 90, vjust = -0.5, color = \"#9b6723\"\n      )\n  }\n\n  return(p)\n}\n\n\n# --- Use purrr::map to create a LIST of plots, one for each sector\nsector_plots &lt;- map(PDOsector_list, f_plot_sector)\n\n# --- (exe) Extract the first plot to display\n# sector_plots$HEALTH\n\n# ---- Optionally print each plot to the console\n# walk(sector_plots, print)\n\n\n# Define the output directory using the 'here' function\noutput_dir &lt;- here(\"analysis\", \"output\", \"figures\")\n\n## Save each plot to a file in the specified directory\n# walk2(sector_plots, names(PDOsector_list),\n#      ~ggsave(filename = file.path(output_dir, paste0(.y, \"_sector_plot.png\")), plot = .x))\n\n# Save iteratively the plots' objects as RDS files\nwalk2(\n  sector_plots, names(PDOsector_list),\n  ~ f_save_plot_obj(.x, paste0(\"pdo_\", .y, \"_sect_p\"))\n)\n\n# pdo_FINANCIAL_sect_p"
  },
  {
    "objectID": "analysis/01b_WB_project_pdo_EDA.html#sector-in-pdo-v.-wdr-publications",
    "href": "analysis/01b_WB_project_pdo_EDA.html#sector-in-pdo-v.-wdr-publications",
    "title": "WB Project PDO text EDA",
    "section": "SECTOR in PDO v. WDR publications",
    "text": "SECTOR in PDO v. WDR publications\nFor the (broadly defined) HEALTH sector, it is quite clear that Covid-19 is the main driver of the peak in 2020.\nWhat about the other sectors? I was struck by the fact that, observing PDOs over time, the broadly defined “sector term” in the PDO always presents at least one peak and I wonder what could trigger it.\nOne possible explanation is that the PDOs somehow reflect the topics discussed by the World Development Reports (WDR) published annually by the World Bank. The WDR is a flagship publication of the World Bank that provides in-depth analysis of a specific aspect of development.\nIt is important to remark that these publications are not some speculative research endeavor, as they are deeply rooted in the concrete information that the Bank retrieves on the ground from projects and operations as they are supported and evaluated. In turn, the WDRs themselves inform the Bank’s policy priorities and operational strategies.\nTherefore, it is reasonable to expect some kind of correlation between the topics discussed in the WDRs and the objectives of projects stated in in the PDOs."
  },
  {
    "objectID": "analysis/01b_WB_project_pdo_EDA.html#ingest-wdr-data",
    "href": "analysis/01b_WB_project_pdo_EDA.html#ingest-wdr-data",
    "title": "WB Project PDO text EDA",
    "section": "Ingest WDR data",
    "text": "Ingest WDR data\nPreviously created, as explained in data/derived_data/_provenance.md\n\n# Read the WDR data\nwdr &lt;- readRDS(here(\"data\", \"derived_data\", \"wdr.rds\"))\n\n— Manually add WDR 2023 ✍🏻\nOKR Full item\n\nlibrary(tibble) # Simple Data Frames # Simple Data Frames\n\n# Create a named list of NA values for subj_11 to subj_46\nna_values &lt;- setNames(rep(NA, 35), paste0(\"subj_\", 12:46))\n\n# Add a new row with the existing columns and NA for subj_11 to subj_46\nwdr &lt;- wdr %&gt;%\n  add_row(\n    date_issued = 2023,\n    decade = \"2020s\",\n    id = NA, # ?\n    ISBN = \"978-1-4648-1941-4\",\n    title = \"Migrants, Refugees, and Societies\",\n    doc_mt_identifier_1 = \"oai:openknowledge.worldbank.org:10986/39696\", # ?\n    subject_miss = NA,\n    abstract = \"Migration is a development challenge. About 184 million people-2.3 percent of the world’s population-live outside of their country of nationality. Almost half of them are in low- and middle-income countries. But what lies ahead? As the world struggles to cope with global economic imbalances, diverging demographic trends, and climate change, migration will become a necessity in the decades to come for countries at all levels of income. If managed well, migration can be a force for prosperity and can help achieve the United Nations’ Sustainable Development Goals. World Development Report 2023 proposes an innovative approach to maximize the development impacts of cross-border movements on both destination and origin countries and on migrants and refugees themselves. The framework it offers, drawn from labor economics and international law, rests on a “Match and Motive Matrix” that focuses on two factors: how closely migrants’ skills and attributes match the needs of destination countries and what motives underlie their movements. This approach enables policy makers to distinguish between different types of movements and to design migration policies for each. International cooperation will be critical to the effective management of migration.\",\n    url_keys = \"https://openknowledge.worldbank.org/handle/10986/39696\",\n    altmetric = 150,\n    all_topic = \"Poverty Reduction,Social Development,Conflict and Development\",\n    all_subj = \"migration,migrants,refugees,force displacement,crss-border mobility,remittances,origin country,international protection,refugee-hosting country,irregular migration,international cooperation\",\n    subj_1 = \"migration\",\n    subj_2 = \"migrants\",\n    subj_3 = \"refugees\",\n    subj_4 = \"force displacement\",\n    subj_5 = \"crss-border mobility\",\n    subj_6 = \"remittances\",\n    subj_7 = \"origin country\",\n    subj_8 = \"international protection\",\n    subj_9 = \"refugee-hosting country\",\n    subj_10 = \"irregular migration\",\n    subj_11 = \"international cooperation\",\n    !!!na_values # Unpack the NA values for subj_12 to subj_46\n  )\n\n— Manually add WDR 2024 ✍🏻\nOKR Full item\n\nlibrary(tibble) # Simple Data Frames # Simple Data Frames\n\n# Create a named list of NA values for subj_11 to subj_46\nna_values &lt;- setNames(rep(NA, 35), paste0(\"subj_\", 12:46))\n# https://documents.worldbank.org/en/publication/documents-reports/documentdetail/099042523192514880/p17826903573340450b2d00e8cfd3baf7ac\n# https://openknowledge.worldbank.org/entities/publication/5e5ac9f1-71ee-4734-825e-60966658395f/full\n\n# Add a new row with the existing columns and NA for subj_11 to subj_46\nwdr &lt;- wdr %&gt;%\n  add_row(\n    date_issued = 2024,\n    decade = \"2020s\",\n    id = NA, # ?\n    ISBN = \"978-1-4648-2078-6\",\n    title = \"The Middle-Income Trap\",\n    doc_mt_identifier_1 = \"oai:openknowledge.worldbank.org:10986/41919\", # ?\n    subject_miss = NA,\n    abstract = \"Middle-income countries are in a race against time. Many of them have done well since the 1990s to escape low-income levels and eradicate extreme poverty, leading to the perception that the last three decades have been great for development. But the ambition of the more than 100 economies with incomes per capita between US$1,100 and US$14,000 is to reach high-income status within the next generation. When assessed against this goal, their record is discouraging. Since the 1970s, income per capita in the median middle-income country has stagnated at less than a tenth of the US level. With aging populations, growing protectionism, and escalating pressures to speed up the energy transition, today’s middle-income economies face ever more daunting odds. To become advanced economies despite the growing headwinds, they will have to make miracles. Drawing on the development experience and advances in economic analysis since the 1950s, World Development Report 2024 identifies pathways for developing economies to avoid the “middle-income trap.” It points to the need for not one but two transitions for those at the middle-income level: the first from investment to infusion and the second from infusion to innovation. Governments in lower-middle-income countries must drop the habit of repeating the same investment-driven strategies and work instead to infuse modern technologies and successful business processes from around the world into their economies. This requires reshaping large swaths of those economies into globally competitive suppliers of goods and services. Upper-middle-income countries that have mastered infusion can accelerate the shift to innovation—not just borrowing ideas from the global frontiers of technology but also beginning to push the frontiers outward. This requires restructuring enterprise, work, and energy use once again, with an even greater emphasis on economic freedom, social mobility, and political contestability. Neither transition is automatic. The handful of economies that made speedy transitions from middle- to high-income status have encouraged enterprise by disciplining powerful incumbents, developed talent by rewarding merit, and capitalized on crises to alter policies and institutions that no longer suit the purposes they were once designed to serve. Today’s middle-income countries will have to do the same.\",\n    url_keys = \"https://openknowledge.worldbank.org/handle/10986/41919\",\n    altmetric = 13,\n    all_topic = \"Macroeconomics,Economic Growth,Business Cycles and Stabilization Policies,Poverty Reduction,Achieving Shared Growth,Science and Technology Development,Innovation\",\n    all_subj = \"middle-income trap,investment,infusion,innovation,technologies,competitive suppliers,economic freedom\",\n    subj_1 = \"middle-income trap\",\n    subj_2 = \"investment\",\n    subj_3 = \"infusion\",\n    subj_4 = \"innovation\",\n    subj_5 = \"technologies\",\n    subj_6 = \"competitive suppliers\",\n    subj_7 = \"economic freedom\",\n    subj_8 = NA,\n    subj_9 = NA,\n    subj_10 = NA,\n    subj_11 = NA,\n    !!!na_values # Unpack the NA values for subj_12 to subj_46\n  )\n\n— Manually correct WDR 2011 ✍🏻\n\n\nwdr$url_keys[wdr$id == \"4389\"] &lt;- \"https://openknowledge.worldbank.org/handle/10986/4389\"\n\nwdr$altmetric[wdr$id == \"4389\"] &lt;- \"210\"\n\nwdr$abstract[wdr$id == \"4389\"] &lt;- \"The 2011 World development report looks across disciplines and experiences drawn from around the world to offer some ideas and practical recommendations on how to move beyond conflict and fragility and secure development. The key messages are important for all countries-low, middle, and high income-as well as for regional and global institutions: first, institutional legitimacy is the key to stability. When state institutions do not adequately protect citizens, guard against corruption, or provide access to justice; when markets do not provide job opportunities; or when communities have lost social cohesion-the likelihood of violent conflict increases. Second, investing in citizen security, justice, and jobs is essential to reducing violence. But there are major structural gaps in our collective capabilities to support these areas. Third, confronting this challenge effectively means that institutions need to change. International agencies and partners from other countries must adapt procedures so they can respond with agility and speed, a longer-term perspective, and greater staying power. Fourth, need to adopt a layered approach. Some problems can be addressed at the country level, but others need to be addressed at a regional level, such as developing markets that integrate insecure areas and pooling resources for building capacity Fifth, in adopting these approaches, need to be aware that the global landscape is changing. Regional institutions and middle income countries are playing a larger role. This means should pay more attention to south-south and south-north exchanges, and to the recent transition experiences of middle income countries.\"\n\nwdr$all_topic[wdr$id == \"4389\"] &lt;- tolower(\"Justice,Jobs,Political Violence and Civil War,Political Violence and War,Organized Crime,Fragility,Conflict and Violence,Crime,Social Cohesion,Public Sector Management,Social Development,Law and Development, Social Protections and Labor,Conflict and Development,Water Supply and Sanitation,Judicial System Reform, Labor Markets,Armed Conflict,Urban Solid Waste Management\")\n\n# Define the subjects to be added for the specific row\nsubjects &lt;- c(\n  \"Armed Conflict\",\n  \"Civil Wars\",\n  \"Conflict Prevention\",\n  \"Conflict Resolution\",\n  \"Development Policy\",\n  \"Fragile States\",\n  \"International Development\",\n  \"Peacebuilding\",\n  \"Political Instability\",\n  \"Post-Conflict Reconstruction\",\n  \"Security and Development\"\n) %&gt;% tolower() # Convert subjects to lowercase\n\n# Ensure id is handled as character and enforce lowercase comparison\nwdr &lt;- wdr %&gt;%\n  mutate(across(\n    starts_with(\"subj_\"),\n    ~ ifelse(id == \"4389\",\n      subjects[as.numeric(sub(\"^subj_\", \"\", cur_column()))],\n      NA_character_\n    )\n  )) %&gt;%\n  mutate(all_subj = if_else(id == \"4389\", paste0(subjects, collapse = \",\"), all_subj))\n\n# Check the result for the row with id == \"4389\"\nwdr %&gt;%\n  filter(id == \"4389\") %&gt;%\n  select(starts_with(\"subj_\")) # Display the updated subject columns\n\n\n# check &lt;- wdr[wdr$id == \"4389\",]\n\n— Remove extra space in title column\n\n# Check and remove leading space in the 'title' column\nwdr &lt;- wdr %&gt;%\n  mutate(title = str_trim(title, side = \"left\"))\n\n— Re-save (upon correction) wrd2.rds\n\n\nwdr2 &lt;- wdr\nwrite_rds(x = wdr2, file = here::here(\"data\", \"derived_data\", \"wdr2.rds\"))\n\n[TBL] World Develompent Reports 2000-2024\nBelow are the titles of the World Development Reports from 2000 to 2024.\n\n\n\n\ndate_issued\ntitle\nurl_keys\n\n\n\n2001\nAttacking Poverty\nhttps://openknowledge.worldbank.org/handle/10986/11856?show=full\n\n\n2002\nBuilding Institutions for Markets\nhttps://openknowledge.worldbank.org/handle/10986/5984?show=full\n\n\n2003\nSustainable Development in a Dynamic World--Transforming Institutions, Growth, and Quality of Life\nhttps://openknowledge.worldbank.org/handle/10986/5985?show=full\n\n\n2004\nMaking Services Work for Poor People\nhttps://openknowledge.worldbank.org/handle/10986/5986?show=full\n\n\n2005\nA Better Investment Climate for Everyone\nhttps://openknowledge.worldbank.org/handle/10986/5987?show=full\n\n\n2006\nEquity and Development\nhttps://openknowledge.worldbank.org/handle/10986/5988?show=full\n\n\n2007\nDevelopment and the Next Generation\nhttps://openknowledge.worldbank.org/handle/10986/5989?show=full\n\n\n2008\nAgriculture for Development\nhttps://openknowledge.worldbank.org/handle/10986/5990?show=full\n\n\n2009\nReshaping Economic Geography\nhttps://openknowledge.worldbank.org/handle/10986/5991?show=full\n\n\n2010\nDevelopment and Climate Change\nhttps://openknowledge.worldbank.org/handle/10986/4387?show=full\n\n\n2011\nConflict, Security, and Development\nhttps://openknowledge.worldbank.org/handle/10986/4389\n\n\n2012\nGender Equality and Development\nhttps://openknowledge.worldbank.org/handle/10986/4391?show=full\n\n\n2013\nJobs\nhttps://openknowledge.worldbank.org/handle/10986/11843?show=full\n\n\n2014\nRisk and Opportunity—Managing Risk for Development\nhttps://openknowledge.worldbank.org/handle/10986/16092?show=full\n\n\n2015\nMind, Society, and Behavior\nhttps://openknowledge.worldbank.org/handle/10986/20597?show=full\n\n\n2016\nDigital Dividends\nhttps://openknowledge.worldbank.org/handle/10986/23347?show=full\n\n\n2017\nGovernance and the Law\nhttps://openknowledge.worldbank.org/handle/10986/25880?show=full\n\n\n2018\nLearning to Realize Education's Promise\nhttps://openknowledge.worldbank.org/handle/10986/28340?show=full\n\n\n2019\nThe Changing Nature of Work\nhttps://openknowledge.worldbank.org/handle/10986/30435?show=full\n\n\n2020\nTrading for Development in the Age of Global Value Chains\nhttps://openknowledge.worldbank.org/handle/10986/32437?show=full\n\n\n2021\nData for Better Lives\nhttps://openknowledge.worldbank.org/handle/10986/35218?show=full\n\n\n2022\nFinance for an Equitable Recovery\nhttps://openknowledge.worldbank.org/handle/10986/36883?show=full\n\n\n2023\nMigrants, Refugees, and Societies\nhttps://openknowledge.worldbank.org/handle/10986/39696\n\n\n2024\nThe Middle-Income Trap\nhttps://openknowledge.worldbank.org/handle/10986/41919\n\n\n\n\n\nQualify: peak or trend (by sector)"
  },
  {
    "objectID": "analysis/01b_WB_project_pdo_EDA.html#add-geomvline-to-sector-plots-v-wdr-title-cmpl",
    "href": "analysis/01b_WB_project_pdo_EDA.html#add-geomvline-to-sector-plots-v-wdr-title-cmpl",
    "title": "WB Project PDO text EDA",
    "section": "Add geomvline to sector plots v WDR title [CMPL 🟠]",
    "text": "Add geomvline to sector plots v WDR title [CMPL 🟠]\n\ntabyl(pdo_train2_t$tok_sector_broad)\n# pdo_train2_t$tok_sector_broad      n  WDR\n\n#                  AGR_FOR_FISH    665 WDR 2008  Agriculture for Development\n#                     EDUCATION   1180 WDR 2004 Making Services Work for Poor People\n#                        ENERGY    886 WDR\n#                     FINANCIAL   1843 WDR\n#                  GENDER_EQUAL    213 WDR 2012  Gender Equality and Development\n#                        HEALTH    946 WDR\n#                           ICT    548 WDR\n#                IND TRADE SERV     60 WDR\n#           INSTITUTIONAL SUPP.   2171 WDR\n#                 MINING_OIL_GAS    299 WDR\n#                     TRANSPORT   1371 WDR\n#                         URBAN    553 WDR\n#                       WAT_SAN   1069 WDR\n\n— ✅ AGR_FOR_FISH ( Agriculture, forestry, and fishing)\nThe WDR of 2008 was titled “Agriculture for Development”, link\n\n# --- Get a LIST of unique sectors (facets) and split the data\nPDOsector_list &lt;- base::split(x = sector_broad_pdo, f = sector_broad_pdo$tok_sector_broad)\n# Specific split df\n# PDOsector_list$'AGR_FOR_FISH'\n\n# Specific plot\npdo_agr_WDR_plot &lt;- sector_plots$\"AGR_FOR_FISH\" +\n  geom_vline(xintercept = 2008, linetype = \"solid\", color = \"#9b6723\", alpha = 0.35) +\n  geom_text(aes(x = 2008, y = max(n) * 0.5, label = \"WDR Agric\"),\n    angle = 90, vjust = -0.5, color = \"#9b6723\"\n  )\npdo_agr_WDR_plot\n\n\n\n\n\n\n\n\n# f_save_plot(\"pdo_agr_plot\", pdo_agr_plot)\nf_save_plot_obj(pdo_agr_WDR_plot, \"pdo_agr_WDR_plot\")\n\n— ✅ EDUCATION\nWDR 2007 was titled “Development and the Next Generation” WDR 2018 was titled “Learning to Realize Education’s Promise”\n\n# Specific split df\n# PDOsector_list$EDUCATION\n\n# Specific plot\npdo_edu_WDR_plot &lt;- sector_plots$EDUCATION +\n  geom_vline(xintercept = 2007, linetype = \"solid\", color = \"#9b6723\", alpha = 0.35) +\n  geom_text(aes(x = 2007, y = max(n) * 0.45, label = \"WDR Youth\"),\n    angle = 90, vjust = -0.5, color = \"#9b6723\"\n  ) +\n  geom_vline(xintercept = 2018, linetype = \"solid\", color = \"#9b6723\", alpha = 0.35) +\n  geom_text(aes(x = 2018, y = max(n) * 0.30, label = \"WDR Educ\"),\n    angle = 90, vjust = -0.5, color = \"#9b6723\"\n  )\n\npdo_edu_WDR_plot\n\n\n\n\n\n\n\n\n# f_save_plot(\"pdo_edu_plot\", pdo_edu_plot)\nf_save_plot_obj(pdo_edu_WDR_plot, \"pdo_edu_WDR_plot\")\n\n— ✅ CLIMATE (climate change)\n\nThe WDR of 2010 was titled ” Development and Climate Change”, link\n\n# --- Get a LIST of unique sectors (facets) and split the data\nPDOsector_list &lt;- base::split(x = sector_broad_pdo, f = sector_broad_pdo$tok_sector_broad)\n# GENDER split df\n# PDOsector_list$'CLIMATE'\n\n# Specific plot\npdo_clim_WDR_plot &lt;- sector_plots$\"CLIMATE\" +\n  # geom_vline(xintercept = 2003, linetype = \"solid\", color = \"#9b6723\",alpha = 0.35) +\n  # geom_text(aes(x = 2003, y = max(n) * 0.5, label = \"WDR Sust Dev\"),\n  #           angle = 90, vjust = -0.5, color = \"#9b6723\")+\n  geom_vline(xintercept = 2010, linetype = \"solid\", color = \"#9b6723\", alpha = 0.35) +\n  geom_text(aes(x = 2010, y = max(n) * 0.45, label = \"WDR Climate change\"),\n    angle = 90, vjust = -0.5, color = \"#9b6723\"\n  )\n\npdo_clim_WDR_plot\n\n\n\n\n\n\n\n\n# f_save_plot(\"pdo_clim_plot\", pdo_clim_plot)\nf_save_plot_obj(pdo_clim_WDR_plot, \"pdo_clim_WDR_plot\")\n\n— ✅ GENDER EQUALITY\nthe WDR of 2012 was titled “Gender Equality and Development”, link\n\n# --- Get a LIST of unique sectors (facets) and split the data\nPDOsector_list &lt;- base::split(x = sector_broad_pdo, f = sector_broad_pdo$tok_sector_broad)\n# GENDER split df\n# PDOsector_list$GENDER_EQUAL\n\n# Specific plot\npdo_gen_WDR_plot &lt;- sector_plots$GENDER_EQUAL +\n  geom_vline(xintercept = 2012, linetype = \"solid\", color = \"#9b6723\", alpha = 0.35) +\n  geom_text(aes(x = 2012, y = max(n) * 0.45, label = \"WDR Gender equal\"),\n    angle = 90, vjust = -0.5, color = \"#9b6723\"\n  )\n\npdo_gen_WDR_plot\n\n\n\n\n\n\n\n\n# f_save_plot(\"pdo_gen_plot\", pdo_gen_plot)\nf_save_plot_obj(pdo_gen_WDR_plot, \"pdo_gen_WDR_plot\")\n\n— SOCIAL PROTECTION\nWDR 2004 ” Making Services Work for Poor People”\n\n# Specific split df\n# PDOsector_list$SOCIAL_PROT\n\n# Specific plot\npdo_soc_WDR_plot &lt;- sector_plots$SOCIAL_PROT +\n  geom_vline(xintercept = 2004, linetype = \"solid\", color = \"#9b6723\", alpha = 0.35) +\n  geom_text(aes(x = 2004, y = max(n) * 0.75, label = \"WDR Services\"),\n    angle = 90, vjust = -0.5, color = \"#9b6723\"\n  )\n\npdo_soc_WDR_plot\n\n\n\n\n\n\n\n\n# f_save_plot(\"pdo_soc_plot\", pdo_soc_plot)\nf_save_plot_obj(pdo_soc_WDR_plot, \"pdo_soc_WDR_plot\")\n\n— INSTITUTIONAL SUPPORT\nWDR 2002 ” Building Institutions for Markets” WDR 2007 ” Governance and the Law”\n\n# Specific split df\n# PDOsector_list$INSTIT_SUP\n\n# Specific plot\npdo_inst_WDR_plot &lt;- sector_plots$INSTIT_SUPP +\n  geom_vline(xintercept = 2002, linetype = \"solid\", color = \"#9b6723\", alpha = 0.35) +\n  geom_text(aes(x = 2002, y = max(n) * 0.75, label = \"WDR Institutions\"),\n    angle = 90, vjust = -0.5, color = \"#9b6723\"\n  ) +\n  geom_vline(xintercept = 2007, linetype = \"solid\", color = \"#9b6723\", alpha = 0.35) +\n  geom_text(aes(x = 2007, y = max(n) * 0.75, label = \"WDR Governance\"),\n    angle = 90, vjust = -0.5, color = \"#9b6723\"\n  )\n\npdo_inst_WDR_plot\n\n\n# f_save_plot(\"pdo_inst_plot\", pdo_inst_plot)\nf_save_plot_obj(pdo_inst_WDR_plot, \"pdo_inst_WDR_plot\")\n\n— ICT\nWDR 2016 ” Digital Dividends” WDR 2021 ” Data for Better Lives”\n\n# Specific split df\n# PDOsector_list$ICT\n\n# Specific plot\npdo_ict_WDR_plot &lt;- sector_plots$ICT +\n  geom_vline(xintercept = 2016, linetype = \"solid\", color = \"#9b6723\", alpha = 0.35) +\n  geom_text(aes(x = 2016, y = max(n) * 0.75, label = \"WDR Digital Div\"),\n    angle = 90, vjust = -0.5, color = \"#9b6723\"\n  ) +\n  geom_vline(xintercept = 2021, linetype = \"solid\", color = \"#9b6723\", alpha = 0.35) +\n  geom_text(aes(x = 2021, y = max(n) * 0.75, label = \"WDR Data\"),\n    angle = 90, vjust = -0.5, color = \"#9b6723\"\n  )\n\npdo_ict_WDR_plot\n\n\n# f_save_plot(\"pdo_ict_plot\", pdo_ict_plot)\nf_save_plot_obj(pdo_ict_WDR_plot, \"pdo_ict_WDR_plot\")\n\n— FINANCIAL\nWDR 2005 ” A Better Investment Climate for Everyone” WDR 2022 ” Finance for an Equitable Recovery”\n\n# Specific split df\n# PDOsector_list$FINANCIAL\n\n# Specific plot\npdo_fin_WDR_plot &lt;- sector_plots$FIN +\n  geom_vline(xintercept = 2005, linetype = \"solid\", color = \"#9b6723\", alpha = 0.35) +\n  geom_text(aes(x = 2005, y = max(n) * 0.80, label = \"WDR Inv Clim\"),\n    angle = 90, vjust = -0.5, color = \"#9b6723\"\n  ) +\n  geom_vline(xintercept = 2022, linetype = \"solid\", color = \"#9b6723\", alpha = 0.35) +\n  geom_text(aes(x = 2022, y = max(n) * 0.75, label = \"WDR Finance\"),\n    angle = 90, vjust = -0.5, color = \"#9b6723\"\n  )\npdo_fin_WDR_plot\n\n\n# f_save_plot(\"pdo_fin_plot\", pdo_fin_plot)\nf_save_plot_obj(pdo_fin_WDR_plot, \"pdo_fin_WDR_plot\")"
  },
  {
    "objectID": "analysis/01b_WB_project_pdo_EDA.html#bigrams",
    "href": "analysis/01b_WB_project_pdo_EDA.html#bigrams",
    "title": "WB Project PDO text EDA",
    "section": "BIGRAMS",
    "text": "BIGRAMS\n\nHere I use [clnp_annotate() output + ] dplyr to combine consecutive tokens into bigrams.\n\n\n# Create bigrams by pairing consecutive tokens by sentence ID and token IDs\nbigrams &lt;- pdo_train2_t %&gt;%\n  # keeping FY with tokens\n  group_by(FY_appr, proj_id, pdo, sid) %&gt;%\n  arrange(tid) %&gt;%\n  # Using mutate() and lead(), we create bigrams from consecutive tokens\n  mutate(\n    next_token = lead(token),\n    bigram = paste(token, next_token)\n  ) %&gt;%\n  # make bigram low case\n  mutate(bigram = tolower(bigram)) %&gt;%\n  # only includes the rows where valid bigrams are formed\n  filter(!is.na(next_token)) %&gt;%\n  ungroup() %&gt;%\n  arrange(FY_appr, proj_id, sid, tid) %&gt;%\n  select(FY_appr, proj_id, pdo, sid, tid, token, bigram)\n\n\n# most frequent bigrams\ncount_bigram &lt;- bigrams %&gt;%\n  count(bigram, sort = TRUE)"
  },
  {
    "objectID": "analysis/01b_WB_project_pdo_EDA.html#clean-bigrams",
    "href": "analysis/01b_WB_project_pdo_EDA.html#clean-bigrams",
    "title": "WB Project PDO text EDA",
    "section": "Clean bigrams",
    "text": "Clean bigrams\nThe challenge is to clean but without separating consecutive words… so I do this split-reunite process to remove stopwords and punctuation. Basically only keeping bigrams made of 2 nouns or ADJ+noun.\n\n# Separate the bigram column into two words\nbigrams_cleaned &lt;- bigrams %&gt;%\n  tidyr::separate(bigram, into = c(\"word1\", \"word2\"), sep = \" \")\n\n# Remove stopwords and bigrams in EACH component word containing punctuation\nbigrams_cleaned &lt;- bigrams_cleaned %&gt;%\n  # custom stop words\n  filter(!word1 %in% custom_stop_words_df$word, !word2 %in% custom_stop_words_df$word) %&gt;%\n  # Remove punctuation\n  filter(!stringr::str_detect(word1, \"[[:punct:]]\"), !stringr::str_detect(word2, \"[[:punct:]]\"))\n\n# Reunite the component cleaned words into the bigram column\nbigrams_cleaned &lt;- bigrams_cleaned %&gt;%\n  unite(bigram, word1, word2, sep = \" \") %&gt;%\n  # Remove too obvious bigrams\n  filter(!bigram %in% c(\n    \"development objective\", \"development objectives\",\n    \"proposed project\", \"project development\", \"program development\"\n  ))\n\n# View the cleaned dataframe\nbigrams_cleaned\n\n# Count the frequency of each bigram\nbigram_freq &lt;- bigrams_cleaned %&gt;%\n  count(bigram, sort = TRUE)\n\n[FIG] most frequent bigrams in PDOs\n\nExcluding bigrams where 1 word is among stopwords or a punctuation sign\nExcluding “development objective/s”, “proposed project”, “program development” because not very informative\n\n\n# ---- Prepare data for plotting\n# Evaluate the title with glue first\ntitle_text &lt;- glue::glue(\"Frequency of bigrams in PDOs over FY {min(pdo_train2_t$FY_appr)}-{max(pdo_train2_t$FY_appr)}\")\n\n# Define the bigrams you want to highlight\nbigrams_to_highlight &lt;- c(\n  \"public sector\", \"private sector\", \"eligible crisis\",\n  \"health care\", \"health services\", \"public health\"\n)\n\n\n# ---- Plot the most frequent bigrams\npdo_bigr_freq &lt;- bigram_freq %&gt;%\n  slice_max(n, n = 25) %&gt;%\n  ggplot(aes(\n    x = reorder(bigram, n), y = n,\n    fill = ifelse(bigram %in% bigrams_to_highlight, bigram, \"Other\")\n  )) +\n  geom_col() +\n  # coord flipped so n is Y axis\n  scale_y_continuous(breaks = seq(min(bigram_freq$n) - 1, max(bigram_freq$n), by = 50)) +\n  scale_fill_manual(values = c(\n    \"public sector\" = \"#005ca1\",\n    \"private sector\" = \"#9b2339\",\n    \"eligible crisis\" = \"#8e550a\",\n    \"health care\" = \"#4C9F38\",\n    \"health services\" = \"#4C9F38\",\n    \"public health\" = \"#4C9F38\",\n    \"Other\" = \"grey\"\n  )) +\n  guides(fill = \"none\") +\n  coord_flip() +\n  labs(\n    title = title_text, subtitle = \"(top 25 bigrams)\",\n    x = \"\", y = \"\"\n  ) +\n  theme(axis.text.y = element_text(\n    # obtain vector of colors 2 match x axis labels color to fill\n    color = bigram_freq %&gt;%\n      slice_max(n, n = 25) %&gt;%\n      # mutate(color = ifelse(bigram %in% bigrams_to_highlight,\n      #                       ifelse(bigram == \"public sector\", \"#005ca1\",\n      #                              ifelse(bigram == \"private sector\", \"#9b2339\", \"#8e550a\")),\n      #                       \"#4c4c4c\"))\n      mutate(color = dplyr::case_when(\n        bigram == \"public sector\" ~ \"#005ca1\",\n        bigram == \"private sector\" ~ \"#9b2339\",\n        bigram == \"eligible crisis\" ~ \"#8e550a\",\n        bigram %in% c(\"health care\", \"health services\", \"public health\") ~ \"#4C9F38\",\n        TRUE ~ \"#4c4c4c\"\n      )) %&gt;%\n      # Ensure the order matches the reordered bigrams (AS BINS)\n      arrange(reorder(bigram, n)) %&gt;%\n      # Extract the color column in bin order as vector to be passed to element_text()\n      pull(color)\n  )) +\n  lulas_theme\n\npdo_bigr_freq\n\n\n\n\n\n\n\nResults are not surprising in terms of frequent bigram recurrence:\n\nSee for example “increase access”, “service delivery” ,“institutional capacity”, “poverty reduction” etc, at the top.\nAlthough, while “health” recurred in several bigrams (e.g. “health services”, “public health”, “health care”) among the top 25, “education” did not appear at all.\nA bit mysterious is perhaps “eligible crisis” (&gt; 100 mentions)?! (coming back to this later)\n[FIG] Changes over time BY 1FY\nBesides huge, counter intuitive, difference between “health” and “education”, “climate change” appears in the top 25 (ranking above “financial sector” and “capacity building”) which begs the question: Has the frequency of these bigrams has changed over time?\n\n#\n# ## too busy to be useful\n#\n# # Step 1: Count the frequency of each bigram by year\n# top_bigrams_1FY &lt;- bigrams_cleaned %&gt;%\n#    group_by(FY_appr, bigram) %&gt;%\n#    summarise(count = n(), .groups = 'drop') %&gt;%\n#    arrange(FY_appr, desc(count)) %&gt;%\n#    # ---  +/- top 10\n#    group_by(FY_appr) %&gt;%\n#    top_n(10, count) %&gt;%\n#    ungroup()\n#    # # ---  STRICT  top 10\n#    # mutate(rank = dense_rank(desc(count))) %&gt;%  # Rank bigrams by frequency\n#    # filter(rank &lt;= 10) %&gt;%  # Keep only the top 10 by rank\n#    # ungroup()\n#\n#\n# # Add specific bigrams to highlight, if any\n# bigrams_to_highlight &lt;- c(\"climate change\",  \"climate resilience\", \"public sector\", \"private sector\")\n#\n# # Step 2: Plot the top bigrams by frequency over time\n# pdo_bigr_FY_freq  &lt;-  top_bigrams_1FY %&gt;%\n#  ggplot(aes(x = reorder(bigram, count),\n#              y = count,\n#              fill = ifelse(bigram %in% bigrams_to_highlight, bigram, \"Other\"))) +\n#   geom_col() +\n#   scale_fill_manual(values = c(\"public sector\" = \"#005ca1\", \"private sector\" = \"#e60066\",\n#                                \"climate change\" = \"#399B23\", \"climate resilience\" = \"#d8e600\",\n#                                \"Other\" = \"grey\")) +\n#   guides(fill = \"none\") +\n#   coord_flip() +\n#   facet_wrap(~ FY_appr, scales = \"free_y\") +\n#   labs(title = \"Top 10 Bigrams by Frequency Over Time\",\n#        subtitle = \"(Faceted by Fiscal Year Approval)\",\n#        x = \"Bigrams\",\n#        y = \"Count\") +\n#   theme_minimal() +\n#   theme(plot.title.position = \"plot\",\n#         axis.text.x = element_text(angle = 45, hjust = 1))+\n#      lulas_theme\n#\n# pdo_bigr_FY_freq\n\n[FIG] Changes over time BY 3FY\nTo reduce the noise and make the plot more readable, we can group the data by 3 fiscal years (FY) intervals.\n\n# generate FY group\nf_generate_year_groups &lt;- function(years, interval) {\n  breaks &lt;- seq(floor(min(years, na.rm = TRUE) / interval) * interval,\n    ceiling(max(years, na.rm = TRUE) / interval) * interval,\n    by = interval\n  )\n\n  labels &lt;- paste(breaks[-length(breaks)], \"-\", breaks[-1] - 1)\n\n  return(list(breaks = breaks, labels = labels))\n}\n\n\n# --- Step 1: Create n-year groups (using `f_generate_year_groups`)\ninterval_i &lt;- 3 # decide the interval\nyear_groups &lt;- f_generate_year_groups(bigrams_cleaned$FY_appr, interval = interval_i)\ntop_n_i &lt;- 12 # decide the top n bigrams to show\n\n# --- Step 2: Add the generated FY breaks and labels to data frame\ntop_bigrams_FYper &lt;- bigrams_cleaned %&gt;%\n  # cut divides the range of x into intervals\n  mutate(FY_group = base::cut(FY_appr,\n    breaks = year_groups$breaks,\n    include.lowest = TRUE,\n    right = FALSE,\n    labels = year_groups$labels\n  )) %&gt;%\n  # Count the frequency of each bigram by n-year groups\n  group_by(FY_group, bigram) %&gt;%\n  summarise(count = n(), .groups = \"drop\") %&gt;%\n  arrange(FY_group, desc(count)) %&gt;%\n  # Top ? bigrams for each n-year period\n  group_by(FY_group) %&gt;%\n  top_n(top_n_i, count) %&gt;%\n  ungroup()\n\n# --- Step 3: Add specific bigrams to highlight, if any\nbigrams_to_highlight &lt;- c(\n  \"climate change\", \"climate resilience\",\n  \"eligible crisis\",\n  \"public sector\", \"private sector\",\n  \"water supply\", \"sanitation services\",\n  \"health care\", \"health services\", \"public health\", \"health preparedness\"\n)\n\n# --- Step 4: Plot the top bigrams by frequency over n-year periods\npdo_bigr_FY_freq &lt;- top_bigrams_FYper %&gt;%\n  ggplot(aes(\n    x = reorder(bigram, count),\n    y = count,\n    fill = ifelse(bigram %in% bigrams_to_highlight, bigram, \"Other\")\n  )) +\n  geom_col() +\n  scale_fill_manual(values = c(\n    # \"public sector\" = \"#005ca1\",\n    # \"private sector\" = \"#e60066\",\n    \"water supply\" = \"#26BDE2\",\n    \"sanitation services\" = \"#26BDE2\",\n    \"climate change\" = \"#3F7E44\",\n    \"climate resilience\" = \"#a6bd23\",\n    \"eligible crisis\" = \"#e68000\",\n    \"health care\" = \"#E5243B\",\n    \"health services\" = \"#E5243B\",\n    \"public health\" = \"#E5243B\",\n    \"Other\" = \"grey\"\n  )) +\n  guides(fill = \"none\") +\n  coord_flip() +\n  facet_wrap(~FY_group, ncol = 3, scales = \"free_y\") +\n  # strip.position = \"top\") +  # Facet wrap with columns\n  labs(\n    title = glue::glue(\"Top 12 Bigrams by Frequency Over {interval_i}-Year Periods\"),\n    subtitle = \"(Some sectors highlighted)\",\n    x = \"\",\n    y = \"\"\n  ) +\n  lulas_theme\n\n\n# print the plot\npdo_bigr_FY_freq\n\n\n\n\n\n\n\n\nFrequency observed over FY intervals is very revealing.\n\n\nInteresting to see the trend of “water supply” and “sanitation services” bigrams, which are quite stable over time.\nThe bigram “health care” and “health services” are also quite stable, while “public health” obviously gained relevance since the 2019-2021 FY period.\nConversely, “private sector” and “public sector” loose importance over time (around mid 2010s), while “climate change” and “climate resilience” gain relevance from the same point on.\nStill quite surprising the bigram “eligible crisis”, which actually appears in the top 12 bigrams starting in FY 2016-2018!\n🤔 Which are the most frequent and persistent Bigrams Over Time?\n\nFor this, I am looking for a ranking that considers Mean frequency across periods arrange(desc(mean_count)) + Stability (low standard deviation) across periods [this is hard bc of NAs], and NOT total count overall…\n\n\nUsing top_bigrams_FYper which had breaks of 3FY\n\n\n# ------------------------------[REPEATED just to see the table]\n\n# --- Step 1: Create n-year groups (using `f_generate_year_groups`)\ninterval_i &lt;- 3 # decide the interval\nyear_groups &lt;- f_generate_year_groups(bigrams_cleaned$FY_appr, interval = interval_i)\ntop_n_i &lt;- 12 # decide the top n bigrams to show\n\n# --- Step 2: Add the generated FY breaks and labels to data frame\ntop_bigrams_FYper &lt;- bigrams_cleaned %&gt;%\n  # cut divides the range of x into intervals\n  mutate(FY_group = base::cut(FY_appr,\n    breaks = year_groups$breaks,\n    include.lowest = TRUE,\n    right = FALSE,\n    labels = year_groups$labels\n  )) %&gt;%\n  # Count the frequency of each bigram by n-year groups\n  group_by(FY_group, bigram) %&gt;%\n  summarise(count = n(), .groups = \"drop\") %&gt;%\n  arrange(FY_group, desc(count)) %&gt;%\n  # Top ? bigrams for each n-year period\n  group_by(FY_group) %&gt;%\n  top_n(top_n_i, count) %&gt;%\n  ungroup()\n\n\nsd() returns NA for bigrams that are not present in any periods (or are present in just 1 period).\n\n\n# Calculate the mean frequency and standard deviation of the counts for each bigram across periods\nstable_and_frequent_bigrams_per &lt;- top_bigrams_FYper %&gt;%\n  group_by(bigram) %&gt;%\n  summarise(\n    mean_count = mean(count, na.rm = TRUE), # Mean frequency across periods\n    sd_count = sd(count, na.rm = TRUE), # Stability (lower sd = more stable)\n    count_non_na = sum(!is.na(count)), # Count non-NA values\n    sd_count2 = if_else(count_non_na &gt;= 1, sd(count, na.rm = TRUE), NA_real_), # Only calculate sd if &gt;= 3 non-NA\n    total_count = sum(count)\n  ) %&gt;% # Total count across all periods (optional)\n  arrange(desc(mean_count)) %&gt;% # Sort by frequency and then stability\n  # Filter out bigrams with low mean frequency or high instability (you can adjust thresholds)\n  # Focus on the top 25% most frequent bigrams\n  filter(mean_count &gt; quantile(mean_count, 0.70, na.rm = TRUE)) # %&gt;%\n# Focus on the most stable 50% (lower sd) ---&gt; NO bc NA values\n# filter( sd_count &lt; quantile(sd_count, 0.5, na.rm = TRUE))\n\n[TBL] Bigrams Over Time [3FY]\n\n# View the most frequent and stable bigrams\nstable_and_frequent_bigrams_per %&gt;%\n  slice_head(n = 15) %&gt;%\n  kableExtra::kable()\n\n\n\n\n\n\n\n\n\n\n\nbigram\nmean_count\nsd_count\ncount_non_na\nsd_count2\ntotal_count\n\n\n\neligible crisis\n36.00000\n7.211103\n3\n7.211103\n108\n\n\nincrease access\n33.00000\n7.852813\n7\n7.852813\n231\n\n\nprivate sector\n29.40000\n9.528903\n5\n9.528903\n147\n\n\npoverty reduction\n29.25000\n10.045729\n4\n10.045729\n117\n\n\nservice delivery\n27.16667\n6.493587\n6\n6.493587\n163\n\n\nimprove access\n26.66667\n8.547904\n6\n8.547904\n160\n\n\npublic sector\n26.25000\n9.708244\n4\n9.708244\n105\n\n\nclimate change\n26.00000\n11.313709\n2\n11.313709\n52\n\n\nmobile applications\n25.00000\nNA\n1\nNA\n25\n\n\nfinancial management\n23.00000\n2.000000\n3\n2.000000\n69\n\n\ninstitutional capacity\n22.87500\n8.542959\n8\n8.542959\n183\n\n\neffective response\n22.50000\n3.535534\n2\n3.535534\n45\n\n\nprovide immediate\n22.50000\n6.363961\n2\n6.363961\n45\n\n\n\n\n\n\nUsing top_bigrams_1FY which had breaks of 1FY\n\n\n# --- Step 1: Create n-year groups (using `f_generate_year_groups`)\ninterval_i &lt;- 1 # decide the interval\nyear_groups &lt;- f_generate_year_groups(bigrams_cleaned$FY_appr, interval = interval_i)\ntop_n_i &lt;- 12 # decide the top n bigrams to show\n\n# --- Step 2: Add the generated FY breaks and labels to data frame\ntop_bigrams_1FY &lt;- bigrams_cleaned %&gt;%\n  # cut divides the range of x into intervals\n  mutate(FY_group = base::cut(FY_appr,\n    breaks = year_groups$breaks,\n    include.lowest = TRUE,\n    right = FALSE,\n    labels = year_groups$labels\n  )) %&gt;%\n  # Count the frequency of each bigram by n-year groups\n  group_by(FY_group, bigram) %&gt;%\n  summarise(count = n(), .groups = \"drop\") %&gt;%\n  arrange(FY_group, desc(count)) %&gt;%\n  # Top ? bigrams for each n-year period\n  group_by(FY_group) %&gt;%\n  top_n(top_n_i, count) %&gt;%\n  ungroup()\n\n\n# Calculate the mean frequency and standard deviation of the counts for each bigram across periods\nstable_and_frequent_bigrams_1FY &lt;- top_bigrams_1FY %&gt;%\n  group_by(bigram) %&gt;%\n  summarise(\n    mean_count = mean(count, na.rm = TRUE), # Mean frequency across periods\n    sd_count = sd(count, na.rm = TRUE), # Stability (lower sd = more stable)\n    total_count = sum(count)\n  ) %&gt;% # Total count across all periods (optional)\n  arrange(desc(mean_count)) %&gt;% # Sort by frequency and then stability\n  # Filter out bigrams with low mean frequency or high instability (you can adjust thresholds)\n  # Focus on the top 25% most frequent bigrams\n  filter(mean_count &gt; quantile(mean_count, 0.70, na.rm = TRUE)) # %&gt;%\n# Focus on the most stable 50% (lower sd) ---&gt; NO bc NA values\n# filter( sd_count &lt; quantile(sd_count, 0.5, na.rm = TRUE))\n\n[TBL] Bigrams Over Time [1FY]\n\n# View the most frequent and stable bigrams\nstable_and_frequent_bigrams_1FY %&gt;%\n  slice_head(n = 15) %&gt;%\n  kableExtra::kable()\n\n\n\nbigram\nmean_count\nsd_count\ntotal_count\n\n\n\nmobile applications\n24.00000\nNA\n24\n\n\nrespond promptly\n14.50000\n9.192388\n29\n\n\neligible crisis\n14.37500\n6.162965\n115\n\n\nprivate sector\n13.10000\n4.433459\n131\n\n\nincrease access\n13.05882\n5.005879\n222\n\n\npoverty reduction\n12.50000\n2.449490\n100\n\n\npublic health\n12.50000\n5.446712\n50\n\n\ncongo basin\n12.00000\nNA\n12\n\n\ninvestment climate\n12.00000\nNA\n12\n\n\nthreat posed\n12.00000\n3.464102\n36\n\n\nhealth preparedness\n11.66667\n4.163332\n35\n\n\nrenewable energy\n11.25000\n3.500000\n45\n\n\nnational systems\n11.00000\nNA\n11\n\n\nroad safety\n11.00000\nNA\n11\n\n\nsustainable development\n11.00000\nNA\n11"
  },
  {
    "objectID": "analysis/01b_WB_project_pdo_EDA.html#explore-specific-bigrams",
    "href": "analysis/01b_WB_project_pdo_EDA.html#explore-specific-bigrams",
    "title": "WB Project PDO text EDA",
    "section": "Explore specific bigrams",
    "text": "Explore specific bigrams\n— Public/Private ~ compare frequency over FY\nA case in which looking at bigrams may be better than tokens is the question whether WB project are more focused on public or private sector. It is not easy to capture this information from the text, because:\n\n“government” may be referred to the subject/counterpart of the project (e.g. “government of Mozambique”)\n“private” is not necessarily referred to the “private sector” (e.g. “private households”)\n“public” is not necessarily referred to the “public sector” (e.g. “public health”)\n\nSo, I narrow down to consecutive bigrams “public sector” and “private sector” to get an indicative frequency of these terms.\n[FIG] Bigrams (“public sector”, “private sector”) freq plot\n\n# Filter for the specific bigrams \"public sector\" and \"private sector\"\nbigrams_pub_priv_sec &lt;- bigrams %&gt;%\n  filter(bigram %in% c(\"public sector\", \"private sector\"))\n\n# Display the result\n# bigrams_pub_priv_sec\n\n# prepare data for plotting (count)\nsector_bigr_df &lt;- bigrams_pub_priv_sec %&gt;%\n  count(FY_appr, bigram) %&gt;%\n  # reorder values by frequency\n  mutate(bigram = factor(bigram, levels = c(\"public sector\", \"private sector\")))\n\n\n# ---- Prepare data for plotting\n# Evaluate the title with glue first\ntitle_text &lt;- glue::glue(\"Frequency of bigrams \\\"public sector\\\" and \\\"private sector\\\" in PDOs over FY {min(sector_bigr_df$FY_appr)}-{max(sector_bigr_df$FY_appr)}\")\n\ntwo_col_contrast &lt;- c(\"#005ca1\", \"#e60066\")\n\n# Create a named vector for the legend labels with totals in a single pipeline\nlegend_labels &lt;- sector_bigr_df %&gt;%\n  group_by(bigram) %&gt;%\n  # Calculate total counts for each bigram\n  summarize(total_n = sum(n)) %&gt;%\n  # Append totals to bigram names\n  mutate(label = paste0(bigram, \" (\", total_n, \")\")) %&gt;%\n  # Create a named vector with bigram as names and labels as values\n  {\n    setNames(.$label, .$bigram)\n  } # curly braces {} in a dplyr pipeline using . as ouptu from previous..\n\n# ---- Plot\npdo_pub_pri_bigr &lt;- ggplot(data = sector_bigr_df, aes(x = FY_appr, y = n, group = bigram, color = bigram)) +\n  geom_line(linetype = \"solid\", alpha = 0.75, size = .5) +\n  geom_point(size = 3) +\n  scale_x_continuous(breaks = seq(2001, 2023, by = 1)) +\n  scale_color_manual(\n    values = two_col_contrast,\n    labels = legend_labels\n  ) + # Use modified labels\n  lulas_theme +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +\n  labs(\n    title = title_text,\n    x = \"\",\n    y = \"\",\n    color = \"\"\n  )\n\n\npdo_pub_pri_bigr\n\n\n\n\n\n\n\n\n# Save the plot\n# f_save_plot(\"pdo_pub_pri_bigr\", pdo_pub_pri_bigr)\nf_save_plot_obj(pdo_pub_pri_bigr, \"pdo_pub_pri_bigr\")\n\n\nNote:\n\n\nthese are much less common than the single words.\nWhat happens in FY 2014-2016 that makes these bigram drop in frequency of mention?"
  },
  {
    "objectID": "analysis/01b_WB_project_pdo_EDA.html#clean-trigrams",
    "href": "analysis/01b_WB_project_pdo_EDA.html#clean-trigrams",
    "title": "WB Project PDO text EDA",
    "section": "Clean trigrams",
    "text": "Clean trigrams\nThe challenge is to clean but without separating consecutive words… so I do this split-reunite process to remove stopwords and punctuation. Basically only keeping bigrams made of 2 nouns or ADJ+noun.\n\n# Split the trigrams into three tokens\ntrigrams_split &lt;- trigrams %&gt;%\n  separate(trigram, c(\"token1\", \"token2\", \"token3\"), sep = \" \")\n\n# Remove stopwords and punctuation\ntrigrams_clean &lt;- trigrams_split %&gt;%\n  filter(\n    !token1 %in% custom_stop_words,\n    !token2 %in% custom_stop_words,\n    !token3 %in% custom_stop_words\n  ) %&gt;%\n  filter(\n    token1 != \"na\",\n    token2 != \"na\",\n    token3 != \"na\"\n  ) %&gt;%\n  # Remove punctuation\n  filter(\n    !stringr::str_detect(token1, \"[[:punct:]]\"),\n    !stringr::str_detect(token2, \"[[:punct:]]\"),\n    !stringr::str_detect(token3, \"[[:punct:]]\")\n  ) %&gt;%\n  unite(trigram, token1, token2, token3, sep = \" \") %&gt;%\n  select(FY_appr, proj_id, pdo, sid, tid, trigram)\n\n\n# Count the frequency of each trigram\ntrigram_freq &lt;- trigrams_clean %&gt;%\n  count(trigram, sort = TRUE)\n\n[FIG] Most frequent trigrams in PDOs\n\nExcluding bigrams where 1 word is among stopwords or a punctuation sign\nExcluding “development objective/s”, “proposed project”, “program development” because not very informative\n\n\n# Evaluate the title with glue first\ntitle_text &lt;- glue::glue(\"Most frequent trigrams in PDOs over FY {min(trigrams_clean$FY_appr)}-{max(trigrams_clean$FY_appr)}\")\n\n# Define colors for specific highlights\nhighlight_colors &lt;- c(\"Health\" = \"#d02e4c\", \"Environment\" = \"#3F7E44\", \"Other\" = \"grey\")\n\n\n# Plot the most frequent trigrams\npdo_trigram_freq_plot &lt;- trigram_freq %&gt;%\n  dplyr::filter(!trigram %in% c(\n    \"project development objective\",\n    \"project development objectives\",\n    \"overall development objective\",\n    \"program development objective\",\n    \"program development objectives\",\n    \"proposed project development\",\n    \"proposed development objectives\",\n    \"proposed development objective\",\n    \"revised project development\"\n  )) %&gt;%\n  top_n(25) %&gt;%\n  # plot the top 25 trigrams\n  ggplot(aes(\n    x = reorder(trigram, n), y = n,\n    fill = dplyr::case_when(\n      stringr::str_detect(trigram, \"health\") ~ \"Health\",\n      stringr::str_detect(trigram, \"environment\") ~ \"Environment\",\n      stringr::str_detect(trigram, \"climate\") ~ \"Environment\",\n      stringr::str_detect(trigram, \"greenhouse\") ~ \"Environment\",\n      # stringr::str_detect(trigram, \"sustain\") ~ \"Environment\",\n      TRUE ~ \"Other\"\n    )\n  )) +\n  geom_col() +\n  # coord flipped so n is Y axis\n  scale_y_continuous(breaks = seq(min(trigram_freq$n) - 1, max(trigram_freq$n), by = 50)) +\n  coord_flip() +\n  labs(\n    title = title_text, subtitle = \"(top 25 trigrams)\",\n    x = \"\", y = \"\"\n  ) +\n  scale_fill_manual(values = highlight_colors) +\n  guides(fill = \"none\") +\n  lulas_theme\n\npdo_trigram_freq_plot"
  },
  {
    "objectID": "analysis/01b_WB_project_pdo_EDA.html#concordances-with-specific-bigrams",
    "href": "analysis/01b_WB_project_pdo_EDA.html#concordances-with-specific-bigrams",
    "title": "WB Project PDO text EDA",
    "section": "Concordances with specific bigrams",
    "text": "Concordances with specific bigrams\nConcordancing is central to analyses of text and they often represents the first step in more sophisticated analyses of language data, because concordances are extremely valuable for understanding how a word or phrase is used, how often it is used, and in which contexts is used.\nA concordance list is a list of all contexts in which a particular token appears in a corpus or text. Here I use it in association with the bigram “eligible crisis” to see in which context it appears in the PDOs.\n\nHere I did it at the level of sentence, i.e. without tokenizing the text into words.\n\n— eligible crisis ~ notable bigrams over FY\n\n# reduce back to the original data\npdo_t &lt;- pdo_train2_t %&gt;%\n  select(\n    proj_id, pdo, pr_name, FY_appr, FY_clos, status, regionname, countryname,\n    sector1, theme1, lendinginstr, env_cat, ESrisk, curr_total_commitment\n  ) %&gt;%\n  group_by(proj_id) %&gt;%\n  slice(1)\n\nFirst of all, let’s see what are the sentence that contain the bigram “eligible crisis” in the PDOs.\n\n# Tokenize the text data into sentences\nsentences &lt;- pdo_t %&gt;%\n  unnest_tokens(sentence, pdo, token = \"sentences\", drop = FALSE)\n\n# Count the number of sentences in each document\nsentence_count &lt;- sentences %&gt;%\n  group_by(proj_id) %&gt;%\n  summarise(num_sentences = n())\n\nn_distinct(sentence_count$proj_id) # number of projects\nsum(sentence_count$num_sentences) # total number of sentences\n\n\n# ---- Define the bigram you want to find\ntarget_bigram &lt;- \"eligible crisis\"\n\n\n# Filter sentences that contain the specific bigram\nsentences_with_targ &lt;- sentences %&gt;%\n  filter(stringr::str_detect(sentence, target_bigram))\n\n# Define how many characters before and after the bigram to extract\nchars_before &lt;- 60 # Number of characters before the bigram\nchars_after &lt;- 60 # Number of characters after the bigram\n\n# Add the extracted bigram and surrounding characters to the same dataframe\nsentences_with_eligcris &lt;- sentences_with_targ %&gt;%\n  mutate(closest_text = str_extract(sentence, paste0(\".{0,\", chars_before, \"}\", target_bigram, \".{0,\", chars_after, \"}\"))) %&gt;%\n  # View the updated dataframe with the closest_text column\n  select(\n    proj_id, # sentence,\n    closest_text\n  )\n\n\n# Define how many words before and after the bigram to extract\nwords_before &lt;- 8 # Number of words before the bigram\nwords_after &lt;- 8 # Number of words after the bigram\n\n# Add the extracted bigram and surrounding words to the same dataframe\nsentences_with_eligcris2 &lt;- sentences_with_targ %&gt;%\n  mutate(closest_text = str_extract(\n    sentence,\n    paste0(\n      \"(\", # Start a capture group\n      # Match preceding words\n      \"(?:\\\\S+\\\\s+){0,\", words_before, \"}\",\n      target_bigram,\n      # Match following words\n      \"(?:\\\\s+\\\\S+){0,\", words_after, \"}\",\n      \")\"\n    )\n  )) %&gt;%\n  # View the updated dataframe with the closest_text column\n  select(\n    proj_id, sentence,\n    closest_text\n  )\n\nn_distinct(sentences_with_eligcris2$proj_id)\n\n\nThere are 112 projects, for which the PDO has a sentences containing the bigram “eligible crisis” in the PDOs.\n\n[TBL] Close phrase around bigram “eligible crisis”\nIt appears “eligible crisis or emergency” is a commonly used phrase in the PDOs, often accompanied by similar phrasing: “to respond promptly and effectively”. as well as “provide immediate and effective response to”. Presumably, a standard sentence that refers to a situation that qualifies for specific types of assistance or intervention under certain policies.\n\n# Define the phrase you want to search for in the vicinity of the target bigram\nphrase_to_search &lt;- \"respond promptly and effectively\"\n\n# Count how often the phrase appears in the vicinity of the target bigram\nphrase_count &lt;- sentences_with_eligcris2 %&gt;%\n  mutate(contains_phrase = stringr::str_detect(closest_text, phrase_to_search)) %&gt;% # Check if the phrase is present\n  summarise(count = sum(contains_phrase)) # Count how many times the phrase is found\n\n# View the result\ntabyl(phrase_count$count)\n\n\n32% of the (112) times, the bigram “eligible crisis” in the PDOs, it is accompanied by the phrase “respond promptly and effectively”.\n\nHere are a few examples of the sentences containing the bigram “eligible crisis” and the phrase “respond promptly and effectively” OR immediate and effective response:\n\nset.seed(555)\n# Filter the sentences that contain the phrase\nsample_with_eligcris2 &lt;- sentences_with_eligcris2 %&gt;%\n  ungroup() %&gt;%\n  # take a random sample of 5 sentences\n  sample_n(10) %&gt;%\n  select(proj_id, closest_text) %&gt;%\n  mutate(\n    closest_text = paste0(\"(...) \", closest_text),\n    # Make \"eligible crisis\" bold by adding &lt;b&gt; tags\n    closest_text = gsub(\"eligible crisis\", \"&lt;b&gt;eligible crisis&lt;/b&gt;\", closest_text),\n    # Highlight by adding &lt;mark&gt; tags\n    closest_text = gsub(\"(?i)(promptly and effectively|immediate and effective response)\", # (?i) makes the match case-insensitive.\n      \"&lt;mark style='background-color: #d8e600;'&gt;\\\\1&lt;/mark&gt;\", closest_text,\n      perl = TRUE\n    )\n  )\n\n# Print out sample in a kable\nelcr_k &lt;- kable(sample_with_eligcris2,\n  format = \"html\",\n  # Display the table with bold formatting\n  escape = FALSE,\n  col.names = c(\"WB Project ID\", \"Excerpt of PDO Sentences with 'Eligible Crisis'\")\n) %&gt;%\n  kable_styling(full_width = FALSE)\n\nelcr_k\n\n\n\nWB Project ID\nExcerpt of PDO Sentences with 'Eligible Crisis'\n\n\n\nP179636\n(...) and (iii) respond effectively in case of an eligible crisis or emergency.\n\n\nP176982\n(...) borrower’s territory; and (iii) in case of an eligible crisis or emergency, respond promptly and effectively to it.\n\n\nP147827\n(...) of associated institutions, and in case of an eligible crisis or emergency, respond promptly andeffectively to it.\n\n\nP177816\n(...) in project areas, and, in case of an eligible crisis or emergency, to respond promptly and effectively to\n\n\nP125961\n(...) an earlyemergency response in the event of an eligible crisis or emergency.\n\n\nP156012\n(...) health services, and, in the event of an eligible crisis or emergency, to provide immediate and effective response\n\n\n\nP171093\n(...) communities and to provide immediate response to an eligible crisis or emergency as needed.\n\n\nP158231\n(...) communities and to provide immediate response to an eligible crisis or emergency as needed.\n\n\nP147280\n(...) and to respond effectively in case of an eligible crisis or emergency\n\n\nP167512\n(...) provide an immediate and effective response to an eligible crisis or emergency.\n\n\n\n\n# Save the table as an HTML file\nwrite_rds(elcr_k, here(\"analysis\", \"output\", \"tables\", \"elcr_k.rds\"))\n\n— climate change ~ notable bigrams over FY [CMPL 🟠]\nFirst of all, let’s see what are the sentence that contain the bigram “eligible crisis” in the PDOs.\n\n# ---- Define the bigram you want to find\ntarget_bigram &lt;- \"climate change\"\n\n# # Filter sentences that contain the specific bigram\n# sentences_with_targ &lt;- sentences %&gt;%\n#    filter(stringr::str_detect(sentence, target_bigram))\n#\n# # Define how many words before and after the bigram to extract\n# words_before &lt;- 8  # Number of words before the bigram\n# words_after &lt;- 8   # Number of words after the bigram\n\n# Add the extracted bigram and surrounding words to the same dataframe\nsentences_with_climchang &lt;- sentences %&gt;%\n  filter(stringr::str_detect(sentence, target_bigram)) %&gt;%\n  mutate(closest_text = str_extract(\n    sentence,\n    paste0(\n      \"(\", # Start a capture group\n      # Match preceding words\n      \"(?:\\\\S+\\\\s+){0,\", words_before, \"}\",\n      target_bigram,\n      # Match following words\n      \"(?:\\\\s+\\\\S+){0,\", words_after, \"}\",\n      \")\"\n    )\n  )) %&gt;%\n  # View the updated dataframe with the closest_text column\n  select(\n    proj_id, pdo, sentence,\n    closest_text\n  )\n\n\nThere are 92 projects, for which the PDO has a sentences containing the bigram “climate change”in the PDOs.\n\n[TBL] Close phrase around bigram “climate change”\nI want to know which of these commonly used phrases are most often found in the vicinity of the bigram “climate change” in the PDOs.\n\n# Count how often the phrase appears in the vicinity of the target bigram\nclose_words &lt;- sentences_with_climchang %&gt;%\n  mutate(contains_what = dplyr::case_when(\n    stringr::str_detect(sentence, \"mitigat\") ~ \"mitigate\",\n    stringr::str_detect(sentence, \"adapt\") ~ \"adapt\",\n    stringr::str_detect(sentence, \"vulnerab\") ~ \"vulnerability\",\n    stringr::str_detect(sentence, \"hazard\") ~ \"hazard\",\n    stringr::str_detect(sentence, \"resil\") ~ \"resilience\",\n    TRUE ~ \"...\"\n  ))\n\n# Count how often the phrase is found\nclose_words_sort &lt;- close_words %&gt;%\n  filter(contains_what != \"...\") %&gt;%\n  group_by(contains_what) %&gt;%\n  summarise(count = n()) %&gt;%\n  mutate(percentage = scales::percent(count / sum(count))) %&gt;%\n  arrange(desc(count))\n\n# Specify the words to highlight\nhighlight_words &lt;- c(\"mitigate\")\nhighlight_words2 &lt;- c(\"resilience\", \"adapt\")\n\n\nclch_close_k &lt;- close_words_sort %&gt;%\n  kable(\n    format = \"html\",\n    col.names = c(\"Near 'climate change'\", \"Count\", \"Percentage\")\n  ) %&gt;%\n  kable_styling(full_width = FALSE) %&gt;%\n  # Light yellow background\n  row_spec(which(close_words_sort$contains_what %in% highlight_words),\n    background = \"#d8e600\"\n  ) %&gt;%\n  row_spec(which(close_words_sort$contains_what %in% highlight_words2),\n    background = \"#a6bd23\"\n  )\n\nclch_close_k\n\n\n\nNear 'climate change'\nCount\nPercentage\n\n\n\nvulnerability\n23\n33.8%\n\n\nresilience\n16\n23.5%\n\n\nmitigate\n14\n20.6%\n\n\nadapt\n9\n13.2%\n\n\nhazard\n6\n8.8%\n\n\n\n\n# save as object\nwrite_rds(clch_close_k, here(\"analysis\", \"output\", \"tables\", \"clch_close_k.rds\"))\n\n\n#   &lt;chr&gt;         &lt;int&gt;       &lt;chr&gt;\n# 1 vulnerab         18       32.1%\n# 2 mitigate         12       21.4%\n# 3 resil            12       21.4%\n# 4 hazard            9       16.1%\n# 5 adapt             5       8.9%\n\nHere are a few examples of the sentences containing the bigram “climate change” and the words “mitigate|adaptation”:\n\nset.seed(888)\n# Filter the sentences that contain the phrase\nsentences_with_climchang2_k &lt;- sentences_with_climchang %&gt;%\n  filter(proj_id != \"P125447\") %&gt;%\n  # add a column to identify the phrases\n  mutate(contains_what = case_when(\n    stringr::str_detect(closest_text, \"mitig\") ~ \"mitig\",\n    stringr::str_detect(closest_text, \"adapt\") ~ \"adapt\",\n    stringr::str_detect(closest_text, \"vulnerab\") ~ \"vulnerab\",\n    stringr::str_detect(closest_text, \"hazard\") ~ \"hazard\",\n    stringr::str_detect(closest_text, \"resil\") ~ \"resil\",\n    TRUE ~ \"...\"\n  )) %&gt;%\n  filter(contains_what != \"...\") %&gt;%\n  # take a random sample of 3  by word\n  group_by(contains_what) %&gt;%\n  slice_sample(n = 3, replace = FALSE) %&gt;%\n  select(contains_what, proj_id, closest_text) %&gt;%\n  mutate(\n    closest_text = paste0(\"(...) \", closest_text),\n    # Make \"mutate\" bold by adding &lt;b&gt; tags\n    closest_text = gsub(\"climate change\", \"&lt;b&gt;climate change&lt;/b&gt;\", closest_text),\n    # highlight the phrases by adding &lt;mark&gt; tags (adapt, mitigate, etc.)\n    closest_text = gsub(\"(?i)(adaptation|resilience)\", # (?i) makes the match case-insensitive.\n      \"&lt;mark style='background-color: #a6bd23;'&gt;\\\\1&lt;/mark&gt;\", closest_text,\n      perl = TRUE\n    ),\n    closest_text = gsub(\"(?i)(mitigation|mitigate)\",\n      \"&lt;mark style='background-color: #8e94d6;'&gt;\\\\1&lt;/mark&gt;\", closest_text,\n      perl = TRUE\n    ),\n    closest_text = gsub(\"(?i)(hazard|vulnerability)\",\n      \"&lt;mark style='background-color: #e28293;'&gt;\\\\1&lt;/mark&gt;\", closest_text,\n      perl = TRUE\n    )\n  )\n\n# save as object\nwrite_rds(sentences_with_climchang2_k, here(\"analysis\", \"output\", \"tables\", \"sentences_with_climchang2_k.rds\"))\n\n\n# paint(sentences_with_climchang2_k)\n\n# Prepare the kable table with subheaders based on 'contains_what'\nsentences_with_climchang2_k %&gt;%\n  ungroup() %&gt;%\n  arrange(contains_what) %&gt;%\n  select(contains_what, proj_id, closest_text) %&gt;%\n  kable(\n    format = \"html\",\n    escape = FALSE,\n    col.names = c(\"Near word (root)\", \"WB Project ID\", \"Closest Text\")\n  ) %&gt;%\n  kable_styling(full_width = FALSE)\n\n\n\nNear word (root)\nWB Project ID\nClosest Text\n\n\n\nadapt\nP128137\n(...) phase i of the disaster risk management and climate change adaptation project are to strengthen the ca pacity\n\n\nadapt\nP091979\n(...) arid and semi-arid lands to plan and implement climate change adaptation measures\n\n\nadapt\nP120134\n(...) support the gom's efforts to foster adaptation to climate change in the water sector, contributing to long-term sustainable\n\n\nhazard\nP177124\n(...) islands to the impacts of natural hazards and climate change\n\n\n\nhazard\nP146768\n(...) buildings and infrastructure due to natural hazards or climate change impacts; and (b) increased capacity of oecs governments\n\n\nhazard\nP123896\n(...) agencies to financial protection from losses caused by climate change and geological hazards.\n\n\nmitig\nP077763\n(...) goal of the fund is to mitigate the climate change and demonstrate the possibilities of public -private partnerships\n\n\nmitig\nP081743\n(...) to help mitigate global climate change through certified carbon emission reductions (cers) of 178,000\n\n\nmitig\nP111940\n(...) developing actions to mitigate the effects of global climate change in the atlantic rain forest, ensuring the conservation\n\n\nresil\nP114294\n(...) implement measures to enhance biodiversity resilie nce to climate change and protect forest carbon assets.\n\n\nresil\nP170052\n(...) iii) strengthening financial resilience to natural disasters and climate change\n\n\n\nresil\nP178141\n(...) in the city, strengthen the city’s resilience to climate change and enhance access to basic services in the\n\n\nvulnerab\nP117871\n(...) at measurably reducing vulnerability to natural hazards and climate change impacts in the eastern caribbean sub-region.\n\n\nvulnerab\nP146768\n(...) at measurably reducing vulnerability to natural hazards and climate change impacts in the eastern caribbean sub-region.\n\n\nvulnerab\nP149259\n(...) at measurably reducing vulnerability to natural hazards and climate change impacts in the eastern caribbean sub-region.\n\n\n\n\n# Add subheaders based on the unique values in `contains_what`\n# group_rows(index = table(sentences_with_climchang2$contains_what))"
  },
  {
    "objectID": "analysis/01b_WB_project_pdo_EDA.html#keyword-in-context-kwic",
    "href": "analysis/01b_WB_project_pdo_EDA.html#keyword-in-context-kwic",
    "title": "WB Project PDO text EDA",
    "section": "— Keyword In Context (KWIC)",
    "text": "— Keyword In Context (KWIC)\n\n\n\nKeyword In Context (KWIC), or concordances, are the most frequently used method in corpus linguistics. The idea is very intuitive: we get to know more about the semantics of a word by examining how it is being used in a wider context.\nUsually, the process involves: 1) tokenizing the text, 2) perform a search for a word and retrieve its concordances from the corpus. Typically, these extractions are displayed through keyword-in-context displays (KWICs), where the search term, also referred to as the node word, is showcased within its surrounding context, comprising both preceding and following words."
  },
  {
    "objectID": "analysis/01b_WB_project_pdo_EDA.html#concordances",
    "href": "analysis/01b_WB_project_pdo_EDA.html#concordances",
    "title": "WB Project PDO text EDA",
    "section": "— Concordances",
    "text": "— Concordances\n\n\n\n\nUsing quanteda\n\nfile:///Users/luisamimmi/Github/slogan_old/docs/01b_WDR_data-exploration_abstracts.html\n\n# ----  Create a corpus from the data frame containing the `pdo` column\npdo_q_corpus &lt;- quanteda::corpus(as.data.frame(projs_train),\n  docid_field = \"proj_id\",\n  text_field = \"pdo\",\n  meta = list(\"pr_name\", \"FY_appr\")\n)\n# I obtain a `corpus` == a `character vector` class(pdo_q_corpus)\n# --- example with individual keyword\n\n# Step 1) tokens\npdo_q_tokens &lt;- quanteda::tokens(\n  x = pdo_q_corpus,\n  remove_punct = TRUE,\n  remove_symbols = TRUE\n  # ,remove_numbers = TRUE\n) %&gt;%\n  quanteda::tokens_tolower() # %&gt;%\n# quanteda::tokens_remove(pattern = custom_stop_words) %&gt;%\n# quanteda::tokens_remove(pattern = c(\"project\", \"development\", \"bank\", \"world\", \"project\", \"projects\"))\n\n# #______ Step 2) kwic (individual exe )\nkwic_pdo_data &lt;- quanteda::kwic(x = pdo_q_tokens, # define text(s)\n                                 # define pattern\n                                 pattern = quanteda::phrase(c(\"gender\", \"climate\", \"sustainab*\")),\n                                 # define window size\n                                 window = 5) %&gt;%\n    # convert into a data frame\n    as_tibble() %&gt;%\n    left_join(projs_train, by = c(\"docname\" =  \"proj_id\")) %&gt;%\n    # remove superfluous columns\n     dplyr::select( 'Year' = FY_appr, 'Prj title' = pr_name, pre, keyword, post) %&gt;%\n  #  slice_sample( n = 50) %&gt;%\n   kbl(align = \"c\") # %&gt;% kable_styling()\n\n\n# ____ Step 2) kwic (on vector)\n# Iterate `quanteda::kwic` over a vector of tokens | regex-modified-keywords\nkeywords &lt;- c(\"gender\", \"climate\", \"sustainab*\", \"conditional*\")\n\n# [ERROR] apply iteratively kwic over a vector of keywords\n# outputs_key &lt;-  map(keywords,\n#       ~quanteda::kwic(pdo_q_tokens,\n#                       pattern =  .x,\n#                       window = 5)) %&gt;%\n#         as_tibble() %&gt;%\n#         left_join(projs_train, by = c(\"docname\" =  \"proj_id\")) %&gt;%\n#         # remove superfluous columns\n#      dplyr::select( 'Year' = FY_appr, 'Prj title' = pr_name, pre, keyword, post)  \n\n\n# [CORRECTED CHATGPT] apply iteratively kwic over a vector of keywords\nkwic_tokens &lt;- keywords %&gt;%\n  set_names() %&gt;%  # names the list using the keywords\n  map(~ quanteda::kwic(\n          x = pdo_q_tokens,\n          pattern = quanteda::phrase(.x),\n          window = 5\n      ) %&gt;%\n      as_tibble() %&gt;%\n      left_join(projs_train, by = c(\"docname\" =  \"proj_id\")) %&gt;%\n      dplyr::select('Year' = FY_appr, 'Prj title' = pr_name, pre, keyword, post)\n  )\n\n\n# # all togetha 4\nn &lt;- length(keywords)\nn\n\n# check the first element\nkwic_tokens$climate %&gt;% kbl(align = \"c\")\nkwic_tokens$gender %&gt;% kbl(align = \"c\")\n\n— create kwic with phrases | purrr + print + save png\n\n# Iterate `quanteda::kwic` over a vector of phrases/bigrams\nkeywords_phrase &lt;- c(\"pro-poor\", \"gender equality\", \"gender mainstreaming\")\n\n# Step 1) tokens\n# (done above) -&gt; abs_q_tokens\n\n# Step 2) kwic\n# apply iteratively kwic over a vector of bigrams\n# Create KWIC tables for each phrase\nkwic_bigrams &lt;- keywords_phrase %&gt;%\n  set_names() %&gt;%\n  map(~ quanteda::kwic(\n          x = pdo_q_tokens,\n          pattern = quanteda::phrase(.x),  # treat each string as a full phrase\n          window = 5\n      ) %&gt;%\n      as_tibble() %&gt;%\n      left_join(projs_train, by = c(\"docname\" =  \"proj_id\")) %&gt;%\n      dplyr::select('Year' = FY_appr, 'Prj title' = pr_name, pre, keyword, post)\n  ) |&gt; \n# drop empty tables\n  discard(~ nrow(.x) == 0)\n\n\n#  number ofo cbigrams\nn_bi &lt;- length(keywords_phrase)\nn_bi # 3\n\n# Render the KWIC table for \"gender equality\":\nkwic_bigrams[[\"gender mainstreaming\"]] %&gt;%\n  kbl(align = \"c\")\n\n# -------------- print all\n#  walk + print -\n# walk(.x = outputs_bigrams2, .f = print)\n\n\n# -------------- save  all -&gt; create multiple tables from a single dataframe and save them as images\n# https://stackoverflow.com/questions/69323569/how-to-save-multiple-tables-as-images-using-kable-and-map/69323893#69323893\n\nout_dir_tab &lt;- here::here(\"analysis\", \"output\", \"tables\")\n\nkwic_bigrams %&gt;%\n  # save_kable(..., file = \"file.png\") relies on the webshot2 package, which  Chromium or Chrome to be installed and accessible. \n  imap(~ save_kable(\n    file = paste0(out_dir_tab, \"/\", \"pdo_kwic_\", .y, \"_.png\"),\n    # bs_theme = 'journal',\n    self_contained = T,\n    x = kbl(.x, booktabs = T, align = c(\"l\", \"l\", \"c\")) %&gt;%\n      kable_styling()\n  ))"
  },
  {
    "objectID": "analysis/01b_WB_project_pdo_EDA.html#compare-pdo-words-v-sector-the-tag",
    "href": "analysis/01b_WB_project_pdo_EDA.html#compare-pdo-words-v-sector-the-tag",
    "title": "WB Project PDO text EDA",
    "section": "COMPARE PDO words v sector (the tag)",
    "text": "COMPARE PDO words v sector (the tag)\nBasically I want to compare the trend over time of the frequency of my custom sector word (pdo_train2_t$tok_sector_broad) in the PDO text, against the frequency of the sector tag in the dataset (sector1).\n—- make sector1_broad\n\n\nTHIS STARTs FROM projs_train bc I needed the PROJECTS\n\n\n# Data input\ntabyl(projs_train$sector1)\n\n# let's select some clear cut sectors e.g. WATER AND SANITATION\nprojs_train &lt;- projs_train %&gt;%\n  mutate(sector1_broad = case_when(\n    # WAT_SAN\n    sector1 == \"Other Water Supply, Sanitation and Waste Management\" ~ \"WAT_SAN\",\n    sector1 == \"Public Administration - Water, Sanitation and Waste Management\" ~ \"WAT_SAN\",\n    sector1 == \"Sanitation\" ~ \"WAT_SAN\",\n    sector1 == \"Water Supply\" ~ \"WAT_SAN\",\n    sector1 == \"Waste Management\" ~ \"WAT_SAN\",\n    # ENERGY\n    sector1 == \"Energy Transmission and Distribution\" ~ \"ENERGY\",\n    sector1 == \"Non-Renewable Energy Generation\" ~ \"ENERGY\",\n    sector1 == \"Other Energy and Extractives\" ~ \"ENERGY\",\n    sector1 == \"Public Administration - Energy and Extractives\" ~ \"ENERGY\",\n    sector1 == \"Renewable Energy Biomass\" ~ \"ENERGY\",\n    sector1 == \"Renewable Energy Geothermal\" ~ \"ENERGY\",\n    sector1 == \"Renewable Energy Hydro\" ~ \"ENERGY\",\n    sector1 == \"Renewable Energy Solar\" ~ \"ENERGY\",\n    sector1 == \"Renewable Energy Wind\" ~ \"ENERGY\",\n    sector1 == \"Renewable energy\" ~ \"ENERGY\",\n\n    # TRANSPORT\n    sector1 == \"Other Transportation\" ~ \"TRANSPORT\",\n    sector1 == \"Public Administration - Transportation\" ~ \"TRANSPORT\",\n    sector1 == \"Urban Transport\" ~ \"TRANSPORT\",\n    sector1 == \"Rural and Inter-Urban Roads\" ~ \"TRANSPORT\",\n    sector1 == \"Roads and highways\" ~ \"TRANSPORT\",\n    sector1 == \"Ports/Waterways\" ~ \"TRANSPORT\",\n    sector1 == \"Railways\" ~ \"TRANSPORT\",\n    sector1 == \"Airports\" ~ \"TRANSPORT\",\n    # URBAN\n    # niente\n    #  MINING_OIL_GAS\n    sector1 == \"MINING_OIL_GAS\" ~ \"MINING_OIL_GAS\",\n    sector1 == \"Oil and Gas\" ~ \"MINING_OIL_GAS\",\n    # ICT\n    sector1 == \"ICT Infrastructure\" ~ \"ICT\",\n    sector1 == \"ICT Services\" ~ \"ICT\",\n    sector1 == \"Public Administration - Information and Communications Technologies\" ~ \"ICT\",\n    sector1 == \"Other Information and Communications Technologies\" ~ \"ICT\",\n    # EDUCATION\n    sector1 == \"Other Education\" ~ \"EDUCATION\",\n    sector1 == \"Primary education\" ~ \"EDUCATION\",\n    sector1 == \"Public Administration - Education\" ~ \"EDUCATION\",\n    sector1 == \"Tertiary education\" ~ \"EDUCATION\",\n    sector1 == \"Secondary education\" ~ \"EDUCATION\",\n    sector1 == \"Workforce Development and Vocational Education\" ~ \"EDUCATION\",\n    sector1 == \"Adult, Basic and Continuing Education\" ~ \"EDUCATION\",\n    sector1 == \"Early Childhood Education\" ~ \"EDUCATION\",\n    # HEALTH\n    sector1 == \"Health\" ~ \"HEALTH\",\n    sector1 == \"Public Administration - Health\" ~ \"HEALTH\",\n    sector1 == \"Health facilities and construction\" ~ \"HEALTH\",\n    # else\n    TRUE ~ sector1\n  ))\n\n# check\npdo_train2_t %&gt;%\n  filter(tok_sector_broad %in%\n    c(\"WAT_SAN\", \"ENERGY\", \"TRANSPORT\", \"MINING_OIL_GAS\", \"ICT\", \"EDUCATION\", \"HEALTH\")) %&gt;%\n  tabyl(tok_sector_broad, show_missing_levels = T)\n\n\nprojs_train %&gt;%\n  filter(sector1_broad %in%\n    c(\"WAT_SAN\", \"ENERGY\", \"TRANSPORT\", \"MINING_OIL_GAS\", \"ICT\", \"EDUCATION\", \"HEALTH\")) %&gt;%\n  tabyl(sector1_broad)\n\n—- prep data sector_broad_tag\n\n\npaint(projs_train)\n\n# prep data\nsector_broad_tag &lt;- projs_train %&gt;%\n#  mutate(FY_appr = boardapprovalFY) %&gt;%\n  filter(!is.na(sector1_broad)) %&gt;%\n  filter(sector1_broad %in%\n    c(\"WAT_SAN\", \"ENERGY\", \"TRANSPORT\", \"MINING_OIL_GAS\", \"ICT\", \"EDUCATION\", \"HEALTH\")) %&gt;%\n  select(FY_appr, sector1_broad) %&gt;%\n  # count(FY_appr, sector1_broad) %&gt;%\n  # filter(n &gt; 0) %&gt;%\n  mutate(sector1_broad = factor(sector1_broad, levels = c(\n    \"WAT_SAN\", \"ENERGY\", \"TRANSPORT\", # \"URBAN\",\n    \"MINING_OIL_GAS\", \"ICT\", \"HEALTH\", \"EDUCATION\"\n  ))) # reorder values by frequency\n# df$FY\n\n\n# data long by sector1_broad (sector_broad_tag)\npaint(sector_broad_tag)\n\n# which tag_sector (gia tolto NA!!!)\n# tabyl(sector_broad_tag$sector1 )\ntabyl(sector_broad_tag$sector1_broad)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n—- [FUNC] Plot each tag sector\nHere I have much bigger numbers\n\n# --- Split data long into a LIST of subset by sector\nsector_list &lt;- base::split(x = sector_broad_tag, f = sector_broad_tag$sector1_broad)\nstr(sector_list)\n\n# --- FUNCTION to plot iteratively each sector (like f_plot_sector)\nf_plot_tag_sector &lt;- function(name_sec) {\n  # Ensure name_sec is treated as a character\n  data_sec &lt;- sector_list[[as.character(name_sec)]]\n  #  data_sec &lt;- sector_list[[\"ENERGY\"]]\n\n  data_sec &lt;- data_sec %&gt;%\n    group_by(FY_appr) %&gt;%\n    count() %&gt;%\n    ungroup() # %&gt;%\n  # #mutate(FY_appr = as.Date(FY_appr, format = \"%Y-%m-%d\"))\n\n  # plot\n  ggplot(data = data_sec, aes(x = FY_appr, y = n)) +\n    geom_line(color = sector_colors[name_sec], linetype = \"dotted\", alpha = 0.5, size = 1) +\n    geom_point(color = sector_colors[name_sec], size = 3) +\n    scale_x_continuous(breaks = seq(2001, 2023, by = 1)) +\n    scale_y_continuous(breaks = seq(0, max(data_sec$n), by = 5)) +\n    labs(title = name_sec, x = \"Year\", y = \"Number of projects\") +\n    # custom\n    lulas_theme +\n    theme(axis.text.x = element_text(angle = 45, hjust = 1)) +\n    labs(\n      title = paste(\"\\\"\", name_sec, \"\\\" in tag by fiscal years of approval\"), # Use facet-specific title\n      subtitle = \"[Using variable \\\"sector1\\\"]\",\n      x = \"\",\n      y = \"\" # Remove y-axis label\n    )\n}\n\n# --- Plot one sector\n# name_sec EDUCATION    ENERGY    HEALTH       ICT     MINING_OIL_GAS TRANSPORT    WAT_SAN\n# data_sec EX sector_list[[\"WAT_SAN\"]]\nf_plot_tag_sector(name_sec = \"WAT_SAN\")\nf_plot_tag_sector(name_sec = \"ENERGY\")\nf_plot_tag_sector(name_sec = \"TRANSPORT\")\nf_plot_tag_sector(name_sec = \"ICT\")\nf_plot_tag_sector(name_sec = \"HEALTH\")\nf_plot_tag_sector(name_sec = \"EDUCATION\")\n# Use purrr::map to apply the function over the names of sector_list\nmap(names(sector_list), f_plot_tag_sector)"
  },
  {
    "objectID": "analysis/01b_WB_project_pdo_EDA.html#combine-two-sets-of-data-sector_broad_tag-and-sector_broad_pdo",
    "href": "analysis/01b_WB_project_pdo_EDA.html#combine-two-sets-of-data-sector_broad_tag-and-sector_broad_pdo",
    "title": "WB Project PDO text EDA",
    "section": "—- Combine two sets of data sector_broad_tag and sector_broad_pdo\n",
    "text": "—- Combine two sets of data sector_broad_tag and sector_broad_pdo\n\n\n# not sure why\nsector_broad_tag &lt;- sector_broad_tag %&gt;%\n  count(FY_appr, sector1_broad)\n\n# Combine two sets of data\nstr(sector_broad_pdo)\nstr(sector_broad_tag)\n\nsector_broad_combo &lt;- left_join(sector_broad_pdo, sector_broad_tag,\n  by = c(\"FY_appr\", \"tok_sector_broad\" = \"sector1_broad\"),\n  suffix = c(\"_pdo\", \"_tag\")\n) %&gt;%\n  filter(!is.na(n_tag))\n\nsector_broad_combo\n\n— [TAB] Kolmorogov-Smirnov test test of similarity with a table\nIn Kolmorogov-Smirnov test:\n\nthe null hypothesis is that the two distributions are the same\n\nThe alternative hypothesis is that the two distributions are different.\n\n\nThe test statistic is the maximum difference between the two cumulative distribution functions. The p-value is the probability of observing a test statistic as extreme as the one observed, assuming the null hypothesis is true.\n\n# Function to calculate KS test results and save to a table without plotting\nks_results_k &lt;- sector_broad_combo %&gt;%\n  group_by(tok_sector_broad) %&gt;%\n  summarize(\n    # ks_alt_hyp = ks.test(\n    #   (n_pdo - min(n_pdo)) / (max(n_pdo) - min(n_pdo)),\n    #   (n_tag - min(n_tag)) / (max(n_tag) - min(n_tag))\n    # )$alternative,\n\n    # a) with normalization\n    ks_statistic = ks.test(\n      (n_pdo - min(n_pdo)) / (max(n_pdo) - min(n_pdo)),\n      (n_tag - min(n_tag)) / (max(n_tag) - min(n_tag))\n    )$statistic,\n    ks_p_value = ks.test(\n      (n_pdo - min(n_pdo)) / (max(n_pdo) - min(n_pdo)),\n      (n_tag - min(n_tag)) / (max(n_tag) - min(n_tag))\n    )$p.value,\n    similarity = ifelse(ks_p_value &gt; 0.05, \"Similar\", \"Dissimilar\"),\n\n    # # b) without normalization\n    #  ks_statistic_raw = ks.test(n_pdo, n_tag)$statistic,\n    #  ks_p_value_raw = ks.test(n_pdo, n_tag)$p.value,\n  ) %&gt;%\n  ungroup() %&gt;%\n  arrange(ks_p_value)\n\n# save as object\nwrite_rds(ks_results_k, here(\"analysis\", \"output\", \"tables\", \"ks_results_k.rds\"))\n\n# Count how often the phrase appears in the vicinity of the target bigram\nks_results_k %&gt;%\n  kable(\n    format = \"html\",\n    col.names = c(\n      \"SECTORS\", \"KS statistic\", \"KS p-value\", \"Distributions\" # , \"KS statistic R\",\"KS p-valueR\"\n    ),\n    # Round  to 4 digits)\n    digits = c(\n      0, 4, 4, 0 # , 4,4\n    )\n  ) %&gt;%\n  kable_styling(full_width = FALSE) %&gt;%\n  row_spec(which(ks_results_k$similarity == \"Dissimilar\"), background = \"#e7d8da\")\n\n\n\nSECTORS\nKS statistic\nKS p-value\nDistributions\n\n\n\nMINING_OIL_GAS\n0.5882\n0.0030\nDissimilar\n\n\nENERGY\n0.4091\n0.0452\nDissimilar\n\n\nEDUCATION\n0.3182\n0.1836\nSimilar\n\n\nTRANSPORT\n0.3182\n0.1976\nSimilar\n\n\nHEALTH\n0.2273\n0.6009\nSimilar\n\n\nICT\n0.2000\n0.7909\nSimilar\n\n\nWAT_SAN\n0.1818\n0.8479\nSimilar\n\n\n\n\n\nAdditional plots\n— [FIG] Kolmogorov-Smirnov Test for Similarity of PDO and TAG Distributions by Sector\n\nks_results_k %&gt;%\n  ggplot(aes(x = reorder(tok_sector_broad, ks_p_value), y = ks_p_value)) +\n  geom_col(fill = \"#0073C2FF\") +\n  geom_text(aes(label = round(ks_p_value, 4)), vjust = -0.5 ) +\n  coord_flip() +\n  labs(\n    title = \"Kolmogorov-Smirnov Test for Similarity of PDO and TAG Distributions by Sector\",\n    x = \"Sector\",\n    y = \"P-Value\"\n  ) +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\nFigure 1: Kolmogorov-Smirnov Test for Similarity of PDO and TAG Distributions by Sector\n\n\n\n\n\n\n\n— [FIG] Correlation between PDO and TAG by sector\nCorrelation Coefficients: Display the correlation between n_pdo and n_tag within each sector\n\nsector_broad_combo %&gt;%\n  ggplot(aes(x = n_pdo, y = n_tag)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  facet_wrap(~tok_sector_broad, scales = \"free\") +\n  labs(\n    title = \"Correlation between PDO and TAG by Sector\",\n    x = \"Normalized PDO\",\n    y = \"Normalized TAG\"\n  ) +\n  theme_minimal()\n\n\n\nFigure 2: Correlation between PDO and TAG by sector"
  },
  {
    "objectID": "analysis/01b_WB_project_pdo_EDA.html#compare-pdo-words-v-sector",
    "href": "analysis/01b_WB_project_pdo_EDA.html#compare-pdo-words-v-sector",
    "title": "WB Project PDO text EDA",
    "section": "COMPARE PDO words v sector ($$)",
    "text": "COMPARE PDO words v sector ($$)\n—- count of PDO with sector words sector_broad_pdo corresponding to each tok_sector_broad per year\nThis is a count of OBS per cell\n\nsector_broad_pdo # 330\n\n—- sum of sum_curr_total_commitment corresponding to each sector1_broad per year\n\nTHIS STARTs FROM projs_train bc I needed the PROJECTS\n\n\n# prep data\nsector_broad_commit &lt;- projs_train %&gt;%\n  select(FY_appr ,#= boardapprovalFY, \nsector1_broad, curr_total_commitment) %&gt;%\n  # group_by(FY_appr, sector1_broad) %&gt;%\n  # summarise(sum_curr_total_commitment = sum(curr_total_commitment)) %&gt;%\n  # ungroup() %&gt;%\n  # mutate(FY_appr = as.character(FY_appr)) %&gt;%\n  filter(sector1_broad %in% c(\"WAT_SAN\", \"ENERGY\", \"TRANSPORT\", \"MINING_OIL_GAS\", \"ICT\", \"HEALTH\", \"EDUCATION\", \"URBAN\")) %&gt;%\n  mutate(FY_appr = as.numeric(FY_appr)) %&gt;%\n  arrange(sector1_broad, FY_appr) %&gt;%\n  group_by(sector1_broad, FY_appr) %&gt;%\n  summarise(sum_commit = sum(curr_total_commitment), .groups = \"drop\") %&gt;%\n  complete(sector1_broad, FY_appr = full_seq(FY_appr, 1), fill = list(sum_commit = 0)) # Fill missing years\n\n\nsector_broad_pdo &lt;- sector_broad_pdo %&gt;%\n  filter(tok_sector_broad %in% c(\"WAT_SAN\", \"ENERGY\", \"TRANSPORT\", \"MINING_OIL_GAS\", \"ICT\", \"HEALTH\", \"EDUCATION\"))\n\npaint(sector_broad_pdo)\nnrow(sector_broad_pdo) # 161\ntabyl(sector_broad_pdo$tok_sector_broad, show_missing_levels = F, show_na = F)\ntabyl(sector_broad_pdo$FY_appr, show_missing_levels = F, show_na = F)\n\npaint(sector_broad_commit)\nnrow(sector_broad_commit) # 161\ntabyl(sector_broad_commit$sector1_broad, show_missing_levels = F, show_na = F)\ntabyl(sector_broad_commit$FY_appr, show_missing_levels = F, show_na = F)\n\n\n# merge the two datasets\nsector_broad_pdo_comm &lt;- left_join(sector_broad_pdo, sector_broad_commit, by = c(\"tok_sector_broad\" = \"sector1_broad\", \"FY_appr\" = \"FY_appr\"))\n\n— [TAB] Kolmorogov-Smirnov test test of similarity with a table\nIn Kolmorogov-Smirnov test:\n\nthe null hypothesis is that the two distributions are the same\n\nThe alternative hypothesis is that the two distributions are different.\n\n\nThe test statistic is the maximum difference between the two cumulative distribution functions. The p-value is the probability of observing a test statistic as extreme as the one observed, assuming the null hypothesis is true.\n\n# Function to calculate KS test results and save to a table without plotting\nks_results2_k &lt;- sector_broad_pdo_comm %&gt;%\n  group_by(tok_sector_broad) %&gt;%\n  # min -max normalization\n  mutate(\n    n_scaled = (n - min(n, na.rm = TRUE)) /\n      (max(n, na.rm = TRUE) - min(n, na.rm = TRUE)),\n    sum_commit_scaled = (sum_commit - min(sum_commit, na.rm = TRUE)) /\n      (max(sum_commit, na.rm = TRUE) - min(sum_commit, na.rm = TRUE))\n  ) %&gt;%\n  summarize(\n    # -- a) with normalization\n    ks_statistic = ks.test(n_scaled, sum_commit_scaled)$statistic,\n    ks_p_value = ks.test(n_scaled, sum_commit_scaled)$p.value,\n    similarity = ifelse(ks_p_value &gt; 0.05, \"Similar\", \"Dissimilar\"),\n    # -- b) without normalization\n    #  ks_statistic_raw = ks.test(n_pdo, n_tag)$statistic,\n    #  ks_p_value_raw = ks.test(n_pdo, n_tag)$p.value,\n  ) %&gt;%\n  ungroup() %&gt;%\n  arrange(ks_p_value)\n\n# save as object\nwrite_rds(ks_results2_k, here(\"analysis\", \"output\", \"tables\", \"ks_results2_k.rds\"))\n\n# Count how often the phrase appears in the vicinity of the target bigram\nks_results2_k %&gt;%\n  kable(\n    format = \"html\",\n    col.names = c(\n      \"SECTORS\", \"KS statistic\", \"KS p-value\", \"Distributions\" # , \"KS statistic R\",\"KS p-valueR\"\n    ),\n    # Round  to 4 digits)\n    digits = c(\n      0, 4, 4, 0 # , 4,4\n    )\n  ) %&gt;%\n  kable_styling(full_width = FALSE) %&gt;%\n  row_spec(which(ks_results2_k$similarity == \"Dissimilar\"), background = \"#e7d8da\")\n\n\n\nSECTORS\nKS statistic\nKS p-value\nDistributions\n\n\n\nICT\n0.6818\n0.0000\nDissimilar\n\n\nMINING_OIL_GAS\n0.5909\n0.0007\nDissimilar\n\n\nEDUCATION\n0.5000\n0.0069\nDissimilar\n\n\nENERGY\n0.2727\n0.3867\nSimilar\n\n\nHEALTH\n0.2727\n0.3937\nSimilar\n\n\nTRANSPORT\n0.2273\n0.6324\nSimilar\n\n\nWAT_SAN\n0.2273\n0.6324\nSimilar\n\n\n\n\n\n— [FUNC] standardize and plot\n\n\nStandardization is done by subtracting the mean and dividing by the standard deviation. This is done for both the n and sum_commit columns. In this way we can compare the two distributions on the same scale.\n\nRobust Scaling: Subtract the median and divide by the IQR. This is more robust to outliers than standardization, but it doesn’t ensure the distributions have the same variance.\n✅ Min-Max Scaling: Rescale both n and sum_commit to a [0, 1] range. This doesn’t assume normality and ensures both distributions are within the same bounds, though it doesn’t account for the shape of the distributions.\n\n\nrobust or min-max scaling alternatives can provide more reliable comparisons, especially with skewed data.\n\n\n\nKolmogorov-Smirnov (KS) Test P-Values: Display the p-value from a Kolmogorov-Smirnov test comparing rescaled trends within each sector producing a p-value that indicates the probability of observing these distributions if they were the same.\n\nDIFFERENT = A low p-value (typically &lt; 0.05) suggests the distributions are significantly different, while a higher p-value suggests similarity.\nSIMILAR = A high p-value does not necessarily mean the distributions are identical, only that there is not enough evidence to reject the null hypothesis of similarity.\nThe KS test is non-parametric and makes no assumptions about the underlying distributions, making it a versatile tool for comparing distributions.\n\n\n\n\n# --- FUNCTION to 1) standardize 2 distributions and 2) plot iteratively each sector\nf_plot_sector_comm &lt;- function(data, sector) {\n  # ---- Filter data for the specified sector\n  sector_data &lt;- data %&gt;%\n    dplyr::filter(tok_sector_broad == sector) %&gt;%\n    dplyr::group_by(tok_sector_broad) %&gt;%\n    # # ---- Standardize n and sum_commit within each sector\n    # mutate(n_standardized = (n - mean(n, na.rm = TRUE)) / sd(n, na.rm = TRUE),\n    #        sum_commit_standardized = (\n    #           sum_commit - mean(sum_commit, na.rm = TRUE)) / sd(sum_commit, na.rm = TRUE)) %&gt;%\n    # ---- Min-Max Scaling\n    dplyr::mutate(\n      n_scaled = (n - min(n, na.rm = TRUE)) /\n        (max(n, na.rm = TRUE) - min(n, na.rm = TRUE)),\n      sum_commit_scaled = (sum_commit - min(sum_commit, na.rm = TRUE)) /\n        (max(sum_commit, na.rm = TRUE) - min(sum_commit, na.rm = TRUE))\n    ) %&gt;%\n    dplyr::ungroup()\n\n  # ---- Calculate Spearman correlation and KS test p-value for the selected sector\n  sector_stats &lt;- sector_data %&gt;%\n    dplyr::summarize(\n      spearman_cor = cor(n_scaled, sum_commit_scaled, method = \"spearman\", use = \"complete.obs\"),\n      ks_p_value = ks.test(n_scaled, sum_commit_scaled)$p.value,\n      similarity = ifelse(ks_p_value &gt; 0.05, \"Similar\", \"Dissimilar\")\n    )\n\n\n  # Extract the color for the sector line (you can set a specific color or use ggplot's color palette)\n  pdo_color &lt;- \"#8e550a\" # Get a color from ggplot's default palette\n  commit_color &lt;- \"#00689D\" # Set a color for the secondary line\n\n  # Plot the data for the selected sector\n  ggplot(sector_data, aes(x = FY_appr)) +\n    # --- geom_bar for rel_freq_n_pdo\n    geom_line(aes(y = n_scaled), color = pdo_color, alpha = 0.75) +\n    geom_point(aes(y = n_scaled), color = pdo_color, alpha = 0.75, size = 2) +\n    # --- geom_line and geom_point for rel_freq_commitment\n    geom_line(aes(y = sum_commit_scaled), color = commit_color, linetype = \"dashed\") +\n    geom_point(aes(y = sum_commit_scaled), color = commit_color, size = 2) +\n    # --- scale\n    scale_x_continuous(breaks = seq(2001, 2023, by = 1)) +\n    scale_y_continuous(\n      name = \"N words in PDOs (mean = 0, sd = 1)\",\n      sec.axis = sec_axis(~., name = \"$$ Committed (mean = 0, sd = 1)\")\n    ) +\n\n    # --- Annotate KS test results directly on the plot\n    annotate(\n      \"text\",\n      x = Inf, y = Inf, label = paste(\"KS test p-value:\", round(sector_stats$ks_p_value, 4)),\n      hjust = 1.1, vjust = 1.1, color = \"black\", size = 4\n    ) +\n\n    # Customize colors and set common legend title\n    labs(\n      title = paste(\"Word frequency in PDO v. amount committed for\", sector),\n      subtitle = \"n_pdo and sum_commit are rescaled with Min-Max scaling \\nKolmogorov-Smirnov test for similarity between the two trends\",\n      x = \"\"\n    ) +\n\n    # custom\n    lulas_theme +\n    theme(\n      legend.position = \"none\",\n      axis.text.x = element_text(angle = 45, hjust = 1),\n      axis.title.y = element_text(color = pdo_color),\n      axis.title.y.right = element_text(color = commit_color)\n    )\n}\n# --- Plot one sector\n# name_sec EDUCATION    ENERGY    HEALTH       ICT     MINING_OIL_GAS TRANSPORT    WAT_SAN\n# data_sec EX sector_list[[\"WAT_SAN\"]]\nf_plot_sector_comm(sector_broad_pdo_comm, sector = \"TRANSPORT\") # KS p-val = 0.42 similar\nf_plot_sector_comm(sector_broad_pdo_comm, sector = \"WAT_SAN\") # KS p-val = 0.42 similar\nf_plot_sector_comm(sector_broad_pdo_comm, sector = \"ENERGY\") # KS p-val = 0.12 similar\nf_plot_sector_comm(sector_broad_pdo_comm, sector = \"ICT\") # KS p-val = 0.0001 DIFFERENT\nf_plot_sector_comm(sector_broad_pdo_comm, sector = \"HEALTH\") # KS p-val = 0.001 DIFFERENT\nf_plot_sector_comm(sector_broad_pdo_comm, sector = \"EDUCATION\") # KS p-val = 0.0001 DIFFERENT\n# many with non commitment\nf_plot_sector_comm(sector_broad_pdo_comm, sector = \"MINING_OIL_GAS\") # KS p-val = 0.0031 DIFFERENT\n\n— [FIG] Plot sector WAT_SAN\n\nvery similar trends in PDO and commitment\n\n\n\n\nFigure 10\n\n\n\n\n\n\n\n— [FIG] Plot sector ICT\n\nvery different trends in PDO and commitment\n\n\n\n\nFigure 11"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Motivation",
    "section": "",
    "text": "Throughout my career as a consultant in multilateral and governmental institutions, I have been exposed to the phenomenon of buzzwords1, catchphrases2 and slogans3, and to how they suddenly emerge in policy discussions, and—at least for some time—take center stage in strategic planning.\nNotably, in the organizations I worked with (including World Bank, Inter-American Development Bank, G20, EU, and the Italian Government) catchphrases and slogans often turn into significant funding allocations, as they can steer the prioritization of policies and programs.\n\n\nSome examples I can recall (drawing on my own experience in economic development policy work) are: South-South Cooperation, Gender Mainstreaming, and Results-Based Financing, which were prominent in earlier periods. More recently, concepts such as Sustainable Development4 and ESG (criteria/reporting/issues) have become quintessential to the development discourse. Understandably, constructs like Build Back Better and Resilient (institutions/infrastructure/communities) have gained traction, especially as part of the legacy of the COVID-19 crisis.\nI’ve noticed how these formulaic expressions would make their way into project documentation and policy papers, quickly gaining prominence—and sometimes becoming pervasive—in both academic and professional discourse.\nThis has led me to question whether such phrases in policy discourse are deliberately chosen with careful intent or if they gain traction almost inadvertently, becoming a convenient shorthand for complex ideas. The latter shouldn’t be surprising. After all, humans rely on mental (and perhaps linguistic) shortcuts to make sense of the world. Recent studies in psychology and behavioral economics explore heuristics, the cognitive strategies people use to simplify decision-making.5 Furthermore, we are now exposed to the paradoxical effect of social media and its recommendation algorithms: rather than broadening our exposure to information, they often restrict it, reinforcing the echo chamber effect.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA further element that interests me is when the meaning of words and phrases progressively detaches from reality and/or from their original intent, sometimes even adopting new reinterpreted meanings that gradually become the dominant interpretation—whether explicitly or subtly. For example, I was struck to learn that the Latin motto I heard as a child—“Mens sana in corpore sano”—is actually a partial quotation adopted in 1861 by John Hulley for his Liverpool Athletic Club. The phrase actually quoted Juvenal’s “Orandum est ut sit mens sana in corpore sano” from Satire X (Juvenal 2004, 10.354–357), which, seen in its entirety, has a whole different meaning!\n Such reductions have real consequences, as Giovanni Gentile notes: “language is not a garment of thought; it is thought’s own body” (Garbini 2003, p3, quoting “Sommario di pedagogia come scienza filosofica”). Language doesn’t merely communicate ideas but shapes and defines them.  Luca D’Auria similarly asserts that “language is performative,” as it colors, alters, and produces reality (D’Auria 2024). While language help us categorize, it can also divide—separating good from bad, truth from falsehood, etc. Knowledge itself, much to the dismay of proponents of political correctness, depends on clear categories to distinguish and define the realities we discuss.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFor a while now, I have wondered why and how certain catchphrases/slogans/buzzwords become so prevalent in a given public policy context at some specific point in time.\nThe problem is that if you are (as I was) just a cog in very complex institutional machine, it is difficult to capture the complex dynamics of this process.\nTherefore, I decided to investigate this phenomenon with a “reverse engineering” approach, starting from the corpora of documents themselves to see if I could identify some patterns or trends over time. (Head over to this page to see my specific research questions.)\n\n\n\nThe World Bank simply presented, from my point of view, a convenient “study case” because:\n\nIts documents (related to lending projects and other publications) are available and openly accessible.\nThe Bank has a long history and a vast amount of digitized documents, which allows for a longitudinal analysis.\nThe Bank’s sectorial and thematic coverage is very broad, which allows for an interesting analysis.\nLast but not least, I am familiar with the Bank’s documents’ nature, structure and jargon, having worked there for several years.\n\n\n\n\nGet in touch if you have questions, comments or suggestions!\n\n\n\n\n\n    Email"
  },
  {
    "objectID": "index.html#examples-in-the-development-economics-arena",
    "href": "index.html#examples-in-the-development-economics-arena",
    "title": "Motivation",
    "section": "",
    "text": "Some examples I can recall (drawing on my own experience in economic development policy work) are: South-South Cooperation, Gender Mainstreaming, and Results-Based Financing, which were prominent in earlier periods. More recently, concepts such as Sustainable Development4 and ESG (criteria/reporting/issues) have become quintessential to the development discourse. Understandably, constructs like Build Back Better and Resilient (institutions/infrastructure/communities) have gained traction, especially as part of the legacy of the COVID-19 crisis.\nI’ve noticed how these formulaic expressions would make their way into project documentation and policy papers, quickly gaining prominence—and sometimes becoming pervasive—in both academic and professional discourse.\nThis has led me to question whether such phrases in policy discourse are deliberately chosen with careful intent or if they gain traction almost inadvertently, becoming a convenient shorthand for complex ideas. The latter shouldn’t be surprising. After all, humans rely on mental (and perhaps linguistic) shortcuts to make sense of the world. Recent studies in psychology and behavioral economics explore heuristics, the cognitive strategies people use to simplify decision-making.5 Furthermore, we are now exposed to the paradoxical effect of social media and its recommendation algorithms: rather than broadening our exposure to information, they often restrict it, reinforcing the echo chamber effect."
  },
  {
    "objectID": "index.html#the-curious-case-of-lexical-domestication",
    "href": "index.html#the-curious-case-of-lexical-domestication",
    "title": "Motivation",
    "section": "",
    "text": "A further element that interests me is when the meaning of words and phrases progressively detaches from reality and/or from their original intent, sometimes even adopting new reinterpreted meanings that gradually become the dominant interpretation—whether explicitly or subtly. For example, I was struck to learn that the Latin motto I heard as a child—“Mens sana in corpore sano”—is actually a partial quotation adopted in 1861 by John Hulley for his Liverpool Athletic Club. The phrase actually quoted Juvenal’s “Orandum est ut sit mens sana in corpore sano” from Satire X (Juvenal 2004, 10.354–357), which, seen in its entirety, has a whole different meaning!\n Such reductions have real consequences, as Giovanni Gentile notes: “language is not a garment of thought; it is thought’s own body” (Garbini 2003, p3, quoting “Sommario di pedagogia come scienza filosofica”). Language doesn’t merely communicate ideas but shapes and defines them.  Luca D’Auria similarly asserts that “language is performative,” as it colors, alters, and produces reality (D’Auria 2024). While language help us categorize, it can also divide—separating good from bad, truth from falsehood, etc. Knowledge itself, much to the dismay of proponents of political correctness, depends on clear categories to distinguish and define the realities we discuss."
  },
  {
    "objectID": "index.html#so-what",
    "href": "index.html#so-what",
    "title": "Motivation",
    "section": "",
    "text": "For a while now, I have wondered why and how certain catchphrases/slogans/buzzwords become so prevalent in a given public policy context at some specific point in time.\nThe problem is that if you are (as I was) just a cog in very complex institutional machine, it is difficult to capture the complex dynamics of this process.\nTherefore, I decided to investigate this phenomenon with a “reverse engineering” approach, starting from the corpora of documents themselves to see if I could identify some patterns or trends over time. (Head over to this page to see my specific research questions.)"
  },
  {
    "objectID": "index.html#why-focus-on-the-world-bank",
    "href": "index.html#why-focus-on-the-world-bank",
    "title": "Motivation",
    "section": "",
    "text": "The World Bank simply presented, from my point of view, a convenient “study case” because:\n\nIts documents (related to lending projects and other publications) are available and openly accessible.\nThe Bank has a long history and a vast amount of digitized documents, which allows for a longitudinal analysis.\nThe Bank’s sectorial and thematic coverage is very broad, which allows for an interesting analysis.\nLast but not least, I am familiar with the Bank’s documents’ nature, structure and jargon, having worked there for several years.\n\n\n\n\nGet in touch if you have questions, comments or suggestions!\n\n\n\n\n\n    Email"
  },
  {
    "objectID": "index.html#footnotes",
    "href": "index.html#footnotes",
    "title": "Motivation",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nBuzzword: The term probably originated in the 1940s as jargon used in business and marketing circles. More recently, it has taken on the meaning “fashionable jargon”, especially common in business, technology, and politics, where they often signal innovation or forward-thinking but may lack clear, practical applications.↩︎\nCatchphrase: The notion is of a short phrase that will “catch” in the mind of the hearer. The word was used in the 1840s of a line of music that was easy to catch and remember. It describes, in the modern sense, a memorable, often simplistic phrase or expression that becomes widely recognized and associated with a particular individual, group, or concept. It is designed to stick in people’s minds and to evoke a specific idea or emotion, usually through repetition in media, advertising, or popular culture.↩︎\nSlogan: from Gaelic sluagh-ghairm, i.e. “battle cry”, used by Scottish Highland or Irish clans. The metaphoric sense of “distinctive word or phrase used by a political or other group” is attested from 1704. It still carries the connotation of a rallying cry, a motto, or a watchword, deliberately crafted to be persuasive, promote a cause or sell an idea.↩︎\nActually, “sustainable development” was first used in 1987 within the so-called Brundtland Report, titled “Our Common Future”, presented to the World Commission on Environment and Development led by Gro Harlem Brundtland.↩︎\nHeuristics can certainly help us solve problems and speed up our decision-making process, but they can also introduce errors, bias, inaccurate judgment and irrational decision-making.↩︎"
  }
]