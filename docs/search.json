[
  {
    "objectID": "analysis/01b_WB_project_pdo_EDA.html",
    "href": "analysis/01b_WB_project_pdo_EDA.html",
    "title": "WB Project PDO text EDA",
    "section": "",
    "text": "Warning\n\n\n\n\n\nWORK IN PROGRESS! (Please expect unfinished sections, and unpolished code. Feedback is welcome!)"
  },
  {
    "objectID": "analysis/01b_WB_project_pdo_EDA.html#tbl-illustrative-pdos-text-in-projects-documents",
    "href": "analysis/01b_WB_project_pdo_EDA.html#tbl-illustrative-pdos-text-in-projects-documents",
    "title": "WB Project PDO text EDA",
    "section": "[TBL] Illustrative PDOs text in Projects‚Äô documents",
    "text": "[TBL] Illustrative PDOs text in Projects‚Äô documents\n\n\n\n\n\n\n\n\n\nProject_ID\nProject_Name\nProject_Development_Objective\n\n\n\nP127665\nSecond Economic Recovery Development Policy Loan\nThis development policy loan supports the Government of Croatia‚Äôs reform efforts with the aim to: (i) enhance fiscal sustainability through expenditure-based consolidation; and (ii) strengthen investment climate.\n\n\nP069934\nPERNAMBUCO INTEGRATED DEVELOPMENT: EDUCATION QUALITY IMPROVEMENT PROJECT\nThe development objectives of the Pernambuco Integrated Development: Education Quality Improvement Project are to (a) improve the quality, efficiency, and inclusiveness of the public education system; (b) modernize and strengthen the managerial, financial, and administrative capacity of the Secretariat of Education to set policies and guidelines for the sector and deliver public education efficiently; and (c) support the overall state modernization effort through interventions to be carried out in the Secretariat of Education and to be replicated in other state institutions."
  },
  {
    "objectID": "analysis/01b_WB_project_pdo_EDA.html#saved-file-projs_train_t-pdo_train_t",
    "href": "analysis/01b_WB_project_pdo_EDA.html#saved-file-projs_train_t-pdo_train_t",
    "title": "WB Project PDO text EDA",
    "section": "[Saved file projs_train_t & pdo_train_t]",
    "text": "[Saved file projs_train_t & pdo_train_t]\n\n# Load Proj train dataset `projs_train_t`\nprojs_train &lt;- readRDS(\"~/Github/slogan/data/derived_data/projs_train.rds\")  \n\n# Load clean tokenized-PDO dataset `pdo_train_t`\npdo_train_t &lt;- readRDS(here::here(\"data\" , \"derived_data\", \"pdo_train_t.rds\"))"
  },
  {
    "objectID": "analysis/01b_WB_project_pdo_EDA.html#previous-tokenization-and-pos-tagging",
    "href": "analysis/01b_WB_project_pdo_EDA.html#previous-tokenization-and-pos-tagging",
    "title": "WB Project PDO text EDA",
    "section": "Previous Tokenization and PoS Tagging",
    "text": "Previous Tokenization and PoS Tagging\nTypically, one of the first steps in this transformation from natural language to feature, or any of kind of text analysis, is tokenization.\ni) Explain Tokenization\nBreaking units of language into components relevant for the research question is called ‚Äútokenization‚Äù. Components can be words, n-grams, sentences, etc. or combining smaller units into larger units.\n\nTokenization is a row-wise operation: it changes the number of rows in the dataset.\nThe choices of tokenization\n\nShould words be lower cased?\nShould punctuation be removed?\n\nShould numbers be replaced by some placeholder?\nShould words be stemmed (also called lemmatization)? ‚òëÔ∏è\nShould bigrams/multi-word phrase be used instead of single word phrases? ‚òëÔ∏è\nShould stopwords (the most common words) be removed? ‚òëÔ∏è\nShould rare words be removed? ‚ùå\nShould hyphenated words be split into two words? ‚ùå\n\n\nfor the moment I keep all as conservatively as possible\n\nii) Explain Pos Tagging\nLinguistic annotation is a common for of enriching text data, i.e.¬†adding information about the text that is not directly present in the text itself.\nUpon this, e.g.¬†classifying noun, verb, adjective, etc., one can discover intent or action in a sentence, or scanning ‚Äúverb-noun‚Äù patterns.\nHere I have a training dataset file with:\n\n\n\n\n\n\n\n\n\n\n\nVariable\nType\nProvenance\nDescription\nExample\n\n\n\nproj_id\nchr\noriginal PDO data\n\n\n\n\npdo\nchr\noriginal PDO data\n\n\n\n\nword\nchr\noriginal PDO data\n\nGovernments\n\n\nsid\nint\noutput cleanNLP\nsentence ID\n\n\n\ntid\nchr\noutput cleanNLP\ntoken ID within sentence\n\n\n\ntoken\nchr\noutput cleanNLP\nTokenized form of the token.\ngovernment\n\n\ntoken_with_ws\nchr\noutput cleanNLP\nToken with trailing whitespace\ngovernment\n\n\nlemma\nchr\noutput cleanNLP\nThe base form of the token\ngovernment\n\n\nstem\nchr\noutput SnowballC\nThe base form of the token\ngovern\n\n\nupos\nchr\noutput cleanNLP\nUniversal part-of-speech tag (e.g., NOUN, VERB, ADJ).\n\n\n\nxpos\nchr\noutput cleanNLP\nLanguage-specific part-of-speech tags.\n\n\n\nfeats\nchr\noutput cleanNLP\nMorphological features of the token\n\n\n\ntid_source\nchr\noutput cleanNLP\nToken ID in the source document\n\n\n\nrelation\nchr\noutput cleanNLP\nDependency relation between the token and its head token\n\n\n\npr_name\nchr\noutput cleanNLP\nName of the parent token\n\n\n\nFY_appr\ndbl\noriginal PDO data\n\n\n\n\nFY_clos\ndbl\noriginal PDO data\n\n\n\n\nstatus\nchr\noriginal PDO data\n\n\n\n\nregionname\nchr\noriginal PDO data\n\n\n\n\ncountryname\nchr\noriginal PDO data\n\n\n\n\nsector1\nchr\noriginal PDO data\n\n\n\n\ntheme1\nchr\noriginal PDO data\n\n\n\n\nlendinginstr\nchr\noriginal PDO data\n\n\n\n\nenv_cat\nchr\noriginal PDO data\n\n\n\n\nESrisk\nchr\noriginal PDO data\n\n\n\n\ncurr_total_commitment\ndbl\noriginal PDO data\n\n\n\n\n\n\n\n‚Äî PoS Tagging: upos (Universal Part-of-Speech)\n\n\n\n\nupos\nn\npercent\nexplan\n\n\n\nADJ\n21261\n0.0852623\nAdjective\n\n\nADP\n27050\n0.1084777\nAdposition\n\n\nADV\n2950\n0.0118303\nAdverb\n\n\nAUX\n3588\n0.0143888\nAuxiliary\n\n\nCCONJ\n14236\n0.0570902\nCoordinating conjunction\n\n\nDET\n21505\n0.0862408\nDeterminer\n\n\nINTJ\n57\n0.0002286\nInterjection\n\n\nNOUN\n70752\n0.2837344\nNoun\n\n\nNUM\n2190\n0.0087825\nNumeral\n\n\nPART\n8691\n0.0348532\nParticle\n\n\nPRON\n2330\n0.0093439\nPronoun\n\n\nPROPN\n14856\n0.0595765\nProper noun\n\n\nPUNCT\n28393\n0.1138635\nPunctuation\n\n\nSCONJ\n2160\n0.0086622\nSubordinating conjunction\n\n\nSYM\n316\n0.0012672\nSymbol\n\n\nVERB\n25806\n0.1034889\nVerb\n\n\nX\n3219\n0.0129090\nOther\n\n\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\n\n\nOn random visual check, these are not always correct, but they are a good starting point for now.\n\n\n\niii) Custom Stopwords\nRemove stop words, which are the most common words in a language.\n\nbut I don‚Äôt want to remove any meaningful word for now\n\n\n# Custom list of articles, prepositions, and pronouns\ncustom_stop_words &lt;- c(\n   # Articles\n   \"the\", \"a\", \"an\",   \n   \"and\", \"but\", \"or\", \"yet\", \"so\", \"for\", \"nor\", \"as\", \"at\", \"by\", \"per\",  \n   # Prepositions\n   \"of\", \"in\", \"on\", \"at\", \"by\", \"with\", \"about\", \"against\", \"between\", \"into\", \"through\", \n   \"during\", \"before\", \"after\", \"above\", \"below\", \"to\", \"from\", \"up\", \"down\", \"under\",\n   \"over\", \"again\", \"further\", \"then\", \"once\",  \n   # Pronouns\n   \"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \"you\", \"your\",\n   \"yours\", \"yourself\", \"yourselves\", \"he\", \"him\", \"his\", \"himself\", \"she\", \"her\", \n   \"hers\", \"herself\", \"it\", \"its\", \"itself\", \"they\", \"them\", \"their\", \"theirs\", \"themselves\" ,\n   \"this\", \"that\", \"these\", \"those\", \"which\", \"who\", \"whom\", \"whose\", \"what\", \"where\",\n   \"when\", \"why\", \"how\", \"all\", \"any\", \"both\", \"each\", \"few\", \"more\", \"most\", \"other\",\n   # \"some\", \"such\", \"no\",  \"not\", \n   # \"too\", \"very\",   \n   # verbs\n   \"is\", \"are\", \"would\", \"could\", \"will\", \"be\", \"e.g\", \"e.g.\", \"i.e.\",\n   \"i\", \"ii\", \"iii\", \"iv\", \"v\",\n   # because tautology\n   \"pdo\"\n)\n\n# Convert to a data frame if needed for consistency with tidytext\ncustom_stop_words_df &lt;- tibble(word = custom_stop_words)\n\n\nsaveRDS(custom_stop_words, here(\"data\" , \"derived_data\", \"custom_stop_words.rds\"))\nsaveRDS(custom_stop_words_df, here(\"data\" , \"derived_data\", \"custom_stop_words_df.rds\"))\n\niv) Stemming\nOften documents contain different versions of one base word, often called a stem. Stemming is the process of reducing words to their base or root form.\nSnowball is one framework released in 1980 with an open-source license that can be found in R package SnowballC.\n\n# Using `SnowballC::wordStem` to stem the words. e.g.\npdo_train_t &lt;- pdo_train_t %&gt;% \n   mutate(stem = SnowballC::wordStem(token_l)) %&gt;%\n   relocate(stem, .after = lemma)\n\nWhy Stemming?: For example, in topic modeling, stemming reduces noise by making it easier for the model to identify core topics without being distracted by grammatical variations. (Lemmatization is more computationally intensive as it requires linguistic context and dictionaries, making it slower, especially on large datasets)\n\n\nToken\nLemma\nStem\n\n\n\ndevelopment\ndevelopment\ndevelop\n\n\nquality\nquality\nqualiti\n\n\nhigh-quality\nhigh-quality\nhigh-qual\n\n\ninclude\ninclude\ninclud\n\n\nlogistics\nlogistic\nlogist\n\n\ngovernment/governance\nGovernemnt/government/governance\ngovern\n\n\n\n\nNOTE: Among word / stems encountered in PDOs, there are a lot of acronyms which may refer to World Bank lingo, or local agencies, etc‚Ä¶ Especially when looked at in low case form they don‚Äôt make much sense‚Ä¶\n\nNotes on sparsity\nSparsity in the context of a document-term matrix refers to the proportion of cells in the matrix that contain zeros. High sparsity means that most terms do not appear in most documents.\n\nremoving stopwords before stemming can reduce sparsity\n\ntidytext::cast_tdm turns a ‚Äútidy‚Äù one-term-per-document-per-row data frame into a Document-Term Matrix (DTM) from the tm package.\n\nthis dataset contains 4403 documents (each of them a PDO) and 11029 terms (distinct words). Notice that this DTM is 100% sparse (100% of document-word pairings are zero, bc most pairings of document and term do not occur (they have the value zero).\n\n\n\n\n# create document-word matrix\nDTM &lt;- pdo_train_t %&gt;% \n   anti_join(custom_stop_words_df, by = c(\"token_l\" = \"word\")) %&gt;% \n   count(proj_id, token_l) %&gt;%\n   tidytext::cast_dtm(proj_id, token_l, n) # HIGH!!!\n\nDTM\n# &lt;&lt;DocumentTermMatrix (documents: 4403, terms: 11029)&gt;&gt;\n# Non-/sparse entries: 129940/48430747\n# Sparsity           : 100%\n# Maximal term length: 34\n# Weighting          : term frequency (tf)\n\nv) Document-term matrix or TF-IDF\n\nThe tf-idf is the product of the term frequency and the inverse document frequency::\n\n\\[\n\\begin{aligned}\ntf(\\text{term}) &= \\frac{n_{\\text{term}}}{n_{\\text{terms in document}}} \\\\\nidf(\\text{term}) &= \\ln{\\left(\\frac{n_{\\text{documents}}}{n_{\\text{documents containing term}}}\\right)} \\\\\ntf\\text{-}idf(\\text{term}) &= tf(\\text{term}) \\times idf(\\text{term})\n\\end{aligned}\n\\]\n‚Äî TF-IDF matrix on train pdo\n\n# reduce size \n\npdo_train_4_tf_idf &lt;- pdo_train_t %&gt;% # 255964\n   # Keep only content words [very restrictive for now]\n   # normally c(\"NOUN\", \"VERB\", \"ADJ\", \"ADV\")\n   filter(upos %in% c(\"NOUN\")) %&gt;% #    72,668 \n   filter(!token_l %in% c(\"development\", \"objective\", \"project\")) %&gt;%   #  66,741\n   # get rid of stop words (from default list)   \n   filter(!token_l %in% custom_stop_words_df$word) %&gt;%   #  66,704\n   # Optional: Remove lemmas of length 1 or shorter\n   filter(nchar(lemma) &gt; 1)  #  66,350\n\nNow, count the occurrences of each lemma for each document. (This is the term frequency or tf)\n\n# This is the term frequency or `tf`\n\n# Count lemmas per document\nlemma_counts &lt;- pdo_train_4_tf_idf %&gt;%\n  count(proj_id, lemma, sort = TRUE)\n# Preview the result\nhead(lemma_counts) \n\nWith the lemma counts prepared, the bind_tf_idf() function from the tidytext package computes the TF-IDF scores.\n\n# Compute the TF-IDF scores\nlemma_tf_idf &lt;- lemma_counts %&gt;%\n  bind_tf_idf(lemma, proj_id, n) %&gt;%\n  arrange(desc(tf_idf))\n\nhead(lemma_tf_idf)\n\n\n\n\n\n\n\nNote\n\n\n\n\n\nWhat to use: token, lemma, or stem?\nGeneral Preference in Real-World NLP:\n\n\nTokens for analyses where word forms matter or for sentiment analysis.\n\nLemmas (*) for most general-purpose NLP tasks where you want to reduce dimensionality while maintaining accuracy and clarity of meaning.\n\nStems for very large datasets, search engines, and applications where speed and simplicity are more important than linguistic precision.\n\n(*) I use lemma, after ‚Äúaggressively‚Äù reducing the number of words to consider, and removing stop words (at least for now)."
  },
  {
    "objectID": "analysis/01b_WB_project_pdo_EDA.html#term-frequency",
    "href": "analysis/01b_WB_project_pdo_EDA.html#term-frequency",
    "title": "WB Project PDO text EDA",
    "section": "Term frequency",
    "text": "Term frequency\nNote: normally, the most frequent words are function words (e.g.¬†determiners, prepositions, pronouns, and auxiliary verbs), which are not very informative. Moreover, even content words (e.g.¬†nouns, verbs, adjectives, and adverbs) can often be quite generic semantically speaking (e.g.¬†‚Äúgood‚Äù may be used for many different things).\nHowever, in this analysis, I do not use the STOPWORD approach, but use the POS tags to reduce ‚Äì in a more controlled way ‚Äì the dataset, filtering the content words such as nouns, verbs, adjectives, and adverbs.\n[FUNC] save plot Ouptput\nPointless bc does not render in the HTML output.\n[FUNC] save plot Object\n[FIG] Overall token freq ggplot\n\nExcluding ‚Äúproject‚Äù ‚Äúdevelop‚Äù,‚Äúobjective‚Äù\n\nIncluding only ‚Äúcontent words‚Äù (NOUN, VERB, ADJ, ADV)\n\n\n# Evaluate the title with glue first\ntitle_text &lt;- glue::glue(\"Most frequent TOKEN in {n_distinct(pdo_train_t$proj_id)} PDOs from projects approved between FY {min(pdo_train_t$FY_appr)}-{max(pdo_train_t$FY_appr)}\") \n\npdo_wrd_freq &lt;- pdo_train_t %&gt;%   # 123,927\n   # include only content words\n   filter(upos %in% c(\"NOUN\", \"VERB\", \"ADJ\", \"ADV\")) %&gt;%\n   #filter (!(upos %in% c(\"AUX\",\"CCONJ\", \"INTJ\", \"DET\", \"PART\",\"ADP\", \"SCONJ\", \"SYM\", \"PART\", \"PUNCT\"))) %&gt;%\n   filter (!(relation %in% c(\"nummod\" ))) %&gt;% # 173,686 \n filter (!(token_l %in% c(\"pdo\",\"project\", \"development\", \"objective\",\"objectives\", \"i\", \"ii\", \"iii\",\n                          \"is\"))) %&gt;% # whne it is VERB\n   count(token_l) %&gt;% \n   filter(n &gt; 800) %&gt;% \n   mutate(token_l = reorder(token_l, n))   # reorder values by frequency\n\n# plot \npdo_wrd_freq_p &lt;- pdo_wrd_freq %&gt;% \n   ggplot(aes(token_l, n)) +\n   geom_col(fill = \"#d7b77b\") +\n   scale_y_continuous(breaks = seq(0, max(pdo_wrd_freq$n), by = 400)) + # directly use 'n' instead of .data$n\n   coord_flip() + # flip x and y coordinates so we can read the words better\n   labs(#title = title_text,\n      subtitle = \"[TOKEN with count &gt; 800]\", y = \"\", x = \"\")+\n   geom_hline(yintercept = 800, linetype = \"dashed\", color = \"#873c4a\") +\n   lulas_theme +\n   theme(# Adjust angle and alignment of x labels\n      axis.text.x = element_text(angle = 45, hjust = 1)) \n\n[FIG] Overall stem freq ggplot\n\nWithout ‚Äúproject‚Äù ‚Äúdevelop‚Äù,‚Äúobjective‚Äù\n\nIncluding only ‚Äúcontent words‚Äù (NOUN, VERB, ADJ, ADV)\n\n\n# Evaluate the title with glue first\ntitle_text &lt;- glue::glue(\"Most frequent STEM in {n_distinct(pdo_train_t$proj_id)} PDOs from projects approved between FY {min(pdo_train_t$FY_appr)}-{max(pdo_train_t$FY_appr)}\") \n\n# Plot\npdo_stem_freq &lt;- pdo_train_t %&gt;%   # 256,632\n   # include only content words\n   filter(upos %in% c(\"NOUN\", \"VERB\", \"ADJ\", \"ADV\")) %&gt;%\n   filter (!(relation %in% c(\"nummod\" ))) %&gt;% # 173,686 \n   filter (!(stem %in% c(\"pdo\", \"project\", \"develop\", \"object\", \"i\", \"ii\", \"iii\"))) %&gt;%\n   count(stem) %&gt;% \n   filter(n &gt; 800) %&gt;%\n   mutate(stem = reorder(stem, n))    # reorder values by frequency\n   \n# plot \npdo_stem_freq_p &lt;-    pdo_stem_freq %&gt;% \n   ggplot(aes(stem, n)) +\n   geom_col(fill = \"#d7b77b\") +\n   scale_y_continuous(breaks = seq(0, max(pdo_stem_freq$n), by = 400)) + # directly use 'n' instead of .data$n\n   coord_flip() + # flip x and y coordinates so we can read the words better\n   labs(#title = title_text,\n      subtitle = \"[STEM with count &gt; 800]\", y = \"\", x = \"\") +\n   geom_hline(yintercept = 800, linetype = \"dashed\", color = \"#873c4a\") +\n   lulas_theme +\n   theme(# Adjust angle and alignment of x labels\n      axis.text.x = element_text(angle = 45, hjust = 1)) \n\n\nEvidently, after stemming, more words (or stems) reach the threshold frequency count of 800 (they have been combined by root).\n\n[FIG] token + stem freq ggplot\n\ntitle2_text &lt;- glue::glue(\"Most frequent TOKEN & STEM in {n_distinct(pdo_train_t$proj_id)} PDOs\") \n\nsubtitle2_text &lt;- glue::glue(\"From projects approved between FY {min(pdo_train_t$FY_appr)}-{max(pdo_train_t$FY_appr)}\") \n\ncombo_freq_p &lt;-  pdo_wrd_freq_p + pdo_stem_freq_p + \n   plot_annotation(title = title2_text,\n                    subtitle = subtitle2_text,\n                   # caption = \"Source: World Bank Project Documents\",\n                   theme = theme(plot.title = element_text(size = 12, face = \"bold\"),\n                                 plot.subtitle = element_text(size = 10, face = \"italic\"),\n                                 plot.caption = element_text(size = 10, face = \"italic\"))\n                   )  \n\ncombo_freq_p\n\n\n\n\n\n\n\n\n#f_save_plot(\"combo_freq_p\", combo_freq_p)\nf_save_plot_obj(combo_freq_p, \"combo_freq_p\")"
  },
  {
    "objectID": "analysis/01b_WB_project_pdo_EDA.html#sector-related-term-frequency",
    "href": "analysis/01b_WB_project_pdo_EDA.html#sector-related-term-frequency",
    "title": "WB Project PDO text EDA",
    "section": "SECTOR-related term frequency",
    "text": "SECTOR-related term frequency\nIsolate SECTOR words and see frequency over years\nTo try and make it a bit more meaningful, let‚Äôs focus on the frequency of the most common words related to SECTORS.\nFrom token_l, I created a ‚Äúbroad SECTOR‚Äù variable to group the sectors in broader definitions:\n\n\nWAT_SAN = water|wastewater|sanitat|Sewer|sewage|Irrigat|Drainag|river basin|groundwater\n\nTRANSPORT = transport|railway|road|airport|waterway|bus|metropolitan|inter-urban|aviation|highway|transit|bridge|port\n\nURBAN = urban|housing|inter-urban|peri-urban|waste manag|slum|city|megacity|intercity|inter-city|town\n\nENERGY = energ|electri|hydroele|hydropow|renewable|transmis|grid|transmission|electric power|geothermal|solar|wind|thermal|nuclear power|energy generation\n\nHEALTH = health|hospital|medicine|drugs|epidem|pandem|covid-19|vaccin|immuniz|diseas|malaria|HIV|AIDS|TB|maternal|clinic|nutrition\n\nEDUCATION = educat|school|vocat|teach|univers|student|literacy|training|curricul|pedagog\n\nAGR_FOR_FISH (Agriculture, Forestry, Fishing) = Agricultural|Agro|Fish|Forest|Crop|livestock|fishery|land|soil\n** MINING_OIL_GAS** = Minin|oil|gas|mineral|quarry|extract|coal|natural gas|mine|petroleum|hydrocarbon\n\nSOCIAL_PROT = Social Protec|social risk|social assistance|living standard|informality|insurance|social choesion|gig economy|human capital|employment|unemploy|productivity|wage lev|intergeneration|lifelong learn|vulnerab|empowerment|sociobehav\n\nFINANCIAL = Bank|finan|Investment|credit|microfinan|loan|financial stability|banking|financial intermed|fintech\n\nICT = Information|Communication|ICT|Internet|telecom|cyber|data|AI|artificial intelligence|blockchain|e-learn|e-commerce|platform|software|hardware|digital\n\nIND_TRADE_SERV = Industry|Trade|Service|manufactur|Tourism|Trade and Services|market|export|import|supply chain|logistic|distribut|e-commerce|retail|wholesale|trade facilitation|trade policy|trade agreement|trade barrier|trade finance|trade promotion|trade integration|trade liberalization|trade balance|trade deficit|trade surplus|trade war|trade dispute|trade negotiation|trade cooperation|trade relation|trade partner|trade route|trade corridor\n\n‚ÄúINSTIT_SUPP‚Äù = Government|Public Admin|Institution|Central Agenc|Sub-national Gov|law|justice|governance|Policy|Regulation|Public Expenditure|Public Investment|Public Procurement\n\n‚ÄúGENDER_EQUAL‚Äù = Gender|Women|Girl|Woman|femal|Gender Equal|gender-base|gender inclus|gender mainstream|gender sensit|gender respons|gender gap|gender-based|gender-sensitive|gender-responsive|gender-transform|gender-equit|gender-balance\n\n‚ÄúCLIMATE‚Äù = Climate|Environment|Sustain|Resilience|Adaptation|Mitigation|Green|Eco|Eco-|carbon|carbon cycle|carbon dioxide|climate change|ecosystem|emission|energy effic|greenhouse|greenhouse gas|temperature anomalies|zero net|green growth|low carbon|climate resilient|climate smart|climate tech|climate variab\n\n\npdo_train_t &lt;- pdo_train_t %&gt;%\n   # dealing with water/watershed/waterway\n   mutate(tok_sector_broad = case_when(\n      stringr::str_detect(token_l, regex(\"water|wastewater|sanitat|Sewer|sewage|Irrigat|Drainag|river basin|groundwater\", ignore_case = T)) ~ \"WAT_SAN\",\n      stringr::str_detect(token_l, regex(\"transport|railway|road|airport|waterway|bus|metropolitan|inter-urban|aviation|highway|transit|bridge|port\", ignore_case = T)) ~ \"TRANSPORT\",\n      stringr::str_detect(token_l, regex(\"urban|housing|inter-urban|peri-urban|waste manag|slum|city|megacity|intercity|inter-city|town\", ignore_case = T)) ~ \"URBAN\",\n      stringr::str_detect(token_l, regex(\"energ|electri|hydroele|hydropow|renewable|transmis|grid|transmission|electric power|geothermal|solar|wind|thermal|nuclear power|energy generation\", ignore_case = T)) ~ \"ENERGY\",   \n      stringr::str_detect(token_l, regex(\"health|hospital|medicine|drugs|epidem|pandem|covid-19|vaccin|immuniz|diseas|malaria|HIV|AIDS|TB|maternal|clinic|nutrition\", ignore_case = T))  ~ \"HEALTH\",\n      stringr::str_detect(token_l, regex(\"educat|school|vocat|teach|univers|student|literacy|training|curricul|pedagog\", ignore_case = T)) ~ \"EDUCATION\",\n      # not infra \n      stringr::str_detect(token_l, regex(\"Agricultural|Agro|Fish|Forest|Crop|livestock|fishery|land|soil\", ignore_case = T)) ~ \"AGR_FOR_FISH\",\n      stringr::str_detect(token_l, regex(\"Minin|oil|gas|mineral|quarry|extract|coal|natural gas|mine|petroleum|hydrocarbon\", ignore_case = T)) ~ \"MINING_OIL_GAS\",\n      stringr::str_detect(token_l, regex(\"Social Protec|social risk|social assistance|living standard|informality|insurance|social choes|gig economy|human capital|employment|unemploy|productivity|wage lev|intergeneration|lifelong learn|vulnerab|empowerment|sociobehav\", ignore_case = T)) ~ \"SOCIAL_PROT\",\n      stringr::str_detect(token_l, regex(\"Bank|finan|Investment|credit|microfinan|loan|financial stability|banking|financial intermed|fintech\", ignore_case = T)) ~ \"FINANCIAL\",\n      stringr::str_detect(token_l, regex(\"Information|Communication|ICT|Internet|telecom|cyber|data|AI|artificial intelligence|blockchain|e-learn|platform|software|hardware|digital\", ignore_case = T)) ~ \"ICT\",\n      stringr::str_detect(token_l, regex(\"Industry|Trade|Service|manufactur|Tourism|Trade and Services|market|export|import|supply chain|logistic|distribut|e-commerce|retail|wholesale|trade facilitation|trade policy|trade agreement|trade barrier|trade finance|trade promotion|trade integration|trade liberalization|trade balance|trade deficit|trade surplus|trade war|trade dispute|trade negotiation|trade cooperation|trade relation|trade partner|trade route|trade corridor\", ignore_case = T)) ~ \"IND_TRADE_SERV\",\n      stringr::str_detect(token_l, regex(\"Government|Public Admin|Institution|Central Agenc|Sub-national Gov|law|justice|governance|Policy|Regulation|Public Expenditure|Public Investment|Public Procurement\", ignore_case = T)) ~ \"INSTIT_SUPP\",\n      stringr::str_detect(token_l, regex(\"Gender|Women|Girl|Woman|femal|Gender Equal|gender-base|gender inclus|gender mainstream|gender sensit|gender respons|gender gap|gender-based|gender-sensitive|gender-responsive|gender-transform|gender-equit|gender-balance\", ignore_case = T)) ~  \"GENDER_EQUAL\" ,\n            stringr::str_detect(token_l, regex(\"Climate chan|Environment|Sustain|Resilience|Adaptation|Mitigation|Green|Eco|Eco-|carbon|carbon cycle|carbon dioxide|climate change|ecosystem|emission|energy effic|greenhouse|greenhouse gas|temperature anomalies|zero net|green growth|low carbon|climate resilient|climate smart|climate tech|climate variab\", ignore_case = T)) ~ \"CLIMATE\" ,\n      TRUE ~ NA_character_)) %&gt;% \n   relocate(tok_sector_broad, .after = token_l) # move the new column to the right of token_l\n\nData prep for sector plots\n\ntabyl(pdo_train_t$tok_sector_broad)\n\n# Create a custom color list for each sector\nsector_colors &lt;- c(\n   \"WAT_SAN\" = \"#26BDE2\",  # SDG 6\n   \"ENERGY\" = \"#FCC30B\", # 7SDG \n   \"MINING_OIL_GAS\" = \"#23399b\", # no SDG! \n   \"URBAN\" = \"#FD9D24\", # SDG 11\n   \"ICT\" = \"#0f7184\",    # no SDG!\n   \"HEALTH\" = \"#4C9F38\", # SDG 3\n   \"EDUCATION\" = \"#C5192D\", # SDG 4\n   # SDGS \n   \"POVERTY\" = \"#E5243B\", # SDG 1\n   \"ZERO_HUNGER\" = \"#DDA63A\", # SDG 2\n   \"GENDER_EQUAL\" = \"#FF3A21\", # SDG 5\n   \"WORK\" = \"#A21942\", # SDG 8\n   \"INDUSTRY\" = \"#FD9D24\", # SDG 9\n   \"INEQUALITY\" =  \"#DD1367\", # SDG 10\n   \"CONSUMPTION\" = \"#BF8B2E\", # SDG 12\n   \"CLIMATE\" = \"#3F7E44\", # SDG 13\n   \"OCEANS\" = \"#0A97D9\", # SDG 14\n   \"BIODIVERSITY\" = \"#56C02B\", # SDG 15\n   \"PEACE\" = \"#00689D\", # SDG 16\n   \"PARTNERSHIP\" = \"#19486A\", # SDG 17\n   # MINE\n   \"TRANSPORT\" =  \"#A6A6A6\", \n   \"AGR_FOR_FISH\" = \"#56C02B\" , \n   \"SOCIAL_PROT\" = \"#e28293\", \n   \"FINANCIAL\" =  \"#e60066\",\n   \"IND_TRADE_SERV\" = \"#85239b\",\n   \"INSTIT_SUPP\" = \"#49239b\"\n   )\n\n# prepare data for plotting (count)\nsector_broad_pdo &lt;- pdo_train_t %&gt;% \n   filter(!is.na(tok_sector_broad)) %&gt;% \n   filter(tok_sector_broad %in% c(\"WAT_SAN\", \"ENERGY\", \"TRANSPORT\", \"URBAN\", \"MINING_OIL_GAS\", \"ICT\", \"HEALTH\", \"EDUCATION\", \n                                  # not infrastructure\n                                  \"AGR_FOR_FISH\", \"GENDER_EQUAL\", \"CLIMATE\",  \"SOCIAL_PROT\", \"FINANCIAL\", \"IND_TRADE_SERV\", \"INSTIT_SUPP\" )) %&gt;%\n   count(tok_sector_broad, FY_appr) %&gt;% \n   #filter(n &gt; 0) %&gt;% \n   mutate(tok_sector_broad = factor(tok_sector_broad, levels = c(\n      \"WAT_SAN\", \"ENERGY\", \"TRANSPORT\",\"URBAN\",\"MINING_OIL_GAS\",\"ICT\", \"HEALTH\", \"EDUCATION\",\n      # not infrastructure\n       \"AGR_FOR_FISH\", \"GENDER_EQUAL\", \"CLIMATE\",  \"SOCIAL_PROT\", \"FINANCIAL\", \"IND_TRADE_SERV\", \"INSTIT_SUPP\"))) # reorder values by frequency\n#df$FY\n\n[FIG] faceted sector (tok_sector_broad) freq ggplot\n\n# Evaluate the title with glue first\ntitle_text &lt;- glue::glue(\"Sector words frequency in PDO over FY {min(pdo_train_t$FY_appr)}-{max(pdo_train_t$FY_appr)}\") \n\n# Plot\npdo_sect_broad_freq &lt;- sector_broad_pdo %&gt;% \n   # only the \"original group\n  filter(tok_sector_broad %in% c(\"WAT_SAN\", \"ENERGY\", \"TRANSPORT\", \"URBAN\", \"MINING_OIL_GAS\", \"ICT\", \"HEALTH\", \"EDUCATION\" )) %&gt;% \n   ggplot(.,\n          aes(x = FY_appr, y = n, \n              group = tok_sector_broad, color = tok_sector_broad)) +\n   geom_line(linetype = \"dotted\", alpha = 0.5, size = 1) +\n   geom_point(size = 2) +\n   scale_x_continuous(breaks =  seq(2001, 2023, by=  1)) +\n   scale_y_continuous(breaks =  seq(0,300, by=  25)) +\n   # ~ SDG colors \n    # scale_color_viridis_d(option = \"magma\", end = 0.9) + \n   scale_color_manual(values = sector_colors) +\n   facet_wrap(~tok_sector_broad, ncol = 3, scales = \"free\")+ \n   guides(color = FALSE) +\n   lulas_theme +\n   theme(# Adjust angle and alignment of x labels\n      axis.text.x = element_text(angle = 45, hjust = 1)) + \n   labs(title = \"Sector words frequency in PDOs by fiscal years of approval\",\n        subtitle = \"[Using \\\"custom\\\" broad sector definition]\",\n        x =  \"\",# \"Board approval FY\", \n        y = \"\"#\"Counts of 'sector' word (tok_sector_broad)\"\n   ) + \n   # Add the reference line at y = 50, red, dashed, and transparent (50% opacity)\n   geom_hline(yintercept = 50, linetype = \"longdash\", color = \"#d02e4c\", alpha = 0.40)   \n   # geom_vline(data = subset(sector_broad_pdo, tok_sector_broad == \"HEALTH\"), \n   #            aes(xintercept = 2020), \n   #            linetype = \"solid\", color = \"#9b6723\",alpha = 0.35) +\n   # geom_text(data = subset(sector_broad_pdo, tok_sector_broad == \"HEALTH\"), \n   #           aes(x = 2020, y = max(sector_broad_pdo$n)*0.65, label = \"Covid\"), \n   #           angle = 90, vjust = -0.5, color = \"#9b6723\") \n\npdo_sect_broad_freq\n\n\n\n\n\n\n\n[FUNC] Figure split sector (tok_sector_broad) freq ggplot\n\n# --- Get a LIST of unique sectors (facets) and split the data\nPDOsector_list &lt;- base::split(x = sector_broad_pdo, f = sector_broad_pdo$tok_sector_broad)\n\n# --- Create a function to plot for each sector with custom colors\nf_plot_sector &lt;- function(data) {\n   # Get the sector name\n   sector &lt;- unique(data$tok_sector_broad)\n   # Create the plot\n   p &lt;-ggplot(data = data, \n              aes(x = FY_appr, y = n, \n                  group = tok_sector_broad, color = tok_sector_broad)) +\n      # By sector ... \n      geom_line(color = sector_colors[sector], linetype = \"dotted\", alpha = 0.5, size = 1) +   \n      geom_point(color = sector_colors[sector], size = 3) +               \n      scale_x_continuous(breaks = seq(2001, 2023, by = 1)) +\n      scale_y_continuous(breaks = seq(0, max(data$n), by = 25)) +\n      # custom\n      lulas_theme + \n      theme(axis.text.x = element_text(angle = 45, hjust = 1)) +\n      labs(\n         title = paste(\"\\\"\",sector,\"\\\" in PDOs by fiscal years of approval\"),  # Use facet-specific title\n         subtitle = \"[Using a \\\"custom\\\" broad sector definition &\\nshowing relevant WDR publication(s)]\",\n         x = \"\", \n         y = \"\"  # Remove y-axis label\n      ) +\n      # Ensure y-axis limit includes 50\n      expand_limits(y = 50) + \n      # Add the reference line at y = 50, red, dashed, and transparent (50% opacity)\n      geom_hline(yintercept = 50, linetype = \"longdash\", color = \"#d02e4c\", alpha = 0.75)\n   # # Add vline and text annotation only for the HEALTH sector\n   if (sector == \"HEALTH\") {\n      p &lt;- p +\n         geom_vline(aes(xintercept = 2020), linetype = \"solid\", color = \"#9b6723\",alpha = 0.35) +\n         geom_text(aes(x = 2020, y = max(n) * 0.75, label = \"Covid\"),\n                   angle = 90, vjust = -0.5, color = \"#9b6723\")\n   }\n\n   return(p)\n}\n\n\n# --- Use purrr::map to create a LIST of plots, one for each sector\nsector_plots &lt;- map(PDOsector_list, f_plot_sector)\n\n# --- (exe) Extract the first plot to display\n#sector_plots$HEALTH\n\n# ---- Optionally print each plot to the console\n#walk(sector_plots, print)\n\n\n# Define the output directory using the 'here' function\noutput_dir &lt;- here(\"analysis\", \"output\", \"figures\")\n\n## Save each plot to a file in the specified directory\n#walk2(sector_plots, names(PDOsector_list), \n#      ~ggsave(filename = file.path(output_dir, paste0(.y, \"_sector_plot.png\")), plot = .x))\n\n# Save iteratively the plots' objects as RDS files\nwalk2(sector_plots, names(PDOsector_list), \n      ~f_save_plot_obj(.x, paste0(\"pdo_\", .y, \"_sect_p\")))\n\n#pdo_FINANCIAL_sect_p"
  },
  {
    "objectID": "analysis/01b_WB_project_pdo_EDA.html#sector-in-pdo-v.-wdr-publications",
    "href": "analysis/01b_WB_project_pdo_EDA.html#sector-in-pdo-v.-wdr-publications",
    "title": "WB Project PDO text EDA",
    "section": "SECTOR in PDO v. WDR publications",
    "text": "SECTOR in PDO v. WDR publications\nFor the (broadly defined) HEALTH sector, it is quite clear that Covid-19 is the main driver of the peak in 2020.\nWhat about the other sectors? I was struck by the fact that, observing PDOs over time, the broadly defined ‚Äúsector term‚Äù in the PDO always presents at least one peak and I wonder what could trigger it.\nOne possible explanation is that the PDOs somehow reflect the topics discussed by the World Development Reports (WDR) published annually by the World Bank. The WDR is a flagship publication of the World Bank that provides in-depth analysis of a specific aspect of development.\nIt is important to remark that these publications are not some speculative research endeavor, as they are deeply rooted in the concrete information that the Bank retrieves on the ground from projects and operations as they are supported and evaluated. In turn, the WDRs themselves inform the Bank‚Äôs policy priorities and operational strategies.\nTherefore, it is reasonable to expect some kind of correlation between the topics discussed in the WDRs and the objectives of projects stated in in the PDOs."
  },
  {
    "objectID": "analysis/01b_WB_project_pdo_EDA.html#ingest-wdr-data",
    "href": "analysis/01b_WB_project_pdo_EDA.html#ingest-wdr-data",
    "title": "WB Project PDO text EDA",
    "section": "Ingest WDR data",
    "text": "Ingest WDR data\nPreviously created, as explained in data/derived_data/_provenance.md\n\n# Read the WDR data\nwdr &lt;- readRDS(here(\"data\",\"derived_data\", \"wdr.rds\"))\n\n‚Äî Manually add WDR 2023 ‚úçüèª\nOKR Full item\n\nlibrary(tibble) # Simple Data Frames # Simple Data Frames\n\n# Create a named list of NA values for subj_11 to subj_46\nna_values &lt;- setNames(rep(NA, 35), paste0(\"subj_\", 12:46))\n\n# Add a new row with the existing columns and NA for subj_11 to subj_46\nwdr &lt;- wdr %&gt;%\n  add_row(\n    date_issued = 2023,\n    decade = \"2020s\",\n    id = NA, # ? \n    ISBN = \"978-1-4648-1941-4\",\n    title = \"Migrants, Refugees, and Societies\",\n    doc_mt_identifier_1 = \"oai:openknowledge.worldbank.org:10986/39696\", #? \n    subject_miss = NA,\n    abstract = \"Migration is a development challenge. About 184 million people-2.3 percent of the world‚Äôs population-live outside of their country of nationality. Almost half of them are in low- and middle-income countries. But what lies ahead? As the world struggles to cope with global economic imbalances, diverging demographic trends, and climate change, migration will become a necessity in the decades to come for countries at all levels of income. If managed well, migration can be a force for prosperity and can help achieve the United Nations‚Äô Sustainable Development Goals. World Development Report 2023 proposes an innovative approach to maximize the development impacts of cross-border movements on both destination and origin countries and on migrants and refugees themselves. The framework it offers, drawn from labor economics and international law, rests on a ‚ÄúMatch and Motive Matrix‚Äù that focuses on two factors: how closely migrants‚Äô skills and attributes match the needs of destination countries and what motives underlie their movements. This approach enables policy makers to distinguish between different types of movements and to design migration policies for each. International cooperation will be critical to the effective management of migration.\",\n    url_keys = \"https://openknowledge.worldbank.org/handle/10986/39696\",\n    altmetric = 150,\n    all_topic = \"Poverty Reduction,Social Development,Conflict and Development\",\n    all_subj = \"migration,migrants,refugees,force displacement,crss-border mobility,remittances,origin country,international protection,refugee-hosting country,irregular migration,international cooperation\",\n    subj_1 = \"migration\",\n    subj_2 = \"migrants\",\n    subj_3 = \"refugees\",\n    subj_4 = \"force displacement\",\n    subj_5 = \"crss-border mobility\",\n    subj_6 = \"remittances\",\n    subj_7 = \"origin country\",\n    subj_8 = \"international protection\",\n    subj_9 = \"refugee-hosting country\",\n    subj_10 = \"irregular migration\",\n    subj_11 = \"international cooperation\",\n    !!!na_values  # Unpack the NA values for subj_12 to subj_46\n  )\n\n‚Äî Manually add WDR 2024 ‚úçüèª\nOKR Full item\n\nlibrary(tibble) # Simple Data Frames # Simple Data Frames\n\n# Create a named list of NA values for subj_11 to subj_46\nna_values &lt;- setNames(rep(NA, 35), paste0(\"subj_\", 12:46))\n# https://documents.worldbank.org/en/publication/documents-reports/documentdetail/099042523192514880/p17826903573340450b2d00e8cfd3baf7ac\n# https://openknowledge.worldbank.org/entities/publication/5e5ac9f1-71ee-4734-825e-60966658395f/full\n\n# Add a new row with the existing columns and NA for subj_11 to subj_46\nwdr &lt;- wdr %&gt;%\n  add_row(\n    date_issued = 2024,\n    decade = \"2020s\",\n    id = NA, # ? \n    ISBN = \"978-1-4648-2078-6\",\n    title = \"The Middle-Income Trap\",\n    doc_mt_identifier_1 = \"oai:openknowledge.worldbank.org:10986/41919\", #? \n    subject_miss = NA,\n    abstract = \"Middle-income countries are in a race against time. Many of them have done well since the 1990s to escape low-income levels and eradicate extreme poverty, leading to the perception that the last three decades have been great for development. But the ambition of the more than 100 economies with incomes per capita between US$1,100 and US$14,000 is to reach high-income status within the next generation. When assessed against this goal, their record is discouraging. Since the 1970s, income per capita in the median middle-income country has stagnated at less than a tenth of the US level. With aging populations, growing protectionism, and escalating pressures to speed up the energy transition, today‚Äôs middle-income economies face ever more daunting odds. To become advanced economies despite the growing headwinds, they will have to make miracles. Drawing on the development experience and advances in economic analysis since the 1950s, World Development Report 2024 identifies pathways for developing economies to avoid the ‚Äúmiddle-income trap.‚Äù It points to the need for not one but two transitions for those at the middle-income level: the first from investment to infusion and the second from infusion to innovation. Governments in lower-middle-income countries must drop the habit of repeating the same investment-driven strategies and work instead to infuse modern technologies and successful business processes from around the world into their economies. This requires reshaping large swaths of those economies into globally competitive suppliers of goods and services. Upper-middle-income countries that have mastered infusion can accelerate the shift to innovation‚Äînot just borrowing ideas from the global frontiers of technology but also beginning to push the frontiers outward. This requires restructuring enterprise, work, and energy use once again, with an even greater emphasis on economic freedom, social mobility, and political contestability. Neither transition is automatic. The handful of economies that made speedy transitions from middle- to high-income status have encouraged enterprise by disciplining powerful incumbents, developed talent by rewarding merit, and capitalized on crises to alter policies and institutions that no longer suit the purposes they were once designed to serve. Today‚Äôs middle-income countries will have to do the same.\",\n    url_keys = \"https://openknowledge.worldbank.org/handle/10986/41919\",\n    altmetric = 13,\n   all_topic = \"Macroeconomics,Economic Growth,Business Cycles and Stabilization Policies,Poverty Reduction,Achieving Shared Growth,Science and Technology Development,Innovation\",\n    all_subj = \"middle-income trap,investment,infusion,innovation,technologies,competitive suppliers,economic freedom\",\n    subj_1   = \"middle-income trap\",\n    subj_2   = \"investment\",\n    subj_3   = \"infusion\",\n    subj_4   = \"innovation\",\n    subj_5   = \"technologies\",\n    subj_6   = \"competitive suppliers\",\n    subj_7   = \"economic freedom\",\n    subj_8   = NA,\n    subj_9   = NA,\n    subj_10  = NA,\n    subj_11  = NA,\n    !!!na_values  # Unpack the NA values for subj_12 to subj_46\n  )\n\n‚Äî Manually correct WDR 2011 ‚úçüèª\n\n\nwdr$url_keys [wdr$id == \"4389\"] &lt;- \"https://openknowledge.worldbank.org/handle/10986/4389\"\n\nwdr$altmetric [wdr$id == \"4389\"] &lt;- \"210\"\n\nwdr$abstract [wdr$id == \"4389\"] &lt;- \"The 2011 World development report looks across disciplines and experiences drawn from around the world to offer some ideas and practical recommendations on how to move beyond conflict and fragility and secure development. The key messages are important for all countries-low, middle, and high income-as well as for regional and global institutions: first, institutional legitimacy is the key to stability. When state institutions do not adequately protect citizens, guard against corruption, or provide access to justice; when markets do not provide job opportunities; or when communities have lost social cohesion-the likelihood of violent conflict increases. Second, investing in citizen security, justice, and jobs is essential to reducing violence. But there are major structural gaps in our collective capabilities to support these areas. Third, confronting this challenge effectively means that institutions need to change. International agencies and partners from other countries must adapt procedures so they can respond with agility and speed, a longer-term perspective, and greater staying power. Fourth, need to adopt a layered approach. Some problems can be addressed at the country level, but others need to be addressed at a regional level, such as developing markets that integrate insecure areas and pooling resources for building capacity Fifth, in adopting these approaches, need to be aware that the global landscape is changing. Regional institutions and middle income countries are playing a larger role. This means should pay more attention to south-south and south-north exchanges, and to the recent transition experiences of middle income countries.\"\n\nwdr$all_topic [wdr$id == \"4389\"] &lt;- tolower(\"Justice,Jobs,Political Violence and Civil War,Political Violence and War,Organized Crime,Fragility,Conflict and Violence,Crime,Social Cohesion,Public Sector Management,Social Development,Law and Development, Social Protections and Labor,Conflict and Development,Water Supply and Sanitation,Judicial System Reform, Labor Markets,Armed Conflict,Urban Solid Waste Management\") \n\n# Define the subjects to be added for the specific row\nsubjects &lt;- c(\n  \"Armed Conflict\",\n  \"Civil Wars\",\n  \"Conflict Prevention\",\n  \"Conflict Resolution\",\n  \"Development Policy\",\n  \"Fragile States\",\n  \"International Development\",\n  \"Peacebuilding\",\n  \"Political Instability\",\n  \"Post-Conflict Reconstruction\",\n  \"Security and Development\"\n) %&gt;% tolower()  # Convert subjects to lowercase\n\n# Ensure id is handled as character and enforce lowercase comparison\nwdr &lt;- wdr %&gt;%\n   mutate(across(starts_with(\"subj_\"),\n                 ~ ifelse(id == \"4389\", \n                          subjects[as.numeric(sub(\"^subj_\", \"\", cur_column()))], \n                          NA_character_))) %&gt;% \n   mutate (all_subj = if_else(id == \"4389\", paste0(subjects, collapse = \",\"), all_subj)) \n\n# Check the result for the row with id == \"4389\"\nwdr %&gt;% filter(id == \"4389\") %&gt;% select(starts_with(\"subj_\"))  # Display the updated subject columns\n\n\n# check &lt;- wdr[wdr$id == \"4389\",] \n\n‚Äî Remove extra space in title column\n\n# Check and remove leading space in the 'title' column\nwdr &lt;- wdr %&gt;%\n  mutate(title = str_trim(title, side = \"left\"))\n\n‚Äî Re-save (upon correction) wrd2.rds\n\n\nwdr2 &lt;-  wdr\nwrite_rds(x = wdr2, file = here::here(\"data\", \"derived_data\",\"wdr2.rds\"))\n\n[TBL] World Develompent Reports 2000-2024\nBelow are the titles of the World Development Reports from 2000 to 2024.\n\n\n\n\n\ndate_issued\ntitle\nurl_keys\n\n\n\n2001\nAttacking Poverty\nhttps://openknowledge.worldbank.org/handle/10986/11856?show=full\n\n\n2002\nBuilding Institutions for Markets\nhttps://openknowledge.worldbank.org/handle/10986/5984?show=full\n\n\n2003\nSustainable Development in a Dynamic World--Transforming Institutions, Growth, and Quality of Life\nhttps://openknowledge.worldbank.org/handle/10986/5985?show=full\n\n\n2004\nMaking Services Work for Poor People\nhttps://openknowledge.worldbank.org/handle/10986/5986?show=full\n\n\n2005\nA Better Investment Climate for Everyone\nhttps://openknowledge.worldbank.org/handle/10986/5987?show=full\n\n\n2006\nEquity and Development\nhttps://openknowledge.worldbank.org/handle/10986/5988?show=full\n\n\n2007\nDevelopment and the Next Generation\nhttps://openknowledge.worldbank.org/handle/10986/5989?show=full\n\n\n2008\nAgriculture for Development\nhttps://openknowledge.worldbank.org/handle/10986/5990?show=full\n\n\n2009\nReshaping Economic Geography\nhttps://openknowledge.worldbank.org/handle/10986/5991?show=full\n\n\n2010\nDevelopment and Climate Change\nhttps://openknowledge.worldbank.org/handle/10986/4387?show=full\n\n\n2011\nConflict, Security, and Development\nhttps://openknowledge.worldbank.org/handle/10986/4389\n\n\n2012\nGender Equality and Development\nhttps://openknowledge.worldbank.org/handle/10986/4391?show=full\n\n\n2013\nJobs\nhttps://openknowledge.worldbank.org/handle/10986/11843?show=full\n\n\n2014\nRisk and Opportunity‚ÄîManaging Risk for Development\nhttps://openknowledge.worldbank.org/handle/10986/16092?show=full\n\n\n2015\nMind, Society, and Behavior\nhttps://openknowledge.worldbank.org/handle/10986/20597?show=full\n\n\n2016\nDigital Dividends\nhttps://openknowledge.worldbank.org/handle/10986/23347?show=full\n\n\n2017\nGovernance and the Law\nhttps://openknowledge.worldbank.org/handle/10986/25880?show=full\n\n\n2018\nLearning to Realize Education's Promise\nhttps://openknowledge.worldbank.org/handle/10986/28340?show=full\n\n\n2019\nThe Changing Nature of Work\nhttps://openknowledge.worldbank.org/handle/10986/30435?show=full\n\n\n2020\nTrading for Development in the Age of Global Value Chains\nhttps://openknowledge.worldbank.org/handle/10986/32437?show=full\n\n\n2021\nData for Better Lives\nhttps://openknowledge.worldbank.org/handle/10986/35218?show=full\n\n\n2022\nFinance for an Equitable Recovery\nhttps://openknowledge.worldbank.org/handle/10986/36883?show=full\n\n\n2023\nMigrants, Refugees, and Societies\nhttps://openknowledge.worldbank.org/handle/10986/39696\n\n\n2024\nThe Middle-Income Trap\nhttps://openknowledge.worldbank.org/handle/10986/41919\n\n\n\n\n\n\nQualify: peak or trend (by sector)"
  },
  {
    "objectID": "analysis/01b_WB_project_pdo_EDA.html#add-geomvline-to-sector-plots-v-wdr-title-cmpl",
    "href": "analysis/01b_WB_project_pdo_EDA.html#add-geomvline-to-sector-plots-v-wdr-title-cmpl",
    "title": "WB Project PDO text EDA",
    "section": "Add geomvline to sector plots v WDR title [CMPL üü†]",
    "text": "Add geomvline to sector plots v WDR title [CMPL üü†]\n\ntabyl(pdo_train_t$tok_sector_broad)\n# pdo_train_t$tok_sector_broad      n  WDR \n\n#                  AGR_FOR_FISH    665 WDR 2008  Agriculture for Development\n#                     EDUCATION   1180 WDR 2004 Making Services Work for Poor People\n#                        ENERGY    886 WDR \n#                     FINANCIAL   1843 WDR \n#                  GENDER_EQUAL    213 WDR 2012  Gender Equality and Development\n#                        HEALTH    946 WDR \n#                           ICT    548 WDR \n#                IND TRADE SERV     60 WDR \n#           INSTITUTIONAL SUPP.   2171 WDR \n#                 MINING_OIL_GAS    299 WDR \n#                     TRANSPORT   1371 WDR \n#                         URBAN    553 WDR \n#                       WAT_SAN   1069 WDR \n\n‚Äî ‚úÖ AGR_FOR_FISH ( Agriculture, forestry, and fishing)\nThe WDR of 2008 was titled ‚ÄúAgriculture for Development‚Äù, link\n\n# --- Get a LIST of unique sectors (facets) and split the data\nPDOsector_list &lt;- base::split(x = sector_broad_pdo, f = sector_broad_pdo$tok_sector_broad)\n# Specific split df \n#PDOsector_list$'AGR_FOR_FISH'\n\n# Specific plot \npdo_agr_WDR_plot &lt;- sector_plots$'AGR_FOR_FISH' +  \n  geom_vline(xintercept = 2008, linetype = \"solid\", color = \"#9b6723\",alpha = 0.35) +\n  geom_text(aes(x = 2008, y = max(n) * 0.5, label = \"WDR Agric\"), \n                   angle = 90, vjust = -0.5, color = \"#9b6723\")\npdo_agr_WDR_plot\n\n\n\n\n\n\n\n\n#f_save_plot(\"pdo_agr_plot\", pdo_agr_plot)\nf_save_plot_obj (pdo_agr_WDR_plot, \"pdo_agr_WDR_plot\")\n\n‚Äî ‚úÖ EDUCATION\nWDR 2007 was titled ‚ÄúDevelopment and the Next Generation‚Äù WDR 2018 was titled ‚ÄúLearning to Realize Education‚Äôs Promise‚Äù\n\n# Specific split df \n#PDOsector_list$EDUCATION\n\n# Specific plot \npdo_edu_WDR_plot &lt;- sector_plots$EDUCATION + \n  geom_vline(xintercept = 2007, linetype = \"solid\", color = \"#9b6723\",alpha = 0.35) +\n  geom_text(aes(x = 2007, y = max(n) * 0.45, label = \"WDR Youth\"), \n                   angle = 90, vjust = -0.5, color = \"#9b6723\") +\n  geom_vline(xintercept = 2018, linetype = \"solid\", color = \"#9b6723\",alpha = 0.35) +\n  geom_text(aes(x = 2018, y = max(n) * 0.30, label = \"WDR Educ\"), \n                   angle = 90, vjust = -0.5, color = \"#9b6723\")\n\npdo_edu_WDR_plot\n\n\n\n\n\n\n\n\n#f_save_plot(\"pdo_edu_plot\", pdo_edu_plot)\nf_save_plot_obj (pdo_edu_WDR_plot, \"pdo_edu_WDR_plot\")\n\n‚Äî ‚úÖ CLIMATE (climate change)\n\nThe WDR of 2010 was titled ‚Äù Development and Climate Change‚Äù, link\n\n# --- Get a LIST of unique sectors (facets) and split the data\nPDOsector_list &lt;- base::split(x = sector_broad_pdo, f = sector_broad_pdo$tok_sector_broad)\n# GENDER split df \n#PDOsector_list$'CLIMATE'\n\n# Specific plot \npdo_clim_WDR_plot &lt;- sector_plots$'CLIMATE' + \n   # geom_vline(xintercept = 2003, linetype = \"solid\", color = \"#9b6723\",alpha = 0.35) +\n   # geom_text(aes(x = 2003, y = max(n) * 0.5, label = \"WDR Sust Dev\"), \n   #           angle = 90, vjust = -0.5, color = \"#9b6723\")+ \n   geom_vline(xintercept = 2010, linetype = \"solid\", color = \"#9b6723\",alpha = 0.35) +\n   geom_text(aes(x = 2010, y = max(n) * 0.45, label = \"WDR Climate change\"), \n             angle = 90, vjust = -0.5, color = \"#9b6723\")\n\npdo_clim_WDR_plot\n\n\n\n\n\n\n\n\n#f_save_plot(\"pdo_clim_plot\", pdo_clim_plot)\nf_save_plot_obj (pdo_clim_WDR_plot, \"pdo_clim_WDR_plot\")\n\n‚Äî ‚úÖ GENDER EQUALITY\nthe WDR of 2012 was titled ‚ÄúGender Equality and Development‚Äù, link\n\n# --- Get a LIST of unique sectors (facets) and split the data\nPDOsector_list &lt;- base::split(x = sector_broad_pdo, f = sector_broad_pdo$tok_sector_broad)\n# GENDER split df \n#PDOsector_list$GENDER_EQUAL\n\n# Specific plot \npdo_gen_WDR_plot &lt;- sector_plots$GENDER_EQUAL +\n  geom_vline(xintercept = 2012, linetype = \"solid\", color = \"#9b6723\",alpha = 0.35) +\n  geom_text(aes(x = 2012, y = max(n) * 0.45, label = \"WDR Gender equal\"), \n                   angle = 90, vjust = -0.5, color = \"#9b6723\")\n\npdo_gen_WDR_plot\n\n\n\n\n\n\n\n\n#f_save_plot(\"pdo_gen_plot\", pdo_gen_plot)\nf_save_plot_obj (pdo_gen_WDR_plot, \"pdo_gen_WDR_plot\")\n\n‚Äî SOCIAL PROTECTION\nWDR 2004 ‚Äù Making Services Work for Poor People‚Äù\n\n# Specific split df \n#PDOsector_list$SOCIAL_PROT\n\n# Specific plot \npdo_soc_WDR_plot &lt;- sector_plots$SOCIAL_PROT + \n  geom_vline(xintercept = 2004, linetype = \"solid\", color = \"#9b6723\",alpha = 0.35) +\n  geom_text(aes(x = 2004, y = max(n) * 0.75, label = \"WDR Services\"), \n                   angle = 90, vjust = -0.5, color = \"#9b6723\")\n\npdo_soc_WDR_plot\n\n\n\n\n\n\n\n\n#f_save_plot(\"pdo_soc_plot\", pdo_soc_plot)\nf_save_plot_obj (pdo_soc_WDR_plot, \"pdo_soc_WDR_plot\")\n\n‚Äî INSTITUTIONAL SUPPORT\nWDR 2002 ‚Äù Building Institutions for Markets‚Äù WDR 2007 ‚Äù Governance and the Law‚Äù\n\n# Specific split df \n# PDOsector_list$INSTIT_SUP\n\n# Specific plot \npdo_inst_WDR_plot &lt;- sector_plots$INSTIT_SUPP + \n   geom_vline(xintercept = 2002, linetype = \"solid\", color = \"#9b6723\",alpha = 0.35) +\n   geom_text(aes(x = 2002, y = max(n) * 0.75, label = \"WDR Institutions\"), \n             angle = 90, vjust = -0.5, color = \"#9b6723\") + \n   geom_vline(xintercept = 2007, linetype = \"solid\", color = \"#9b6723\",alpha = 0.35) +\n   geom_text(aes(x = 2007, y = max(n) * 0.75, label = \"WDR Governance\"), \n             angle = 90, vjust = -0.5, color = \"#9b6723\")\n\npdo_inst_WDR_plot\n\n\n#f_save_plot(\"pdo_inst_plot\", pdo_inst_plot)\nf_save_plot_obj (pdo_inst_WDR_plot, \"pdo_inst_WDR_plot\")\n\n‚Äî ICT\nWDR 2016 ‚Äù Digital Dividends‚Äù WDR 2021 ‚Äù Data for Better Lives‚Äù\n\n# Specific split df \n# PDOsector_list$ICT\n\n# Specific plot \npdo_ict_WDR_plot &lt;- sector_plots$ICT + \n   geom_vline(xintercept = 2016, linetype = \"solid\", color = \"#9b6723\",alpha = 0.35) +\n   geom_text(aes(x = 2016, y = max(n) * 0.75, label = \"WDR Digital Div\"), \n             angle = 90, vjust = -0.5, color = \"#9b6723\") + \n   geom_vline(xintercept = 2021, linetype = \"solid\", color = \"#9b6723\",alpha = 0.35) +\n   geom_text(aes(x = 2021, y = max(n) * 0.75, label = \"WDR Data\"), \n             angle = 90, vjust = -0.5, color = \"#9b6723\")\n\npdo_ict_WDR_plot\n\n\n#f_save_plot(\"pdo_ict_plot\", pdo_ict_plot)\nf_save_plot_obj (pdo_ict_WDR_plot, \"pdo_ict_WDR_plot\")\n\n‚Äî FINANCIAL\nWDR 2005 ‚Äù A Better Investment Climate for Everyone‚Äù WDR 2022 ‚Äù Finance for an Equitable Recovery‚Äù\n\n# Specific split df \n#PDOsector_list$FINANCIAL\n\n# Specific plot \npdo_fin_WDR_plot &lt;- sector_plots$FIN + \n  geom_vline(xintercept = 2005, linetype = \"solid\", color = \"#9b6723\",alpha = 0.35) +\n  geom_text(aes(x = 2005, y = max(n) * 0.80, label = \"WDR Inv Clim\"), \n                   angle = 90, vjust = -0.5, color = \"#9b6723\") +\n  geom_vline(xintercept = 2022, linetype = \"solid\", color = \"#9b6723\",alpha = 0.35) +\n  geom_text(aes(x = 2022, y = max(n) * 0.75, label = \"WDR Finance\"), \n                   angle = 90, vjust = -0.5, color = \"#9b6723\")\npdo_fin_WDR_plot\n\n\n#f_save_plot(\"pdo_fin_plot\", pdo_fin_plot)\nf_save_plot_obj (pdo_fin_WDR_plot, \"pdo_fin_WDR_plot\")"
  },
  {
    "objectID": "analysis/01b_WB_project_pdo_EDA.html#bigrams",
    "href": "analysis/01b_WB_project_pdo_EDA.html#bigrams",
    "title": "WB Project PDO text EDA",
    "section": "BIGRAMS",
    "text": "BIGRAMS\n\nHere I use [clnp_annotate() output + ] dplyr to combine consecutive tokens into bigrams.\n\n\n# Create bigrams by pairing consecutive tokens by sentence ID and token IDs\nbigrams &lt;- pdo_train_t %&gt;%\n   # keeping FY with tokens\n   group_by(FY_appr, proj_id, pdo, sid ) %&gt;%\n   arrange(tid) %&gt;%\n   # Using mutate() and lead(), we create bigrams from consecutive tokens \n   mutate(next_token = lead(token), \n          bigram = paste(token, next_token)) %&gt;%\n   # make bigram low case\n   mutate(bigram = tolower(bigram)) %&gt;%\n   # only includes the rows where valid bigrams are formed\n   filter(!is.na(next_token)) %&gt;%\n   ungroup() %&gt;%\n   arrange(FY_appr, proj_id, sid, tid) %&gt;%\n   select(FY_appr,proj_id, pdo,sid, tid, token, bigram) \n\n\n# most frequent bigrams \ncount_bigram &lt;- bigrams %&gt;% \n   count(bigram, sort = TRUE)"
  },
  {
    "objectID": "analysis/01b_WB_project_pdo_EDA.html#clean-bigrams",
    "href": "analysis/01b_WB_project_pdo_EDA.html#clean-bigrams",
    "title": "WB Project PDO text EDA",
    "section": "Clean bigrams",
    "text": "Clean bigrams\nThe challenge is to clean but without separating consecutive words‚Ä¶ so I do this split-reunite process to remove stopwords and punctuation. Basically only keeping bigrams made of 2 nouns or ADJ+noun.\n\n# Separate the bigram column into two words\nbigrams_cleaned &lt;- bigrams %&gt;%\n  tidyr::separate(bigram, into = c(\"word1\", \"word2\"), sep = \" \")\n\n# Remove stopwords and bigrams in EACH component word containing punctuation\nbigrams_cleaned &lt;- bigrams_cleaned %&gt;%\n   # custom stop words\n   filter(!word1 %in% custom_stop_words_df$word, !word2 %in% custom_stop_words_df$word) %&gt;% \n   # Remove punctuation   \n   filter(!stringr::str_detect(word1, \"[[:punct:]]\"), !stringr::str_detect(word2, \"[[:punct:]]\"))  \n\n# Reunite the component cleaned words into the bigram column\nbigrams_cleaned &lt;- bigrams_cleaned %&gt;%\n   unite(bigram, word1, word2, sep = \" \") %&gt;% \n   # Remove too obvious bigrams \n   filter(!bigram %in% c(\"development objective\", \"development objectives\", \n                         \"proposed project\", \"project development\", \"program development\"))\n\n# View the cleaned dataframe\nbigrams_cleaned\n\n# Count the frequency of each bigram\nbigram_freq &lt;- bigrams_cleaned %&gt;%\n  count(bigram, sort = TRUE)\n\n[FIG] most frequent bigrams in PDOs\n\nExcluding bigrams where 1 word is among stopwords or a punctuation sign\nExcluding ‚Äúdevelopment objective/s‚Äù, ‚Äúproposed project‚Äù, ‚Äúprogram development‚Äù because not very informative\n\n\n# ---- Prepare data for plotting\n# Evaluate the title with glue first\ntitle_text &lt;- glue::glue(\"Frequency of bigrams in PDOs over FY {min(pdo_train_t$FY_appr)}-{max(pdo_train_t$FY_appr)}\") \n\n# Define the bigrams you want to highlight\nbigrams_to_highlight &lt;- c(\"public sector\", \"private sector\", \"eligible crisis\",\n                          \"health care\", \"health services\", \"public health\")   \n\n \n# ---- Plot the most frequent bigrams\npdo_bigr_freq &lt;- bigram_freq %&gt;%\n   slice_max(n, n = 25) %&gt;%\n   ggplot(aes(x = reorder(bigram, n), y = n,\n              fill = ifelse(bigram %in% bigrams_to_highlight, bigram, \"Other\"))) +\n   geom_col() +\n   # coord flipped so n is Y axis\n   scale_y_continuous(breaks = seq(min(bigram_freq$n)-1, max(bigram_freq$n), by = 50)) +\n   scale_fill_manual(values = c(\"public sector\" = \"#005ca1\", \n                                \"private sector\" = \"#9b2339\", \n                                \"eligible crisis\"= \"#8e550a\", \n                                \"health care\"= \"#4C9F38\",\n                                \"health services\"= \"#4C9F38\",\n                                \"public health\"= \"#4C9F38\", \n                                \"Other\" = \"grey\")) +\n   guides(fill = \"none\") +\n   coord_flip() +\n   labs(title = title_text, subtitle = \"(top 25 bigrams)\",\n        x = \"\", y = \"\") +\n   theme(axis.text.y = element_text(\n            # obtain vector of colors 2 match x axis labels color to fill\n            color = bigram_freq %&gt;%\n               slice_max(n, n = 25) %&gt;%\n               # mutate(color = ifelse(bigram %in% bigrams_to_highlight,\n               #                       ifelse(bigram == \"public sector\", \"#005ca1\",\n               #                              ifelse(bigram == \"private sector\", \"#9b2339\", \"#8e550a\")),\n               #                       \"#4c4c4c\")) \n               mutate(color = dplyr::case_when (\n                  bigram == \"public sector\" ~ \"#005ca1\",\n                  bigram == \"private sector\" ~ \"#9b2339\",\n                  bigram == \"eligible crisis\" ~ \"#8e550a\",\n                  bigram %in% c(\"health care\", \"health services\", \"public health\") ~ \"#4C9F38\",\n                  TRUE ~ \"#4c4c4c\")) %&gt;%\n               # Ensure the order matches the reordered bigrams (AS BINS)\n               arrange(reorder(bigram, n)) %&gt;%  \n               # Extract the color column in bin order as vector to be passed to element_text()\n               pull(color)\n            )\n         ) + lulas_theme\n\npdo_bigr_freq\n\n\n\n\n\n\n\nResults are not surprising in terms of frequent bigram recurrence:\n\nSee for example ‚Äúincrease access‚Äù, ‚Äúservice delivery‚Äù ,‚Äúinstitutional capacity‚Äù, ‚Äúpoverty reduction‚Äù etc, at the top.\nAlthough, while ‚Äúhealth‚Äù recurred in several bigrams (e.g.¬†‚Äúhealth services‚Äù, ‚Äúpublic health‚Äù, ‚Äúhealth care‚Äù) among the top 25, ‚Äúeducation‚Äù did not appear at all.\nA bit mysterious is perhaps ‚Äúeligible crisis‚Äù (&gt; 100 mentions)?! (coming back to this later)\n[FIG] Changes over time BY 1FY\nBesides huge, counter intuitive, difference between ‚Äúhealth‚Äù and ‚Äúeducation‚Äù, ‚Äúclimate change‚Äù appears in the top 25 (ranking above ‚Äúfinancial sector‚Äù and ‚Äúcapacity building‚Äù) which begs the question: Has the frequency of these bigrams has changed over time?\n\n# \n# ## too busy to be useful\n# \n# # Step 1: Count the frequency of each bigram by year\n# top_bigrams_1FY &lt;- bigrams_cleaned %&gt;%\n#    group_by(FY_appr, bigram) %&gt;%\n#    summarise(count = n(), .groups = 'drop') %&gt;%\n#    arrange(FY_appr, desc(count)) %&gt;%\n#    # ---  +/- top 10  \n#    group_by(FY_appr) %&gt;%\n#    top_n(10, count) %&gt;%\n#    ungroup()\n#    # # ---  STRICT  top 10  \n#    # mutate(rank = dense_rank(desc(count))) %&gt;%  # Rank bigrams by frequency\n#    # filter(rank &lt;= 10) %&gt;%  # Keep only the top 10 by rank\n#    # ungroup()\n# \n#   \n# # Add specific bigrams to highlight, if any\n# bigrams_to_highlight &lt;- c(\"climate change\",  \"climate resilience\", \"public sector\", \"private sector\")\n# \n# # Step 2: Plot the top bigrams by frequency over time   \n# pdo_bigr_FY_freq  &lt;-  top_bigrams_1FY %&gt;% \n#  ggplot(aes(x = reorder(bigram, count), \n#              y = count,\n#              fill = ifelse(bigram %in% bigrams_to_highlight, bigram, \"Other\"))) +\n#   geom_col() +\n#   scale_fill_manual(values = c(\"public sector\" = \"#005ca1\", \"private sector\" = \"#e60066\", \n#                                \"climate change\" = \"#399B23\", \"climate resilience\" = \"#d8e600\",\n#                                \"Other\" = \"grey\")) +\n#   guides(fill = \"none\") +\n#   coord_flip() +\n#   facet_wrap(~ FY_appr, scales = \"free_y\") +\n#   labs(title = \"Top 10 Bigrams by Frequency Over Time\",\n#        subtitle = \"(Faceted by Fiscal Year Approval)\",\n#        x = \"Bigrams\",\n#        y = \"Count\") +\n#   theme_minimal() +\n#   theme(plot.title.position = \"plot\",\n#         axis.text.x = element_text(angle = 45, hjust = 1))+\n#      lulas_theme\n# \n# pdo_bigr_FY_freq\n\n[FIG] Changes over time BY 3FY\nTo reduce the noise and make the plot more readable, we can group the data by 3 fiscal years (FY) intervals.\n\n# generate FY group \nf_generate_year_groups &lt;- function(years, interval) {\n  breaks &lt;- seq(floor(min(years, na.rm = TRUE) / interval) * interval, \n                ceiling(max(years, na.rm = TRUE) / interval) * interval, \n                by = interval)\n  \n  labels &lt;- paste(breaks[-length(breaks)], \"-\", breaks[-1] - 1)\n  \n  return(list(breaks = breaks, labels = labels))\n}\n\n\n# --- Step 1: Create n-year groups (using `f_generate_year_groups`)\ninterval_i = 3 # decide the interval\nyear_groups &lt;- f_generate_year_groups(bigrams_cleaned$FY_appr, interval = interval_i)\ntop_n_i = 12 # decide the top n bigrams to show\n\n# --- Step 2: Add the generated FY breaks and labels to data frame\ntop_bigrams_FYper &lt;- bigrams_cleaned %&gt;%\n   # cut divides the range of x into intervals\n   mutate(FY_group = base::cut(FY_appr, \n                               breaks = year_groups$breaks, \n                               include.lowest = TRUE, \n                               right = FALSE, \n                               labels = year_groups$labels)) %&gt;% \n   # Count the frequency of each bigram by n-year groups\n   group_by(FY_group, bigram) %&gt;%\n   summarise(count = n(), .groups = 'drop') %&gt;%\n   arrange(FY_group, desc(count)) %&gt;%\n   # Top ? bigrams for each n-year period\n   group_by(FY_group) %&gt;%\n   top_n(top_n_i, count) %&gt;%\n   ungroup()\n\n# --- Step 3: Add specific bigrams to highlight, if any\nbigrams_to_highlight &lt;- c(\"climate change\",  \"climate resilience\", \n                          \"eligible crisis\",  \n                          \"public sector\", \"private sector\",\n                          \"water supply\", \"sanitation services\",\n                          \"health care\", \"health services\", \"public health\", \"health preparedness\"\n                          )\n\n# --- Step 4: Plot the top bigrams by frequency over n-year periods\npdo_bigr_FY_freq  &lt;-  top_bigrams_FYper %&gt;% \n ggplot(aes(x = reorder(bigram, count), \n             y = count,\n             fill = ifelse(bigram %in% bigrams_to_highlight, bigram, \"Other\"))) +\n  geom_col() +\n  scale_fill_manual(values = c(\n     # \"public sector\" = \"#005ca1\", \n     # \"private sector\" = \"#e60066\", \n     \"water supply\" = \"#26BDE2\",\n      \"sanitation services\" = \"#26BDE2\",\n     \"climate change\" = \"#3F7E44\", \n     \"climate resilience\" = \"#a6bd23\",\n     \"eligible crisis\" = \"#e68000\",  \n     \"health care\" = \"#E5243B\",\n     \"health services\" = \"#E5243B\",\n     \"public health\" = \"#E5243B\",\n     \"Other\" = \"grey\")) +\n  guides(fill = \"none\") +\n  coord_flip() +\n  facet_wrap(~ FY_group, ncol = 3 , scales = \"free_y\" )+ \n              #strip.position = \"top\") +  # Facet wrap with columns\n  labs(title = glue::glue(\"Top 12 Bigrams by Frequency Over {interval_i}-Year Periods\"),\n       subtitle =  \"(Some sectors highlighted)\",\n       x = \"\",\n       y = \"\") +\n     lulas_theme\n\n\n# print the plot\npdo_bigr_FY_freq\n\n\n\n\n\n\n\n\nFrequency observed over FY intervals is very revealing.\n\n\nInteresting to see the trend of ‚Äúwater supply‚Äù and ‚Äúsanitation services‚Äù bigrams, which are quite stable over time.\nThe bigram ‚Äúhealth care‚Äù and ‚Äúhealth services‚Äù are also quite stable, while ‚Äúpublic health‚Äù obviously gained relevance since the 2019-2021 FY period.\nConversely, ‚Äúprivate sector‚Äù and ‚Äúpublic sector‚Äù loose importance over time (around mid 2010s), while ‚Äúclimate change‚Äù and ‚Äúclimate resilience‚Äù gain relevance from the same point on.\nStill quite surprising the bigram ‚Äúeligible crisis‚Äù, which actually appears in the top 12 bigrams starting in FY 2016-2018!\nü§î Which are the most frequent and persistent Bigrams Over Time?\n\nFor this, I am looking for a ranking that considers Mean frequency across periods arrange(desc(mean_count)) + Stability (low standard deviation) across periods [this is hard bc of NAs], and NOT total count overall‚Ä¶\n\n\nUsing top_bigrams_FYper which had breaks of 3FY\n\n\n# ------------------------------[REPEATED just to see the table]\n\n# --- Step 1: Create n-year groups (using `f_generate_year_groups`)\ninterval_i = 3 # decide the interval\nyear_groups &lt;- f_generate_year_groups(bigrams_cleaned$FY_appr, interval = interval_i)\ntop_n_i = 12 # decide the top n bigrams to show\n\n# --- Step 2: Add the generated FY breaks and labels to data frame\ntop_bigrams_FYper &lt;- bigrams_cleaned %&gt;%\n   # cut divides the range of x into intervals\n   mutate(FY_group = base::cut(FY_appr, \n                               breaks = year_groups$breaks, \n                               include.lowest = TRUE, \n                               right = FALSE, \n                               labels = year_groups$labels)) %&gt;% \n   # Count the frequency of each bigram by n-year groups\n   group_by(FY_group, bigram) %&gt;%\n   summarise(count = n(), .groups = 'drop') %&gt;%\n   arrange(FY_group, desc(count)) %&gt;%\n   # Top ? bigrams for each n-year period\n   group_by(FY_group) %&gt;%\n   top_n(top_n_i, count) %&gt;%\n   ungroup()\n\n\nsd() returns NA for bigrams that are not present in any periods (or are present in just 1 period).\n\n\n# Calculate the mean frequency and standard deviation of the counts for each bigram across periods\nstable_and_frequent_bigrams_per &lt;- top_bigrams_FYper %&gt;%\n   group_by(bigram) %&gt;%\n   summarise(mean_count = mean(count, na.rm = TRUE),     # Mean frequency across periods\n             sd_count = sd(count, na.rm = TRUE),         # Stability (lower sd = more stable)\n             count_non_na = sum(!is.na(count)),  # Count non-NA values\n             sd_count2 = if_else(count_non_na &gt;= 1, sd(count, na.rm = TRUE), NA_real_),  # Only calculate sd if &gt;= 3 non-NA\n             total_count = sum(count)) %&gt;%               # Total count across all periods (optional)\n   arrange(desc(mean_count)) %&gt;%                      # Sort by frequency and then stability\n   # Filter out bigrams with low mean frequency or high instability (you can adjust thresholds)\n   # Focus on the top 25% most frequent bigrams\n   filter(mean_count &gt; quantile(mean_count, 0.70, na.rm = TRUE)) #%&gt;% \n   # Focus on the most stable 50% (lower sd) ---&gt; NO bc NA values\n   #filter( sd_count &lt; quantile(sd_count, 0.5, na.rm = TRUE))\n\n[TBL] Bigrams Over Time [3FY]\n\n# View the most frequent and stable bigrams\nstable_and_frequent_bigrams_per %&gt;% \n   slice_head(n = 15)  %&gt;% kableExtra::kable()\n\n\n\n\n\n\n\n\n\n\n\nbigram\nmean_count\nsd_count\ncount_non_na\nsd_count2\ntotal_count\n\n\n\nincrease access\n39.83333\n6.080022\n6\n6.080022\n239\n\n\neligible crisis\n37.33333\n1.527525\n3\n1.527525\n112\n\n\nthreat posed\n33.00000\nNA\n1\nNA\n33\n\n\nprivate sector\n31.20000\n10.917875\n5\n10.917875\n156\n\n\nhealth preparedness\n31.00000\nNA\n1\nNA\n31\n\n\nstrengthen national\n28.00000\nNA\n1\nNA\n28\n\n\nservice delivery\n27.71429\n5.313953\n7\n5.313953\n194\n\n\nclimate change\n27.00000\n2.828427\n2\n2.828427\n54\n\n\npoverty reduction\n27.00000\n14.514361\n4\n14.514361\n108\n\n\npublic health\n25.50000\n16.263456\n2\n16.263456\n51\n\n\npublic sector\n25.25000\n8.301606\n4\n8.301606\n101\n\n\ninstitutional capacity\n24.87500\n6.577831\n8\n6.577831\n199\n\n\nimprove access\n24.57143\n8.521681\n7\n8.521681\n172\n\n\nnational systems\n24.00000\nNA\n1\nNA\n24\n\n\n\n\n\n\nUsing top_bigrams_1FY which had breaks of 1FY\n\n\n# --- Step 1: Create n-year groups (using `f_generate_year_groups`)\ninterval_i = 1 # decide the interval\nyear_groups &lt;- f_generate_year_groups(bigrams_cleaned$FY_appr, interval = interval_i)\ntop_n_i = 12 # decide the top n bigrams to show\n\n# --- Step 2: Add the generated FY breaks and labels to data frame\ntop_bigrams_1FY &lt;- bigrams_cleaned %&gt;%\n   # cut divides the range of x into intervals\n   mutate(FY_group = base::cut(FY_appr, \n                               breaks = year_groups$breaks, \n                               include.lowest = TRUE, \n                               right = FALSE, \n                               labels = year_groups$labels)) %&gt;% \n   # Count the frequency of each bigram by n-year groups\n   group_by(FY_group, bigram) %&gt;%\n   summarise(count = n(), .groups = 'drop') %&gt;%\n   arrange(FY_group, desc(count)) %&gt;%\n   # Top ? bigrams for each n-year period\n   group_by(FY_group) %&gt;%\n   top_n(top_n_i, count) %&gt;%\n   ungroup()\n\n\n# Calculate the mean frequency and standard deviation of the counts for each bigram across periods\nstable_and_frequent_bigrams_1FY &lt;- top_bigrams_1FY %&gt;%\n   group_by( bigram) %&gt;%\n   summarise(mean_count = mean(count, na.rm = TRUE),     # Mean frequency across periods\n             sd_count = sd(count, na.rm = TRUE),         # Stability (lower sd = more stable)\n             total_count = sum(count)) %&gt;%               # Total count across all periods (optional)\n   arrange(desc(mean_count)) %&gt;%                      # Sort by frequency and then stability\n   # Filter out bigrams with low mean frequency or high instability (you can adjust thresholds)\n   # Focus on the top 25% most frequent bigrams\n   filter(mean_count &gt; quantile(mean_count, 0.70, na.rm = TRUE)) #%&gt;% \n   # Focus on the most stable 50% (lower sd) ---&gt; NO bc NA values\n   #filter( sd_count &lt; quantile(sd_count, 0.5, na.rm = TRUE))\n\n[TBL] Bigrams Over Time [1FY]\n\n# View the most frequent and stable bigrams\nstable_and_frequent_bigrams_1FY %&gt;% \n   slice_head(n = 15)   %&gt;% kableExtra::kable()\n\n\n\nbigram\nmean_count\nsd_count\ntotal_count\n\n\n\nmobile applications\n21.00000\nNA\n21\n\n\npublic health\n16.66667\n3.0550505\n50\n\n\nthreat posed\n16.50000\n2.1213203\n33\n\n\nhealth preparedness\n15.50000\n0.7071068\n31\n\n\nincrease access\n14.64706\n5.1713293\n249\n\n\neligible crisis\n14.62500\n10.1971635\n117\n\n\nstrengthen national\n14.00000\n2.8284271\n28\n\n\nvulnerable households\n13.00000\nNA\n13\n\n\nrespond promptly\n12.50000\n10.6066017\n25\n\n\naction plan\n12.00000\nNA\n12\n\n\ndisaster risk\n12.00000\nNA\n12\n\n\nlocal governments\n12.00000\nNA\n12\n\n\nnational systems\n12.00000\n1.4142136\n24\n\n\nworld bank\n12.00000\nNA\n12\n\n\nclimate resilience\n11.66667\n4.5092498\n35"
  },
  {
    "objectID": "analysis/01b_WB_project_pdo_EDA.html#explore-specific-bigrams",
    "href": "analysis/01b_WB_project_pdo_EDA.html#explore-specific-bigrams",
    "title": "WB Project PDO text EDA",
    "section": "Explore specific bigrams",
    "text": "Explore specific bigrams\n‚Äî Public/Private ~ compare frequency over FY\nA case in which looking at bigrams may be better than tokens is the question whether WB project are more focused on public or private sector. It is not easy to capture this information from the text, because:\n\n‚Äúgovernment‚Äù may be referred to the subject/counterpart of the project (e.g.¬†‚Äúgovernment of Mozambique‚Äù)\n‚Äúprivate‚Äù is not necessarily referred to the ‚Äúprivate sector‚Äù (e.g.¬†‚Äúprivate households‚Äù)\n‚Äúpublic‚Äù is not necessarily referred to the ‚Äúpublic sector‚Äù (e.g.¬†‚Äúpublic health‚Äù)\n\nSo, I narrow down to consecutive bigrams ‚Äúpublic sector‚Äù and ‚Äúprivate sector‚Äù to get an indicative frequency of these terms.\n[FIG] Bigrams (‚Äúpublic sector‚Äù, ‚Äúprivate sector‚Äù) freq plot\n\n# Filter for the specific bigrams \"public sector\" and \"private sector\"\nbigrams_pub_priv_sec &lt;- bigrams %&gt;%\n   filter(bigram %in% c(\"public sector\", \"private sector\"))\n\n# Display the result\n#bigrams_pub_priv_sec\n\n# prepare data for plotting (count)\nsector_bigr_df &lt;- bigrams_pub_priv_sec %&gt;% \n   count(FY_appr, bigram) %&gt;% \n   # reorder values by frequency\n   mutate(bigram = factor(bigram, levels = c(\"public sector\", \"private sector\")))\n\n\n# ---- Prepare data for plotting\n# Evaluate the title with glue first\ntitle_text &lt;- glue::glue(\"Frequency of bigrams \\\"public sector\\\" and \\\"private sector\\\" in PDOs over FY {min(sector_bigr_df$FY_appr)}-{max(sector_bigr_df$FY_appr)}\") \n\ntwo_col_contrast &lt;- c( \"#005ca1\",  \"#e60066\" )\n\n# Create a named vector for the legend labels with totals in a single pipeline\nlegend_labels &lt;- sector_bigr_df %&gt;%\n   group_by(bigram) %&gt;%\n   # Calculate total counts for each bigram\n   summarize(total_n = sum(n)) %&gt;% \n   # Append totals to bigram names\n   mutate(label = paste0(bigram, \" (\", total_n, \")\")) %&gt;%  \n   # Create a named vector with bigram as names and labels as values\n   {setNames(.$label, .$bigram)} # curly braces {} in a dplyr pipeline using . as ouptu from previous..\n\n# ---- Plot\npdo_pub_pri_bigr &lt;- ggplot(data = sector_bigr_df, aes(x = FY_appr, y = n, group = bigram, color = bigram)) +\n   geom_line(linetype = \"solid\", alpha = 0.75, size = .5) +\n   geom_point(size = 3) +\n   scale_x_continuous(breaks = seq(2001, 2023, by = 1)) +\n   scale_color_manual(values = two_col_contrast, \n                      labels = legend_labels) +  # Use modified labels\n   lulas_theme +\n   theme(axis.text.x = element_text(angle = 45, hjust = 1)) +\n   labs(title = title_text, \n        x = \"\", \n        y = \"\", \n        color = \"\") \n   \n\npdo_pub_pri_bigr\n\n\n\n\n\n\n\n\n# Save the plot\n#f_save_plot(\"pdo_pub_pri_bigr\", pdo_pub_pri_bigr)\nf_save_plot_obj(pdo_pub_pri_bigr, \"pdo_pub_pri_bigr\")\n\n\nNote:\n\n\nthese are much less common than the single words.\nWhat happens in FY 2014-2016 that makes these bigram drop in frequency of mention?"
  },
  {
    "objectID": "analysis/01b_WB_project_pdo_EDA.html#clean-trigrams",
    "href": "analysis/01b_WB_project_pdo_EDA.html#clean-trigrams",
    "title": "WB Project PDO text EDA",
    "section": "Clean trigrams",
    "text": "Clean trigrams\nThe challenge is to clean but without separating consecutive words‚Ä¶ so I do this split-reunite process to remove stopwords and punctuation. Basically only keeping bigrams made of 2 nouns or ADJ+noun.\n\n# Split the trigrams into three tokens\ntrigrams_split &lt;- trigrams %&gt;% \n   separate(trigram, c(\"token1\", \"token2\", \"token3\"), sep = \" \")  \n\n# Remove stopwords and punctuation\ntrigrams_clean &lt;- trigrams_split %&gt;% \n   filter(!token1 %in% custom_stop_words,\n          !token2 %in% custom_stop_words,\n          !token3 %in% custom_stop_words) %&gt;%\n   filter(token1 != \"na\",\n          token2 != \"na\",\n          token3 != \"na\") %&gt;%\n   # Remove punctuation   \n   filter(!stringr::str_detect(token1, \"[[:punct:]]\"), \n          !stringr::str_detect(token2, \"[[:punct:]]\"),\n          !stringr::str_detect(token3, \"[[:punct:]]\"))  %&gt;% \n   unite(trigram, token1, token2, token3, sep = \" \") %&gt;%\n   select(FY_appr, proj_id, pdo, sid, tid, trigram)\n\n\n# Count the frequency of each trigram\ntrigram_freq &lt;- trigrams_clean %&gt;% \n   count(trigram, sort = TRUE) \n\n[FIG] Most frequent trigrams in PDOs\n\nExcluding bigrams where 1 word is among stopwords or a punctuation sign\nExcluding ‚Äúdevelopment objective/s‚Äù, ‚Äúproposed project‚Äù, ‚Äúprogram development‚Äù because not very informative\n\n\n# Evaluate the title with glue first\ntitle_text &lt;- glue::glue(\"Most frequent trigrams in PDOs over FY {min(trigrams_clean$FY_appr)}-{max(trigrams_clean$FY_appr)}\")\n \n# Define colors for specific highlights\nhighlight_colors &lt;- c(\"Health\" =  \"#d02e4c\", \"Environment\" =\"#3F7E44\", \"Other\" = \"grey\")\n\n\n# Plot the most frequent trigrams\npdo_trigram_freq_plot &lt;- trigram_freq %&gt;%\n   dplyr::filter(!trigram %in% c(\"project development objective\",\n                          \"project development objectives\",\n                          \"overall development objective\",\n                          \"program development objective\",\n                          \"program development objectives\",\n                          \"proposed project development\", \n                          \"proposed development objectives\", \n                          \"proposed development objective\",\n                          \"revised project development\"\n                          )) %&gt;%\n   top_n(25) %&gt;%\n   # plot the top 25 trigrams\n   ggplot(aes(x = reorder(trigram, n), y = n,\n              fill = dplyr::case_when(\n                 stringr::str_detect(trigram, \"health\") ~ \"Health\",\n                 stringr::str_detect(trigram, \"environment\") ~ \"Environment\",\n                 stringr::str_detect(trigram, \"climate\") ~ \"Environment\",\n                 stringr::str_detect(trigram, \"greenhouse\") ~ \"Environment\",\n                 # stringr::str_detect(trigram, \"sustain\") ~ \"Environment\",\n                 TRUE ~ \"Other\"))) +\n   geom_col() +\n   # coord flipped so n is Y axis\n   scale_y_continuous(breaks = seq(min(trigram_freq$n)-1, max(trigram_freq$n), by = 50)) +\n   coord_flip() +\n   labs(title = title_text, subtitle = \"(top 25 trigrams)\",\n        x = \"\", y = \"\") +\n   scale_fill_manual(values = highlight_colors) +\n   guides(fill = \"none\") +\n   lulas_theme\n\npdo_trigram_freq_plot"
  },
  {
    "objectID": "analysis/01b_WB_project_pdo_EDA.html#concordances-with-specific-bigrams",
    "href": "analysis/01b_WB_project_pdo_EDA.html#concordances-with-specific-bigrams",
    "title": "WB Project PDO text EDA",
    "section": "Concordances with specific bigrams",
    "text": "Concordances with specific bigrams\nConcordancing is central to analyses of text and they often represents the first step in more sophisticated analyses of language data, because concordances are extremely valuable for understanding how a word or phrase is used, how often it is used, and in which contexts is used.\nA concordance list is a list of all contexts in which a particular token appears in a corpus or text. Here I use it in association with the bigram ‚Äúeligible crisis‚Äù to see in which context it appears in the PDOs.\n\nHere I did it at the level of sentence, i.e.¬†without tokenizing the text into words.\n\n‚Äî eligible crisis ~ notable bigrams over FY\n\n# reduce back to the original data\npdo_t &lt;- pdo_train_t %&gt;% \n   select(proj_id, pdo,pr_name, FY_appr, FY_clos, status, regionname, countryname,\n          sector1, theme1,lendinginstr, env_cat, ESrisk, curr_total_commitment) %&gt;%\n   group_by(proj_id) %&gt;% \n   slice(1)\n\nFirst of all, let‚Äôs see what are the sentence that contain the bigram ‚Äúeligible crisis‚Äù in the PDOs.\n\n# Tokenize the text data into sentences\nsentences &lt;- pdo_t %&gt;%\n   unnest_tokens(sentence, pdo, token = \"sentences\", drop = FALSE)\n\n# Count the number of sentences in each document\nsentence_count &lt;- sentences %&gt;%\n   group_by(proj_id) %&gt;%\n   summarise(num_sentences = n())\n\nn_distinct(sentence_count$proj_id)  # number of projects\nsum(sentence_count$num_sentences)   # total number of sentences\n\n\n# ---- Define the bigram you want to find\ntarget_bigram &lt;- \"eligible crisis\"\n\n\n# Filter sentences that contain the specific bigram\nsentences_with_targ &lt;- sentences %&gt;%\n   filter(stringr::str_detect(sentence, target_bigram))\n\n# Define how many characters before and after the bigram to extract\nchars_before &lt;- 60  # Number of characters before the bigram\nchars_after &lt;- 60   # Number of characters after the bigram\n\n# Add the extracted bigram and surrounding characters to the same dataframe\nsentences_with_eligcris &lt;- sentences_with_targ %&gt;%\n   mutate(closest_text = str_extract(sentence, paste0(\".{0,\", chars_before, \"}\", target_bigram, \".{0,\", chars_after, \"}\"))) %&gt;% \n   # View the updated dataframe with the closest_text column\n   select(proj_id, #sentence, \n          closest_text)\n\n\n# Define how many words before and after the bigram to extract\nwords_before &lt;- 8  # Number of words before the bigram\nwords_after &lt;- 8   # Number of words after the bigram\n\n# Add the extracted bigram and surrounding words to the same dataframe\nsentences_with_eligcris2 &lt;- sentences_with_targ %&gt;%\n   mutate(closest_text = str_extract(sentence, \n                                     paste0(\"(\", # Start a capture group\n                                            # Match preceding words\n                                            \"(?:\\\\S+\\\\s+){0,\", words_before, \"}\", \n                                            target_bigram, \n                                            # Match following words\n                                            \"(?:\\\\s+\\\\S+){0,\", words_after, \"}\", \n                                            \")\"\n                                           ))) %&gt;% \n   # View the updated dataframe with the closest_text column\n   select(proj_id,  sentence, \n          closest_text)\n\nn_distinct(sentences_with_eligcris2$proj_id)\n\n\nThere are 112 projects, for which the PDO has a sentences containing the bigram ‚Äúeligible crisis‚Äù in the PDOs.\n\n[TBL] Close phrase around bigram ‚Äúeligible crisis‚Äù\nIt appears ‚Äúeligible crisis or emergency‚Äù is a commonly used phrase in the PDOs, often accompanied by similar phrasing: ‚Äúto respond promptly and effectively‚Äù. as well as ‚Äúprovide immediate and effective response to‚Äù. Presumably, a standard sentence that refers to a situation that qualifies for specific types of assistance or intervention under certain policies.\n\n# Define the phrase you want to search for in the vicinity of the target bigram\nphrase_to_search &lt;- \"respond promptly and effectively\"\n\n# Count how often the phrase appears in the vicinity of the target bigram\nphrase_count &lt;- sentences_with_eligcris2 %&gt;%\n  mutate(contains_phrase = stringr::str_detect(closest_text, phrase_to_search)) %&gt;%  # Check if the phrase is present\n  summarise(count = sum(contains_phrase))  # Count how many times the phrase is found\n\n# View the result\ntabyl(phrase_count$count)\n\n\n32% of the (112) times, the bigram ‚Äúeligible crisis‚Äù in the PDOs, it is accompanied by the phrase ‚Äúrespond promptly and effectively‚Äù.\n\nHere are a few examples of the sentences containing the bigram ‚Äúeligible crisis‚Äù and the phrase ‚Äúrespond promptly and effectively‚Äù OR immediate and effective response:\n\nset.seed(555)\n# Filter the sentences that contain the phrase\nsample_with_eligcris2 &lt;- sentences_with_eligcris2 %&gt;% \n   ungroup() %&gt;% \n   # take a random sample of 5 sentences\n   sample_n(10) %&gt;%\n   select(proj_id, closest_text) %&gt;% \n   mutate(\n     closest_text = paste0(\"(...) \", closest_text),\n     # Make \"eligible crisis\" bold by adding &lt;b&gt; tags\n     closest_text = gsub(\"eligible crisis\", \"&lt;b&gt;eligible crisis&lt;/b&gt;\", closest_text),\n     # Highlight by adding &lt;mark&gt; tags\n     closest_text = gsub(\"(?i)(promptly and effectively|immediate and effective response)\", # (?i) makes the match case-insensitive.\n                         \"&lt;mark style='background-color: #d8e600;'&gt;\\\\1&lt;/mark&gt;\", closest_text, perl = TRUE)\n     )\n\n# Print out sample in a kable \nelcr_k &lt;- kable(sample_with_eligcris2, format = \"html\", \n                # Display the table with bold formatting\n                escape = FALSE,\n                col.names = c(\"WB Project ID\",\"Excerpt of PDO Sentences with 'Eligible Crisis'\")) %&gt;% \n   kable_styling(full_width = FALSE)\n\nelcr_k\n\n\n\n\nWB Project ID\nExcerpt of PDO Sentences with 'Eligible Crisis'\n\n\n\nP179499\n(...) and effective response in the case of an eligible crisis or emergency.\n\n\nP176608\n(...) promptly and effectively in the event of an eligible crisis or emergency.\n\n\nP151442\n(...) assistance programs and, in the event of an eligible crisis or emergency, to provide immediate and effective response\n\n\n\nP177329\n(...) eligible crisis or emergency, respond promptly and effectively to it.\n\n\nP127338\n(...) capacity to respond promptly and effectively in an eligible crisis or emergency, asrequired.\n\n\nP158504\n(...) immediate and effective response in case of an eligible crisis or emergency.\n\n\nP173368\n(...) immediate and effective response in case of an eligible crisis or emergency in the kingdom of cambodia.\n\n\nP178816\n(...) the project regions and to respond to an eligible crisis\n\n\n\nP160505\n(...) theproject area, and, in the event of an eligible crisis or emergency, to provide immediate and effective response\n\n\n\nP149377\n(...) mozambique to respond promptly and effectively to an eligible crisis or emergency.\n\n\n\n\n\n# Save the table as an HTML file\nwrite_rds(elcr_k, here(\"analysis\", \"output\", \"tables\" ,\"elcr_k.rds\"))\n\n‚Äî climate change ~ notable bigrams over FY [CMPL üü†]\nFirst of all, let‚Äôs see what are the sentence that contain the bigram ‚Äúeligible crisis‚Äù in the PDOs.\n\n# ---- Define the bigram you want to find\ntarget_bigram &lt;- \"climate change\"\n\n# # Filter sentences that contain the specific bigram\n# sentences_with_targ &lt;- sentences %&gt;%\n#    filter(stringr::str_detect(sentence, target_bigram))\n# \n# # Define how many words before and after the bigram to extract\n# words_before &lt;- 8  # Number of words before the bigram\n# words_after &lt;- 8   # Number of words after the bigram\n\n# Add the extracted bigram and surrounding words to the same dataframe\nsentences_with_climchang &lt;- sentences %&gt;%\n   filter(stringr::str_detect(sentence, target_bigram)) %&gt;% \n   mutate(closest_text = str_extract(sentence, \n                                     paste0(\"(\", # Start a capture group\n                                            # Match preceding words\n                                            \"(?:\\\\S+\\\\s+){0,\", words_before, \"}\", \n                                            target_bigram, \n                                            # Match following words\n                                            \"(?:\\\\s+\\\\S+){0,\", words_after, \"}\", \n                                            \")\"\n                                           ))) %&gt;% \n   # View the updated dataframe with the closest_text column\n   select(proj_id, pdo, sentence,\n          closest_text)\n\n\nThere are 92 projects, for which the PDO has a sentences containing the bigram ‚Äúclimate change‚Äùin the PDOs.\n\n[TBL] Close phrase around bigram ‚Äúclimate change‚Äù\nI want to know which of these commonly used phrases are most often found in the vicinity of the bigram ‚Äúclimate change‚Äù in the PDOs.\n\n# Count how often the phrase appears in the vicinity of the target bigram\nclose_words &lt;- sentences_with_climchang %&gt;%\n  mutate(contains_what = dplyr::case_when(\n     stringr::str_detect(sentence, \"mitigat\") ~ \"mitigate\",\n     stringr::str_detect(sentence, \"adapt\") ~ \"adapt\",\n     stringr::str_detect(sentence, \"vulnerab\") ~ \"vulnerability\",\n     stringr::str_detect(sentence, \"hazard\") ~ \"hazard\",\n     stringr::str_detect(sentence, \"resil\") ~ \"resilience\",\n     TRUE ~ \"...\"))\n \n# Count how often the phrase is found\nclose_words_sort &lt;-  close_words %&gt;% \n   filter(contains_what != \"...\") %&gt;% \n  group_by(contains_what) %&gt;% \n  summarise(count = n()) %&gt;% \n  mutate(percentage = scales::percent(count/sum(count))) %&gt;% \n   arrange(desc(count))\n\n# Specify the words to highlight\nhighlight_words &lt;- c(\"mitigate\")\nhighlight_words2 &lt;- c(  \"resilience\", \"adapt\")\n\n\nclch_close_k &lt;- close_words_sort %&gt;% \n   kable(format = \"html\", \n         col.names = c(\"Near 'climate change'\", \"Count\", \"Percentage\")) %&gt;% \n   kable_styling(full_width = FALSE) %&gt;% \n    # Light yellow background\n   row_spec(which(close_words_sort$contains_what %in% highlight_words), \n            background = \"#d8e600\") %&gt;% \n   row_spec(which(close_words_sort$contains_what %in% highlight_words2), \n            background = \"#a6bd23\")  \n\nclch_close_k\n\n\n\n\nNear 'climate change'\nCount\nPercentage\n\n\n\nvulnerability\n25\n39.1%\n\n\nmitigate\n14\n21.9%\n\n\nresilience\n14\n21.9%\n\n\nadapt\n6\n9.4%\n\n\nhazard\n5\n7.8%\n\n\n\n\n\n# save as object\nwrite_rds(clch_close_k, here(\"analysis\", \"output\", \"tables\" ,\"clch_close_k.rds\"))\n\n\n#   &lt;chr&gt;         &lt;int&gt;       &lt;chr&gt;     \n# 1 vulnerab         18       32.1%     \n# 2 mitigate         12       21.4%     \n# 3 resil            12       21.4%     \n# 4 hazard            9       16.1%     \n# 5 adapt             5       8.9% \n\nHere are a few examples of the sentences containing the bigram ‚Äúclimate change‚Äù and the words ‚Äúmitigate|adaptation‚Äù:\n\nset.seed(888)\n# Filter the sentences that contain the phrase\nsentences_with_climchang2_k &lt;-  sentences_with_climchang %&gt;%\n   filter(proj_id != \"P125447\") %&gt;%\n   # add a column to identify the phrases\n   mutate(contains_what = case_when(\n      stringr::str_detect(closest_text, \"mitig\") ~ \"mitig\",\n      stringr::str_detect(closest_text, \"adapt\") ~ \"adapt\",\n      stringr::str_detect(closest_text, \"vulnerab\") ~ \"vulnerab\",\n      stringr::str_detect(closest_text, \"hazard\") ~ \"hazard\",\n      stringr::str_detect(closest_text, \"resil\") ~ \"resil\",\n      TRUE ~ \"...\")) %&gt;% \n   filter(contains_what != \"...\") %&gt;% \n   # take a random sample of 3  by word \n   group_by(contains_what) %&gt;% \n   slice_sample(n = 3, replace = FALSE ) %&gt;%\n   select(contains_what, proj_id, closest_text) %&gt;%\n   mutate(closest_text = paste0(\"(...) \", closest_text),\n          # Make \"mutate\" bold by adding &lt;b&gt; tags\n          closest_text = gsub(\"climate change\", \"&lt;b&gt;climate change&lt;/b&gt;\", closest_text), \n          # highlight the phrases by adding &lt;mark&gt; tags (adapt, mitigate, etc.)\n          closest_text = gsub(\"(?i)(adaptation|resilience)\", # (?i) makes the match case-insensitive.\n                              \"&lt;mark style='background-color: #a6bd23;'&gt;\\\\1&lt;/mark&gt;\", closest_text, perl = TRUE),\n          closest_text = gsub(\"(?i)(mitigation|mitigate)\",  \n                              \"&lt;mark style='background-color: #8e94d6;'&gt;\\\\1&lt;/mark&gt;\", closest_text, perl = TRUE),\n          closest_text = gsub(\"(?i)(hazard|vulnerability)\", \n                              \"&lt;mark style='background-color: #e28293;'&gt;\\\\1&lt;/mark&gt;\", closest_text, perl = TRUE)\n          )\n\n# save as object\nwrite_rds(sentences_with_climchang2_k, here(\"analysis\", \"output\", \"tables\" ,\"sentences_with_climchang2_k.rds\"))\n\n\n#paint(sentences_with_climchang2_k)\n\n# Prepare the kable table with subheaders based on 'contains_what'\nsentences_with_climchang2_k %&gt;%\n  ungroup() %&gt;%\n  arrange(contains_what) %&gt;%\n  select(contains_what, proj_id, closest_text) %&gt;%\n  kable(format = \"html\", \n        escape = FALSE,\n        col.names = c(\"Near word (root)\", \"WB Project ID\", \"Closest Text\")) %&gt;%\n  kable_styling(full_width = FALSE)   \n\n\n\n\nNear word (root)\nWB Project ID\nClosest Text\n\n\n\nadapt\nP090731\n(...) pilot adaptation measures addressing primarily, the impacts of climate change on their natural resource base, focused on biodiversity\n\n\nadapt\nP120170\n(...) a multi-sectoral dpl to enhance climate change adaptation capacity is anticipated in the cps.\n\n\nadapt\nP129375\n(...) objectives of the project are to: (i) integrate climate change adaptation and disaster risk reduction across the recipient‚Äôs\n\n\nhazard\nP174191\n(...) and health-related hazards, including the adverse effects of climate change and disease outbreaks.\n\n\nhazard\nP123896\n(...) agencies to financial protection from losses caused by climate change and geological hazards.\n\n\nhazard\nP117871\n(...) buildings and infrastructure due to natural hazards or climate change impacts; and (b) increased capacity of oecs governments\n\n\nmitig\nP074619\n(...) to help mitigate global climate change through carbon emission reductions (ers) of 138,000 tco2e\n\n\nmitig\nP164588\n(...) institutional capacity for sustainable agriculture, forest conservation and climate change mitigation.\n\n\nmitig\nP094154\n(...) removing carbon from the atmosphere and to mitigateclimate change in general.\n\n\nresil\nP154784\n(...) to increase agricultural productivity and build resilience to climate change risks in the targeted smallholder farming and pastoralcommunities\n\n\nresil\nP112615\n(...) the resilience of kiribati to the impacts of climate change on freshwater supply and coastal infrastructure.\n\n\nresil\nP157054\n(...) to improve durability and enhance resilience to climate change\n\n\n\nvulnerab\nP149259\n(...) to measurably reduce vulnerability to natural hazards and climate change impacts in grenada and in the eastern caribbean\n\n\nvulnerab\nP146768\n(...) at measurably reducing vulnerability to natural hazards and climate change impacts in the eastern caribbean sub-region.\n\n\nvulnerab\nP117871\n(...) at measurably reducing vulnerability to natural hazards and climate change impacts in the eastern caribbean sub-region.\n\n\n\n\n\n   # Add subheaders based on the unique values in `contains_what`\n  #group_rows(index = table(sentences_with_climchang2$contains_what))"
  },
  {
    "objectID": "analysis/01b_WB_project_pdo_EDA.html#keyword-in-context-kwic",
    "href": "analysis/01b_WB_project_pdo_EDA.html#keyword-in-context-kwic",
    "title": "WB Project PDO text EDA",
    "section": "‚Äî Keyword In Context (KWIC)",
    "text": "‚Äî Keyword In Context (KWIC)\n\n\n\nKeyword In Context (KWIC), or concordances, are the most frequently used method in corpus linguistics. The idea is very intuitive: we get to know more about the semantics of a word by examining how it is being used in a wider context.\nUsually, the process involves: 1) tokenizing the text, 2) perform a search for a word and retrieve its concordances from the corpus. Typically, these extractions are displayed through keyword-in-context displays (KWICs), where the search term, also referred to as the node word, is showcased within its surrounding context, comprising both preceding and following words."
  },
  {
    "objectID": "analysis/01b_WB_project_pdo_EDA.html#concordances",
    "href": "analysis/01b_WB_project_pdo_EDA.html#concordances",
    "title": "WB Project PDO text EDA",
    "section": "‚Äî Concordances",
    "text": "‚Äî Concordances\n\n\n\n\nUsing quanteda\n\nfile:///Users/luisamimmi/Github/slogan_old/docs/01b_WDR_data-exploration_abstracts.html\n\n# I use again data = pdo_words\npdo_q_corpus &lt;- quanteda::corpus(as.data.frame(projs_train), \n                               docid_field = \"id\",\n                               text_field = \"pdo\",\n                               meta = list(\"pr_name\", \"boardApprovalFY\")\n)\n \n# --- example with individual keyword \n# Step 1) tokens\npdo_q_tokens &lt;- quanteda::tokens(x = pdo_q_corpus,\n                       remove_punct = TRUE,\n                       remove_symbols = TRUE#,remove_numbers = TRUE\n ) %&gt;% \n  quanteda::tokens_tolower() #%&gt;%\n #quanteda::tokens_remove(pattern = custom_stop_words) %&gt;%\n #quanteda::tokens_remove(pattern = c(\"project\", \"development\", \"bank\", \"world\", \"project\", \"projects\"))\n                                      \n# #______ Step 2) kwic (individual exe )\n# kwic_pdo_data &lt;- quanteda::kwic(x = pdo_q_tokens, # define text(s)\n#                                  # define pattern\n#                                  pattern = quanteda::phrase(c(\"gender\", \"climate\", \"sustainab*\")),\n#                                  # define window size\n#                                  window = 5) %&gt;%\n#     # convert into a data frame\n#     as_tibble() %&gt;%\n#     left_join(projs_train, by = c(\"docname\" =  \"id\")) %&gt;%\n#     # remove superfluous columns\n#      dplyr::select( 'Year' = boardapprovalFY, 'Prj title' = pr_name, pre, keyword, post) %&gt;%\n#   #  slice_sample( n = 50) %&gt;%\n#    kbl(align = \"c\") # %&gt;% kable_styling()\n \n# ____ Step 2) kwic (on vector)\n# Iterate `quanteda::kwic` over a vector of tokens | regex-modified-keywords\nkeywords &lt;- c(\"gender\", \"climate\", \"sustainab*\", \"conditional*\" )\n\n# apply iteratively kwic over a vector of keywords\noutputs_key &lt;-  map(keywords, \n      ~quanteda::kwic(pdo_q_tokens,\n                      pattern =  .x,\n                      window = 5) %&gt;% \n        as_tibble() %&gt;%\n        left_join(projs_train, by = c(\"docname\" =  \"id\")) %&gt;%  \n        # remove superfluous columns\n       dplyr::select( 'Year' = boardapprovalFY, 'Prj title' = pr_name, pre, keyword, post)\n  )\n\n# # all togetha 3\nn = length(keywords)\n\n# check the first element  \noutputs_key[[1]] %&gt;%\n   kbl(align = \"c\")\noutputs_key[[2]] %&gt;%\n   kbl(align = \"c\")\n\n# this list  has no element names \nnames(outputs_key)\n\n‚Äî create kwic with phrases | purrr + print + save png\n\n# Iterate `quanteda::kwic` over a vector of phrases/bigrams \nkeywords_phrase &lt;- c(\"pro-poor\", \"gender equality\", \"gender mainstreaming\" )\n \n# Step 1) tokens\n# (done above) -&gt; abs_q_tokens\n\n# Step 2) kwic \n# apply iteratively kwic over a vector of bigrams\noutputs_bigrams &lt;- map(keywords_phrase,\n                       ~quanteda::kwic(x = pdo_q_tokens, # define text(s) \n                                       # define pattern\n                                       pattern = quanteda::phrase(.x),\n                                       # define window size\n                                       window = 5) %&gt;%\n                          # convert into a data frame\n                          as_tibble() %&gt;%\n                          left_join(projs_train, by = c(\"docname\" =  \"id\")) %&gt;%  \n                          ## remove superfluous columns\n                          dplyr::select( 'Year' = boardapprovalFY, 'Prj title' = pr_name, pre, keyword, post)\n)  \n\n#  number ofo cbigrams \nn_bi = length(keywords_phrase)\nn_bi # 7\n# name this list's elements \noutputs_bigrams &lt;- outputs_bigrams %&gt;% \n  set_names(paste0(\"kwic_\", keywords_phrase))  \n\n# get rid of empty output dfs in list  \noutputs_bigrams2 &lt;- outputs_bigrams[sapply(\n  outputs_bigrams, function(x) dim(x)[1]) &gt; 0] # 4 left!\n \n#or \noutputs_bigrams3 &lt;- purrr::keep(outputs_bigrams, ~nrow(.) &gt; 0)  # 4 left!\n\n# -------------- print all \n#  walk + print -\n#walk(.x = outputs_bigrams2, .f = print)\n\n\n# -------------- save  all -&gt; create multiple tables from a single dataframe and save them as images\n# https://stackoverflow.com/questions/69323569/how-to-save-multiple-tables-as-images-using-kable-and-map/69323893#69323893\n\nout_dir_tab &lt;-  here::here(\"analysis\", \"output\",\"tables\")\n \noutputs_bigrams2  %&gt;%\n  imap(~save_kable(file = paste0(out_dir_tab, '/', 'pdo_', .y, '_.png'),\n                   # bs_theme = 'journal', \n                   self_contained = T, \n                   x = kbl(.x, booktabs = T, align = c('l','l', 'c')) %&gt;%\n                     kable_styling() \n  )\n  )"
  },
  {
    "objectID": "analysis/01b_WB_project_pdo_EDA.html#compare-pdo-words-v-sector-the-tag",
    "href": "analysis/01b_WB_project_pdo_EDA.html#compare-pdo-words-v-sector-the-tag",
    "title": "WB Project PDO text EDA",
    "section": "COMPARE PDO words v sector (the tag)",
    "text": "COMPARE PDO words v sector (the tag)\nBasically I want to compare the trend over time of the frequency of my custom sector word (pdo_train_t$tok_sector_broad) in the PDO text, against the frequency of the sector tag in the dataset (sector1).\n‚Äî- make sector1_broad\n\n\nTHIS STARTs FROM projs_train bc I needed the PROJECTS\n\n\n# Data input\ntabyl(projs_train$sector1)   \n\n# let's select some clear cut sectors e.g. WATER AND SANITATION\nprojs_train &lt;- projs_train %&gt;%\n   mutate (sector1_broad = case_when(\n      # WAT_SAN\n      sector1 == \"Other Water Supply, Sanitation and Waste Management\" ~ \"WAT_SAN\",\n      sector1 == \"Public Administration - Water, Sanitation and Waste Management\" ~ \"WAT_SAN\",\n      sector1 == \"Sanitation\" ~ \"WAT_SAN\",\n      sector1 == \"Water Supply\" ~ \"WAT_SAN\",\n      sector1 == \"Waste Management\" ~ \"WAT_SAN\",\n      # ENERGY\n      sector1 == \"Energy Transmission and Distribution\" ~ \"ENERGY\",\n      sector1 == \"Non-Renewable Energy Generation\" ~ \"ENERGY\",\n      sector1 == \"Other Energy and Extractives\" ~ \"ENERGY\",\n      sector1 == \"Public Administration - Energy and Extractives\" ~ \"ENERGY\",\n      sector1 == \"Renewable Energy Biomass\"  ~ \"ENERGY\", \n      sector1 == \"Renewable Energy Geothermal\"  ~ \"ENERGY\", \n      sector1 == \"Renewable Energy Hydro\"  ~ \"ENERGY\", \n      sector1 == \"Renewable Energy Solar\"  ~ \"ENERGY\", \n      sector1 == \"Renewable Energy Wind\"  ~ \"ENERGY\", \n      sector1 == \"Renewable energy\"  ~ \"ENERGY\", \n      \n      # TRANSPORT\n      sector1 == \"Other Transportation\" ~ \"TRANSPORT\",\n      sector1 == \"Public Administration - Transportation\" ~ \"TRANSPORT\",\n      sector1 == \"Urban Transport\" ~ \"TRANSPORT\",\n      sector1 == \"Rural and Inter-Urban Roads\" ~ \"TRANSPORT\",\n      sector1 == \"Roads and highways\" ~ \"TRANSPORT\",\n      sector1 == \"Ports/Waterways\" ~ \"TRANSPORT\",\n      sector1 == \"Railways\" ~ \"TRANSPORT\",\n      sector1 == \"Airports\" ~ \"TRANSPORT\",\n        # URBAN\n      #niente  \n      #  MINING_OIL_GAS\n      sector1 == \"MINING_OIL_GAS\" ~ \"MINING_OIL_GAS\",\n      sector1 == \"Oil and Gas\" ~ \"MINING_OIL_GAS\",\n      # ICT\n      sector1 == \"ICT Infrastructure\" ~ \"ICT\",\n      sector1 == \"ICT Services\" ~ \"ICT\",\n      sector1 == \"Public Administration - Information and Communications Technologies\" ~ \"ICT\",\n      sector1 == \"Other Information and Communications Technologies\" ~ \"ICT\",\n     # EDUCATION\n      sector1 == \"Other Education\" ~ \"EDUCATION\",\n      sector1 == \"Primary education\" ~ \"EDUCATION\",\n      sector1 == \"Public Administration - Education\" ~ \"EDUCATION\",\n      sector1 == \"Tertiary education\" ~ \"EDUCATION\",\n      sector1 == \"Secondary education\" ~ \"EDUCATION\",\n      sector1 == \"Workforce Development and Vocational Education\" ~ \"EDUCATION\",\n      sector1 == \"Adult, Basic and Continuing Education\" ~ \"EDUCATION\",\n      sector1 == \"Early Childhood Education\" ~ \"EDUCATION\",\n      # HEALTH\n      sector1 == \"Health\" ~ \"HEALTH\",\n      sector1 == \"Public Administration - Health\" ~ \"HEALTH\",\n      sector1 == \"Health facilities and construction\" ~ \"HEALTH\",\n       # else\n      TRUE ~ sector1\n   ))\n\n# check\npdo_train_t %&gt;% \n   filter(tok_sector_broad %in% \n             c(\"WAT_SAN\", \"ENERGY\", \"TRANSPORT\", \"MINING_OIL_GAS\", \"ICT\", \"EDUCATION\", \"HEALTH\")) %&gt;%\n   tabyl(tok_sector_broad, show_missing_levels =  T) \n\n\nprojs_train %&gt;% \n   filter(sector1_broad %in% \n             c(\"WAT_SAN\", \"ENERGY\", \"TRANSPORT\", \"MINING_OIL_GAS\",           \"ICT\", \"EDUCATION\", \"HEALTH\")) %&gt;% \n   tabyl(sector1_broad) \n\n‚Äî- prep data sector_broad_tag\n\n\npaint(projs_train )\n\n# prep data\nsector_broad_tag &lt;- projs_train %&gt;% \n      mutate(FY_appr = boardapprovalFY) %&gt;%\n   filter(!is.na(sector1_broad)) %&gt;% \n   filter(sector1_broad %in% \n             c(\"WAT_SAN\", \"ENERGY\", \"TRANSPORT\", \"MINING_OIL_GAS\",           \"ICT\", \"EDUCATION\", \"HEALTH\")) %&gt;% \n   select (FY_appr,  sector1_broad )  %&gt;% \n   # count(FY_appr, sector1_broad) %&gt;% \n   # filter(n &gt; 0) %&gt;% \n   mutate(sector1_broad = factor(sector1_broad, levels = c(\n      \"WAT_SAN\", \"ENERGY\", \"TRANSPORT\",#\"URBAN\",\n      \"MINING_OIL_GAS\",\"ICT\", \"HEALTH\", \"EDUCATION\" ))) # reorder values by frequency\n#df$FY\n\n\n# data long by sector1_broad (sector_broad_tag)\npaint(sector_broad_tag)\n\n# which tag_sector (gia tolto NA!!!)\n#tabyl(sector_broad_tag$sector1 )\ntabyl(sector_broad_tag$sector1_broad )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n‚Äî- [FUNC] Plot each tag sector\nHere I have much bigger numbers\n\n# --- Split data long into a LIST of subset by sector\nsector_list &lt;- base::split(x =  sector_broad_tag, f = sector_broad_tag$sector1_broad)\nstr(sector_list) \n \n# --- FUNCTION to plot iteratively each sector (like f_plot_sector)\nf_plot_tag_sector &lt;- function( name_sec){\n # Ensure name_sec is treated as a character\n   data_sec &lt;- sector_list[[as.character(name_sec)]]\n #  data_sec &lt;- sector_list[[\"ENERGY\"]]\n\n      data_sec &lt;- data_sec %&gt;% \n      group_by(FY_appr)  %&gt;% \n       count() %&gt;% \n       ungroup() #%&gt;% \n      # #mutate(FY_appr = as.Date(FY_appr, format = \"%Y-%m-%d\"))\n   \n   # plot\n   ggplot(data = data_sec, aes(x = FY_appr, y = n)) +\n      geom_line(color = sector_colors[name_sec], linetype = \"dotted\", alpha = 0.5, size = 1) +   \n      geom_point(color = sector_colors[name_sec], size = 3) +     \n      scale_x_continuous(breaks = seq(2001, 2023, by = 1)) +\n      scale_y_continuous(breaks = seq(0, max(data_sec$n), by = 5)) +\n      labs(title = name_sec, x = \"Year\", y = \"Number of projects\") +\n      # custom\n      lulas_theme + \n      theme(axis.text.x = element_text(angle = 45, hjust = 1)) +\n      labs(\n         title = paste(\"\\\"\",name_sec,\"\\\" in tag by fiscal years of approval\"),  # Use facet-specific title\n         subtitle = \"[Using variable \\\"sector1\\\"]\",\n         x = \"\", \n         y = \"\"  # Remove y-axis label\n      ) \n\n}\n\n# --- Plot one sector \n# name_sec EDUCATION    ENERGY    HEALTH       ICT     MINING_OIL_GAS TRANSPORT    WAT_SAN \n# data_sec EX sector_list[[\"WAT_SAN\"]]\nf_plot_tag_sector( name_sec = \"WAT_SAN\")\nf_plot_tag_sector( name_sec = \"ENERGY\")\nf_plot_tag_sector( name_sec = \"TRANSPORT\")\nf_plot_tag_sector( name_sec = \"ICT\")\nf_plot_tag_sector( name_sec = \"HEALTH\")\nf_plot_tag_sector( name_sec = \"EDUCATION\")\n# Use purrr::map to apply the function over the names of sector_list\n map(names(sector_list), f_plot_tag_sector)"
  },
  {
    "objectID": "analysis/01b_WB_project_pdo_EDA.html#combine-two-sets-of-data-sector_broad_tag-and-sector_broad_pdo",
    "href": "analysis/01b_WB_project_pdo_EDA.html#combine-two-sets-of-data-sector_broad_tag-and-sector_broad_pdo",
    "title": "WB Project PDO text EDA",
    "section": "‚Äî- Combine two sets of data sector_broad_tag and sector_broad_pdo\n",
    "text": "‚Äî- Combine two sets of data sector_broad_tag and sector_broad_pdo\n\n\n# not sure why \nsector_broad_tag &lt;- sector_broad_tag %&gt;% \n   count(FY_appr, sector1_broad)\n\n# Combine two sets of data\nstr(sector_broad_pdo)\nstr(sector_broad_tag)\n\nsector_broad_combo &lt;- left_join(sector_broad_pdo, sector_broad_tag, \n                                by = c(\"FY_appr\", \"tok_sector_broad\" = \"sector1_broad\") ,\n                                suffix = c(\"_pdo\", \"_tag\")\n                                ) %&gt;% \n   filter (!is.na(n_tag))\n\nsector_broad_combo\n\n‚Äî [TAB] Kolmorogov-Smirnov test test of similarity with a table\nIn Kolmorogov-Smirnov test:\n\nthe null hypothesis is that the two distributions are the same\n\nThe alternative hypothesis is that the two distributions are different.\n\n\nThe test statistic is the maximum difference between the two cumulative distribution functions. The p-value is the probability of observing a test statistic as extreme as the one observed, assuming the null hypothesis is true.\n\n# Function to calculate KS test results and save to a table without plotting\nks_results_k &lt;- sector_broad_combo  %&gt;%\n   group_by(tok_sector_broad) %&gt;%\n   summarize(\n      # ks_alt_hyp = ks.test(\n      #   (n_pdo - min(n_pdo)) / (max(n_pdo) - min(n_pdo)),\n      #   (n_tag - min(n_tag)) / (max(n_tag) - min(n_tag))\n      # )$alternative, \n \n      # a) with normalization     \n      ks_statistic = ks.test(\n         (n_pdo - min(n_pdo)) / (max(n_pdo) - min(n_pdo)),\n         (n_tag - min(n_tag)) / (max(n_tag) - min(n_tag)))$statistic, \n      \n      ks_p_value = ks.test(\n         (n_pdo - min(n_pdo)) / (max(n_pdo) - min(n_pdo)),\n         (n_tag - min(n_tag)) / (max(n_tag) - min(n_tag)))$p.value,\n      similarity = ifelse(ks_p_value &gt; 0.05, \"Similar\", \"Dissimilar\"),\n      \n      # # b) without normalization\n      #  ks_statistic_raw = ks.test(n_pdo, n_tag)$statistic,\n      #  ks_p_value_raw = ks.test(n_pdo, n_tag)$p.value,\n       ) %&gt;%\n   ungroup()  %&gt;% \n   arrange(ks_p_value)\n\n# save as object\nwrite_rds(ks_results_k, here(\"analysis\", \"output\", \"tables\" ,\"ks_results_k.rds\"))\n\n# Count how often the phrase appears in the vicinity of the target bigram\nks_results_k %&gt;% \n   kable(format = \"html\", \n         col.names = c(\"SECTORS\", \"KS statistic\",\"KS p-value\",  \"Distributions\"#, \"KS statistic R\",\"KS p-valueR\" \n                       ),\n         # Round  to 4 digits) \n         digits = c(0, 4, 4, 0#, 4,4\n                    )) %&gt;% \n   kable_styling(full_width = FALSE) %&gt;%\n   row_spec(which(ks_results_k$similarity == \"Dissimilar\"), background = \"#e7d8da\")\n\n\n\n\nSECTORS\nKS statistic\nKS p-value\nDistributions\n\n\n\nENERGY\n0.6522\n0.0001\nDissimilar\n\n\nHEALTH\n0.3913\n0.0487\nDissimilar\n\n\nWAT_SAN\n0.3913\n0.0544\nSimilar\n\n\nEDUCATION\n0.3478\n0.1002\nSimilar\n\n\nICT\n0.2857\n0.3399\nSimilar\n\n\nMINING_OIL_GAS\n0.3333\n0.3442\nSimilar\n\n\nTRANSPORT\n0.2174\n0.6410\nSimilar\n\n\n\n\n\n\nAdditional plots\n‚Äî [FIG] Kolmogorov-Smirnov Test for Similarity of PDO and TAG Distributions by Sector\n\nks_results_k %&gt;% \n   ggplot(aes(x = reorder(tok_sector_broad, ks_p_value), y = ks_p_value)) +\n   geom_col(fill = \"#0073C2FF\") +\n   geom_text(aes(label = round(ks_p_value, 4)), vjust = -0.5) +\n   coord_flip() +\n   labs(\n      title = \"Kolmogorov-Smirnov Test for Similarity of PDO and TAG Distributions by Sector\",\n      x = \"Sector\",\n      y = \"P-Value\"\n   ) +\n   theme_minimal() +\n   theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\nFigure¬†1: Kolmogorov-Smirnov Test for Similarity of PDO and TAG Distributions by Sector\n\n\n\n\n\n\n\n‚Äî [FIG] Correlation between PDO and TAG by sector\nCorrelation Coefficients: Display the correlation between n_pdo and n_tag within each sector\n\nsector_broad_combo %&gt;%\n   ggplot(aes(x = n_pdo, y = n_tag)) +\n   geom_point() +\n   geom_smooth(method = \"lm\", se = FALSE) +\n   facet_wrap(~tok_sector_broad, scales = \"free\") +\n   labs(\n      title = \"Correlation between PDO and TAG by Sector\",\n      x = \"Normalized PDO\",\n      y = \"Normalized TAG\"\n   ) +\n   theme_minimal()\n\n\n\nFigure¬†2: Correlation between PDO and TAG by sector"
  },
  {
    "objectID": "analysis/01b_WB_project_pdo_EDA.html#compare-pdo-words-v-sector",
    "href": "analysis/01b_WB_project_pdo_EDA.html#compare-pdo-words-v-sector",
    "title": "WB Project PDO text EDA",
    "section": "COMPARE PDO words v sector ($$)",
    "text": "COMPARE PDO words v sector ($$)\n‚Äî- count of PDO with sector words sector_broad_pdo corresponding to each tok_sector_broad per year\nThis is a count of OBS per cell\n\nsector_broad_pdo   # 345\n\n‚Äî- sum of sum_curr_total_commitment corresponding to each sector1_broad per year\n\nTHIS STARTs FROM projs_train bc I needed the PROJECTS\n\n\n# prep data\nsector_broad_commit &lt;- projs_train %&gt;% \n   select(FY_appr = boardapprovalFY, sector1_broad, curr_total_commitment) %&gt;% \n   # group_by(FY_appr, sector1_broad) %&gt;% \n   # summarise(sum_curr_total_commitment = sum(curr_total_commitment)) %&gt;% \n   # ungroup() %&gt;% \n   # mutate(FY_appr = as.character(FY_appr)) %&gt;% \n   filter (sector1_broad %in% c(\"WAT_SAN\", \"ENERGY\",\"TRANSPORT\", \"MINING_OIL_GAS\", \"ICT\", \"HEALTH\", \"EDUCATION\", \"URBAN\")) %&gt;% \n   mutate(FY_appr = as.numeric(FY_appr)) %&gt;% \n   arrange(sector1_broad,FY_appr) %&gt;% \n   group_by(sector1_broad, FY_appr ) %&gt;% \n   summarise(sum_commit = sum(curr_total_commitment) , .groups = \"drop\") %&gt;%\n   complete(sector1_broad, FY_appr = full_seq(FY_appr, 1), fill = list(sum_commit = 0))  # Fill missing years\n\n\nsector_broad_pdo &lt;- sector_broad_pdo %&gt;% \n   filter(tok_sector_broad %in% c(\"WAT_SAN\", \"ENERGY\",\"TRANSPORT\", \"MINING_OIL_GAS\", \"ICT\", \"HEALTH\", \"EDUCATION\" ))  \n\npaint(sector_broad_pdo)\nnrow(sector_broad_pdo) # 161 \ntabyl(sector_broad_pdo$tok_sector_broad, show_missing_levels = F, show_na = F)  \ntabyl(sector_broad_pdo$FY_appr, show_missing_levels = F, show_na = F)  \n\npaint(sector_broad_commit)\nnrow(sector_broad_commit) # 161  \ntabyl(sector_broad_commit$sector1_broad, show_missing_levels = F, show_na = F)  \ntabyl(sector_broad_commit$FY_appr, show_missing_levels = F, show_na = F)  \n\n\n# merge the two datasets\nsector_broad_pdo_comm &lt;-  left_join (sector_broad_pdo, sector_broad_commit, by = c(\"tok_sector_broad\" = \"sector1_broad\", \"FY_appr\" = \"FY_appr\"))  \n\n‚Äî [TAB] Kolmorogov-Smirnov test test of similarity with a table\nIn Kolmorogov-Smirnov test:\n\nthe null hypothesis is that the two distributions are the same\n\nThe alternative hypothesis is that the two distributions are different.\n\n\nThe test statistic is the maximum difference between the two cumulative distribution functions. The p-value is the probability of observing a test statistic as extreme as the one observed, assuming the null hypothesis is true.\n\n# Function to calculate KS test results and save to a table without plotting\nks_results2_k &lt;- sector_broad_pdo_comm  %&gt;%\n   group_by(tok_sector_broad) %&gt;%\n   # min -max normalization\n   mutate(n_scaled = (n - min(n, na.rm = TRUE)) / \n             (max(n, na.rm = TRUE) - min(n, na.rm = TRUE)),\n          sum_commit_scaled = (sum_commit - min(sum_commit, na.rm = TRUE)) / \n             (max(sum_commit, na.rm = TRUE) - min(sum_commit, na.rm = TRUE)) ) %&gt;%\n   summarize(\n      # -- a) with normalization     \n      ks_statistic = ks.test(n_scaled, sum_commit_scaled)$statistic,\n      ks_p_value = ks.test(n_scaled, sum_commit_scaled )$p.value,\n      similarity = ifelse(ks_p_value &gt; 0.05, \"Similar\", \"Dissimilar\"),\n      # -- b) without normalization\n      #  ks_statistic_raw = ks.test(n_pdo, n_tag)$statistic,\n      #  ks_p_value_raw = ks.test(n_pdo, n_tag)$p.value,\n       ) %&gt;%\n   ungroup()  %&gt;% \n   arrange(ks_p_value)\n\n# save as object\nwrite_rds(ks_results2_k, here(\"analysis\", \"output\", \"tables\" ,\"ks_results2_k.rds\"))\n\n# Count how often the phrase appears in the vicinity of the target bigram\nks_results2_k %&gt;% \n   kable(format = \"html\", \n         col.names = c(\"SECTORS\", \"KS statistic\",\"KS p-value\",  \"Distributions\"#, \"KS statistic R\",\"KS p-valueR\" \n                       ),\n         # Round  to 4 digits) \n         digits = c(0, 4, 4, 0#, 4,4\n                    )) %&gt;% \n   kable_styling(full_width = FALSE) %&gt;%\n   row_spec(which(ks_results2_k$similarity == \"Dissimilar\"), background = \"#e7d8da\")\n\n\n\n\nSECTORS\nKS statistic\nKS p-value\nDistributions\n\n\n\nEDUCATION\n0.6522\n0.0001\nDissimilar\n\n\nICT\n0.6522\n0.0001\nDissimilar\n\n\nHEALTH\n0.5652\n0.0010\nDissimilar\n\n\nMINING_OIL_GAS\n0.5217\n0.0031\nDissimilar\n\n\nENERGY\n0.3478\n0.1235\nSimilar\n\n\nTRANSPORT\n0.2609\n0.4218\nSimilar\n\n\nWAT_SAN\n0.2609\n0.4218\nSimilar\n\n\n\n\n\n\n‚Äî [FUNC] standardize and plot\n\n\nStandardization is done by subtracting the mean and dividing by the standard deviation. This is done for both the n and sum_commit columns. In this way we can compare the two distributions on the same scale.\n\nRobust Scaling: Subtract the median and divide by the IQR. This is more robust to outliers than standardization, but it doesn‚Äôt ensure the distributions have the same variance.\n‚úÖ Min-Max Scaling: Rescale both n and sum_commit to a [0, 1] range. This doesn‚Äôt assume normality and ensures both distributions are within the same bounds, though it doesn‚Äôt account for the shape of the distributions.\n\n\nrobust or min-max scaling alternatives can provide more reliable comparisons, especially with skewed data.\n\n\n\nKolmogorov-Smirnov (KS) Test P-Values: Display the p-value from a Kolmogorov-Smirnov test comparing rescaled trends within each sector producing a p-value that indicates the probability of observing these distributions if they were the same.\n\nDIFFERENT = A low p-value (typically &lt; 0.05) suggests the distributions are significantly different, while a higher p-value suggests similarity.\nSIMILAR = A high p-value does not necessarily mean the distributions are identical, only that there is not enough evidence to reject the null hypothesis of similarity.\nThe KS test is non-parametric and makes no assumptions about the underlying distributions, making it a versatile tool for comparing distributions.\n\n\n\n\n# --- FUNCTION to 1) standardize 2 distributions and 2) plot iteratively each sector  \nf_plot_sector_comm &lt;- function(data, sector) {\n\n      # ---- Filter data for the specified sector\n   sector_data &lt;- data %&gt;%\n      filter(tok_sector_broad == sector) %&gt;%\n      group_by(tok_sector_broad) %&gt;%\n      # # ---- Standardize n and sum_commit within each sector\n      # mutate(n_standardized = (n - mean(n, na.rm = TRUE)) / sd(n, na.rm = TRUE),\n      #        sum_commit_standardized = (\n      #           sum_commit - mean(sum_commit, na.rm = TRUE)) / sd(sum_commit, na.rm = TRUE)) %&gt;%\n      # ---- Min-Max Scaling\n      mutate(n_scaled = (n - min(n, na.rm = TRUE)) / \n                (max(n, na.rm = TRUE) - min(n, na.rm = TRUE)),\n             sum_commit_scaled = (sum_commit - min(sum_commit, na.rm = TRUE)) / \n                (max(sum_commit, na.rm = TRUE) - min(sum_commit, na.rm = TRUE)) ) %&gt;%\n      ungroup()\n\n      # ---- Calculate Spearman correlation and KS test p-value for the selected sector\n   sector_stats &lt;- sector_data %&gt;%\n      summarize(\n         spearman_cor = cor(n_scaled, sum_commit_scaled, method = \"spearman\", use = \"complete.obs\"),\n         ks_p_value = ks.test(n_scaled, sum_commit_scaled)$p.value,\n         similarity = ifelse(ks_p_value &gt; 0.05, \"Similar\", \"Dissimilar\")\n      )\n\n      \n   # Extract the color for the sector line (you can set a specific color or use ggplot's color palette)\n   pdo_color &lt;- \"#8e550a\" # Get a color from ggplot's default palette\n   commit_color &lt;-\"#00689D\"  # Set a color for the secondary line\n   \n   # Plot the data for the selected sector\n   ggplot(sector_data, aes(x = FY_appr)) +\n      # --- geom_bar for rel_freq_n_pdo\n      geom_line(aes(y = n_scaled), color = pdo_color, alpha = 0.75) +\n      geom_point(aes(y = n_scaled), color = pdo_color, alpha = 0.75, size = 2) +\n      # --- geom_line and geom_point for rel_freq_commitment\n      geom_line(aes(y = sum_commit_scaled ), color = commit_color, linetype = \"dashed\" ) +\n      geom_point(aes(y = sum_commit_scaled), color = commit_color, size = 2) +\n      # --- scale\n      scale_x_continuous(breaks = seq(2001, 2023, by = 1)) +\n      scale_y_continuous(\n         name = \"N words in PDOs (mean = 0, sd = 1)\",\n         sec.axis = sec_axis(~., name = \"$$ Committed (mean = 0, sd = 1)\")\n      ) +\n      \n            # --- Annotate KS test results directly on the plot\n      annotate(\n         \"text\", x = Inf, y = Inf, label = paste(\"KS test p-value:\", round(sector_stats$ks_p_value, 4)),\n         hjust = 1.1, vjust = 1.1, color = \"black\", size = 4\n      ) +\n      \n       # Customize colors and set common legend title\n        labs(\n         title = paste(\"Word frequency in PDO v. amount committed for\", sector),\n         subtitle = \"n_pdo and sum_commit are rescaled with Min-Max scaling \\nKolmogorov-Smirnov test for similarity between the two trends\",\n         x = \"\"\n      ) +\n     \n      # custom\n      lulas_theme + \n      theme(   legend.position = \"none\",\n               axis.text.x = element_text(angle = 45, hjust = 1),\n               axis.title.y = element_text(color = pdo_color ), \n               axis.title.y.right = element_text(color =  commit_color ) \n      )\n}\n# --- Plot one sector \n# name_sec EDUCATION    ENERGY    HEALTH       ICT     MINING_OIL_GAS TRANSPORT    WAT_SAN \n# data_sec EX sector_list[[\"WAT_SAN\"]]\nf_plot_sector_comm(sector_broad_pdo_comm, sector = \"TRANSPORT\") # KS p-val = 0.42 similar\nf_plot_sector_comm(sector_broad_pdo_comm, sector = \"WAT_SAN\") # KS p-val = 0.42 similar \nf_plot_sector_comm(sector_broad_pdo_comm, sector = \"ENERGY\") # KS p-val = 0.12 similar\nf_plot_sector_comm(sector_broad_pdo_comm, sector = \"ICT\") # KS p-val = 0.0001 DIFFERENT\nf_plot_sector_comm(sector_broad_pdo_comm, sector = \"HEALTH\") # KS p-val = 0.001 DIFFERENT\nf_plot_sector_comm(sector_broad_pdo_comm, sector = \"EDUCATION\") # KS p-val = 0.0001 DIFFERENT\n# many with non commitment \nf_plot_sector_comm(sector_broad_pdo_comm, sector = \"MINING_OIL_GAS\") # KS p-val = 0.0031 DIFFERENT\n\n‚Äî [FIG] Plot sector WAT_SAN\n\nvery similar trends in PDO and commitment\n\n\n\n\nFigure¬†10\n\n\n\n\n\n\n\n‚Äî [FIG] Plot sector ICT\n\nvery different trends in PDO and commitment\n\n\n\n\nFigure¬†11"
  },
  {
    "objectID": "R/Callouts.html",
    "href": "R/Callouts.html",
    "title": "Callouts",
    "section": "",
    "text": "TOOL\n\n\n\n\n\n\n Nerdy Note\n\n\n\n\n\nXXX\n\n\n\n\n\n\n\n\n\nTip\n\n\n\n\n\nxxx\n\n\n\n\n\n\n\n\n\nCaution\n\n\n\n\n\nxxx\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\n\n\nxxx\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\n\n\nxxx\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\n\nxxx"
  }
]