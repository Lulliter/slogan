[
  {
    "objectID": "data/raw_data/project2/all_projects_as_of29ago2024descr-stats.html",
    "href": "data/raw_data/project2/all_projects_as_of29ago2024descr-stats.html",
    "title": "Source",
    "section": "",
    "text": "Source\nALL WB projects available as of 31/08/2024 using simply the Excel button on this page this WBG Projects.\n\n\nFormat\n\nSaved HUUUGE .xls file in data/raw_data/project2/all_projects_as_of29ago2024.xls\nalso saved/copied as all_projects_as_of29ago2024.Rdata\n\n\n\nNote\nI tried using the API from link but it was too slow to lead and I couldn’t ….\n\n\nData sampling frame\n\nData on WBG projects from boardapprovalFY 1947 to 2026\nColumns\n\n…1 chr Project ID |id …2 chr Region |regionname …3 chr Country |countryname …4 chr Project Status |projectstatusdisplay …5 chr Last Stage Reached Name |last_stage_reached_name …6 chr Project Name |project_name …7 chr Project Development Objective |pdo …8 chr Implementing Agency |impagency …9 chr Consultant Services Required |cons_serv_reqd_ind …10 chr Project URL |url …11 chr Board Approval Date |boardapprovaldate …12 chr Project Closing Date |closingdate …13 chr Financing Type |projectfinancialtype …14 chr Current Project Cost |curr_project_cost …15 chr IBRD Commitment |curr_ibrd_commitment …16 chr IDA Commitment |curr_ida_commitment …17 chr Total IDA and IBRD Commitment |curr_total_commitme~ …18 chr Grant Amount |grantamt …19 chr Borrower |borrower …20 chr Lending Instrument |lendinginstr …21 chr Environmental Assessment Category |envassesmentcategorycode …22 chr Environmental and Social Risk |esrc_ovrl_risk_rate …23 chr Sector 1 |sector1 …24 chr Sector 2 |sector2 …25 chr Sector 3 |sector3 …26 chr Theme 1 |theme1 …27 chr Theme 2 |theme2\n\n\nData collection date(s)"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "analysis/hypotheses.html",
    "href": "analysis/hypotheses.html",
    "title": "Research Questions & Hypotheses",
    "section": "",
    "text": "The research question depends (in terms of its nature) on the type of analysis we intend to carry out (See fig 3.9 in Francom 2024):\n\n\nTable 1: Overview of analysis types\n\n\n\n\n\n\n\n\n\n\nType\nAims\nApproach\nMethods\nEvaluation\n\n\n\nExploratory\nExplore: gain insight\nInductive, data-driven, and iterative\nDescriptive, pattern detection with machine learning (unsupervised)\nAssociative\n\n\nPredictive\nPredict: validate associations\nSemi-deductive, data-/ theory-driven, and iterative\nPredictive modeling with machine learning (supervised)\nModel performance, feature importance, and associative\n\n\nInferential\nExplain: test hypotheses\nDeductive, theory-driven, and non-iterative\nHypothesis testing with statistical tests\nCausal\n\n\n\n\n\n\n\n\nIs there a pattern in the WBG project document corpus1 that shows non random variation in the incidence of certain policy concepts2 over time?\n\n\nCould the WDR 3 “explain” or at least have a correlation to the appearance-prevalence of said concepts?\n\nFor the moment, the present study’s research aim (See Table 1) is mainly TO EXPLORE (trends over time in concepts use), and possibly to PREDICT (conjecture about WRD traction effect).",
    "crumbs": [
      "Analysis",
      "Questions",
      "Research Questions & Hypotheses"
    ]
  },
  {
    "objectID": "analysis/hypotheses.html#rq-1.1",
    "href": "analysis/hypotheses.html#rq-1.1",
    "title": "Research Questions & Hypotheses",
    "section": "RQ #1.1",
    "text": "RQ #1.1\nThe launch of a “policy slogan” carries intrinsic motivations to shift the PDO in a certain direction.\n\n\nH1: if WDR mentions a certain concept, I expect it will appear more frequently in the PDO of the subsequne few FY.",
    "crumbs": [
      "Analysis",
      "Questions",
      "Research Questions & Hypotheses"
    ]
  },
  {
    "objectID": "analysis/hypotheses.html#footnotes",
    "href": "analysis/hypotheses.html#footnotes",
    "title": "Research Questions & Hypotheses",
    "section": "Footnotes",
    "text": "Footnotes\n\nWBG project document observed in this case are Project Development Objectives (PDO) descriptive short texts.↩︎\nconcepts means ….↩︎\nWDRs are the flagship reports of the World Bank group…↩︎\nAnnex to the Giuseppe Fioravanti’s book (Fioravanti 2006)↩︎",
    "crumbs": [
      "Analysis",
      "Questions",
      "Research Questions & Hypotheses"
    ]
  },
  {
    "objectID": "posts/2023-11-15-revisione-libro-il-sistema-invisibile/index.html",
    "href": "posts/2023-11-15-revisione-libro-il-sistema-invisibile/index.html",
    "title": "Revisione libro: Il Sistema invisibile, di Marcello Foa del 15 Novembre 2023",
    "section": "",
    "text": "Da “4.4. Come disarticolare una società”\n\n\nVladimis Volkoff (Il montaggio) “Quando si tratta di mobilitare le masse in realta non si ha che uno scopo: immobilizzarle” (…) tante volte la nostra rabbia e’ stata indirizzata verso bersagli di facile presa, il politico corrotto che abusa dell’auto blu, mentre i veri sprechi e gli scandali rilevanti erano altri, come nel caso di Autostrade Italiane. Tante, troppe volte l’indignazione delle masse e’ stata indirizzata contro bersagli fittizzi o sproporzionati, che non hanno prodotto altro effetto che generare sfiducia socaile verso le autorita costituite, i partiti e le istituzioni (p. 104, 105)\n\n\n\n\nCitazioneBibTeX@online{m. mimmi2023,\n  author = {M. Mimmi, Luisa and M. Mimmi, Luisa},\n  title = {Revisione libro: Il Sistema invisibile, di Marcello Foa del\n    15 Novembre 2023},\n  date = {2023-11-15},\n  url = {https://lulliter.github.io/slogan/posts/2023-11-15-revisione-libro-il-sistema-invisibile},\n  langid = {it}\n}\nPer favore citare questo lavoro come:\nM. Mimmi, Luisa, and Luisa M. Mimmi. 2023. “Revisione libro: Il\nSistema invisibile, di Marcello Foa del 15 Novembre 2023.”\nNovember 15, 2023. https://lulliter.github.io/slogan/posts/2023-11-15-revisione-libro-il-sistema-invisibile."
  },
  {
    "objectID": "intro_NLP.html",
    "href": "intro_NLP.html",
    "title": "Intro to NLP",
    "section": "",
    "text": "Text Mining (or Text Analytics) is the process of deriving meaningful information from unstructured text data, which involves techniques that can identify patterns, trends, and correlations in text data. As a research method, it is versatile in its aplications and can be combined with different disciplines and techniques.\n\n\nText analytics often utilizes NLP techniques to process and analyze text.\nA subfield of artificial intelligence (AI), NLP focuses on enabling computers to understand, interpret, and generate human language.\n\nWhile NLP provides the tools and algorithms (like tokenization, parsing, and entity recognition), text mining applies these tools to extract specific information from large text corpora.\n\n\n\nCorpus Linguistics is a branch of text analysis research applied to linguistics (e.g. the role of frequency and phonotactics in affix ordering in English, study of idiomatic expressions, geographic spread of neologisms, etc.) or where insight from language is sought\n\nSometimes, Text analytics serves as a method to support other research methods.\n\n\n\n\nPopulation (in language) ~ Any (idealized) compendium of words that we are interested in analysing… (most) populations are amorphous moving targets.\n\n\nCorpus (pl. Corpora) ~ A language population is called a corpus, a collection of similar documents | objects that typically contain raw strings annotated with additional metadata and details\n\n\nreference corpora, e.g. the American National Corpus\nspecialized corpora\nparrallel and comparable corpora\n\n\n\nUnstructured Data ~ data which does not have a machine-readable internal structure. This is the case for plain text files (.txt), which are simply a sequence of characters (as opposed to structured data that conforms to tabular format and is machine readable)\n\n\n.json format is somwhere in the middle /~ semi-structured data /~ which reflects the autho rpreferences\n\n\n\nString ~ in computational approaches, a string is a specific type of data that represents text and is often encoded in specific format, e.g., Latin1 or UTF8.\n\nTidy text ~ refers both to the structural (physical) and infor- mational (semantic) organization of the dataset. Physically, a tidy dataset is a tabular data structure, where each row is an observation (e.g. token) and each column is a variable that contains measures of a feature or attribute of each observation.\n\n\nToken ~ is a meaningful unit of text, such as a word, that we are interested in using for analysis\n\nTypes ~ refers to the unique tokens in a term variable (&lt; of tokens if repetition)\n\n\n\nBigrams/n-gram ~ Sequential groupings of characters and words (e.g.sentence, or paragraph)\n\nCollocations ~ words that are attracted to each other (and that co-occur or co-locate together), e.g., Merry Christmas, Good Morning, No worries.\n\n\n\nText normalization ~ standardizing text to convert the text into a uniform format and reduce unwanted variation and noise. (e.g. eliminating missing, redundant, or anoma- lous observations, changing the case of the text, removing punctuation, stan- dardizing forms, etc.)\n\nText Tokenization ~ splitting text into tokens, i.e. adapting the text so it reflects the target lin- guistic unit that will be used in the analysis. (involves expanding or reducing the number of rows depending on the linguistic unit of analysis)\n\nEnrichment transformations ~ to add new attributes to the dataset (e.g. generation, recoding, and integration of observations and/or variables.)\n\nStemming ~ is the process of reducing inflected words to their word stem, base, or root form. E.g.: believe --\\&gt; believ\n\nA stem is the base part of a word to which affixes can be attached for derivatives\n\n\n\nLemmatization ~ is the process of reducing inflected words to their dictionary form, or lemma. E.g.: gone\\|going --\\&gt; go\n\n\nDocument-term matrix (DTM) ~ rows = documents | cols = words | cells = [0,1]/frequencies. A sparse matrix describing a collection (i.e., a corpus) of documents with one row for each document and one column for each term. (WDR_com)\n\nTerm-Document matrix (TDM) ~ rows = words | cols = documents | cells = [0,1]/frequencies. A sparse matrix describing a collection (i.e., a corpus) of documents with one row for each document and one column for each term. (WDR_com)\n\nA part of computer science and AI that deals with human languages, Natural Language Processing (NLP) has evolved significantly over the past few decades, driven by advances in computational power, machine learning, and the availability of large datasets.\nBroadly speaking, these were some key steps in its evolution:\n\nEarly Years (1950s - 1980s) - Rule-based Systems (Early NLP systems were based on rule-based methods, which relied on handcrafted rules for tasks like translation, parsing, and information retrieval).\n1970s - 1980s: Statistical Methods and Linguistic Models (The introduction of the Chomskyan Linguistic Models influenced NLP research, focusing on syntax and grammar, while statistical methods began to emerge, laying the groundwork for more data-driven approaches)\n1990s: Statistical NLP (significant shift towards statistical approaches due to the availability of larger text corpora and more powerful computers, Hidden Markov Models (HMMs) and n-grams became popular for tasks such as part-of-speech tagging, speech recognition, and machine translation)\n2000s: Machine Learning and Data-Driven Methods (rise of machine learning in NLP, particularly supervised learning methods: Support Vector Machines (SVMs), Maximum Entropy models, etc. The development of large annotated corpora and platforms fueled progress in areas such as parsing, word sense disambiguation, and sentiment analysis.)\n2010s: Deep Learning Revolution (Neural networks, particularly recurrent neural networks (RNNs) and later long short-term memory (LSTM) networks, became the standard for many NLP tasks. The introduction of word embeddings allowed words to be represented as continuous vectors in a high-dimensional space, capturing semantic relationships between them. Convolutional Neural Networks (CNNs) were applied to text classification and other tasks, although they were more commonly used in computer vision. The development of sequence-to-sequence (Seq2Seq) models enabled advancements in machine translation, summarization, and other sequence generation tasks. Transformers outperformed RNNs on many tasks and led to the development of large-scale pre-trained language models.)\nLate 2010s - Present: Pre-trained Language Models and NLP at Scale (Pre-trained language models like BERT (2018) by Google and GPT (Generative Pre-trained Transformer) by OpenAI revolutionized NLP by providing powerful, general-purpose models that could be fine-tuned for specific tasks with minimal training data. The concept of transfer learning became central, where models trained on massive datasets could be adapted to specific tasks. ChatGPT, BERT, T5, and FLAN-T5 continue to push the boundaries of what NLP can achieve, leading to increasingly sophisticated and human-like interactions.)\n2020s - Future Directions Multimodal models: Integrating NLP with other forms of data, such as images and audio, to create more comprehensive models. Explainability and interpretability: As models grow in complexity, understanding their decision-making processes becomes more important.\n\n\n\n\nNatural Language Processing (NLP) ~ is an interdisciplinary field in computer science that has specialized on processing natural language data using computational and mathematical methods.\n\nNetwork Analysis ~ the most common way to visualize relationships between entities. Networks, also called graphs, consist of nodes (typically represented as dots) and edges (typically represented as lines) and they can be directed or undirected networks.\n\nText Classification ~ a supervised learning method of learning and predicting the category or the class of a document given its text content.\n\nNamed Entity Recognition ~ NER is the task of classifying words or key phrases of a text into predefined entities of interest.\n\nText Summarization ~ a language generation task of summarizing the input text into a shorter paragraph of text.\n\nEntailment ~ the task of classifying the binary relation between two natural-language texts, text and hypothesis, to determine if the text agrees with the hypothesis or not.\n\nQuestion Answering ~ QA is the task of retrieving or generating a valid answer for a given query in natural language, provided with a passage related to the query.\n\nSentence Similarity ~ the process of computing a similarity score given a pair of text documents.\n\nEmbeddings ~ the process of converting a word or a piece of text to a continuous vector space of real number, usually, in low dimension.\n\nSentiment Analysis ~ Provides an example of train and use Aspect Based Sentiment Analysis with Azure ML and Intel NLP Architect.\n\nSemantic Analysis ~ Allows to analyze the semantic (semantics) fo texts. Such analyses often rely on semantic tagsets that are based on word meaning or meaning families/categories.\n\nPart-of-Speech (PoS) ~ Tagging identifies the word classes of words (e.g., noun, adjective, verb, etc.) in a text and adds part-of-speech tags to each word.\n\nTopic Modeling ~ Topic Modeling is a machine learning method seeks to answer the question: given a collection of documents, can we identify what they are about? Topic model algorithms look for patterns of co-occurrences of words in documents.\n\nThis goes beyond my scope, but just to lay out some important elements ust recall\n\nHermeneutics: from Greek \\(ἑρμηνευτική (τέχνη)\\) - hermeneutikè (téchne), deriving from the verb \\(ἑρμηνεύ\\) (hermēneuō) - is the science and practice of interpretation. Over history, it has been mainly applied to sacred or juridical texts. Two are the main approaches: 1. reconstructing the original intention of the authors 2. adapting the interpretation based on the person who receives the text\nexegesis: from Greek \\(ἐξήγησις\\) - exégesis - deriving from the verb \\(ἐξηγέομαι\\) (exegéomai, ~ bring out) - indicates “studying to explain”, but referring to the maximum level of depth is seeked (hence it is referred to sacred or normaitive texts for which every nuance can be poignant).\nphilology: from Greek \\(ϕιλολογία\\) - philologĭa - composed from \\(φίλος\\)- phìlos and \\(λόγος\\) - lògos - indicates literally interest/love for the word/reason. The term indicates the study of texts and their history, but changed a little with the Latin philologia, and later embraced the sense of ‘love of literature’.\nlinguistics\n\n…\n\n\nAn adage is an ancient saying or maxim, brief and sometimes mysterious, that has become accepted as conventional wisdom. In classical rhetoric, an adage is also known as a rhetorical proverb or paroemia. Often it’s a type of metaphor. It can express the values of a culture. (Nordquist 2018)\n\nExample(s): “The early bird gets the worm”, “Better late than never.”\n\n\nThe English word slogan has a Scottish Gaelic origin and derives from the combination of sluagh (army) + gairm (shout), i.e. a “battle cry”. Nowadays, it signifies a short, memorable and concise phrase used for marketing or political campaigns. Marketing slogans are often called taglines in the United States\n\nExample(s): “…”, “…”\n\n…"
  },
  {
    "objectID": "intro_NLP.html#what-is-text-mining-text-analytics",
    "href": "intro_NLP.html#what-is-text-mining-text-analytics",
    "title": "Intro to NLP",
    "section": "",
    "text": "Text Mining (or Text Analytics) is the process of deriving meaningful information from unstructured text data, which involves techniques that can identify patterns, trends, and correlations in text data. As a research method, it is versatile in its aplications and can be combined with different disciplines and techniques.\n\n\nText analytics often utilizes NLP techniques to process and analyze text.\nA subfield of artificial intelligence (AI), NLP focuses on enabling computers to understand, interpret, and generate human language.\n\nWhile NLP provides the tools and algorithms (like tokenization, parsing, and entity recognition), text mining applies these tools to extract specific information from large text corpora.\n\n\n\nCorpus Linguistics is a branch of text analysis research applied to linguistics (e.g. the role of frequency and phonotactics in affix ordering in English, study of idiomatic expressions, geographic spread of neologisms, etc.) or where insight from language is sought\n\nSometimes, Text analytics serves as a method to support other research methods.\n\n\n\n\nPopulation (in language) ~ Any (idealized) compendium of words that we are interested in analysing… (most) populations are amorphous moving targets.\n\n\nCorpus (pl. Corpora) ~ A language population is called a corpus, a collection of similar documents | objects that typically contain raw strings annotated with additional metadata and details\n\n\nreference corpora, e.g. the American National Corpus\nspecialized corpora\nparrallel and comparable corpora\n\n\n\nUnstructured Data ~ data which does not have a machine-readable internal structure. This is the case for plain text files (.txt), which are simply a sequence of characters (as opposed to structured data that conforms to tabular format and is machine readable)\n\n\n.json format is somwhere in the middle /~ semi-structured data /~ which reflects the autho rpreferences\n\n\n\nString ~ in computational approaches, a string is a specific type of data that represents text and is often encoded in specific format, e.g., Latin1 or UTF8.\n\nTidy text ~ refers both to the structural (physical) and infor- mational (semantic) organization of the dataset. Physically, a tidy dataset is a tabular data structure, where each row is an observation (e.g. token) and each column is a variable that contains measures of a feature or attribute of each observation.\n\n\nToken ~ is a meaningful unit of text, such as a word, that we are interested in using for analysis\n\nTypes ~ refers to the unique tokens in a term variable (&lt; of tokens if repetition)\n\n\n\nBigrams/n-gram ~ Sequential groupings of characters and words (e.g.sentence, or paragraph)\n\nCollocations ~ words that are attracted to each other (and that co-occur or co-locate together), e.g., Merry Christmas, Good Morning, No worries.\n\n\n\nText normalization ~ standardizing text to convert the text into a uniform format and reduce unwanted variation and noise. (e.g. eliminating missing, redundant, or anoma- lous observations, changing the case of the text, removing punctuation, stan- dardizing forms, etc.)\n\nText Tokenization ~ splitting text into tokens, i.e. adapting the text so it reflects the target lin- guistic unit that will be used in the analysis. (involves expanding or reducing the number of rows depending on the linguistic unit of analysis)\n\nEnrichment transformations ~ to add new attributes to the dataset (e.g. generation, recoding, and integration of observations and/or variables.)\n\nStemming ~ is the process of reducing inflected words to their word stem, base, or root form. E.g.: believe --\\&gt; believ\n\nA stem is the base part of a word to which affixes can be attached for derivatives\n\n\n\nLemmatization ~ is the process of reducing inflected words to their dictionary form, or lemma. E.g.: gone\\|going --\\&gt; go\n\n\nDocument-term matrix (DTM) ~ rows = documents | cols = words | cells = [0,1]/frequencies. A sparse matrix describing a collection (i.e., a corpus) of documents with one row for each document and one column for each term. (WDR_com)\n\nTerm-Document matrix (TDM) ~ rows = words | cols = documents | cells = [0,1]/frequencies. A sparse matrix describing a collection (i.e., a corpus) of documents with one row for each document and one column for each term. (WDR_com)"
  },
  {
    "objectID": "intro_NLP.html#what-is-natural-language-processing-nlp",
    "href": "intro_NLP.html#what-is-natural-language-processing-nlp",
    "title": "Intro to NLP",
    "section": "",
    "text": "A part of computer science and AI that deals with human languages, Natural Language Processing (NLP) has evolved significantly over the past few decades, driven by advances in computational power, machine learning, and the availability of large datasets.\nBroadly speaking, these were some key steps in its evolution:\n\nEarly Years (1950s - 1980s) - Rule-based Systems (Early NLP systems were based on rule-based methods, which relied on handcrafted rules for tasks like translation, parsing, and information retrieval).\n1970s - 1980s: Statistical Methods and Linguistic Models (The introduction of the Chomskyan Linguistic Models influenced NLP research, focusing on syntax and grammar, while statistical methods began to emerge, laying the groundwork for more data-driven approaches)\n1990s: Statistical NLP (significant shift towards statistical approaches due to the availability of larger text corpora and more powerful computers, Hidden Markov Models (HMMs) and n-grams became popular for tasks such as part-of-speech tagging, speech recognition, and machine translation)\n2000s: Machine Learning and Data-Driven Methods (rise of machine learning in NLP, particularly supervised learning methods: Support Vector Machines (SVMs), Maximum Entropy models, etc. The development of large annotated corpora and platforms fueled progress in areas such as parsing, word sense disambiguation, and sentiment analysis.)\n2010s: Deep Learning Revolution (Neural networks, particularly recurrent neural networks (RNNs) and later long short-term memory (LSTM) networks, became the standard for many NLP tasks. The introduction of word embeddings allowed words to be represented as continuous vectors in a high-dimensional space, capturing semantic relationships between them. Convolutional Neural Networks (CNNs) were applied to text classification and other tasks, although they were more commonly used in computer vision. The development of sequence-to-sequence (Seq2Seq) models enabled advancements in machine translation, summarization, and other sequence generation tasks. Transformers outperformed RNNs on many tasks and led to the development of large-scale pre-trained language models.)\nLate 2010s - Present: Pre-trained Language Models and NLP at Scale (Pre-trained language models like BERT (2018) by Google and GPT (Generative Pre-trained Transformer) by OpenAI revolutionized NLP by providing powerful, general-purpose models that could be fine-tuned for specific tasks with minimal training data. The concept of transfer learning became central, where models trained on massive datasets could be adapted to specific tasks. ChatGPT, BERT, T5, and FLAN-T5 continue to push the boundaries of what NLP can achieve, leading to increasingly sophisticated and human-like interactions.)\n2020s - Future Directions Multimodal models: Integrating NLP with other forms of data, such as images and audio, to create more comprehensive models. Explainability and interpretability: As models grow in complexity, understanding their decision-making processes becomes more important.\n\n\n\n\nNatural Language Processing (NLP) ~ is an interdisciplinary field in computer science that has specialized on processing natural language data using computational and mathematical methods.\n\nNetwork Analysis ~ the most common way to visualize relationships between entities. Networks, also called graphs, consist of nodes (typically represented as dots) and edges (typically represented as lines) and they can be directed or undirected networks.\n\nText Classification ~ a supervised learning method of learning and predicting the category or the class of a document given its text content.\n\nNamed Entity Recognition ~ NER is the task of classifying words or key phrases of a text into predefined entities of interest.\n\nText Summarization ~ a language generation task of summarizing the input text into a shorter paragraph of text.\n\nEntailment ~ the task of classifying the binary relation between two natural-language texts, text and hypothesis, to determine if the text agrees with the hypothesis or not.\n\nQuestion Answering ~ QA is the task of retrieving or generating a valid answer for a given query in natural language, provided with a passage related to the query.\n\nSentence Similarity ~ the process of computing a similarity score given a pair of text documents.\n\nEmbeddings ~ the process of converting a word or a piece of text to a continuous vector space of real number, usually, in low dimension.\n\nSentiment Analysis ~ Provides an example of train and use Aspect Based Sentiment Analysis with Azure ML and Intel NLP Architect.\n\nSemantic Analysis ~ Allows to analyze the semantic (semantics) fo texts. Such analyses often rely on semantic tagsets that are based on word meaning or meaning families/categories.\n\nPart-of-Speech (PoS) ~ Tagging identifies the word classes of words (e.g., noun, adjective, verb, etc.) in a text and adds part-of-speech tags to each word.\n\nTopic Modeling ~ Topic Modeling is a machine learning method seeks to answer the question: given a collection of documents, can we identify what they are about? Topic model algorithms look for patterns of co-occurrences of words in documents."
  },
  {
    "objectID": "intro_NLP.html#methodological-notes",
    "href": "intro_NLP.html#methodological-notes",
    "title": "Intro to NLP",
    "section": "",
    "text": "This goes beyond my scope, but just to lay out some important elements ust recall\n\nHermeneutics: from Greek \\(ἑρμηνευτική (τέχνη)\\) - hermeneutikè (téchne), deriving from the verb \\(ἑρμηνεύ\\) (hermēneuō) - is the science and practice of interpretation. Over history, it has been mainly applied to sacred or juridical texts. Two are the main approaches: 1. reconstructing the original intention of the authors 2. adapting the interpretation based on the person who receives the text\nexegesis: from Greek \\(ἐξήγησις\\) - exégesis - deriving from the verb \\(ἐξηγέομαι\\) (exegéomai, ~ bring out) - indicates “studying to explain”, but referring to the maximum level of depth is seeked (hence it is referred to sacred or normaitive texts for which every nuance can be poignant).\nphilology: from Greek \\(ϕιλολογία\\) - philologĭa - composed from \\(φίλος\\)- phìlos and \\(λόγος\\) - lògos - indicates literally interest/love for the word/reason. The term indicates the study of texts and their history, but changed a little with the Latin philologia, and later embraced the sense of ‘love of literature’.\nlinguistics\n\n…"
  },
  {
    "objectID": "intro_NLP.html#essential-list-of-rhetorical-devices",
    "href": "intro_NLP.html#essential-list-of-rhetorical-devices",
    "title": "Intro to NLP",
    "section": "",
    "text": "An adage is an ancient saying or maxim, brief and sometimes mysterious, that has become accepted as conventional wisdom. In classical rhetoric, an adage is also known as a rhetorical proverb or paroemia. Often it’s a type of metaphor. It can express the values of a culture. (Nordquist 2018)\n\nExample(s): “The early bird gets the worm”, “Better late than never.”\n\n\nThe English word slogan has a Scottish Gaelic origin and derives from the combination of sluagh (army) + gairm (shout), i.e. a “battle cry”. Nowadays, it signifies a short, memorable and concise phrase used for marketing or political campaigns. Marketing slogans are often called taglines in the United States\n\nExample(s): “…”, “…”\n\n…"
  },
  {
    "objectID": "posts/2024-05-29-sviluppo-sostenibile/index.html",
    "href": "posts/2024-05-29-sviluppo-sostenibile/index.html",
    "title": "Sviluppo sostenibile",
    "section": "",
    "text": "«Perfezionamento, miglioramento, progresso verso un superiore livello tecnico o metodologico, qualitativo, estetico o stilistico in un determinato ambito pratico o intellettuale. […] Incremento, potenziamento di un’attività industriale, terziaria, agricola, di una produzione o più genericamente di un sistema economico. […] Processo di crescita verso la piena maturità psicofisica, mentale, sessuale; irrobustimento del fisico, di una parte del corpo. […] Processo di crescita fino alla compiuta formazione di un organo o di un organismo animale o vegetale. […] » (Garbini 2003. p. 80)\n\n\n\n\n\nEntrato nella lingua italiana nel XVIII secolo (forse dall’Inglese o dal Francese), il termine sviluppo trova la sua origine nel latino tardo (X sec.) falŭppa definito “scarti di paglia minutissimi o ramoscelli minuti” incrociatosi con un derivativo del verbo volvere “avviluppare” e successivamente accresciuto dal prefisso estrattivo-durativo (Garbini 2003. p. 80)."
  },
  {
    "objectID": "posts/2024-05-29-sviluppo-sostenibile/index.html#definizione",
    "href": "posts/2024-05-29-sviluppo-sostenibile/index.html#definizione",
    "title": "Sviluppo sostenibile",
    "section": "",
    "text": "«Perfezionamento, miglioramento, progresso verso un superiore livello tecnico o metodologico, qualitativo, estetico o stilistico in un determinato ambito pratico o intellettuale. […] Incremento, potenziamento di un’attività industriale, terziaria, agricola, di una produzione o più genericamente di un sistema economico. […] Processo di crescita verso la piena maturità psicofisica, mentale, sessuale; irrobustimento del fisico, di una parte del corpo. […] Processo di crescita fino alla compiuta formazione di un organo o di un organismo animale o vegetale. […] » (Garbini 2003. p. 80)"
  },
  {
    "objectID": "posts/2024-05-29-sviluppo-sostenibile/index.html#etimologia",
    "href": "posts/2024-05-29-sviluppo-sostenibile/index.html#etimologia",
    "title": "Sviluppo sostenibile",
    "section": "",
    "text": "Entrato nella lingua italiana nel XVIII secolo (forse dall’Inglese o dal Francese), il termine sviluppo trova la sua origine nel latino tardo (X sec.) falŭppa definito “scarti di paglia minutissimi o ramoscelli minuti” incrociatosi con un derivativo del verbo volvere “avviluppare” e successivamente accresciuto dal prefisso estrattivo-durativo (Garbini 2003. p. 80)."
  },
  {
    "objectID": "posts/2024-05-29-sviluppo-sostenibile/index.html#sviluppo-economico",
    "href": "posts/2024-05-29-sviluppo-sostenibile/index.html#sviluppo-economico",
    "title": "Sviluppo sostenibile",
    "section": "SVILUPPO ECONOMICO",
    "text": "SVILUPPO ECONOMICO\n\n«La nozione di sviluppo [economico sostenibile], concetto maggiore e di marca Onu di metà XX secolo, è una parola chiave sulla quale si sono incontrate tutte le vulgate politico-ideologiche dei decenni Cinquanta e Sessanta. Ma è stata veramente pensata? (Garbini 2003. p. 81)."
  },
  {
    "objectID": "posts/2024-05-29-sviluppo-sostenibile/index.html#sviluppo-economico-sostenibile",
    "href": "posts/2024-05-29-sviluppo-sostenibile/index.html#sviluppo-economico-sostenibile",
    "title": "Sviluppo sostenibile",
    "section": "SVILUPPO (ECONOMICO) SOSTENIBILE",
    "text": "SVILUPPO (ECONOMICO) SOSTENIBILE\n\nDa dove viene\nNel 1972, la conferenza ONU sull’ “Ambiente Umano” (Stoccolma, 5-16 giugno 1972) è considerata la prima in cui (almeno i …. paesi presenti) prendono in considerazione il problema della conservazione dell’ambiente e della gestione delle risrse naturali come questione fondamentale. La conferenza promulga un DICHIARAZIONE con 7 proclami e 26 Principi. Qualche stralcio:\n(non ho piu visto questa definizione) &gt; “human environment”\n\nProclama 5: The natural growth of population continuously presents problems for the preservation of the environment, and adequate policies and measures should be adopted, as appropriate, to face this problems. Of all things in teh world, people are the most precious.\n\n\nPronciple 2: The natural resources of the earth, including the air, water, land, flora and fauna and especially representative samples of natural ecosustems, must be safeguarded for the benefit of present and future generations through careful planning or management\n\n(controversa la discussione se la crescita demografica sia un problmea o noma questo finisce nelle raccomandazioni) &gt; Principle 16: Demographic policies which are without prejudcie to basic human rights and which are deemed appropriate by Governments concerned should be applied in those regions where the rate of populatioon growth or excessive population concentration are likely to have adverse effects on the environemnr of the human environment and impede development\n\nRecommendation 12: I. It is recommended that the World Health Organization and other United Nations agencies should provide increased assistance to Governments which so request in the field of family planning programmes without delay. 2. It is further recommended that the World Health Organization should promote and intensify research endeavour in the field of human reproduction, so that the serious consequences of population explosion on human environment can be prevented.\n\n\nConclusions: 48. Many speakers, from both developing and developed countries, agreed that the ruthless pursuit of gross national product, without consideration for other factors, produced conditions of life that were an affront to the dignity of man. The requirements of clean air, water, shelter and health were undeniable needs and rights of man.\n\n\n\n[POPULAZION] Several speakers expressed regret that population problems took so minor a place in the agenda of the Conference. They argued that all strategies for develop- ment and environment would be fatally damaged unless the rate of population increase was reduced. Other speakers said that the population increase was not the problem; the real challenge was the fact that so large a number of the people of the world had such a small expectation for a fruitful, happy and long life. In the opinion of certain delegations there was no incompatibility between population growth and preservation of the environment.\n\n\n\n\nUna definizione vera e propria?\nIl concetto di “sviluppo sostenibile” appare per la prima volta nel 1987, nel cosiddetto rapporto Brundtland del 1987 presentato alla Commissione mondiale per l’ambiente e lo sviluppo denominato anche “Our Common Future” guidata da Gro Harlem Brundtland. Qui si menzionano:\n\nuno sviluppo che deve avere dei limiti\n\n\nHumanity has the ability to make development sustainable to ensure that it meets the needs of the present without compromising the ability of future generations to meet their own needs.\nThe concept of sustainable development does imply limits - not absolute limits but limitations imposed by the present state of technology and social organization on environmental resources and by the ability of the biosphere to absorb the effects of human activities. (Environment and Development. 1987, 16)\n\n\nuno sviluppo che adotta stili di vita diversi in base alla ricchezza del paese\n\n\nSustainable global development requires that those who are more affluent adopt life-styles within the planet’s ecological means - in their use of energy, for example. Further, rapidly growing populations can increase the pressure on resources and slow any rise in livingstandards; thus sustainable development can only be pursued if population size and growth are in harmony with the changing productive potential of the ecosystem. (Environment and Development. 1987, 17)\n\n\nuno sviluppo che considera il futuro\n\n\nsustainable development is not a fixed state of harmony, but rather a process of change in which the exploitation of resources, the direction of investments, the orientation of technological development, and institutional change are made consistent with future as well as present needs. (Environment and Development. 1987, 17)\n\n\nuno sviluppo che considera “poggia” su precisi indirizzi politici\n\n\nWe do not pretend that the process is easy or straightforward. Painful choices have to be made. Thus, in the final analysis, sustainable development must rest on political will. (Environment and Development. 1987, 17)\n\n\nla crescita della popolazione e’ superiore alle risorse disponibili\n\n\nThe issue is not just numbers of people, but how those numbers relate to available resources. Thus the ‘population problem’ must be dealt with in part by efforts to eliminate mass poverty, in order to assure more equitable access to resources, and by education to improve human potential to manage those resources. Urgent steps are needed to limit extreme rates of population growth… providing people with facilities and education that allow them to choose the size of their families is a way of assuring - especially for women - the basic human right of self-determination. (Environment and Development. 1987, 18)\n\n\nla crescita della popolazione e’ un problema IN CERTI LUOGHI\n\n\nOur human world of 5 billion must make room in a finite environment for another human world. The population could stabilize at between 8 and 14 billion sometime next century, according to UN projections. More than 90 per cent of the increase will occur in the poorest countries, and 90 per cent of that growth in already bursting cities.(Environment and Development. 1987, 13)\n\nSince that time, sustainable development has emerged as a core idea of international development theory and policy. However, some experts have criticized certain features of the concept, including:\n\nIts generality or vagueness, which has led to a great deal of debate over which forms or aspects of development qualify as “sustainable”\nIts lack of quantifiable or objectively measurable goals\nIts assumption of the inevitability and desirability of industrialization and economic development\nIts failure to ultimately prioritize human needs or environmental commitments, either of which may reasonably be considered more important in certain circumstances\n\n\n\nQuali assunti contiene\n\npopolazione\nLo stesso Economist diceva che ESG mette insieme 3 concetti disparati e (potenzialmente) in contraddizione (The Economist 2022)\npossono organi consultivi (o di indirizzo politico) dare direttive tecniche (con tanto di deadline) su temi sui quali non sono d’accordo nemmeno gli scienziati specialisti?\nnucleare no ma fotovoltaico si\n\n\n\nQuali principi dovrebbe contenere\n\nLCA ?\nintensita di energia?\n\n\n\nDove arriva\na cose molto concrete dove frasi un po vagheggianti spostano some dollari estremamente sonanti\n\nDirettive\nNel 2001, l’UE ha adottato una strategia a favore dello “sviluppo sostenibile” e successivamente ha introdotto il concetto in altri documenti, incluso l’Articolo (3) del Trattato sull’Unione Europea.\nIn 2015 the United Nations General Assembly adopted the 2030 Agenda for Sustainable Development, which included 17 sweeping goals designed to create a globally equitable society alongside a thriving environment.\n\n\nESG\n\n\nONG l’anello debole e piu ricattabile\nxche on the receiving end of funding\n\n\n\nIndirizzo politico o diktat tecnico?\nLa catena principi - obiettivi - metodi - tecnologie\nAbbiamo i proncipi? e se si li stiamo perdendo di vista?\n\nLe direttive non sono neutrali\nvedi come devi produrre le auto per"
  },
  {
    "objectID": "posts/2024-05-29-sviluppo-sostenibile/index.html#riferimentirinvii-1",
    "href": "posts/2024-05-29-sviluppo-sostenibile/index.html#riferimentirinvii-1",
    "title": "Sviluppo sostenibile",
    "section": "Riferimenti/rinvii",
    "text": "Riferimenti/rinvii"
  },
  {
    "objectID": "posts/2024-05-29-sviluppo-sostenibile/index.html#riduzionismo",
    "href": "posts/2024-05-29-sviluppo-sostenibile/index.html#riduzionismo",
    "title": "Sviluppo sostenibile",
    "section": "Riduzionismo",
    "text": "Riduzionismo"
  },
  {
    "objectID": "analysis/01b_WB_project_pdo_anal.html",
    "href": "analysis/01b_WB_project_pdo_anal.html",
    "title": "WB Project PDO text analysis",
    "section": "",
    "text": "Work in progress"
  },
  {
    "objectID": "analysis/01b_WB_project_pdo_anal.html#saved-file-projs_train_t",
    "href": "analysis/01b_WB_project_pdo_anal.html#saved-file-projs_train_t",
    "title": "WB Project PDO text analysis",
    "section": "[Saved file projs_train_t ]",
    "text": "[Saved file projs_train_t ]\n\nDone in ** analysis/_01a_WB_project_pdo_prep.qmd “**\n\n\nI retrieved manually ALL WB projects approved between FY 1947 and 2026 as of 31/08/2024 using simply the Excel button on this page WBG Projects\n\nBy the way this is the link “list-download-excel”\n\nthen saved HUUUGE .xls files in data/raw_data/project2/all_projects_as_of29ago2024.xls\n\n(plus a Rdata copy of the original file )\n\n\n\n\nSplit the dataset and keep only projs_train (50% of projects with PDO text, i.e. 4413 PDOs)\nClean the dataset and save projs_train_t (cleaned train dataset)\nObtain PoS tagging + tokenization with cleanNLP package (functions cnlp_init_udpipe() + cnlp_annotate()) and saved projs_train_t (cleaned train dataset).\n\n\n# Load clean Proj PDO train dataset `pdo_train_t`\npdo_train_t &lt;- readRDS(here::here(\"data\" , \"derived_data\", \"pdo_train_t.rds\"))"
  },
  {
    "objectID": "analysis/01b_WB_project_pdo_anal.html#explain-tokenization-and-pos-tagging",
    "href": "analysis/01b_WB_project_pdo_anal.html#explain-tokenization-and-pos-tagging",
    "title": "WB Project PDO text analysis",
    "section": "Explain Tokenization and PoS Tagging",
    "text": "Explain Tokenization and PoS Tagging\ni) Tokenization\nWhere a word is more abstract, a “type” is a concrete term used in actual language, and a “token” is the particular instance we’re interested in (e.g. abstract things (‘wizards’) and individual instances of the thing (‘Harry Potter.’). Breaking a piece of text into words is thus called “tokenization”, and it can be done in many ways.\nThe choices of tokenization\n\nShould words be lower cased?\n\nShould punctuation be removed?\n\nShould numbers be replaced by some placeholder?\nShould words be stemmed (also called lemmatization). ☑️\nShould bigrams/multi-word phrase be used instead of single word phrases?\nShould stopwords (the most common words) be removed? ☑️\nShould rare words be removed?\nShould hyphenated words be split into two words? ❌\n\n\nfor the moment I keep all as conservatively as possible\n\nii) Pos Tagging\nClassifying noun, verb, adjective, etc. can help discover intent or action in a sentence, or scanning “verb-noun” patterns. Here I have a training dataset file with:\n\n\n\n\n\nVariable\nType\nProvenance\nDescription\n\n\n\nproj_id\nchr\noriginal PDO data\n\n\n\npdo\nchr\noriginal PDO data\n\n\n\nword_original\nchr\noriginal PDO data\n\n\n\nsid\nint\noutput cleanNLP\nsentence ID\n\n\ntid\nchr\noutput cleanNLP\ntoken ID within sentence\n\n\ntoken\nchr\noutput cleanNLP\nTokenized form of the token.\n\n\ntoken_with_ws\nchr\noutput cleanNLP\nToken with trailing whitespace\n\n\nlemma\nchr\noutput cleanNLP\nThe base form of the token\n\n\nupos\nchr\noutput cleanNLP\nUniversal part-of-speech tag (e.g., NOUN, VERB, ADJ).\n\n\nxpos\nchr\noutput cleanNLP\nLanguage-specific part-of-speech tags.\n\n\nfeats\nchr\noutput cleanNLP\nMorphological features of the token\n\n\ntid_source\nchr\noutput cleanNLP\nToken ID in the source document\n\n\nrelation\nchr\noutput cleanNLP\nDependency relation between the token and its head token\n\n\npr_name\nchr\noutput cleanNLP\nName of the parent token\n\n\nFY_appr\ndbl\noriginal PDO data\n\n\n\nFY_clos\ndbl\noriginal PDO data\n\n\n\nstatus\nchr\noriginal PDO data\n\n\n\nregionname\nchr\noriginal PDO data\n\n\n\ncountryname\nchr\noriginal PDO data\n\n\n\nsector1\nchr\noriginal PDO data\n\n\n\ntheme1\nchr\noriginal PDO data\n\n\n\nlendinginstr\nchr\noriginal PDO data\n\n\n\nenv_cat\nchr\noriginal PDO data\n\n\n\nESrisk\nchr\noriginal PDO data\n\n\n\ncurr_total_commitment\ndbl\noriginal PDO data\n\n\n\n\n\n\n\n— PoS Tagging: upos (Universal Part-of-Speech)\n\n\n\n\n\nupos\nn\npercent\nexplan\n\n\n\nADJ\n21852\n0.0853714\nAdjective\n\n\nADP\n27848\n0.1087965\nAdposition\n\n\nADV\n3010\n0.0117595\nAdverb\n\n\nAUX\n3738\n0.0146036\nAuxiliary\n\n\nCCONJ\n14486\n0.0565939\nCoordinating conjunction\n\n\nDET\n22121\n0.0864223\nDeterminer\n\n\nINTJ\n81\n0.0003165\nInterjection\n\n\nNOUN\n72668\n0.2838993\nNoun\n\n\nNUM\n2285\n0.0089270\nNumeral\n\n\nPART\n8846\n0.0345595\nParticle\n\n\nPRON\n2351\n0.0091849\nPronoun\n\n\nPROPN\n14860\n0.0580550\nProper noun\n\n\nPUNCT\n29442\n0.1150240\nPunctuation\n\n\nSCONJ\n2219\n0.0086692\nSubordinating conjunction\n\n\nSYM\n348\n0.0013596\nSymbol\n\n\nVERB\n26397\n0.1031278\nVerb\n\n\nX\n3412\n0.0133300\nOther\n\n\n\n\n\n\niii) Make low case\n\npdo_train_t &lt;- pdo_train_t %&gt;% \n  mutate(token_l = tolower(token)) %&gt;% \n   relocate(token_l, .after = token) %&gt;% \n   select(-token_with_ws) %&gt;%\n  #Replace variations of \"hyphenword\" with \"-\"\n  mutate(\n    lemma = str_replace_all(lemma, regex(\"hyphenword|hyphenwor\", ignore_case = TRUE), \"-\")\n  ) %&gt;%\n   mutate(stem = wordStem(token_l)) %&gt;%\n   relocate(stem, .after = lemma)\n\niv) Stemming"
  },
  {
    "objectID": "analysis/01b_WB_project_pdo_anal.html#frequencies-of-documentswordsstems",
    "href": "analysis/01b_WB_project_pdo_anal.html#frequencies-of-documentswordsstems",
    "title": "WB Project PDO text analysis",
    "section": "Frequencies of documents/words/stems",
    "text": "Frequencies of documents/words/stems\n\n# Count words\ncounts_pdo &lt;- pdo_train_t %&gt;%\n     count(pdo, sort = TRUE)  # 4,071\n\ncounts_words &lt;- pdo_train_t %&gt;%\n     count(word_original, sort = TRUE)  # 13,441\n\ncounts_token &lt;- pdo_train_t %&gt;%\n  count(token, sort = TRUE)   # 13,420\n\ncounts_lemma &lt;- pdo_train_t %&gt;%\n  count(lemma, sort = TRUE)   # 11,705\n\ncounts_stem &lt;- pdo_train_t %&gt;%\n  count(stem, sort = TRUE)   # 8,812\n\nWe are looking at pdo_train_t which has 134,858 rows and 7 columns.\n\nPDOs = 4071 in projects\n\nranging from 2001 to 2023\n\n\n\n\n\n\n\n\nColumn1\nColumn2\n\n\n\nN proj\n4413\n\n\nN PDOs\n4071\n\n\nN words\n13231\n\n\nN token\n11399\n\n\nN lemma\n11474\n\n\nN stem\n8812\n\n\n\n\n\n\nWe are looking at pdo_train_t which has 134,858 rows and 7 columns.\n[FUNC] save plots\n\nf_save_plot &lt;- function(plot_name, plot_object) {\n  # Print the plot, save as PDF and PNG\n  plot_object %T&gt;%\n    print() %T&gt;%\n    ggsave(., filename = here(\"analysis\", \"output\", \"figures\", paste0(plot_name, \".pdf\")),\n           # width = 4, height = 2.25, units = \"in\",\n           device = cairo_pdf) %&gt;%\n    ggsave(., filename = here(\"analysis\", \"output\", \"figures\", paste0(plot_name, \".png\")),\n           # width = 4, height = 2.25, units = \"in\",\n           type = \"cairo\", dpi = 300)\n}\n\n# Example of using the function\n# f_save_plot(\"proj_wrd_freq\", proj_wrd_freq)\n\n[FIG] Overall token freq ggplot\n\nWithout “project” “develop”,“objective”\n\n\n# Evaluate the title with glue first\ntitle_text &lt;- glue::glue(\"Most frequent token in {n_distinct(pdo_train_t$proj_id)} PDOs from projects approved between FY {min(pdo_train_t$boardapprovalFY)} and {max(pdo_train_t$boardapprovalFY)}\") \n\nproj_wrd_freq &lt;- pdo_train_t %&gt;%   # 256,632\n   filter (!(upos %in% c(\"AUX\",\"CCONJ\", \"INTJ\", \"DET\", \"PART\",\"ADP\", \"SCONJ\", \"SYM\", \"PART\", \"PUNCT\"))) %&gt;%\n   filter (!(relation %in% c(\"nummod\" ))) %&gt;% # 173,686 \n filter (!(token_l %in% c(\"pdo\",\"project\", \"development\", \"objective\",\"objectives\", \"i\", \"ii\", \"iii\",\n                          \"is\"))) %&gt;% # whne it is VERB\n   count(token_l) %&gt;% \n   filter(n &gt; 800) %&gt;% \n   mutate(token_l = reorder(token_l, n)) %&gt;%  # reorder values by frequency\n   # plot \n   ggplot(aes(token_l, n)) +\n   geom_col(fill = \"gray\") +\n   coord_flip() + # flip x and y coordinates so we can read the words better\n   labs(title = title_text,\n        subtitle = \"[token_l count &gt; 800]\", y = \"\", x = \"\")\n\nproj_wrd_freq\n\n\n\n\n\n\n\n\nf_save_plot(\"proj_wrd_freq\", proj_wrd_freq)\n\n[FIG] Overall stem freq ggplot\n\nWithout “project” “develop”,“objective”\n\n\n# Evaluate the title with glue first\ntitle_text &lt;- glue::glue(\"Most frequent STEM in {n_distinct(pdo_train_t$proj_id)} PDOs from projects approved between FY {min(pdo_train_t$boardapprovalFY)} and {max(pdo_train_t$boardapprovalFY)}\") \n\nproj_stem_freq &lt;- pdo_train_t %&gt;%   # 256,632\n   filter (!(upos %in% c(\"AUX\",\"CCONJ\", \"INTJ\", \"DET\", \"PART\",\"ADP\", \"SCONJ\", \"SYM\", \"PART\", \"PUNCT\"))) %&gt;%\n   filter (!(relation %in% c(\"nummod\" ))) %&gt;% # 173,686 \n filter (!(stem %in% c(\"pdo\",\"project\", \"develop\", \"object\", \"i\", \"ii\", \"iii\"))) %&gt;%\n   count(stem) %&gt;% \n   filter(n &gt; 800) %&gt;% \n   mutate(stem = reorder(stem, n)) %&gt;%  # reorder values by frequency\n   # plot \n   ggplot(aes(stem, n)) +\n   geom_col(fill = \"gray\") +\n   coord_flip() + # flip x and y coordinates so we can read the words better\n   labs(title = title_text,\n        subtitle = \"[stem count &gt; 800]\", y = \"\", x = \"\")\n\nproj_stem_freq\n\n\n\n\n\n\n\n\nf_save_plot(\"proj_stem_freq\", proj_stem_freq)\n\nEvidently, after stemming, more words (or stems) reach the threshold frequency count of 800."
  },
  {
    "objectID": "analysis/01b_WB_project_pdo_anal.html#term-frequency",
    "href": "analysis/01b_WB_project_pdo_anal.html#term-frequency",
    "title": "WB Project PDO text analysis",
    "section": "Term frequency",
    "text": "Term frequency"
  },
  {
    "objectID": "analysis/01b_WB_project_pdo_anal.html#word-and-document-frequency-tf-idf",
    "href": "analysis/01b_WB_project_pdo_anal.html#word-and-document-frequency-tf-idf",
    "title": "WB Project PDO text analysis",
    "section": "Word and document frequency: Tf-idf",
    "text": "Word and document frequency: Tf-idf\nThe goal is to quantify what a document is about. What is the document about?\n\n\nterm frequency (tf) = how frequently a word occurs in a document… but there are words that occur many time and are not important\nterm’s inverse document frequency (idf) = decreases the weight for commonly used words and increases the weight for words that are not used very much in a collection of documents.\n\nstatistic tf-idf (= tf-idf) = an alternative to using stopwords is the frequency of a term adjusted for how rarely it is used. [It measures how important a word is to a document in a collection (or corpus) of documents, but it is still a rule-of-thumb or heuristic quantity]\n\n\nThe tf-idf is the product of the term frequency and the inverse document frequency::"
  },
  {
    "objectID": "analysis/01b_WB_project_pdo_anal.html#n-grams",
    "href": "analysis/01b_WB_project_pdo_anal.html#n-grams",
    "title": "WB Project PDO text analysis",
    "section": "N-Grams",
    "text": "N-Grams\n…"
  },
  {
    "objectID": "analysis/01b_WB_project_pdo_anal.html#co-occurrence",
    "href": "analysis/01b_WB_project_pdo_anal.html#co-occurrence",
    "title": "WB Project PDO text analysis",
    "section": "Co-occurrence",
    "text": "Co-occurrence\n…"
  },
  {
    "objectID": "analysis/01b_WB_project_pdo_anal.html#named-entity-recognition-using-cleannlp-and-spacy",
    "href": "analysis/01b_WB_project_pdo_anal.html#named-entity-recognition-using-cleannlp-and-spacy",
    "title": "WB Project PDO text analysis",
    "section": "Named Entity Recognition using CleanNLP and spaCy",
    "text": "Named Entity Recognition using CleanNLP and spaCy\nNER is especially useful for analyzing unstructured text.\n— Summarise the tokens by parts of speech\n\n# Initialize the spacy backend\ncnlp_init_spacy() \n\n\nquarto render analysis/01b_WB_project_pdo_anal.qmd --to html\nopen ./docs/analysis/01b_WB_project_pdo_anal.html"
  },
  {
    "objectID": "analysis/model-idea.html",
    "href": "analysis/model-idea.html",
    "title": "Hypothetical model",
    "section": "",
    "text": "Modeling strategy\n\nWhich model could help me detect the change over time of a certain concept in a corpora of company documents\n\nTo detect the change over time of a certain concept in a corpus of company documents, you can use a combination of Topic Modeling, Word Embeddings, and Temporal Analysis techniques. Here are some models and methods that can help with this task:\n1. Dynamic Topic Models (DTM)\n\n\n\nDynamic Topic Models extend traditional topic modeling approaches like Latent Dirichlet Allocation (LDA) to account for changes in topics over time.\nDTM models the evolution of topics in a time series, allowing you to see how the prominence of certain topics or concepts shifts over different time periods.\nThis is useful if you want to track how the discussion around a concept evolves within your document corpus.\n\n\n# Analyze topics over different time periods\nlibrary(topicmodels)\nlibrary(tm)\nlibrary(ldatuning)\n\n# Preprocess the corpus\ncorpus &lt;- Corpus(VectorSource(documents))\ndtm &lt;- DocumentTermMatrix(corpus)\n\n# Fit LDA model\nlda_model &lt;- LDA(dtm, k = num_topics)\n\n2. Temporal Word Embeddings\n\n\n\nTemporal Word Embeddings are extensions of static word embeddings like Word2Vec or GloVe that capture how the meaning of words changes over time.\nBy training word embeddings separately for different time periods (e.g., by splitting your corpus into time segments), you can analyze how the vector representation of a concept shifts over time.\nModels like Temporal Word2Vec or Dynamic Word Embeddings can help identify changes in how a concept is discussed or understood over time.\n\n\nlibrary(wordVectors)\n\n# Train word2vec model for each time period\nmodel_t1 &lt;- train_word2vec(\"text_time_period_1.txt\", output_file = \"vec_t1.bin\")\nmodel_t2 &lt;- train_word2vec(\"text_time_period_2.txt\", output_file = \"vec_t2.bin\")\n\n# Compare word embeddings over time\n\n3. BERT with Temporal Fine-tuning\n\n\n\nBERT (Bidirectional Encoder Representations from Transformers) can be fine-tuned on your document corpus, with separate fine-tuning stages for different time periods.\nBy comparing the contextual embeddings of the concept across different time periods, you can analyze how the context and usage of the concept have changed over time.\nAlternatively, you can use a time-aware variant like BERTime, which is designed to capture temporal dynamics in textual data.\n\n\nnormally done with python\n\n\nlibrary(bertR)\n\n# Load pre-trained BERT model and fine-tune on time-specific data\nmodel &lt;- bert_load(\"bert-base-uncased\")\nfine_tuned_model &lt;- bert_finetune(model, train_data)\n\n# Generate embeddings and analyze changes over time\n\n4. Sentence Transformers with Time-Based Clustering\n\n\nUse Sentence Transformers (e.g., SBERT) to generate embeddings for sentences or paragraphs discussing the concept.\nApply clustering algorithms (with packages like cluster or factoextra) to these embeddings over different time periods to see how the clustering of topics around the concept changes.\nThis approach is effective for tracking nuanced changes in how the concept is discussed at different points in time.\n\n\nlibrary(text)\nlibrary(cluster)\n\n# Generate sentence embeddings\nembeddings &lt;- textEmbed(texts = your_text_data)\n\n# Perform clustering analysis\nclustering &lt;- kmeans(embeddings, centers = num_clusters)\n\n5. Sequential Neural Networks for Temporal Sequence Prediction\n\n\nRecurrent Neural Networks (RNNs) or Long Short-Term Memory (LSTM) networks can be used to model sequences of documents or sentences over time.\nBy training these models to predict future text sequences, you can analyze how the patterns associated with a concept evolve, which can help you detect changes in its importance or context.\n\n\nlibrary(keras)\nlibrary(tensorflow)\n\n# Define and train LSTM model\nmodel &lt;- keras_model_sequential() %&gt;%\n  layer_lstm(units = 50, return_sequences = TRUE, input_shape = c(time_steps, features)) %&gt;%\n  layer_dense(units = 1)\n\nmodel %&gt;% compile(loss = 'mse', optimizer = 'adam')\nhistory &lt;- model %&gt;% fit(x_train, y_train, epochs = 20, batch_size = 32)\n\n6. Change Point Detection Algorithms\n\n\nIntegrating change point detection algorithms (e.g., Bayesian change point detection) with NLP techniques allows you to pinpoint when significant shifts in the usage or context of a concept occur.\nThis is particularly useful if you’re interested in identifying specific events or periods that caused a shift in how a concept is discussed.\n\n\nlibrary(changepoint)\nlibrary(bcp)\n\n# Apply change point detection to time series of concept scores\ncpt &lt;- cpt.meanvar(your_time_series_data)\nplot(cpt)\n\nWorkflow Example:\n\nData Collection: The researchers collected a large corpus of CSR reports from Fortune 500 companies spanning multiple years (e.g., from the 1990s to the 2010s).\nPreprocessing: The text from these reports was cleaned, tokenized, and prepared for analysis. Segment your document corpus by time (e.g., by year or quarter).\n\nModeling:\n\nApply Dynamic Topic Models, e.g. Latent Dirichlet Allocation (LDA) to identify key topics discussed in the reports. By applying LDA to different time segments (e.g., reports from the 1990s, 2000s, 2010s), you could track the prominence of each topic over time.\nor train Temporal Word Embeddings for each time segment.\nAlternatively, fine-tune BERT or Sentence Transformers for each time segment.\n\n\n\n\nES (Zhang, Kim, and Xing 2015) This study utilizes Dynamic Topic Models (DTM) to analyze and track changes in market competition by analyzing text data from financial news articles and company reports. The focus is on understanding how discussions about companies and markets evolve over time and how these changes correlate with stock market returns.\n\n\n\nTemporal Analysis:\n\nTrack the changes in topic distributions or embedding vectors associated with your concept.\nUse clustering, cosine similarity, or other distance metrics to quantify the change over time.\nApply change point detection to identify periods of significant shift.\n\n\n\nWord Embeddings:\n\nWord2Vec models were trained on CSR reports from different time periods to observe how the semantic meaning of key terms (e.g., “sustainability,” “diversity”) evolved.\nBy comparing the embeddings of these terms over time, they assessed shifts in how these concepts were discussed.\n\n\n\nChange Detection:\n\n\nThe study utilized statistical methods to identify significant change points in the emphasis on particular CSR themes. This helped pinpoint specific events or external pressures (like new regulations) that may have driven changes in corporate communication.\n\nBy using these models, you can systematically detect and analyze how the concept of interest changes over time in your corpus of company documents.\n\n\n\n\n— Text processing in R\n\nZhang, Hao, Gunhee Kim, and Eric P. Xing. 2015. “Dynamic Topic Modeling for Monitoring Market Competition from Online Text and Image Data.” Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, August, 1425–34. https://doi.org/10.1145/2783258.2783293.",
    "crumbs": [
      "Analysis",
      "Models",
      "Hypothetical model"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Motivation",
    "section": "",
    "text": "I was really struck the first time I realized how the Latin motto I had heard since childhood: “Mens sana in corpore sano”, was actually a partial quotation used in 1861 by the Englishman John Hulley, as a motto for his Liverpool Athletic Club. In fact, it was extrapolated from a longer sentence by Juvenal poet: “Orandum est ut sit mens sana in corpore sano” (Satire X, 356). Taken in its entirety, the statement has quite a different meaning than its widely used shortened version: for one thing, it revolved around prayer and not fitness!\nI have seen this same dynamic (the meaning of words/phrases progressively detaching from the reality or from their originally intended purpose) at work also in the field of public policy, so I decided to dig deeper.\nIn fact, quoting Giovanni Gentile’s “Sommario di pedagogia come scienza filosofica”: “Language is not a garment of thought: it is its own body” (Garbini 2003, p3), i.e. language does not merely communicate our ideas to others, but it is also the tool of our thought, the wings for it to fly.  D’Auria put it in another way (D’Auria 2024), asserting that language is “performative”, in that it colors, deforms, produces reality. For this reason, it is necessarily a divisive instrument because, as soon as it is able to categorize, it is also capable of separating (good from bad, truth from lie, etc.). Knowledge itself, as much as it may scare the proponents of political correctness, lives on categories and categories, including linguistic ones, must precisely distinguish and correctly identify the ontological characteristics of what we are talking about."
  },
  {
    "objectID": "index.html#at-the-core-of-reduction",
    "href": "index.html#at-the-core-of-reduction",
    "title": "Motivation",
    "section": "",
    "text": "I was really struck the first time I realized how the Latin motto I had heard since childhood: “Mens sana in corpore sano”, was actually a partial quotation used in 1861 by the Englishman John Hulley, as a motto for his Liverpool Athletic Club. In fact, it was extrapolated from a longer sentence by Juvenal poet: “Orandum est ut sit mens sana in corpore sano” (Satire X, 356). Taken in its entirety, the statement has quite a different meaning than its widely used shortened version: for one thing, it revolved around prayer and not fitness!\nI have seen this same dynamic (the meaning of words/phrases progressively detaching from the reality or from their originally intended purpose) at work also in the field of public policy, so I decided to dig deeper.\nIn fact, quoting Giovanni Gentile’s “Sommario di pedagogia come scienza filosofica”: “Language is not a garment of thought: it is its own body” (Garbini 2003, p3), i.e. language does not merely communicate our ideas to others, but it is also the tool of our thought, the wings for it to fly.  D’Auria put it in another way (D’Auria 2024), asserting that language is “performative”, in that it colors, deforms, produces reality. For this reason, it is necessarily a divisive instrument because, as soon as it is able to categorize, it is also capable of separating (good from bad, truth from lie, etc.). Knowledge itself, as much as it may scare the proponents of political correctness, lives on categories and categories, including linguistic ones, must precisely distinguish and correctly identify the ontological characteristics of what we are talking about."
  },
  {
    "objectID": "index.html#issues-of-modern-english",
    "href": "index.html#issues-of-modern-english",
    "title": "Motivation",
    "section": "Issues of modern English",
    "text": "Issues of modern English\nBelow is the list of red flags detected in Orwell’s “Politics and the English Language”(Orwell 1946):\n\nDying metaphors, or worn−out metaphors which have lost all evocative power and are merely used because they save people the trouble of inventing phrases for themselves. \nOperators, or verbal false limbs, which save the trouble of picking out appropriate verbs and nouns, and at the same time pad each sentence with extra syllables which give it an appearance of symmetry. Characteristic phrases are: render inoperative, militate against, prove unacceptable, make contact with, be subjected to, give rise to, give grounds for, having the effect of, play a leading part (role) in, make itself felt, take effect, exhibit a tendency to, serve the purpose of, etc.\nPretentious diction (used to dress up simple statements and give an air of scientific impartiality to biased judgments), foreign words and expressions, jargon\nMeaningless words, as in certain kinds of writing, particularly in art criticism and literary criticism, where they not only do not point to any discoverable object, but are hardly even expected to do so by the reader (like romantic, plastic, values, human, dead, sentimental, natural, vitality), it is normal to come across long passages which are almost completely lacking in meaning. Many political words are similarly abused: The words democracy, socialism, freedom, patriotic, realistic, justice, have each of them several different meanings which cannot be reconciled with one another. Words of this kind are often used in a consciously dishonest way. That is, the person who uses them has his own private definition, but allows his hearer to think he means something quite different."
  },
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "Some examples of “domesticated lexicon”",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n\n\n\n\n\n\n\n\n\n\nSviluppo sostenibile\n\n\n\n\n\n\n🇮🇹\n\n\nlexicon\n\n\n\n[lessico addomesticato] \n\n\n\n\n\nMay 29, 2024\n\n\nLuisa M. Mimmi, Luisa M. Mimmi\n\n\n\n\n\n\n\n\n\n\n\n\nRevisione libro: Il Sistema invisibile, di Marcello Foa del 15 Novembre 2023\n\n\n\n\n\n\nspunti\n\n\n🇮🇹\n\n\n\n[libri] \n\n\n\n\n\nNov 15, 2023\n\n\nLuisa M. Mimmi, Luisa M. Mimmi\n\n\n\n\n\n\nNo matching items"
  }
]