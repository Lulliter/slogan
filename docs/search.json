[
  {
    "objectID": "data/raw_data/project2/all_projects_as_of29ago2024descr-stats.html",
    "href": "data/raw_data/project2/all_projects_as_of29ago2024descr-stats.html",
    "title": "Source",
    "section": "",
    "text": "Source\nALL WB projects available as of 31/08/2024 using simply the Excel button on this page this WBG Projects.\n\n\nFormat\n\nSaved HUUUGE .xls file in data/raw_data/project2/all_projects_as_of29ago2024.xls\nalso saved/copied as all_projects_as_of29ago2024.Rdata\n\n\n\nNote\nI tried using the API from link but it was too slow to lead and I couldn’t ….\n\n\nData sampling frame\n\nData on WBG projects from boardapprovalFY 1947 to 2026\nColumns\n\n…1 chr Project ID |id …2 chr Region |regionname …3 chr Country |countryname …4 chr Project Status |projectstatusdisplay …5 chr Last Stage Reached Name |last_stage_reached_name …6 chr Project Name |project_name …7 chr Project Development Objective |pdo …8 chr Implementing Agency |impagency …9 chr Consultant Services Required |cons_serv_reqd_ind …10 chr Project URL |url …11 chr Board Approval Date |boardapprovaldate …12 chr Project Closing Date |closingdate …13 chr Financing Type |projectfinancialtype …14 chr Current Project Cost |curr_project_cost …15 chr IBRD Commitment |curr_ibrd_commitment …16 chr IDA Commitment |curr_ida_commitment …17 chr Total IDA and IBRD Commitment |curr_total_commitme~ …18 chr Grant Amount |grantamt …19 chr Borrower |borrower …20 chr Lending Instrument |lendinginstr …21 chr Environmental Assessment Category |envassesmentcategorycode …22 chr Environmental and Social Risk |esrc_ovrl_risk_rate …23 chr Sector 1 |sector1 …24 chr Sector 2 |sector2 …25 chr Sector 3 |sector3 …26 chr Theme 1 |theme1 …27 chr Theme 2 |theme2\n\n\nData collection date(s)"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "analysis/hypotheses.html",
    "href": "analysis/hypotheses.html",
    "title": "Research Questions & Hypotheses",
    "section": "",
    "text": "The research question depends (in terms of its nature) on the type of analysis we intend to carry out (See fig 3.9 in Francom 2024):\n\n\nTable 1: Overview of analysis types\n\n\n\n\n\n\n\n\n\n\nType\nAims\nApproach\nMethods\nEvaluation\n\n\n\nExploratory\nExplore: gain insight\nInductive, data-driven, and iterative\nDescriptive, pattern detection with machine learning (unsupervised)\nAssociative\n\n\nPredictive\nPredict: validate associations\nSemi-deductive, data-/ theory-driven, and iterative\nPredictive modeling with machine learning (supervised)\nModel performance, feature importance, and associative\n\n\nInferential\nExplain: test hypotheses\nDeductive, theory-driven, and non-iterative\nHypothesis testing with statistical tests\nCausal\n\n\n\n\n\n\n\n\nIs there a pattern in the WBG project document corpus1 that shows non random variation in the incidence of certain policy concepts2 over time?\n\n\nCould the WDR 3 “explain” or at least have a correlation to the appearance-prevalence of said concepts?\n\nFor the moment, the present study’s research aim (See Table 1) is mainly TO EXPLORE (trends over time in concepts use), and possibly to PREDICT (conjecture about WRD traction effect).",
    "crumbs": [
      "Analysis",
      "Questions",
      "Research Questions & Hypotheses"
    ]
  },
  {
    "objectID": "analysis/hypotheses.html#rq-1.1",
    "href": "analysis/hypotheses.html#rq-1.1",
    "title": "Research Questions & Hypotheses",
    "section": "RQ #1.1",
    "text": "RQ #1.1\nThe launch of a “policy slogan” carries intrinsic motivations to shift the PDO in a certain direction.\n\n\nH1: if WDR mentions a certain concept, I expect it will appear more frequently in the PDO of the subsequne few FY.",
    "crumbs": [
      "Analysis",
      "Questions",
      "Research Questions & Hypotheses"
    ]
  },
  {
    "objectID": "analysis/hypotheses.html#footnotes",
    "href": "analysis/hypotheses.html#footnotes",
    "title": "Research Questions & Hypotheses",
    "section": "Footnotes",
    "text": "Footnotes\n\nWBG project document observed in this case are Project Development Objectives (PDO) descriptive short texts.↩︎\nconcepts means ….↩︎\nWDRs are the flagship reports of the World Bank group…↩︎\nAnnex to the Giuseppe Fioravanti’s book (Fioravanti 2006)↩︎",
    "crumbs": [
      "Analysis",
      "Questions",
      "Research Questions & Hypotheses"
    ]
  },
  {
    "objectID": "analysis/model-idea.html",
    "href": "analysis/model-idea.html",
    "title": "Hypothetical model",
    "section": "",
    "text": "Modeling strategy\n\nWhich model could help me detect the change over time of a certain concept in a corpora of company documents\n\nTo detect the change over time of a certain concept in a corpus of company documents, you can use a combination of Topic Modeling, Word Embeddings, and Temporal Analysis techniques. Here are some models and methods that can help with this task:\n1. Dynamic Topic Models (DTM)\n\n\n\nDynamic Topic Models extend traditional topic modeling approaches like Latent Dirichlet Allocation (LDA) to account for changes in topics over time.\nDTM models the evolution of topics in a time series, allowing you to see how the prominence of certain topics or concepts shifts over different time periods.\nThis is useful if you want to track how the discussion around a concept evolves within your document corpus.\n\n\n# Analyze topics over different time periods\nlibrary(topicmodels)\nlibrary(tm)\nlibrary(ldatuning)\n\n# Preprocess the corpus\ncorpus &lt;- Corpus(VectorSource(documents))\ndtm &lt;- DocumentTermMatrix(corpus)\n\n# Fit LDA model\nlda_model &lt;- LDA(dtm, k = num_topics)\n\n2. Temporal Word Embeddings\n\n\n\nTemporal Word Embeddings are extensions of static word embeddings like Word2Vec or GloVe that capture how the meaning of words changes over time.\nBy training word embeddings separately for different time periods (e.g., by splitting your corpus into time segments), you can analyze how the vector representation of a concept shifts over time.\nModels like Temporal Word2Vec or Dynamic Word Embeddings can help identify changes in how a concept is discussed or understood over time.\n\n\nlibrary(wordVectors)\n\n# Train word2vec model for each time period\nmodel_t1 &lt;- train_word2vec(\"text_time_period_1.txt\", output_file = \"vec_t1.bin\")\nmodel_t2 &lt;- train_word2vec(\"text_time_period_2.txt\", output_file = \"vec_t2.bin\")\n\n# Compare word embeddings over time\n\n3. BERT with Temporal Fine-tuning\n\n\n\nBERT (Bidirectional Encoder Representations from Transformers) can be fine-tuned on your document corpus, with separate fine-tuning stages for different time periods.\nBy comparing the contextual embeddings of the concept across different time periods, you can analyze how the context and usage of the concept have changed over time.\nAlternatively, you can use a time-aware variant like BERTime, which is designed to capture temporal dynamics in textual data.\n\n\nnormally done with python\n\n\nlibrary(bertR)\n\n# Load pre-trained BERT model and fine-tune on time-specific data\nmodel &lt;- bert_load(\"bert-base-uncased\")\nfine_tuned_model &lt;- bert_finetune(model, train_data)\n\n# Generate embeddings and analyze changes over time\n\n4. Sentence Transformers with Time-Based Clustering\n\n\nUse Sentence Transformers (e.g., SBERT) to generate embeddings for sentences or paragraphs discussing the concept.\nApply clustering algorithms (with packages like cluster or factoextra) to these embeddings over different time periods to see how the clustering of topics around the concept changes.\nThis approach is effective for tracking nuanced changes in how the concept is discussed at different points in time.\n\n\nlibrary(text)\nlibrary(cluster)\n\n# Generate sentence embeddings\nembeddings &lt;- textEmbed(texts = your_text_data)\n\n# Perform clustering analysis\nclustering &lt;- kmeans(embeddings, centers = num_clusters)\n\n5. Sequential Neural Networks for Temporal Sequence Prediction\n\n\nRecurrent Neural Networks (RNNs) or Long Short-Term Memory (LSTM) networks can be used to model sequences of documents or sentences over time.\nBy training these models to predict future text sequences, you can analyze how the patterns associated with a concept evolve, which can help you detect changes in its importance or context.\n\n\nlibrary(keras)\nlibrary(tensorflow)\n\n# Define and train LSTM model\nmodel &lt;- keras_model_sequential() %&gt;%\n  layer_lstm(units = 50, return_sequences = TRUE, input_shape = c(time_steps, features)) %&gt;%\n  layer_dense(units = 1)\n\nmodel %&gt;% compile(loss = 'mse', optimizer = 'adam')\nhistory &lt;- model %&gt;% fit(x_train, y_train, epochs = 20, batch_size = 32)\n\n6. Change Point Detection Algorithms\n\n\nIntegrating change point detection algorithms (e.g., Bayesian change point detection) with NLP techniques allows you to pinpoint when significant shifts in the usage or context of a concept occur.\nThis is particularly useful if you’re interested in identifying specific events or periods that caused a shift in how a concept is discussed.\n\n\nlibrary(changepoint)\nlibrary(bcp)\n\n# Apply change point detection to time series of concept scores\ncpt &lt;- cpt.meanvar(your_time_series_data)\nplot(cpt)\n\nWorkflow Example:\n\nData Collection: The researchers collected a large corpus of CSR reports from Fortune 500 companies spanning multiple years (e.g., from the 1990s to the 2010s).\nPreprocessing: The text from these reports was cleaned, tokenized, and prepared for analysis. Segment your document corpus by time (e.g., by year or quarter).\n\nModeling:\n\nApply Dynamic Topic Models, e.g. Latent Dirichlet Allocation (LDA) to identify key topics discussed in the reports. By applying LDA to different time segments (e.g., reports from the 1990s, 2000s, 2010s), you could track the prominence of each topic over time.\nor train Temporal Word Embeddings for each time segment.\nAlternatively, fine-tune BERT or Sentence Transformers for each time segment.\n\n\n\n\nES (Zhang, Kim, and Xing 2015) This study utilizes Dynamic Topic Models (DTM) to analyze and track changes in market competition by analyzing text data from financial news articles and company reports. The focus is on understanding how discussions about companies and markets evolve over time and how these changes correlate with stock market returns.\n\n\n\nTemporal Analysis:\n\nTrack the changes in topic distributions or embedding vectors associated with your concept.\nUse clustering, cosine similarity, or other distance metrics to quantify the change over time.\nApply change point detection to identify periods of significant shift.\n\n\n\nWord Embeddings:\n\nWord2Vec models were trained on CSR reports from different time periods to observe how the semantic meaning of key terms (e.g., “sustainability,” “diversity”) evolved.\nBy comparing the embeddings of these terms over time, they assessed shifts in how these concepts were discussed.\n\n\n\nChange Detection:\n\n\nThe study utilized statistical methods to identify significant change points in the emphasis on particular CSR themes. This helped pinpoint specific events or external pressures (like new regulations) that may have driven changes in corporate communication.\n\nBy using these models, you can systematically detect and analyze how the concept of interest changes over time in your corpus of company documents.\n\n\n\n\n— Text processing in R\n\nZhang, Hao, Gunhee Kim, and Eric P. Xing. 2015. “Dynamic Topic Modeling for Monitoring Market Competition from Online Text and Image Data.” Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, August, 1425–34. https://doi.org/10.1145/2783258.2783293.",
    "crumbs": [
      "Analysis",
      "Models",
      "Hypothetical model"
    ]
  },
  {
    "objectID": "analysis/04_desc_stats.html",
    "href": "analysis/04_desc_stats.html",
    "title": "?? DEscriptive Statistics",
    "section": "",
    "text": "Work in progress\n# Load required packages \nif (!require(\"pacman\")) install.packages('pacman' )\npacman::p_load(here, tidyverse, tibble, janitor,\n      #  emo, \n       readr, \n       gt,\n       #captioner,\n       knitr, \n       flextable, # better bc OK in html & word\n       officer, # to save flextable in word\n       shiny, \n       stringi,\n       wordcountaddin # You may have to download this\n       )  \n\n#pacman::p_load_gh(\"crsh/citr\") # not on CRAN (as of Nov 2021!)\n\n\n# Knitr options\nknitr::opts_chunk$set(\n  echo = FALSE, warning = FALSE, message = FALSE,\n  tidy.opts = list(width.cutoff = 120)#,  # Code width\n  #fig.retina = 3, dpi = conditional_dpi,\n  #fig.width = 7, fig.asp = 0.618,\n  #fig.align = conditional_align, out.width = \"100%\",\n  #fig.path = \"analysis/output/figures/\",\n  #cache.path = \"analysis/output/_cache/\"#,\n  # fig.process = function(x) {  # Remove \"-1\" from figure names\n  #   x2 = sub('-\\\\d+([.][a-z]+)$', '\\\\1', x)\n  #   if (file.rename(x, x2)) x2 else x\n  # }\n)"
  },
  {
    "objectID": "analysis/04_desc_stats.html#what-is-the-purpose-of-wdrs",
    "href": "analysis/04_desc_stats.html#what-is-the-purpose-of-wdrs",
    "title": "?? DEscriptive Statistics",
    "section": "What is the purpose of WDRs?",
    "text": "What is the purpose of WDRs?\n\n\na trend-follower (“synthesizing the”truth” as it was revealed”)? or\n\n\na policy trend-maker (“vehicle for the Bank to lead”)"
  },
  {
    "objectID": "analysis/04_desc_stats.html#is-it-an-observation-of-what-is-emerging-as-important",
    "href": "analysis/04_desc_stats.html#is-it-an-observation-of-what-is-emerging-as-important",
    "title": "?? DEscriptive Statistics",
    "section": "Is it an observation of what is emerging as important?",
    "text": "Is it an observation of what is emerging as important?\n\nThe topics not only provide a window on the Bank’s perception of what mattered or matters in the sphere of development at a particular time, but also give an indicator of the current fashions in development econom- ics that are attracting a significant amount of attention from researchers. (Yusuf 2008, 35)"
  },
  {
    "objectID": "analysis/04_desc_stats.html#or-is-it-a-vehicle-to-push-an-agenda",
    "href": "analysis/04_desc_stats.html#or-is-it-a-vehicle-to-push-an-agenda",
    "title": "?? DEscriptive Statistics",
    "section": "Or is it a vehicle to push an agenda?",
    "text": "Or is it a vehicle to push an agenda?\n\nWith the publication of the first of an annual series, the Bank took it upon itself to try to filter and systematize the knowledge on development so as to enhance the operational utility of such knowledge. In producing the WDR, the Bank was not seeking intellectual leadership or attempting to break new ground in the development field. Instead, the WDR was seen as a vehicle for persuading the Bank’s member governments to broadly unite behind a strategy and to cooperate in making it succeed. (Yusuf 2008, 34)\n\n\nThe three best remembered WDRs are those on fertility, on poverty, and on health. + The 1984 fertility report took the previously standard, though even then rapidly fading, view that population growth was indeed a problem for economic development, that more mouths meant less for each (the lump fallacy), and that the “tragedy of the commons” meant that the individual decisions of parents about their fertility were unlikely to lead to good outcomes. [… ] the Bank dropped the issue after the report, and indeed the tide was turning against the international population control movement from the mid-1980s on. Yet much harm had already been done, as documented in Matthew Connelly’s (2008) Fatal Misconceptions […]. + The 1990 poverty report is famous for introducing the international dollar-a-day poverty standard and the associated counts. These counts have continued to date, regularly updated by Martin Ravallion and his team, who were also the original authors. They have had an immense effect on development practice and on development debate, not least through the use of the dollar-a-day standard to define the first of the Millennium Development Goals (MDGs) and the appointment of the Bank as the subsequent scorekeeper. + The 1993 WDR on health is also famous, mostly for its introduction of the disability-adjusted life year (DALY), although this concept and its subsequent sweeping of the world is attributable less to Bank researchers than to Chris Murray, who was a consultant to the WDR. The DALY, like the dollar-a-day standard, has become a central tool of health measurement around the world for computing the burden of disease associated with different conditions, for permitting a combination of mortality and morbidity, and for assigning priorities. But the 1993 WDR, more than any other Bank report, put the Bank on the map as a major player in global health. It is also famous for reputedly persuading Bill Gates that international health was important, certainly an excellent example of the WDR mobilizing global opinion and shaping strategy. (Angus Deaton in Yusuf 2008)\n\n… the World Development Indicators (WDI), later spun off into an immensely successful stand-alone product."
  },
  {
    "objectID": "analysis/04_desc_stats.html#or-both-or-one-then-the-other-at-different-points",
    "href": "analysis/04_desc_stats.html#or-both-or-one-then-the-other-at-different-points",
    "title": "?? DEscriptive Statistics",
    "section": "Or both? (or one then the other at different points?)",
    "text": "Or both? (or one then the other at different points?)\nBoth\nAccording to Kemal Derviş, more than politics and political ideology what influenced a lot the WRDs shifting emphasis over time has been academic thinking. (Kemal Derviş in Yusuf 2008)"
  },
  {
    "objectID": "analysis/04_desc_stats.html#is-is-more-about-ideas-or-facts",
    "href": "analysis/04_desc_stats.html#is-is-more-about-ideas-or-facts",
    "title": "?? DEscriptive Statistics",
    "section": "Is is more about ideas or facts?",
    "text": "Is is more about ideas or facts?\nSomething new in the 1970s (not inundated by facts as we are today),\n\nthe World Bank used its resources and its access to data to generate a decent statistical picture of development. For the first time, many statements on the dynamics of development could be buttressed by facts. For the first time, readers were given a good sense of the magnitudes involved, and this insight was vital for determining the scale of problems and for calibrating policies. In effect, the WDRs helped to make development economics more numerate and altered the nature of discourse. (Yusuf 2008, 37)\n\nFigure @ref(fig:CommonWordTit) shows …"
  },
  {
    "objectID": "analysis/04_desc_stats.html#historical-overview",
    "href": "analysis/04_desc_stats.html#historical-overview",
    "title": "?? DEscriptive Statistics",
    "section": "Historical overview",
    "text": "Historical overview\n- 50s-70s - post-decolonization world: “growth”\n-   ***industrialization***\n-   ***capital-output ratio***. How much growth a country derives from each incremental unit of capital is a function of this conversion factor.\n-   ***two-gap model*****.** it soon became apparent that growth would be constrained not only by the scarcity of domestic capital but also by the paucity of foreign exchange to finance purchases of capital goods and other needed intermediate and consumption goods.\n-   ***I-O models, turnpike models, \"golden rule\" models*****,** and other dynamic optimizing models employing mathematical techniques that were borrowed from the engineering sciences\n- 70s-early80s - economic stagnation in the 1970s and the interrelated oil, financial, and adjustment crises of the early 1980s: “war on poverty”\n-   embedding of ***poverty in the idea of development*****,** making poverty alleviation an inextricable, if not prime, objective of development.\n    -   (poverty became tangible and measured)\n-   McNamara (1973: 10) warned that \"growth is not ***equitably reaching the poor***. And the poor are not significantly contributing to growth.\" McNamara called for an eradication of absolute poverty by the end of the 20th century, and he indicated that essential to accomplishing this goal would be an *increase in the productivity of **small-scale agriculture***.\n-   ***basic needs***\n-   ***pro-poor policies*** (1980 WDR---the Bank's first report on poverty, stressed the importance of managing health, education, and population growth)\n-   (after second energy shock) ***macroeconomic stability and resource equilibrium***\n-   ***structural adjustments*** was seen as vital to ridding the economy of many distorting regulatory encrustations, shrinking the state, and (most important) optimizing the allocation of resources by getting the prices right---one of the enduring mantras of the 1980s and emblematic of the notorious \"Washington Consensus\"\n- 1980s - pendulum shifts to market-based system that scaled down the role of the state in a globalizing environment\n-   ***market-based economics*** began to dominate, echoing a similar change in mindset among mainstream American academics. *Anne Krueger*, who replaced Hollis Chenery, was a staunch advocate of market solutions, and by prevailing over the views of her peers in the manage- ment group, she hitched the Bank's approach to development firmly to market forces.\n-   ***Tax and public sector reform*** **and *privatization***, quite suddenly, became immensely popular with policy makers,13 not the least because public sector deficits in some industrial countries made greater revenue effort a matter of urgency.\n-   ***Freer trade***---already promoted by the Kennedy and Tokyo Rounds and the creation of the General Agreement on Tariffs and Trade---was boosted by the ambitious Uruguay Round launched in 1986\n-   ***ODA, private resources, FDI***\n- 1990s - growth and poverty, analyzing them from microeconomic, institutional, and sectoral perspectives\n-   redefining the role of the ***state*** , identifying missing institutions, proposing blueprints for these institutions\n-   ***regulatory*** responsibilities\n-   \"***50 years is enough***,\" protests against the Bretton Woods Institutions, civil society\n_ emphasis shifted to ***micro*** issues (framing and testing of narrow hypotheses and are greatly preoccupied with the minutiae of economic plumbing)\n-   ***access to knowledge*** ( Stiglitz) - In a globalizing world, information, information technologies, the Internet, and institutions that were transforming the sharing of information had become as intrinsic to growth as physical capital, opening a whole new range of opportunities for developing countries and at the same time bringing them face to face with a fresh sheaf of policy issues.\n-   1999/00 four major forces---***globalization*** arising from flows of trade, capital, people and ideas; ***climate and environmental changes***; localization stemming from the combined effects of ***fiscal and administrative decentralization***; and ***rapid urbanization***\n-   PPPs\n- growth could be made ***environmentally friendly*** (WDRs 1992, 2003, )\n- 2000s - emphasis on institutions that affect market functioning and the entry, innovativeness, and growth of firms\n\n\ngovernance of regulatory bodies\n\ninformation gaps and asymmetries—the cause of market failures and countless economic ills.\nconcern over barriers to the entry and functioning of firms, created in part by regulations that curtailed competition.\ninvestment climate"
  },
  {
    "objectID": "analysis/04_desc_stats.html#looking-at-titles",
    "href": "analysis/04_desc_stats.html#looking-at-titles",
    "title": "?? DEscriptive Statistics",
    "section": "Looking at titles:",
    "text": "Looking at titles:"
  },
  {
    "objectID": "analysis/04_desc_stats.html#looking-at-subjects",
    "href": "analysis/04_desc_stats.html#looking-at-subjects",
    "title": "?? DEscriptive Statistics",
    "section": ">>> Looking at subjects",
    "text": "&gt;&gt;&gt; Looking at subjects\n01c_WDR_data-exploration_subjects.Rmd ….."
  },
  {
    "objectID": "analysis/04_desc_stats.html#viz-x--frequency-of-words-within-subsets-of-topics",
    "href": "analysis/04_desc_stats.html#viz-x--frequency-of-words-within-subsets-of-topics",
    "title": "?? DEscriptive Statistics",
    "section": "viz x-> Frequency of words within subsets of topics",
    "text": "viz x-&gt; Frequency of words within subsets of topics\nIt could be interesting to verify whether among those WDRs that focused on the same general topic, the choice of words has changed. This seems a perfect application of the “TF-IDF” metrics1.\nSee figure @ref(fig:gg_pov_tfidf) for poverty (looking at abstracts) ….\n\nknitr::include_graphics(here(\"analysis\", \"output\", \"figures\", \"gg_pov_tfidf.png\"))\n\nSee figure @ref(fig:gg_pov_tfidf) for environment/climate (looking at abstracts)…\n\nknitr::include_graphics(here(\"analysis\", \"output\", \"figures\", \"gg_env_tfidf.png\"))\n\nSee figure @ref(fig:gg_pov_tfidf) for knowledge/data (looking at abstracts)…\n\nknitr::include_graphics(here(\"analysis\", \"output\", \"figures\", \"gg_knowl_tfidf.png\"))"
  },
  {
    "objectID": "analysis/04_desc_stats.html#viz-which-words-seem-are-attracted-to-each-other",
    "href": "analysis/04_desc_stats.html#viz-which-words-seem-are-attracted-to-each-other",
    "title": "?? DEscriptive Statistics",
    "section": "[viz?] Which words seem are attracted to each other?",
    "text": "[viz?] Which words seem are attracted to each other?\nCollocations are words that are attracted to each other (and that co-occur or co-locate together),\nCorrelations\nCollocation stenght - poverty\nCollocation stenght - poverty different way"
  },
  {
    "objectID": "analysis/04_desc_stats.html#viz-groups-by-semantic-meaning",
    "href": "analysis/04_desc_stats.html#viz-groups-by-semantic-meaning",
    "title": "?? DEscriptive Statistics",
    "section": "[viz?] groups by semantic meaning?",
    "text": "[viz?] groups by semantic meaning?\n\ncan I use any WBG own taxonomy to classify keywords?"
  },
  {
    "objectID": "analysis/04_desc_stats.html#viz-frequency-of-words-over-time",
    "href": "analysis/04_desc_stats.html#viz-frequency-of-words-over-time",
    "title": "?? DEscriptive Statistics",
    "section": "[viz?] frequency of words over time?",
    "text": "[viz?] frequency of words over time?\n\ntime series ?"
  },
  {
    "objectID": "analysis/04_desc_stats.html#viz-matching-ideas",
    "href": "analysis/04_desc_stats.html#viz-matching-ideas",
    "title": "?? DEscriptive Statistics",
    "section": "[viz?] matching ideas",
    "text": "[viz?] matching ideas\n\nsuch as poverty in ? ? ?\nenvironm / climate in ? ? ?\ninstitutions/regul / governance in ? ? ?\nstate / government ? ? ?\nmarket | finance | PPPs | liberalization & privatization ???"
  },
  {
    "objectID": "analysis/04_desc_stats.html#is-there-an-increaased-appearance-of-certain-policies-among-subsequent-years-projects-pdos",
    "href": "analysis/04_desc_stats.html#is-there-an-increaased-appearance-of-certain-policies-among-subsequent-years-projects-pdos",
    "title": "?? DEscriptive Statistics",
    "section": "Is there an increaased appearance of certain policies among subsequent years’ projects PDOs?",
    "text": "Is there an increaased appearance of certain policies among subsequent years’ projects PDOs?"
  },
  {
    "objectID": "analysis/04_desc_stats.html#if-yes-for-how-long-with-what-level-of-priority",
    "href": "analysis/04_desc_stats.html#if-yes-for-how-long-with-what-level-of-priority",
    "title": "?? DEscriptive Statistics",
    "section": "If yes, for how long? | With what level of priority?",
    "text": "If yes, for how long? | With what level of priority?"
  },
  {
    "objectID": "analysis/04_desc_stats.html#footnotes",
    "href": "analysis/04_desc_stats.html#footnotes",
    "title": "?? DEscriptive Statistics",
    "section": "Footnotes",
    "text": "Footnotes\n\nThe TF-IDF basically finds the important words that distinguish a specific documents within a corpus of related ones. Technically, it is a word frequency that is weighted by decreasing the weight of common words that occur across all documents and increasing the weight for words that are more rare in the collection as a whole.↩︎"
  },
  {
    "objectID": "analysis/03_WDR_abstracs_explor.html",
    "href": "analysis/03_WDR_abstracs_explor.html",
    "title": "Process and merge data - WDR abstracts",
    "section": "",
    "text": "Work in progress\n#knitr::opts_chunk$set(include = TRUE, warning = FALSE)\n# Pckgs -------------------------------------\n#if (!require (\"pacman\")) (install.packages(\"pacman\"))\n\n#p_install_gh(\"luisDVA/annotater\")\n#p_install_gh(\"HumanitiesDataAnalysis/hathidy\")\n# devtools::install_github(\"HumanitiesDataAnalysis/HumanitiesDataAnalysis\") \nlibrary(here)\nlibrary(fs)\nlibrary(paint) \nlibrary(tidyverse) \nlibrary(magrittr)\nlibrary(skimr)\nlibrary(scales) \nlibrary(colorspace)\nlibrary(httr)\nlibrary(DT) # an R interface to the JavaScript library DataTables\nlibrary(knitr)\nlibrary(kableExtra) \nlibrary(flextable) \nlibrary(splitstackshape)  #Stack and Reshape Datasets After Splitting Concatenated Values\nlibrary(tm) # Text Mining Package\nlibrary(tidytext) # Text Mining using 'dplyr', 'ggplot2', and Other Tidy Tools\n# this requires pre-requirsites to install : https://github.com/quanteda/quanteda\nlibrary(quanteda)\nlibrary(igraph)\nlibrary(sjmisc) # Data and Variable Transformation Functions\nlibrary(ggraph) # An Implementation of Grammar of Graphics for Graphs and Networks\nlibrary(widyr) # Widen, Process, then Re-Tidy Data\nlibrary(SnowballC) # Snowball Stemmers Based on the C 'libstemmer' UTF-8 Library\n# library(#HumanitiesDataAnalysis, # Data and Code for Teaching Humanities Data Analysis\nlibrary(sentencepiece) # Text Tokenization using Byte Pair Encoding and Unigram Modelling\nlibrary(sysfonts) \nlibrary(ggdendro)\nlibrary(network)\nlibrary(GGally)\n\nlibrary(topicmodels)                #  with dep   FAILED !!!!!!\n\n# extra steo needed to install github version \n#if (!require(\"devtools\")) install.packages(\"devtools\")\n#library(devtools)\n#install_github(\"husson/FactoMineR\")     FAILED !!!!!!\n# library(FactoMineR)\n#library(factoextra)\n\n# Plot Theme(s) -------------------------------------\n#source(here(\"R\", \"ggplot_themes.R\"))\nggplot2::theme_set(theme_minimal())\n# color paletts -----\nmycolors_gradient &lt;- c(\"#ccf6fa\", \"#80e8f3\", \"#33d9eb\", \"#00d0e6\", \"#0092a1\")\nmycolors_contrast &lt;- c(\"#E7B800\", \"#a19100\", \"#0084e6\",\"#005ca1\", \"#e60066\" )\n\n\n# Function(s) -------------------------------------\n\n# Data -------------------------------------\n\n# -------------------- {cut bc made too heavy} -------------------------------------\n# # Tables [AH knit setup when using kbl() ]------------------------------------\nknit_print.data.frame &lt;- function(x, ...) {\n  res &lt;- paste(c('', '', kable_styling(kable(x, booktabs = TRUE))), collapse = '\\n')\n  asis_output(res)\n}\n\nregisterS3method(\"knit_print\", \"data.frame\", knit_print.data.frame)\nregisterS3method(\"knit_print\", \"grouped_df\", knit_print.data.frame)"
  },
  {
    "objectID": "analysis/03_WDR_abstracs_explor.html#i-pre-processing",
    "href": "analysis/03_WDR_abstracs_explor.html#i-pre-processing",
    "title": "Process and merge data - WDR abstracts",
    "section": "I) Pre-processing",
    "text": "I) Pre-processing\nI.ii) – Set stopwords [more…]\n\n# --- alt stop words\n# mystopwords &lt;- tibble(word = c(\"eq\", \"co\", \"rc\", \"ac\", \"ak\", \"bn\", \n#                                    \"fig\", \"file\", \"cg\", \"cb\", \"cm\",\n#                                \"ab\", \"_k\", \"_k_\", \"_x\"))\n\n# --- set up stop words\nstop_words &lt;- as_tibble(stop_words) %&gt;% # in the tidytext dataset \n  add_row(word = \"WDR\", lexicon = NA_character_) %&gt;%\n  # add_row(word = \"world\", lexicon = NA_character_) %&gt;%\n  add_row(word = \"report\", lexicon = NA_character_) %&gt;%\n  # add_row(word = \"development\", lexicon = NA_character_) %&gt;%\n  add_row(word = \"1978\", lexicon = NA_character_) %&gt;%\n  add_row(word = \"1979\", lexicon = NA_character_) %&gt;%\n  add_row(word = \"1980\", lexicon = NA_character_) %&gt;%\n  add_row(word = \"1981\", lexicon = NA_character_) %&gt;%\n  add_row(word = \"1982\", lexicon = NA_character_) %&gt;%\n  add_row(word = \"1983\", lexicon = NA_character_) %&gt;%\n  add_row(word = \"1984\", lexicon = NA_character_) %&gt;%\n  add_row(word = \"1985\", lexicon = NA_character_) %&gt;%\n  add_row(word = \"1986\", lexicon = NA_character_) %&gt;%\n  add_row(word = \"1987\", lexicon = NA_character_) %&gt;%\n  add_row(word = \"1988\", lexicon = NA_character_) %&gt;%\n  add_row(word = \"1989\", lexicon = NA_character_) %&gt;%\n  add_row(word = \"1990\", lexicon = NA_character_) %&gt;%\n  add_row(word = \"1991\", lexicon = NA_character_) %&gt;%\n  add_row(word = \"1992\", lexicon = NA_character_) %&gt;%\n  add_row(word = \"1993\", lexicon = NA_character_) %&gt;%\n  add_row(word = \"1994\", lexicon = NA_character_) %&gt;%\n  add_row(word = \"1995\", lexicon = NA_character_) %&gt;%\n  add_row(word = \"1996\", lexicon = NA_character_) %&gt;%\n  add_row(word = \"1997\", lexicon = NA_character_) %&gt;%\n  add_row(word = \"1998\", lexicon = NA_character_) %&gt;%\n  add_row(word = \"1999\", lexicon = NA_character_) %&gt;%\n  add_row(word = \"2000\", lexicon = NA_character_) %&gt;%\n  add_row(word = \"2001\", lexicon = NA_character_) %&gt;%\n  add_row(word = \"2002\", lexicon = NA_character_) %&gt;%\n  add_row(word = \"2003\", lexicon = NA_character_) %&gt;%\n  add_row(word = \"2004\", lexicon = NA_character_) %&gt;%\n  add_row(word = \"2005\", lexicon = NA_character_) %&gt;%\n  add_row(word = \"2006\", lexicon = NA_character_) %&gt;%\n  add_row(word = \"2007\", lexicon = NA_character_) %&gt;%\n  add_row(word = \"2008\", lexicon = NA_character_) %&gt;%\n  add_row(word = \"2009\", lexicon = NA_character_) %&gt;%\n  add_row(word = \"2010\", lexicon = NA_character_) %&gt;%\n  add_row(word = \"2011\", lexicon = NA_character_) %&gt;%\n  add_row(word = \"2012\", lexicon = NA_character_) %&gt;%\n  add_row(word = \"2013\", lexicon = NA_character_) %&gt;%\n  add_row(word = \"2014\", lexicon = NA_character_) %&gt;%\n  add_row(word = \"2015\", lexicon = NA_character_) %&gt;%\n  add_row(word = \"2016\", lexicon = NA_character_) %&gt;%\n  add_row(word = \"2017\", lexicon = NA_character_) %&gt;%\n  add_row(word = \"2018\", lexicon = NA_character_) %&gt;%\n  add_row(word = \"2019\", lexicon = NA_character_) %&gt;%\n  add_row(word = \"2020\", lexicon = NA_character_) %&gt;%\n  add_row(word = \"2021\", lexicon = NA_character_) %&gt;%\n  add_row(word = \"2022\", lexicon = NA_character_) %&gt;% \n  # filter (word != \"changes\") %&gt;% \n   # filter (word != \"value\") %&gt;% \n   filter (word != \"member\") %&gt;% \n   filter (word != \"part\") %&gt;% \n   filter (word != \"possible\") %&gt;% \n   filter (word != \"point\") %&gt;% \n   filter (word != \"present\") %&gt;% \n   # filter (word != \"zero\") %&gt;% \n     filter (word != \"young\") %&gt;% \n     filter (word != \"old\") %&gt;% \n     filter (word != \"trying\") \n\n\n# --- set up stop words stemmed\nstop_words_stem &lt;- stop_words  %&gt;% \nmutate (word = SnowballC::wordStem(word ))"
  },
  {
    "objectID": "analysis/03_WDR_abstracs_explor.html#ii-data-ingestion-loading-cleaning",
    "href": "analysis/03_WDR_abstracs_explor.html#ii-data-ingestion-loading-cleaning",
    "title": "Process and merge data - WDR abstracts",
    "section": "II) Data (ingestion), loading & cleaning",
    "text": "II) Data (ingestion), loading & cleaning\nIngestion of WDR basic metadata was done in ./_my_stuff/WDR-data-ingestion.Rmd and the result saved as ./data/raw_data/WDR.rds &lt;– (Being somewhat computational intensive, I only did it once.)\n\n\nWDR = tibble [45, 8]\n\ndoc_mt_identifier_1 chr oai:openknowledge.worldbank.org:109~\n\ndoc_mt_identifier_2 chr http://www-wds.worldbank.org/extern~\n\ndoc_mt_title chr Development Economics through the ~\n\ndoc_mt_date  chr 2012-03-19T10:02:25Z 2012-03-19T19:~\n\ndoc_mt_creator  chr Yusuf, Shahid World Bank World Bank~\n\ndoc_mt_subject chr ABSOLUTE POVERTY AGGLOMERATION BENE~\n\ndoc_mt_description chr The World Development Report (WDR) ~\n\ndoc_mt_set_spec chr oai:openknowledge.worldbank.org:109~\n\n\nIngestion of WDR lists of subjects was available among metadata but presented issues (difficulty to extract, many records with repetition,apparently wrong) so I reconstructed them manually in data/raw_data/WDR_subjects_corrected2010_2011.xlsx taking them from site https://elibrary.worldbank.org/ which lists keywords correctly e.g. see 2022 WDR\n\n# WRD metadata taken with API get (issues) \nWDR &lt;- readr::read_rds(here::here(\"data\", \"raw_data\", \"WDR.rds\" )) %&gt;% \n  # Extract only the portion of string AFTER the backslash {/}\n  mutate(id = as.numeric(stringr::str_extract(doc_mt_identifier_1, \"[^/]+$\"))) %&gt;% \n  dplyr::relocate(id, .before = doc_mt_identifier_1) %&gt;% \n  mutate(url_keys = paste0(\"https://openknowledge.worldbank.org/handle/10986/\", id , \"?show=full\"))  %&gt;% \n # eliminate NON WDR book\n  dplyr::filter(id != \"2586\") \n\n# WRD subject/date_issued taken by manual review \nWDR_subjects &lt;- readxl::read_excel(here::here(\"data\", \"raw_data\", \n                                              \"WDR_subjects_corrected2010_2011.xlsx\")) %&gt;%\n  drop_na(id) %&gt;% \n # eliminate NON WDR book\n  dplyr::filter(id != \"2586\") \n\n# delete empty cols \nColNums_NotAllMissing &lt;- function(df){ # helper function\n  as.vector(which(colSums(is.na(df)) != nrow(df)))\n}\n\nWDR_subjects &lt;- WDR_subjects  %&gt;% \n  select(ColNums_NotAllMissing(.))\n # # convert all columns that start with \"subj_\" to lowercase\n # WDR_subjects[3:218] &lt;- sapply(WDR_subjects[3:218], function(x) tolower(x))\n\n# join\nWDR_com &lt;- left_join(WDR, WDR_subjects, by = \"id\") %&gt;% \n  dplyr::relocate(date_issued, .before = id ) %&gt;% \n  # drop useles clmns \n  dplyr::select(#-doc_mt_identifier_1, \n                -doc_mt_identifier_2, -doc_mt_date, \n                -doc_mt_subject, -doc_mt_creator, -doc_mt_set_spec) %&gt;% \n  # dplyr::relocate(url_keys, .after = subj_216 ) %&gt;% \n  dplyr::rename(abstract = doc_mt_description) %&gt;% \n  # correct titles -&gt; portion after {:}\n  dplyr::mutate(., title = str_extract(doc_mt_title,\"[^:]+$\")) %&gt;% \n  dplyr::relocate(title, .after = id)  %&gt;% \n  dplyr::rename(title_miss = doc_mt_title) %&gt;% \n  dplyr::mutate(title_miss = case_when(\n    str_starts(title, \"World Development Report\") ~ \"Y\",\n    TRUE ~ NA_character_) \n  ) %&gt;% \n  dplyr::mutate(subject_miss = if_else(is.na(subj_1), \n                                       \"Y\", \n                                       NA_character_)) %&gt;% \n  dplyr::relocate(subject_miss, .after = title_miss)    %&gt;% \n  dplyr::relocate(ISBN, .after = id)    \n  \n#paint(WDR_com)\n\n# convert all columns that start with \"subj_\" to lowercase (maybe redundant)\nWDR_com[, grep(\"^subj_\", names(WDR_com))] &lt;- sapply(WDR_com[, grep(\"^subj_\", names(WDR_com))], function(x) tolower(x))\n\n# combine all `subj_...` vars into a vector separated by comma\ncol_subj &lt;- names(WDR_com[, grep(\"^subj_\", names(WDR_com))] )\n\nWDR_com &lt;- WDR_com %&gt;% tidyr::unite(\n  col = \"all_subj\", \n  subj_1:subj_46, \n  sep = \",\",\n  remove = FALSE,\n  na.rm = TRUE) %&gt;% \n  arrange(date_issued)\n\n#paint(WDR_com)\n\n– Some manual correction of wrong metadata\n\n# adding actual titles \n#WDR_com[WDR_com$date_issued == \"1978\", c(\"date_issued\", \"id\", \"title\")]\nWDR_com$title[WDR_com$id == \"5961\"] &lt;- \"Prospects for Growth and Alleviation of Poverty\"\n\n#WDR_com[WDR_com$date_issued == \"1979\", c(\"date_issued\", \"id\", \"title\")]\nWDR_com$title[WDR_com$id == \"5962\"] &lt;- \"Structural Change and Development Policy\"\n\n#WDR_com[WDR_com$date_issued == \"1980\", c(\"date_issued\", \"id\", \"title\")]\nWDR_com$title[WDR_com$id == \"5963\"] &lt;- \"Poverty and Human Development\"\n\n#WDR_com[WDR_com$date_issued == \"1981\", c(\"date_issued\", \"id\", \"title\")]\nWDR_com$title[WDR_com$id == \"5964\"] &lt;- \"National and International Adjustment\"\n\n#WDR_com[WDR_com$date_issued == \"1982\", c(\"date_issued\", \"id\", \"title\")]\nWDR_com$title[WDR_com$id == \"5965\"] &lt;- \"Agriculture and Economic Development\"\n\n#WDR_com[WDR_com$date_issued == \"1983\", c(\"date_issued\", \"id\", \"title\")]\nWDR_com$title[WDR_com$id == \"5966\"] &lt;- \"Management in Development\"\n\n#WDR_com[WDR_com$date_issued == \"1984\", c(\"date_issued\", \"id\", \"title\")]\nWDR_com$title[WDR_com$id == \"5967\"] &lt;- \"Population Change and Development\"\n\n#WDR_com[WDR_com$date_issued == \"1985\", c(\"date_issued\", \"id\", \"title\")]\nWDR_com$title[WDR_com$id == \"5968\"] &lt;- \"International Capital and Economic Development\"\n\n#WDR_com[WDR_com$date_issued == \"1986\", c(\"date_issued\", \"id\", \"title\")]\nWDR_com$title[WDR_com$id == \"5969\"] &lt;- \"Trade and Pricing Policies in World Agriculture\"\n\n#WDR_com[WDR_com$date_issued == \"1987\", c(\"date_issued\", \"id\", \"title\")]\nWDR_com$title[WDR_com$id == \"5970\"] &lt;- \"Industrialization and Foreign Trade\"\n\n#WDR_com[WDR_com$date_issued == \"1988\", c(\"date_issued\", \"id\", \"title\")]\nWDR_com$title[WDR_com$id == \"5971\"] &lt;- \"Public Finance in Development\"\n\n# wrong year \n#WDR_com[WDR_com$date_issued %in% c( \"2011\",\"2012\",\"2013\", \"2014\", \"2015\"), c(\"date_issued\", \"id\", \"title\")]\nWDR_com$date_issued[WDR_com$id == \"11843\"] &lt;- \"2013\"\nWDR_com$date_issued[WDR_com$id == \"16092\"] &lt;- \"2014\"\n\nII.i) Troubleshooting some documents\n\nPROBLEM: some of the subjects collections are evidently wrong (either they are the same of another WDR or the list is impossibly long)\n\n\nMY SOLUTION #1: I took them manually from the website “elibrary” https://elibrary.worldbank.org/action/showPublications?SeriesKey=b02\n\nBut, there still is WDR 2011 (“Conflict, Security, and Development”) which misses keywords\n\nMY SOLUTION #2: I take the abstract and I create my own “plausible list” of subjects\n\n— Extrapolate subjects from abstracts - for record with missing subjects/keywords\n(*) There will remain a problem: this corrected records have tokens and not bi|n-grams (which make more sense)!\n\n# --- identify wrong subject obs\nWDR_wr &lt;- WDR_com %&gt;% \n  filter( subject_miss == \"Y\") \n# names(WDR_com)\n\n– WDR caseid = 4389\n\n# --- tokenize abstract \nWDR_4389 &lt;- WDR_wr %&gt;% \n  filter(id ==\"4389\") %&gt;% \n  select (abstract) %&gt;% as_tibble() %&gt;% \n  tidytext::unnest_tokens(word, abstract) # -&gt; 251 words \n\n# --- remove stop words\n# --- isolate meaningful tokens\nWDR_4389 &lt;- WDR_4389 %&gt;% \n  anti_join( stop_words , by = \"word\") %&gt;%   # -&gt; 131  words \n  # Count observations by group\n  count(word, sort = T)  # -&gt; 101  words \n\n# rename column in result corrected   \nWDR_4389_w &lt;- t(WDR_4389) %&gt;% as_tibble()  \nnames(WDR_4389_w) &lt;- gsub(x = names(WDR_4389_w), pattern = \"\\\\V\", replacement = \"subj_\")\n # --- graph words\n  # p &lt;- WDR_4389 %&gt;% count(word, sort = TRUE) %&gt;%\n  #   filter(n &gt; 600) %&gt;%\n  #   mutate(word = reorder(word, n)) %&gt;%\n  #   ggplot(aes(n, word)) +\n  #   geom_col() +\n  #   labs(y = NULL)\n  # p  \n  #   \n\n# --- replace as subjects \nWDR_4389_w  &lt;- WDR_4389_w %&gt;%  \n  filter(row_number() == 1 ) %&gt;% \n  mutate(id = 4389) %&gt;% \n  relocate(id, .before = subj_1)\n\n# names(WDR_4389_w)\nWDR_4389_w &lt;- WDR_4389_w %&gt;% \n  tidyr::unite(\n    col = \"all_subj\", \n    subj_1:subj_100, \n    sep = \",\",\n    remove = FALSE) \n\n\n#names(WDR_com)\nWDR_com_2 &lt;- WDR_com %&gt;% \n  relocate(subj_1 ,.after = all_subj)\n\n# # create a vector of column names to add to reach the n length(WDR_com)\n# col_names &lt;-  as.vector(paste0('subj_', length(WDR_4389_w):105))\n# \n# # make a df with those cols and 1 row made of Nas values \n# to_add &lt;- bind_rows(setNames(rep(\"\", length(col_names)), col_names))[NA_character_, ]\n# \n# # --- pad until subj_216 with nas.........  \n# WDR_4389_w_pad &lt;- bind_cols(WDR_4389_w, to_add)\n\n#### -- NOW: replace corrected single WDR case into master df \n  # # initial check ---\n  # WDR_com$subj_1[WDR_com$id == 4389] \n  # WDR_com$subj_3[WDR_com$id == 4389] \n  # #--- \n\n#id &lt;-  4389 \ncol_subj_names &lt;- names(WDR_4389_w)[-(1)] # without \"id\"  \"all_subj\"\n\n# pick the id \ni &lt;- 4389\n\n# # --- function --- NO JOY!\n# for (j in 1:length(col_subj_names)) {\n#   col &lt;-  col_subj_names[j]\n#   # print(col) # nolint\n#   WDR_com %&gt;%\n#     dplyr::filter (id == i) %&gt;% \n#     dplyr::mutate (., col = WDR_4389_w_pad$col) \n#   WDR_com_2 &lt;- WDR_com\n# }\n\n# ---------# Solution from SO guy \n# r is vectorized so \nWDR_com_2[WDR_com_2$id %in% WDR_4389_w$id, 9:55] &lt;- WDR_4389_w[, 2:48] \n\n* I cut some …so this record WDR_4389 will be incomplete\n\nII.ii) – SAVE wdr and cleanenv\n\nwdr &lt;- WDR_com_2 %&gt;% \n  select(-title_miss) %&gt;% \n  mutate(decade = case_when(\n    str_detect (string = date_issued, pattern = \"^197\") ~ \"1970s\", \n    str_detect (string = date_issued, pattern = \"^198\") ~ \"1980s\", \n    str_detect (string = date_issued, pattern = \"^199\") ~ \"1990s\", \n    str_detect (string = date_issued, pattern = \"^200\") ~ \"2000s\", \n    str_detect (string = date_issued, pattern = \"^201\") ~ \"2010s\", \n    str_detect (string = date_issued, pattern = \"^202\") ~ \"2020s\"\n  )) %&gt;% \n  relocate(decade, .after = date_issued) %&gt;% \n# correct some datatype\nmutate_at(vars(date_issued, altmetric), as.numeric)  \n\ndataDir &lt;- fs::path_abs(here::here(\"data\",\"derived_data\"))\nfileName &lt;- \"/wdr.rds\"\nDir_File &lt;- paste0(dataDir, fileName)\nwrite_rds(x = wdr, file = Dir_File)\n\n# # ls objects\n# list_old_WDR &lt;-  ls(# pattern = \"^WDR\", \n#                     all.names = TRUE)\n# list_old_WDR\n# rm(list = setdiff(list_old_WDR, c(\"stop_words\", \"stop_words_stem\")))\n\n\nwdr &lt;- readr::read_rds(here::here(\"data\", \"derived_data\", \"wdr.rds\" ))\n\nI.iii) &gt; &gt; Part of Speech Tagging\nTagging segments of speech for part-of-speech (nouns, verbs, adjectives, etc.) or entity recognition (person, place, company, etc.) https://m-clark.github.io/text-analysis-with-R/part-of-speech-tagging.html\n– tagging with cleanNLP\n\nAH: https://datavizs22.classes.andrewheiss.com/example/13-example/#sentiment-analysis\nHere’s the general process for tagging (or “annotating”) text with the cleanNLP package:\n\nMake a dataset where one column is the id (line number, chapter number, book+chapter, etc.), and another column is the text itself.\nInitialize the NLP tagger. You can use any of these:\n\n\ncnlp_init_udpipe(): Use an R-only tagger that should work without installing anything extra (a little slower than the others, but requires no extra steps!)\n\ncnlp_init_spacy(): Use spaCy (if you’ve installed it on your computer with Python)\n\ncnlp_init_corenlp(): Use Stanford’s NLP library (if you’ve installed it on your computer with Java)\n\n\nFeed the data frame from step 1 into the cnlp_annotate() function and wait.\nSave the tagged data on your computer so you don’t have to re-tag it every time."
  },
  {
    "objectID": "analysis/03_WDR_abstracs_explor.html#iii.i-tokenization",
    "href": "analysis/03_WDR_abstracs_explor.html#iii.i-tokenization",
    "title": "Process and merge data - WDR abstracts",
    "section": "III.i) Tokenization",
    "text": "III.i) Tokenization\nWhere a word is more abstract, a “type” is a concrete term used in actual language, and a “token” is the particular instance we’re interested in (e.g. abstract things (‘wizards’) and individual instances of the thing (‘Harry Potter.’). Breaking a piece of text into words is thus called “tokenization”, and it can be done in many ways.\n— The choices of tokenization\n\nShould words be lowercased? x\nShould punctuation be removed? x\nShould numbers be replaced by some placeholder?\nShould words be stemmed (also called lemmatization). x\nShould bigrams/multi-word phrase be used instead of single word phrases?\nShould stopwords (the most common words) be removed? x\nShould rare words be removed?\n— Tokenization using regular expression syntax\nThe R function strsplit lets us do just this: split a string into pieces. *Note, for example, that this makes the word “Don’t” into two words.\n\ntok_simple &lt;- as_tibble(wdr$abstract[1] ) %&gt;%\n  str_split(\"[^A-Za-z]\") # “split on anything that isn’t a letter between A and Z.”\n\nstr(tok_simple) # list of characters \n\n#tok_simple[[1]]\n\n— Tokenization using tidytext\n\nThe simplest way is to remove anything that isn’t a letter. The workhorse function in tidytext is unnest_tokens. It creates a new columns (here called ‘words’) from each of the individual ones in text.\n\nabs_1 &lt;- as_tibble(wdr$abstract[1] )\n\n# LIST OF features I can add to `unnest_tokens`\ntok_feat_l &lt;- list(\n  # 1) all 2 lowercase \n  abs_1 %&gt;% unnest_tokens(word, value) %&gt;% select(lowercase = word),\n  # 4) `SnowballC::wordStem` extracts stems of each given words in the vector.\n  abs_1 %&gt;% unnest_tokens(word, value) %&gt;% rowwise() %&gt;% \n    mutate(word = SnowballC::wordStem(word)) %&gt;% select(stemmed = word),\n  # 1.b) keep uppercase if there are \n  abs_1 %&gt;% unnest_tokens(word, value, to_lower = F) %&gt;% \n    select(uppercase = word),\n  # 2) keep punctuation {default is rid} \n  abs_1 %&gt;% unnest_tokens(word, value, to_lower = F, strip_punc = FALSE) %&gt;% \n    select(punctuations = word),\n  # 5) bigram\n  abs_1 %&gt;% \n    unnest_tokens(word, value, token = \"ngrams\", n = 2, to_lower = F) %&gt;%\n    select(bigrams = word)\n)\n\n# Return a data frame created by column-binding.\ntok_feat_df &lt;- map_dfc(tok_feat_l  , ~ .x %&gt;% head(10))\ntok_feat_df\n\n# # my choice \n# abs_1_t_mod &lt;- abs_1 %&gt;% \n#   # no punctuation, yes capitalized\n#   unnest_tokens(word, value, to_lower = F, strip_punc = TRUE) %&gt;% # 249 obs\n#   # exclude stopwords \n#   anti_join(stop_words) # 109 obs\n# \n# head(abs_1_t_mod, 15)\n\n— Tokenizing ALL abstracts\n\n# isolate only abstracts \nabs_all &lt;- wdr %&gt;% \n  dplyr::select(id, date_issued, title, abstract)\n\nabs_all_token &lt;- abs_all %&gt;% \n  unnest_tokens(output =  word,\n                input = abstract ,\n                to_lower = T, # otherwise cannot match the stop_words  \n                strip_punc = TRUE\n  ) %&gt;% #10018\n  anti_join(stop_words) # 4613\n\n# Count words\nwordcounts &lt;- abs_all_token %&gt;%\n  group_by(word) %&gt;%\n  summarize(n = n()) %&gt;%\n  #arrange(-n) %&gt;%\n  # head(5) %&gt;% \n  mutate(rank = rank(-n)) %&gt;%\n  filter(n &gt; 2, word != \"\")"
  },
  {
    "objectID": "analysis/03_WDR_abstracs_explor.html#iii.ii-word-and-document-frequency-tf-idf",
    "href": "analysis/03_WDR_abstracs_explor.html#iii.ii-word-and-document-frequency-tf-idf",
    "title": "Process and merge data - WDR abstracts",
    "section": "III.ii) Word and document frequency: TF-IDF",
    "text": "III.ii) Word and document frequency: TF-IDF\nThe goal is to quantify what a document is about. What is the document about?\n\n\nterm frequency (tf) = how frequently a word occurs in a document… but there are words that occur many time and are not important\nterm’s inverse document frequency (idf) = decreases the weight for commonly used words and increases the weight for words that are not used very much in a collection of documents.\n\nstatistic tf-idf (= tf-idf) = an alternative to using stopwords is the frequency of a term adjusted for how rarely it is used. [It measures how important a word is to a document in a collection (or corpus) of documents, but it is still a rule-of-thumb or heuristic quantity]\n\n\nThe tf-idf is the product of the term frequency and the inverse document frequency::\n\n\\[\n\\begin{aligned}\ntf(\\text{term}) &= \\frac{n_{\\text{term}}}{n_{\\text{terms in document}}} \\\\\nidf(\\text{term}) &= \\ln{\\left(\\frac{n_{\\text{documents}}}{n_{\\text{documents containing term}}}\\right)} \\\\\ntf\\text{-}idf(\\text{term}) &= tf(\\text{term}) \\times idf(\\text{term})\n\\end{aligned}\n\\]\n— Pre-process 4 TF-IDF\n\n# PREP X TF-IDF\n#paint(abs_all)\nskimr::n_unique(abs_all$title)\nskimr::n_unique(abs_all$date_issued)\n\n# Count words (with stopwords)\ntemp &lt;-  abs_all %&gt;% \n  unnest_tokens(output =  word,\n                input = abstract,\n                to_lower = T, # otherwise cannot match the stop_words  \n                strip_punc = TRUE) %&gt;%  #9769 = tot words\n  # mutate(word = SnowballC::wordStem(word)) %&gt;% \n  ## other important pre-process step\n  # mutate(title = factor(title, ordered = TRUE))  %&gt;% \n  # mutate(date_issued = factor(date_issued, ordered = TRUE)) %&gt;% \n  ## implicit group by ...\n  group_by(date_issued) %&gt;%\n  # Count observations by group\n  count(word, sort = TRUE) %&gt;% # 5860 # unique words\n  ungroup() \n\n\nabs_words &lt;- left_join(temp, abs_all, by = \"date_issued\")\npaint(abs_words)\n\nmodo I) {Kumaran Ponnambalam} Create a Word Frequency Table\ntm way\n\n# abs_words_freq &lt;-  abs_words %&gt;%\n#   anti_join(stop_words, by= \"word\" )  %&gt;%\n#   tidyr::pivot_wider(names_from = word, values_from = n, values_fill = 0)\n# \n# # skim(abs_words_freq)\n# \n# #Generate the Document Term matrix\n# abs_words_freq_matrix &lt;- as.matrix(abs_words_freq)\n# \n# abs_words_freq_matrix[ , 'gender']\n# \n# # .... uses {tm} \n\n#str(abs_words_freq_matrix)\n\nmodo II) {Julia Silge and David Robinson} tidytext::bind_tf_idf\ntidytext way\nThe idea of tf-idf is to find the important words for the content of each document by decreasing the weight for commonly used words and increasing the weight for words that are not used very much in a collection or corpus of documents. Calculating tf-idf attempts to find the words that are important (i.e., common) in a text, but not too common. Let’s do that now.\n\n### ---  abstracts with totals \n\n# Calculate the total appearances of each words per doc\nabs_total_words &lt;- abs_words %&gt;%\n  dplyr::group_by(title) %&gt;%\n  dplyr::summarise(total = sum(n))  \n\n# Join the total appearances of each words per doc\nabs_words_T &lt;- left_join(abs_words, abs_total_words) %&gt;% \n  select(id, date_issued, title, abstract, word, n, total )\n\n# The usual suspects are here, “the”, “and”, “to”, and so forth. \n# ggplot(abs_words_T, aes(n/total, fill = title)) +\n#   geom_histogram(show.legend = FALSE) +\n#    xlim(NA, 0.01) +\n#   facet_wrap(~title, ncol = 2, scales = \"free_y\")\n\n\ntidytext::bind_tf_idf: Calculate and bind the term frequency and inverse document frequency of a tidy text dataset, along with the product, tf-idf, to the dataset. Each of these values are added as columns. This function supports non-standard evaluation through the tidyeval framework.\n\n\nabs_words_tf_idf &lt;- abs_words_T %&gt;%\n  #bind_tf_idf(tbl, term, document, n)\n  bind_tf_idf(      word, title, n) %&gt;% # 5860\n  # get rid of stopwords anyway &...\n  anti_join(stop_words, by = \"word\") %&gt;% # 3703\n  # fileter most importantly weighted \n  # filter(tf_idf &gt; 0.01 ) %&gt;% # 2278\n  arrange(date_issued, desc(tf_idf))\n\nabs_words_tf_idf\n\nNotice that idf and thus tf-idf are zero for these extremely common words. These are all words that appear in all docs, so the idf term (which will then be the natural log of 1) is zero.\n\nThe inverse document frequency (and thus tf-idf) is very low (near zero) for words that occur in many of the documents in a collection; this is how this approach decreases the weight for common words. IDF will be a higher number for words that occur in fewer of the documents in the collection.\n\nLet’s look at recurring terms terms with high tf-idf in WDRs.\n\n# wdr[ wdr$id %in% c(\"4391\" ) , c(\"date_issued\", \"title\" )]\n\n# let's look specifically at \"Gender Equality and Development\" \ntf_idf_2012 &lt;- abs_words_tf_idf %&gt;%\n  filter(date_issued == \"2012\") %&gt;%\n  select(-total) %&gt;%\n  arrange(desc(tf_idf))\n\n# These words are, as measured by tf-idf, the most important to \"Gender Equality and Development\"  and most readers would likely agree.\ntf_idf_2012[tf_idf_2012$word %in% c(\"gender\", \"equality\", \"development\") ,]\n\n# # A tibble: 3 × 7\n#   date_issued title                              word            n     tf   idf tf_idf\n#   &lt;chr&gt;       &lt;chr&gt;                              &lt;chr&gt;       &lt;int&gt;  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n# 1 2012        \" Gender Equality and Development\" gender         13 0.0588  3.09  0.182\n# 2 2012        \" Gender Equality and Development\" equality        6 0.0271  3.78  0.103\n# 3 2012        \" Gender Equality and Development\" development    10 0.0452  0     0 \n\n— TF-IDF tables/ viz for selected WDRs\nInterestingly, some themes are recurrent in cycles (as per (Yusuf 2008)). So I wanted to check TF_IDF in these “subsets” of WDRs\nPoverty\n\n # wdr[ wdr$id %in% c(\"5961\", \"5963\", \"5973\", \"11856\" ) , c(\"date_issued\", \"title\" )]\n\n# SIMPLE TABLE WITH FILTER \ntf_idf_poverty  &lt;-  abs_words_tf_idf %&gt;% \n  dplyr::filter(date_issued %in%  c(\"1978\", \"1980\", \"1990\", \"2001\")) %&gt;% \n    dplyr::filter(n &gt; 1) %&gt;% \n  select(-id, -abstract) %&gt;%\n  dplyr::arrange(date_issued, desc(tf_idf)) \n\n\nWhat this TF-IDF measure shows is the specific words that distinguish each WDR in this subset themed on poverty: i.e. the point of tf-idf is to identify words that are important to one document within a collection of documents.\n\n…viz\n\nlibrary(forcats) # Tools for Working with Categorical Variables (Factors)\n\ngg_pov_tfidf &lt;-  tf_idf_poverty %&gt;%\n  mutate (title2 = paste( \"WDR of \", date_issued )) %&gt;% \n  group_by(title2) %&gt;%\n  slice_max(tf_idf, n = 50) %&gt;%\n  ungroup() %&gt;% \n  ggplot(aes(tf_idf, fct_reorder(word, tf_idf), fill = title2)) +\n  geom_col(show.legend = FALSE) + \n  #my_theme() +\n  scale_fill_manual(values = mycolors_contrast) + # my palette\n  # labs(x = \"tf-idf for \", y = NULL) +\n  labs(title= bquote(\"TF-IDF ranking in 4 WDRs focused on \"~bold(\"poverty topic\")),  \n     subtitle=\"(overall 50 tokens with highest TF-IDF)\",\n     #caption=\"Source: ????\",\n     x=\"TF-IDF values\",\n     y=\"\"\n     ) + \n    facet_wrap(~title2, ncol = 2, scales = \"free\")  \n\ngg_pov_tfidf \n\ngg_pov_tfidf %T&gt;% \n  ggsave(., filename = here(\"analysis\", \"output\", \"figures\", \"gg_pov_tfidf.pdf\"),\n        # width = 2.75, height = 1.5, units = \"in\", device = cairo_pdf\n        ) %&gt;%\n  ggsave(., filename = here(\"analysis\", \"output\", \"figures\", \"gg_pov_tfidf.png\"),\n         #width = 2.75, height = 1.5, units = \"in\", type = \"cairo\", dpi = 300\n         )\n\nEnvironment/Climate\n\n#wdr[ wdr$id %in% c(\"5975\", \"4387\" ) , c(\"date_issued\", \"title\" )]\n\n# SIMPLE TABLE WITH FILTER \ntf_idf_env  &lt;-  abs_words_tf_idf %&gt;% \n  dplyr::filter(date_issued %in%  c(\"1992\", \"2010\")) %&gt;% \n    dplyr::filter(n &gt; 1) %&gt;% \n  select(-id, -abstract) %&gt;%\n  dplyr::arrange(date_issued, desc(tf_idf)) \n\n…viz\nEvident how in the 2010 WDR, words like “warming” and “temperatures” appear, while they were unimportant in the 1992 flagship report.\n\ngg_env_tfidf &lt;- tf_idf_env %&gt;%\n  mutate (title2 = paste( \"WDR of \", date_issued )) %&gt;% \n  group_by(title2) %&gt;%\n  slice_max(tf_idf, n = 50) %&gt;%\n  ungroup()  %&gt;% \n  ggplot(aes(tf_idf, fct_reorder(word, tf_idf), fill = title2)) +\n  geom_col(show.legend = FALSE) + \n  #my_theme() +\n  scale_fill_manual(values = mycolors_contrast) + # my palette\n  # labs(x = \"tf-idf for \", y = NULL) +\n  labs(title= bquote(\"TF-IDF ranking in 4 WDRs foucused on \"~bold(\"environment/climate change\")),      #title=\"TF-IDF ranking in 4 WDRs dedicated to environment/climate change topic\", \n    subtitle=\"(overall 50 tokens with highest TF-IDF)\",\n     #caption=\"Source: ????\",\n     x=\"TF-IDF values\",\n     y=\"\"\n     ) + \n    facet_wrap(~title2, ncol = 2, scales = \"free\")  \n\ngg_env_tfidf \n\ngg_env_tfidf %T&gt;% \n  ggsave(., filename = here(\"analysis\", \"output\", \"figures\", \"gg_env_tfidf.pdf\"),\n        # width = 2.75, height = 1.5, units = \"in\", device = cairo_pdf\n        ) %&gt;%\n  ggsave(., filename = here(\"analysis\", \"output\", \"figures\", \"gg_env_tfidf.png\"),\n         #width = 2.75, height = 1.5, units = \"in\", type = \"cairo\", dpi = 300\n         )\n\nKnowledge/data\n\n# skim(abs_words_tf_idf$tf_idf)\n# wdr[ wdr$id %in% c(\"5981\", \"35218\" ) , c(\"date_issued\", \"title\" )]\n\n# SIMPLE TABLE WITH FILTER \ntf_idf_knowl  &lt;-  abs_words_tf_idf %&gt;% \n  dplyr::filter(date_issued %in%  c(\"1998\", \"2021\")) %&gt;% \n    dplyr::filter(n &gt; 1) %&gt;% \n  select(-id, -abstract) %&gt;%\n  dplyr::arrange(date_issued, desc(tf_idf))\n\n…viz\n\ngg_knowl_tfidf  &lt;- tf_idf_knowl %&gt;% \n  dplyr::filter(date_issued %in%  c(\"1998\", \"2021\")) %&gt;%\n  mutate (title2 = paste( \"WDR of \", date_issued )) %&gt;% \n  group_by(title2) %&gt;%\n  slice_max(tf_idf, n = 50) %&gt;%\n  ungroup()  %&gt;% \n  ggplot(aes(tf_idf, fct_reorder(word, tf_idf), fill = title2)) +\n  geom_col(show.legend = FALSE) + \n  #my_theme() +\n  scale_fill_manual(values = mycolors_contrast) + # my palette\n  # labs(x = \"tf-idf for \", y = NULL) +\n  labs(title= bquote(\"TF-IDF ranking in 4 WDRs foucused on \"~bold(\"knowledge/data\")),\n       #title=\"TF-IDF ranking in 4 WDRs dedicated to knowledge/data change topic\", \n     subtitle=\"(overall 50 tokens with highest TF-IDF)\",\n     #caption=\"Source: ????\",\n     x=\"TF-IDF values\",\n     y=\"\"\n     ) + \n    facet_wrap(~title2, ncol = 2, scales = \"free\")  \n\ngg_knowl_tfidf \n\ngg_knowl_tfidf %T&gt;% \n  ggsave(., filename = here(\"analysis\", \"output\", \"figures\", \"gg_knowl_tfidf.pdf\"),\n        # width = 2.75, height = 1.5, units = \"in\", device = cairo_pdf\n        ) %&gt;%\n  ggsave(., filename = here(\"analysis\", \"output\", \"figures\", \"gg_knowl_tfidf.png\"),\n         #width = 2.75, height = 1.5, units = \"in\", type = \"cairo\", dpi = 300\n         )"
  },
  {
    "objectID": "analysis/03_WDR_abstracs_explor.html#iii.iii-word-frequency-histogram-meaningless",
    "href": "analysis/03_WDR_abstracs_explor.html#iii.iii-word-frequency-histogram-meaningless",
    "title": "Process and merge data - WDR abstracts",
    "section": "III.iii) Word frequency histogram {meaningless}\n",
    "text": "III.iii) Word frequency histogram {meaningless}\n\n— SCHMIDT’s Plotting most frequent words (all abstractS) —————-\nhttp://benschmidt.org/HDA/texts-as-data.html\nThe simple plot gives a very skewed curve: As always, you should experiment with multiple scales, and especially think about logarithms. Putting logarithmic scales on both axes reveals something interesting about the way that data is structured; this turns into a straight line.\n\n“Zipf’s law:” the most common word is twice as common as the second most common word, three times as common as the third most common word, four times as common as the fourth most common word, and so forth.\n\n\n# Putting logarithmic scales on both axes  \nggplot(wordcounts) +\n  aes(x = rank, y = n, label = word) +\n  geom_point(alpha = .3, color = \"grey\") +\n  geom_text(check_overlap = TRUE) +\n  scale_x_continuous(trans = \"log\") +\n  scale_y_continuous(trans = \"log\") +\n  labs(title = \"Zipf's Law\",\n       subtitle=\"The log-frequency of a term is inversely correlated with the logarithm of its rank.\")\n# ...the logarithm of rank decreases linearly with the logarithm of count\n\n[the logarithm of rank decreases linearily with the logarithm of count.] –&gt; common words are very common indeed, and logarithmic scales are more often appropriate for plotting than linear ones. \n— SILGE’s Plotting most frequent words (all abstractS) —————-\n\nOKKIO n instead of n/total\n\n\n# abs_words_T --&gt; 5860\n\n# let's eliminate stopwords \nabs_words2 &lt;-  anti_join(x = abs_words_T, y = stop_words, by= \"word\" )  %&gt;% \n  #filter(n &gt; 1) %&gt;% \n select(date_issued, title, word, n, total)\n\nabs_words2 #--&gt; 3699\n# paint(abs_words2)\n# here there is one row for each word-WDR(abs) combination \n# `n` is the number of times that word is used in that book and \n# `total` is the total words in that abstract\n\nfrequency = let’s look at the distribution of n/total for each doc, the number of times a word appears in a doc divided by the total number of terms (words) in that doc\n\nI actually use n instead because I have only small numbers having used the abstracts alone\n\n\none &lt;- abs_words2 %&gt;% \n  filter ( date_issued == \"2021\") %&gt;% \n  mutate (freq = n/total)\n \nggplot(data = one, \n       mapping = aes(x = n, fill = title)) + # y axis not needed ... R will count\n  geom_histogram(binwidth = 1,\n                 color = \"white\") +\n  scale_y_continuous(breaks= pretty_breaks()) +\n  xlim(0,10) +\n  labs(# title =  title, \n    x = \"frequency\",\n    y = \"N of words @ that frequency\") + \n  guides( fill = \"none\")\n\n# #  skim(one$freq)\n# ggplot(data = one, \n#        mapping = aes(x = freq, fill = title)) + # y axis not needed ... R will count\n#   geom_histogram(binwidth = 1,\n#                  color = \"white\") +\n#   scale_y_continuous(breaks= pretty_breaks()) +\n#    xlim(0, 0.1) +\n#   labs(# title =  title, \n#     x = \"frequency\",\n#     y = \"N of words @ that frequency\") + \n#   guides( fill = \"none\")\n\n\n# overlayed mess!\nggplot(abs_words2, aes(n, fill = title)) +\n  geom_histogram(binwidth = 1,\n                 color = \"white\") +\n  scale_y_continuous(breaks= pretty_breaks()) +\n  xlim(0, 20) +\n  labs(#title = ~date_issued, \n    x = \"frequency\",\n    y = \"N of words @ that frequency\") + \n  guides( fill = \"none\")\n\n\nggplot(abs_words2, aes(n, fill = title)) +\n  geom_histogram(binwidth = 1,\n                 color = \"white\") +\n  scale_y_continuous(breaks= pretty_breaks()) +\n  xlim(0, 10) +\n  labs(#title = ~date_issued, \n    x = \"frequency\",\n    y = \"N of words @ that frequency\") + \n  facet_wrap( ~date_issued ) + # , ncol = 2, scales = \"free_y\")\n  guides( fill = \"none\") # way to turn legend off\n\n— Multiple plots of Word Freq with [purrr]\n\nJennifer Thompson, An Intro to the Magic of purrr\nautomate ggplots while using variable labels as title and axis titles\n\n\n# ---- NON Capisco \n# # Preferred approach\n# histos &lt;- abs_words2 %&gt;%\n#   group_by(title) %&gt;%\n#   nest() %&gt;%\n#   mutate(plot = map2(data, title, \n#                      ~ggplot(data = .x , aes(n/total, fill = title)) + \n#                       geom_histogram( show.legend = FALSE) + \n#                        xlim(NA, 0.05) +\n#                        # ggtitle(.y) +\n#                        ylab(\"Words Frequency\") +\n#                        xlab(\"Distribution per WDR title\")))\n# \n#  histos$plot[[1]]\n\n\n# ---- Capisco \n# https://stackoverflow.com/questions/60671725/ggplot-add-title-based-on-the-variable-in-the-dataframe-used-for-plotting\nlist_plot &lt;- abs_words2 %&gt;%\n  dplyr::group_split(date_issued) %&gt;% # Split data frame by groups\n  map(~ggplot(., \n              mapping = aes(x = n, fill = title)) +\n        geom_histogram(binwidth = 1,\n                       color = \"white\") +\n        scale_y_continuous(breaks= pretty_breaks()) + # integer ticks {scales}\n        xlim(0, 18) + # max of freq is 17\n        labs(title = .$date_issued, \n             x = \"frequency\",\n             y = \"N of words @ that frequency\")\n  )\n\nlist_plot[[1]]\nlist_plot[[44]]\nlist_plot[[40]]\n#grid.arrange(grobs = list_plot, ncol = 1)\n\n— Zipf’s law for WDR’s abstracts\nExamine Zipf’s law for WDR’s abstracts with just a few lines of dplyr functions. &gt; The rank column here tells us the rank of each word within the frequency table; the table was already ordered by n so we could use row_number() to find the rank\n\nfreq_by_rank &lt;- abs_words2 %&gt;% \n  arrange(desc(n)) %&gt;% # order by n... \n  group_by(title) %&gt;% \n  mutate(rank = row_number(), # so this can act as the rank\n         `term frequency` = n/total) %&gt;% # term frequency\n  ungroup()\nfreq_by_rank\n\nZipf’s law is often visualized by plotting rank on the x-axis and term frequency on the y-axis, on logarithmic scales. Plotting this way, an inversely proportional relationship will have a constant, negative slope.\n\nfreq_by_rank %&gt;% \n  filter (date_issued == \"2021\" | date_issued == \"1998\") %&gt;% \n  ggplot(aes(rank, `term frequency`, color = title)) + \n  geom_line(size = 1.1, alpha = 0.8, show.legend = FALSE) + \n  scale_x_log10() +\n  scale_y_log10() + \n  labs(title = \"Zipf’s law seen for  knowledge (1998) & data (2021) WDRs\",\n       subtitle = \"(1998) = blue | (2021) = red\",\n       x = \"rank (log)\",\n       y = \"term frequency (log)\",\n       color = \"Legend\")   \n\nhttps://www.tidytextmining.com/tfidf.html#zipfs-law\nperhaps we could view this as a broken power law with, say, three sections. Let’s see what the exponent of the power law is for the middle section of the rank range.\n\nrank_subset &lt;- freq_by_rank %&gt;% \n  filter(rank &lt; 500,\n         rank &gt; 10)\n\nlm(log10(`term frequency`) ~ log10(rank), data = rank_subset)\n\nLet’s plot this fitted power law with the obtaied data to see how it looks\n\nfreq_by_rank %&gt;% \n  filter (date_issued == \"2021\" | date_issued == \"1998\") %&gt;% \n  ggplot(aes(rank, `term frequency`, color = title)) + \n  geom_abline(intercept = -1.80, slope = -0.33, \n              color = \"gray50\", linetype = 2) +\n  geom_line(size = 1.1, alpha = 0.8, show.legend = FALSE) + \n  scale_x_log10() +\n  scale_y_log10()\n\n\nThe deviations we see here at high rank are not uncommon for many kinds of language; a corpus of language often contains fewer rare words than predicted by a single power law."
  },
  {
    "objectID": "analysis/03_WDR_abstracs_explor.html#iii.iv-relationships-between-words-n-grams-and-correlations",
    "href": "analysis/03_WDR_abstracs_explor.html#iii.iv-relationships-between-words-n-grams-and-correlations",
    "title": "Process and merge data - WDR abstracts",
    "section": "III.iv) Relationships between words: n-grams and correlations",
    "text": "III.iv) Relationships between words: n-grams and correlations\nhttps://www.tidytextmining.com/ngrams.html https://bookdown.org/Maxine/tidy-text-mining/tokenizing-by-n-gram.html\n\nThe one-token-per-row framework can be extended from single words to n-grams and other meaningful units of text(e.g. to see which words tend to follow others immediately, or that tend to co-occur within the same documents.)\n\n\n\ntidytext::token = \"ngrams\" argument is a method tidytext offers for calculating and visualizing relationships between words in your text dataset. It tokenizes by pairs of adjacent words rather than by individual ones.\n\nggraph extends ggplot2 to construct network plots,\n\nwidyr calculates pairwise correlations and distances within a tidy data frame.\n\n— Tokenizing by n-gram\nThe unnest_tokens function can also be used to tokenize into consecutive sequences of words, called n-grams. By seeing how often word X is followed by word Y, we can then build a model of the relationships between them.\n\n# break the text [e.g. 1 abstract] into bi-gram \nabs_2022_bigrams &lt;- wdr %&gt;% \n  dplyr::filter(id == 36883) %&gt;% \n  select(abstract) %&gt;% \n  as_tibble() %&gt;% \n  unnest_tokens(., output = bigram, input = abstract, token = \"ngrams\", n=2 )  \n# notice how these bigrams overlap\nhead(abs_2022_bigrams)\n\n\n# # my choice \n# abs_1_t_mod &lt;- abs_1 %&gt;% \n#   # no punctuation, yes capitalized\n#   unnest_tokens(word, value, to_lower = F, strip_punc = TRUE) %&gt;% # 249 obs\n#   # exclude stopwords \n#   anti_join(stop_words) # 109 obs\n\nabs_all_bigram &lt;- abs_all %&gt;% \n  unnest_tokens(., output = bigram, input = abstract, token = \"ngrams\", n=2 )  \n\nhead(abs_all_bigram[c(\"date_issued\",\"bigram\")], 10)\n\n\nThis data structure is still a variation of the tidy text format. It is structured as one-token-per-row but each token now represents a bigram.\n\n— Operations on n-grams: counting and filtering\n\nbigrams_counts &lt;- abs_all_bigram %&gt;% \n   count(bigram, sort = TRUE)\n\nhead(bigrams_counts)\n\n\nNot surprisingly, a lot are pairs of stopwords\n\nHere, I can use tidyr::separate(), which splits a column into multiple based on a delimiter. This separate it into two columns, “word1” and “word2”, to then remove cases where either is a stop-word.\nIn other analyses, we may want to work with the recombined words. tidyr’s unite() function is the inverse of separate(), and lets us recombine the columns into one.\n\n# separate words \nbigrams_separated &lt;- abs_all_bigram %&gt;% \n  tidyr::separate(bigram, c(\"word1\", \"word2\"), sep = \" \")\n\n# since many are bigram with a stopword\nbigrams_filtered &lt;- bigrams_separated %&gt;% \n  # remove cases where either is a stop-word\n  filter(!word1 %in% stop_words$word) %&gt;%\n  filter(!word2 %in% stop_words$word)\n\n# OPPOSITE: reunite words \nbigrams_united &lt;- bigrams_filtered  %&gt;% \n  unite(bigram, word1, word2, sep = \" \")\n\n— (Trigrams)\nIn other analyses you may be interested in the most common trigrams, which are consecutive sequences of 3 words. We can find this by setting n = 3:\n\ntrigram &lt;- abs_all %&gt;%\n  unnest_tokens(trigram, abstract, token = \"ngrams\", n = 3) %&gt;%\n  separate(trigram, c(\"word1\", \"word2\", \"word3\"), sep = \" \") %&gt;%\n  filter(!word1 %in% stop_words$word,\n         !word2 %in% stop_words$word,\n         !word3 %in% stop_words$word) %&gt;%\n  count(word1, word2, word3, sort = TRUE)\n\n— Bigram ~ potential meaningful SLOGANs?\n Which of this bigram might be a SLOGAN candidate?\n\n# new bigram counts:\nbigrams_counts_clean &lt;- bigrams_filtered %&gt;% \n  # Count observations by group\n  count(word1, word2, sort = TRUE)\n\nhead(bigrams_counts_clean, 20) # cleaned up stopwords\n\n …Maybe some of these bigram with high tf-idf\n\nexternal finance\ngender equality\ndevelopment impact\ndigital revolution\ninvestment climate\naccelerating growth\nalleviating poverty\n…\n— Analyzing bigrams\nThis one-bigram-per-row format is helpful for exploratory analyses of the text. Let’s see what comes before “poverty”, “change”, “knowledge”…\n\nbigrams_filtered %&gt;%\n  filter(word2 == \"poverty\") %&gt;%\n  count(date_issued, word1, sort = TRUE)\n\nbigrams_filtered %&gt;%\n  filter(word2 == \"change\") %&gt;%\n  count(date_issued, word1, sort = TRUE)\n\nbigrams_filtered %&gt;%\n  filter(word2 == \"knowledge\") %&gt;%\n  count(date_issued, word1, sort = TRUE)\n\n…or after “human”, “finance”, “bottom”:\n\n# after  \nbigrams_filtered %&gt;%\n  filter(word1 == \"human\") %&gt;%\n  count(date_issued, word2, sort = TRUE)\n\nbigrams_filtered %&gt;%\n  filter(word1 == \"finance\") %&gt;%\n  count(date_issued, word2, sort = TRUE)\n\nbigrams_filtered %&gt;%\n  filter(word1 == \"bottom\") %&gt;%\n  count(date_issued, word2, sort = TRUE)\n\n— Analyzing bigrams: tf-idf\n\nThere are advantages and disadvantages to examining the tf-idf of bigrams rather than individual words. Pairs of consecutive words might capture structure that isn’t present when one is just counting single words, and may provide context that makes tokens more understandable. However, the per-bigram counts are also sparser (a typical two-word pair is rarer than either of its component words).\n\n\nbigram_tf_idf &lt;- abs_all_bigram %&gt;% \n  # reconstruct the  separated + filtered + united\n  separate(bigram, c(\"word1\", \"word2\"), sep = \" \") %&gt;% \n   filter(!word1 %in% stop_words$word) %&gt;%\n   filter(!word2 %in% stop_words$word) %&gt;% \n  # Put the two word columns back together\n  unite(bigram, word1, word2, sep = \" \") %&gt;% \n\n  # then on that calculate tf-idf\n count(date_issued, bigram) %&gt;%\n  bind_tf_idf(bigram, date_issued, n) %&gt;%\n  arrange(desc(tf_idf))\n\nhead(bigram_tf_idf)\n\n— (Using bigrams to provide context in sentiment analysis)\n…\n— Visualizing a network of bigrams with ggraph\n\nThe igraph package has many powerful functions for manipulating and analyzing networks.\nOne way to create an igraph object from tidy data is the igraph::graph_from_data_frame() function, which takes a data frame of edges with columns for “from”, “to”, and edge attributes (in this case “n”):\n\nIf vertices is NULL, then the first two columns of df (e.g. word1 = FROM & word2 = TO) are used as a symbolic edge list and additional columns (e.g. n) as edge attributes/weight. The names of the attributes are taken from the names of the columns.\n\nHere, a graph can be constructed from the tidy object bigrams_counts_clean since it has three variables.\n\nlibrary(igraph) # Network Analysis and Visualization\nbigrams_counts_clean\n\n# filter for only relatively common combinations\nbigram_graph &lt;- bigrams_counts_clean %&gt;%\n  filter(n &gt; 2) %&gt;%\n  # create an igraph graph from data frames containing the (symbolic) edge list and edge/vertex attributes. \n  igraph::graph_from_data_frame()\n\nThen we can convert an igraph object into a ggraph with the ggraph function (extension of ggplot2), after which we can add layers to it, much like layers are added in ggplot2. For example, for a basic graph we need to add three layers: “nodes”, “edges”, and “text”.\n\n#convert an igraph object into a ggraph with the ggraph function\nlibrary(ggraph) # An Implementation of Grammar of Graphics for Graphs and Networks\nset.seed(2022)\n\nggraph(bigram_graph, layout = \"fr\") +\n  # needed basic arguments passed\n  geom_edge_link() +\n  geom_node_point() +\n  geom_node_text(aes(label = name), vjust = 1, hjust = 1)\n\n\nI can already see some common center nodes\n\nWe conclude with a few polishing operations to make a better looking graph (Figure 4.5):\n\nWe add the edge_alpha aesthetic to the link layer to make links transparent based on how common or rare the bigram is (= n)\nWe add directionality with an arrow, constructed using grid::arrow(), including an end_cap option that tells the arrow to end before touching the node\nWe tinker with the options to the node layer to make the nodes more attractive (larger, blue points)\nWe add a theme that’s useful for plotting networks, theme_void()\n\n\n\nset.seed(2022)\n\na &lt;- grid::arrow(type = \"closed\", length = unit(.08, \"inches\"))\n\nabs_bigram_graph &lt;- ggraph(bigram_graph, layout = \"fr\") +\n  # LINK layer\n  geom_edge_link(aes(edge_alpha = n), # transparency of link based on n\n                 show.legend = FALSE,\n                 # direction\n                 arrow = a,\n                 # arrow to end before touch node\n                 end_cap = circle(.08, 'inches')) +\n  # NODE layer\n  geom_node_point(color = \"lightblue\", size = 3) +\n  geom_node_text(aes(label = name), \n                 vjust = 1, hjust = 1,\n                 check_overlap = TRUE, \n                 repel = FALSE  # adds more lines\n                 \n  ) +\n  # THEME\n  theme_void() +\n  ggtitle(\"Word Network in WDR's abstracts\")  \n\nabs_bigram_graph\n\nabs_bigram_graph %T&gt;% \n  print() %T&gt;%\n  ggsave(., filename = here(\"analysis\", \"output\", \"figures\", \"abs_bigram_graph.pdf\"),\n         #width = 4, height = 2.25, units = \"in\",\n         device = cairo_pdf) %&gt;% \n  ggsave(., filename = here(\"analysis\", \"output\", \"figures\", \"abs_bigram_graph.png\"),\n         #width = 4, height = 2.25, units = \"in\", \n         type = \"cairo\", dpi = 300)  \n\n— Counting and correlating among sections\nNotes for “Text Mining with R: A Tidy Approach”\nThe widyr package makes operations such as computing counts and correlations easy, by simplifying the pattern of “widen data -&gt; perform an operation -&gt; then re-tidy data”. We’ll focus on a set of functions that make pairwise comparisons between groups of observations (for example, between documents, or sections of text).\n\n# divide abstracts into 5-line sections \nabs_section_words &lt;- abs_all %&gt;%\n  mutate(text = stringi::stri_split_lines(abstract, omit_empty = FALSE)\n) %&gt;% \n  #filter(date_issued == \"1978\") %&gt;%\n  mutate(section = row_number(.$abstract) %/% 5) %&gt;%\n  filter(section &gt; 0) %&gt;%\n  unnest_tokens(output =  word,\n                input = abstract) %&gt;%\n  filter(!word %in% stop_words$word)\n\nwidyr::pairwise_counts() counts the number of times each pair of items (words) appear together within a group defined by “feature” (section). &gt; note it still returns a tidy data frame, although the underlying computation took place in a matrix form :\n\nabs_section_words %&gt;% \n  widyr::pairwise_count(item = word, feature = section, sort = TRUE) %&gt;% \n# Since pairwise_count records both the counts of (word_A, word_B) and \n#(word_B, word_B), it does not matter we filter at item1 or item2\n  filter(item1 == \"developing\")\n\n— Pairwise correlation\nWe may want to examine correlation among words, which indicates how often they appear together relative to how often they appear separately.\nwe compute the \\(\\phi\\) coefficient. Introduced by Karl Pearson, this measure is similar to the Pearson correlation coefficient in its interpretation. In fact, a Pearson correlation coefficient estimated for two binary variables will return the \\(\\phi\\) coefficient. The phi coefficient is related to the chi-squared statistic for a 2 × 2 contingency table\n\\[\n\\phi = \\sqrt{\\frac{\\chi^2}{n}}\n\\]\nwhere \\(n\\) denotes sample size. In the case of pairwise counts, \\(\\phi\\) is calculated by\n\n\\[\n\\phi = \\frac{n_{11}n_{00} - n_{10}n_{01}}{\\sqrt{n_{1·}n_{0·}n_{·1}n_{·0}}}\n\\]\nWe see, from the above equation, that \\(\\phi\\) is “standardized” by individual counts, so various word pair with different individual frequency can be compared to each other:\nThe computation of \\(\\phi\\) can be simply done by pairwise_cor (other choice of correlation coefficients specified by method). The procedure can be somewhat computationally expensive, so we filter out uncommon words\n\nword_cors &lt;- abs_section_words %&gt;% \n  add_count(word) %&gt;% \n  filter(n &gt;= 20) %&gt;% \n  select(-n) %&gt;%\n  pairwise_cor(word, section, sort = TRUE)\n\nWhich word is most correlated with “poor”? [health,people,governments, data ]\n\nword_cors %&gt;% \n  filter(item1 == \"poor\")\n\nThis lets us pick particular interesting words and find the other words most associated with them\n\nsource(\"R/f_facetted_bar_plot.R\")\n\np_ass_words &lt;- word_cors %&gt;%\n  filter(item1 %in% c( \"people\", \"governments\", \"markets\", \"institutions\")) %&gt;%\n  group_by(item1) %&gt;%\n  top_n(6) %&gt;%\n  ungroup() %&gt;%\n  facet_bar(y = item2, x = correlation, by = item1) +\n  labs(title=\"Words most correlated to selected words of interest\", \n    subtitle=\"(Taken from WDRs' abstracts)\",\n     #caption=\"Source: ????\",\n     )   \n\np_ass_words %T&gt;% \n  print() %T&gt;%\n  ggsave(., filename = here(\"analysis\", \"output\", \"figures\", \"p_ass_words.pdf\"),\n         #width = 4, height = 2.25, units = \"in\",\n         device = cairo_pdf) %&gt;% \n  ggsave(., filename = here(\"analysis\", \"output\", \"figures\", \"p_ass_words.png\"),\n         #width = 4, height = 2.25, units = \"in\", \n         type = \"cairo\", dpi = 300)  \n\nHow about a network visualization to see the overall correlation pattern?\n\nword_cors %&gt;%\n  filter(correlation &gt; .15) %&gt;%\n  tidygraph::as_tbl_graph() %&gt;%\n  ggraph(layout = \"fr\") +\n  geom_edge_link(aes(edge_alpha = correlation), show.legend = FALSE) +\n  geom_node_point(color = \"lightblue\", size = 5) +\n  geom_node_text(aes(label = name), repel = TRUE)\n\nNote that unlike the bigram analysis, the relationships here are symmetrical, rather than directional (there are no arrows)."
  },
  {
    "objectID": "analysis/03_WDR_abstracs_explor.html#iii.iv-concordances---kwic---collocation",
    "href": "analysis/03_WDR_abstracs_explor.html#iii.iv-concordances---kwic---collocation",
    "title": "Process and merge data - WDR abstracts",
    "section": "III.iv) Concordances -> KWIC -> Collocation",
    "text": "III.iv) Concordances -&gt; KWIC -&gt; Collocation\n\n{Following LADAL Tutorial}\n{Following Ben Schmidt, Chp 8.2.3}\nhttps://alvinntnu.github.io/NTNU_ENC2036_LECTURES/corpus-analysis-a-start.html\n\nIn the language sciences, concordancing refers to the extraction of words from a given text or texts. Concordances are commonly displayed in the form of keyword-in-context displays (KWICs) where the search term is shown in context, i.e. with preceding and following words.\nConcordancing is central to analyses of text and they often represents the first step in more sophisticated analyses of language data, because concordances are extremely valuable for understanding how a word or phrase is used, how often it is used, and in which contexts is used. As concordances allow us to analyze the context in which a word or phrase occurs and provide frequency information about word use, they also enable us to analyze collocations or the collocational profiles of words and phrases (Stefanowitsch 2020, 50–51). Finally, concordances can also be used to extract examples and it is a very common procedure.\n— Concordances\n\n\nhttps://ladal.edu.au/textanalysis.html#Concordancing\nhttps://www.quantumjitter.com/project/deal/\n\n\nUsing quanteda\n\n— create kwic with individual keyword | purrr + print + save png\n\n# I use again data = abs_words\nabs_q_corpus &lt;- quanteda::corpus(as.data.frame(abs_all), \n                               docid_field = \"title\",\n                               text_field = \"abstract\",\n                               meta = list(\"id\", \"date_issued\")\n)\n \n# --- example with individual keyword \n# Step 1) tokens\nabs_q_tokens &lt;- tokens(x = abs_q_corpus,\n                       remove_punct = TRUE,\n                       remove_symbols = TRUE#,remove_numbers = TRUE\n )\n# Step 2) kwic (individual exe )\n# kwic_abs_data &lt;- quanteda::kwic(x = abs_q_tokens, # define text(s) \n#                                  # define pattern\n#                                  pattern = phrase(c(\"data\", \"knowledge\")),\n#                                  # define window size\n#                                  window = 5) %&gt;%\n#     # convert into a data frame\n#     as_tibble() %&gt;%\n#     left_join(abs_all, by = c(\"docname\" =  \"title\")) %&gt;%  \n#     # remove superfluous columns\n#      dplyr::select( 'Year' = date_issued, 'WDR title' = docname, pre, keyword, post) %&gt;%\n#   #  slice_sample( n = 50) %&gt;% \n#    kbl(align = \"c\") # %&gt;% kable_styling()\n \n# Step 2) kwic (on vector)\n# Iterate `quanteda::kwic` over a vector of tokens | regex-modified-keywords\nkeywords &lt;- c(\"data\", \"globalization\", \"sustainab*\", \"conditionalit*\", \"regulat*\", \"ODA\"  )\n\n# apply iteratively kwic over a vector of keywords\noutputs_key &lt;-  map(keywords, \n      ~quanteda::kwic(abs_q_tokens,\n                      pattern =  .x,\n                      window = 5) %&gt;% \n        as_tibble() %&gt;%\n        left_join(abs_all, by = c(\"docname\" =  \"title\")) %&gt;%  \n        # remove superfluous columns\n        dplyr::select( 'Year' = date_issued, 'WDR title' = docname, pre, keyword, post) \n  )\n\n# # all togetha 3\nn = length(keywords)\n  \n# outputs_key[[1]] %&gt;% \n#    kbl(align = \"c\") \n\n# this list  has no element names \nnames(outputs_key)\nn = length(keywords)\n# set names for elements \noutputs_key &lt;- outputs_key %&gt;% \n  set_names(paste0(\"kwic_\", keywords))\n\n# get rid of empty output dfs in list  \noutputs_key &lt;- outputs_key[sapply(\n  outputs_key, function(x) dim(x)[1]) &gt; 0] # 4 left!\n \n# -------------- print all \n# Modo 1 - walk + print -\nwalk(.x = outputs_key, .f = print)  \n\n# Modo 2 - walk + kbl -\n#walk(.x = outputs_key, .f = kbl)\n\n# # Modo 3 - imap??? + kbl -\n# purrr::imap(.x = outputs_key,\n#             .f = ~ {\n#               kbl(x = .x,\n#                   align = \"c\",\n#                   #format  = \"html\",\n#                   caption =.y\n#               ) # %&gt;% kable_styling()\n#             }\n# )\n\n# MODO 4 -&gt; create multiple tables from a single dataframe and save them as images\n# https://stackoverflow.com/questions/69323569/how-to-save-multiple-tables-as-images-using-kable-and-map/69323893#69323893\noutputs_key  %&gt;%\n  imap(~save_kable(file = paste0('analysis/output/tables/', .y, '_.png'),\n                 # bs_theme = 'journal', \n                  self_contained = T, \n                  x = kbl(.x, booktabs = T, align = c('l','l', 'c')) %&gt;%\n                    kable_styling() \n                   )\n    )\n\n— create kwic with phrases | purrr + print + save png\n\n# Iterate `quanteda::kwic` over a vector of phrases/bigrams \nkeywords_phrase &lt;- c(\"climate change\", \"investment climate\", \"pro-poor\", \n                     \"gender equality\", \"maximizing finance\", \"digital revolution\",\n                     \"private finance\")\n\n# Step 1) tokens\n# (done above) -&gt; abs_q_tokens\n\n# Step 2) kwic \n# apply iteratively kwic over a vector of bigrams\noutputs_bigrams &lt;- map(keywords_phrase,\n                       ~quanteda::kwic(x = abs_q_tokens, # define text(s) \n                                       # define pattern\n                                       pattern = quanteda::phrase(.x),\n                                       # define window size\n                                       window = 5) %&gt;%\n                         # convert into a data frame\n                         as_tibble() %&gt;%\n                         left_join(abs_all, by = c(\"docname\" =  \"title\")) %&gt;%  \n                         # remove superfluous columns\n                         dplyr::select( 'Year' = date_issued,\n                                        'WDR title' = docname, pre, keyword, post)\n                       )  \n\n#  number ofo cbigrams \nn_bi = length(keywords_phrase)\nn_bi # 7\n# name this list's elements \noutputs_bigrams &lt;- outputs_bigrams %&gt;% \n  set_names(paste0(\"kwic_\", keywords_phrase))  \n\n# get rid of empty output dfs in list  \noutputs_bigrams2 &lt;- outputs_bigrams[sapply(\n  outputs_bigrams, function(x) dim(x)[1]) &gt; 0] # 4 left!\n \n#or \noutputs_bigrams3 &lt;- purrr::keep(outputs_bigrams, ~nrow(.) &gt; 0)  # 4 left!\n\n# -------------- print all \n#  walk + print -\nwalk(.x = outputs_bigrams2, .f = print)  \n\n\n# -------------- save  all -&gt; create multiple tables from a single dataframe and save them as images\n# https://stackoverflow.com/questions/69323569/how-to-save-multiple-tables-as-images-using-kable-and-map/69323893#69323893\noutputs_bigrams2  %&gt;%\n  imap(~save_kable(file = paste0('analysis/output/tables/', .y, '_.png'),\n                   # bs_theme = 'journal', \n                   self_contained = T, \n                   x = kbl(.x, booktabs = T, align = c('l','l', 'c')) %&gt;%\n                     kable_styling() \n  )\n  )\n\n— Collocation\n\n\nhttps://ladal.edu.au/coll.html#2_Finding_Collocations\n\nCollocations are words that are attracted to each other (and that co-occur or co-locate together), e.g., Merry Christmas, Good Morning, No worries. Any word in any given language has collocations, i.e., others words that are attracted/attractive to that word. This allows us to anticipate what word comes next and collocations are context/text type specific. There are various different statistical measures are used to define the strength of the collocations, like the Mutual Information (MI) score and log-likelihood (see here for an over view of different association strengths measures).\n–&gt; EXE: Collocation for subset on poverty WDR\n\nIn a first step, we will split the Abstract into individual sentences.\n\n\n# reduce to just one long concatenated string \nabs_pov &lt;- abs_all %&gt;% \n  dplyr::filter(date_issued %in%  c(\"1978\", \"1980\", \"1990\", \"2001\")) %&gt;%  \n  select( abstract) %&gt;%\n summarize(text = str_c(abstract, collapse = \". \")) %&gt;% \n  as.character()\n  \n# read in and process text\nabs_pov_sentences &lt;-  abs_pov %&gt;%\n  stringr::str_squish() %&gt;%\n  # divide into sentences\n  tokenizers::tokenize_sentences(.) %&gt;%\n  unlist() %&gt;%\n  stringr::str_remove_all(\"- \") %&gt;%\n  stringr::str_replace_all(\"\\\\W\", \" \") %&gt;%\n  stringr::str_squish()\n\n# inspect data\nhead(abs_pov_sentences)\n\nIn a next step, we will create a matrix that shows how often each word co-occurred with each other word in the data.\n\n# convert into corpus\nabs_pov_corpus &lt;- Corpus(VectorSource(abs_pov_sentences))\n\n# create vector with words to remove\nextrawords &lt;- c(\"the\", \"can\", \"get\", \"got\", \"can\", \"one\", \n                \"dont\", \"even\", \"may\", \"but\", \"will\", \n                \"much\", \"first\", \"but\", \"see\", \"new\", \n                \"many\", \"less\", \"now\", \"well\", \"like\", \n                \"often\", \"every\", \"said\", \"two\")\n\n# clean corpus\nabs_pov_corpus_clean &lt;- abs_pov_corpus %&gt;%\n  tm::tm_map(removePunctuation) %&gt;%\n  tm::tm_map(removeNumbers) %&gt;%\n  tm::tm_map(tolower) %&gt;%\n  tm::tm_map(removeWords, stopwords()) %&gt;%\n  tm::tm_map(removeWords, extrawords)\n\n# create document term matrix\nabs_pov_dtm &lt;- DocumentTermMatrix(\n  abs_pov_corpus_clean, \n  control=list(bounds = list(global=c(1, Inf)),\n               weighting = weightBin))\n\n# convert dtm into sparse matrix\nabs_pov_sdtm &lt;- Matrix::sparseMatrix(i = abs_pov_dtm$i, j = abs_pov_dtm$j, \n                           x = abs_pov_dtm$v, \n                           dims = c(abs_pov_dtm$nrow, abs_pov_dtm$ncol),\n                           dimnames = dimnames(abs_pov_dtm))\n# calculate co-occurrence counts\ncoocurrences &lt;- t(abs_pov_sdtm) %*% abs_pov_sdtm\n# convert into matrix\ncollocates &lt;- as.matrix(coocurrences)\n\nWe can inspect this co-occurrence matrix and check how many terms (words or elements) it represents using the ncol function from base R. We can also check how often terms occur in the data using the summary function from base R.\n\n# inspect size of matrix\nncol(collocates) # 239\nsummary(rowSums(collocates))\n\nThe ncol function reports that the data represents 239 words and that the most frequent word occurs 163 times in the text.\nThe output of the summary function tells us that the minimum frequency of a word in the data is 5 with a maximum of 163. The difference between the median (18) and the mean (22) indicates that the frequencies are distributed non-normally - which is common for language data.\n–&gt; (EXE) Visualizing Collocations EXE “poverty”\nWe will now use an example of one individual word ( poverty ) to show, how collocation strength for individual terms is calculated and how it can be visualized.\nThe function calculateCoocStatistics is taken from Wiedemann and Niekler (n.d.) and applied to the abs_pov_sdtm SPARSE DOCUMENT TEXT MATRIX\nVisualizing Collocations\n\n# load function for co-occurrence calculation\nsource(\"https://slcladal.github.io/rscripts/calculateCoocStatistics.R\")\n# define term\ncoocTerm &lt;- \"development\"\n# calculate co-occurrence statistics\ncoocs &lt;- calculateCoocStatistics(coocTerm, abs_pov_sdtm, measure=\"LOGLIK\")\n# inspect results\ncoocs[1:20]\n\n# define term # 2 \ncoocTerm2 &lt;- \"poverty\"\n# calculate co-occurrence statistics\ncoocs2 &lt;- calculateCoocStatistics(coocTerm2, abs_pov_sdtm, measure=\"LOGLIK\")\n# inspect results\ncoocs2[1:20]\n\nThe output shows that the word most strongly associated with development in the poverty WDR subset is issues - here there is no substantive strength (a substantive strength of the association would indicate these term are definitely collocates and almost - if not already - a lexicalized construction)\n–&gt; (EXE) Association Strength\nThere are various visualizations options for collocations. Which visualization method is appropriate depends on what the visualizations should display.\nWe start with the most basic and visualize the collocation strength using a simple dot chart. We use the vector of association strengths generated above and transform it into a table. Also, we exclude elements with an association strength lower than 30.\n\ncoocdf &lt;- coocs2 %&gt;%\n  as.data.frame() %&gt;%\n  dplyr::mutate(CollStrength = coocs2,\n                Term = names(coocs2)) %&gt;%\n  dplyr::filter(CollStrength &gt; 0.1) # this is kind of weak but for exe's sake\n\n…[viz] association strengths\nWe can now visualize the association strengths as shown in the code chunk below.\n\np_ass_words_poverty &lt;- ggplot(coocdf, aes(x = reorder(Term, CollStrength, mean), y = CollStrength)) +\n  geom_point() +\n  coord_flip() +\n  #theme_void() +\n  labs(title = \"Association to word \\\"poverty\\\"\",\n       subtitle = \"Collocation strenght measured by log-likelihood\",\n       caption = \"Source: https://ladal.edu.au/coll.html#Association_Strength\",\n       y = \"\", \n       x = \"\"\n       )\n\np_ass_words_poverty %T&gt;% \n  print() %T&gt;%\n  ggsave(., filename = here(\"analysis\", \"output\", \"figures\", \"p_ass_words_poverty.pdf\"),\n         #width = 4, height = 2.25, units = \"in\",\n         device = cairo_pdf) %&gt;% \n  ggsave(., filename = here(\"analysis\", \"output\", \"figures\", \"p_ass_words_poverty.png\"),\n         #width = 4, height = 2.25, units = \"in\", \n         type = \"cairo\", dpi = 300)  \n\nThe dot chart shows that poverty is collocating more strongly with economic compared to any other term.\n–&gt; (EXE) Dendrograms\nAnother method for visualizing collocations are dendrograms. Dendrograms (also called tree-diagrams) show how similar elements are based on one or many features. As such, dendrograms are used to indicate groupings as they show elements (words) that are notably similar or different with respect to their association strength. To use this method, we first need to generate a distance matrix from our co-occurrence matrix.\n\ncoolocs &lt;- c(coocdf$Term, \"poverty\")\n# remove non-collocating terms\ncollocates_redux &lt;- collocates[rownames(collocates) %in% coolocs, ]\ncollocates_redux &lt;- collocates_redux[, colnames(collocates_redux) %in% coolocs]\n# create distance matrix\ndistmtx &lt;- dist(collocates_redux)\n\nclustertexts &lt;- hclust(    # hierarchical cluster object\n  distmtx,                 # use distance matrix as data\n  method=\"ward.D2\")        # ward.D as linkage method\n\nggdendrogram(clustertexts) +\n  ggtitle(\"Terms strongly collocating with *poverty*\")\n\n–&gt; (EXE) Network Graphs\nNetwork graphs are a very useful tool to show relationships (or the absence of relationships) between elements. Network graphs are highly useful when it comes to displaying the relationships that words have among each other and which properties these networks of words have.\n–&gt; (EXE) Basic Network Graphs\nIn order to display a network, we need to create a network graph by using the network function from the network package.\n\nnet = network::network(collocates_redux, \n                       directed = FALSE,\n                       ignore.eval = FALSE,\n                       names.eval = \"weights\")\n# vertex names\nnetwork.vertex.names(net) = rownames(collocates_redux)\n# inspect object\nnet\n\nNow that we have generated a network object, we visualize the network with GGally::ggnet2.\n\nGGally::ggnet2(net, \n       label = TRUE, \n       label.size = 4,\n       alpha = 0.2,\n       size.cut = 3,\n       edge.alpha = 0.3) +\n  guides(color = FALSE, size = FALSE)\n\nWe can customize the network object so that the visualization becomes more appealing and informative. To add information, we create vector of words that contain different groups, e.g. terms that rarely, sometimes, and frequently collocate with poverty (I used the dendrogram which displayed the cluster analysis as the basis for the categorization).\nBased on these vectors, we can then change or adapt the default values of certain attributes or parameters of the network object (e.g. weights. linetypes, and colors).\n\n# create vectors with collocation occurrences as categories\nmid &lt;- c(\"dimensions\", \"major\", \"developing\", \"social\", \"health\")\nhigh &lt;- c(\"economic\", \"countries\")\ninfreq &lt;- colnames(collocates_redux)[!colnames(collocates_redux) %in% mid & !colnames(collocates_redux) %in% high]\n# add color by group\nnet %v% \"Collocation\" = ifelse(network.vertex.names(net) %in% infreq, \"weak\", \n                   ifelse(network.vertex.names(net) %in% mid, \"medium\", \n                   ifelse(network.vertex.names(net) %in% high, \"strong\", \"other\")))\n# modify color\nnet %v% \"color\" = ifelse(net %v% \"Collocation\" == \"weak\", \"gray60\", \n                  ifelse(net %v% \"Collocation\" == \"medium\", \"orange\", \n                  ifelse(net %v% \"Collocation\" == \"strong\", \"indianred4\", \"gray60\")))\n# rescale edge size\nnetwork::set.edge.attribute(net, \"weights\", ifelse(net %e% \"weights\" &lt; 1, 0.1, \n                                   ifelse(net %e% \"weights\" &lt;= 2, .5, 1)))\n# define line type\nnetwork::set.edge.attribute(net, \"lty\", ifelse(net %e% \"weights\" &lt;=.1, 3, \n                               ifelse(net %e% \"weights\" &lt;= .5, 2, 1)))\n\nWe can now display the network object and make use of the added information.\n\np_ggnet_poverty &lt;- GGally::ggnet2(net, \n                                color = \"color\", \n                                label = TRUE, \n                                label.size = 4,\n                                alpha = 0.2,\n                                size = \"degree\",\n                                edge.size = \"weights\",\n                                edge.lty = \"lty\",\n                                edge.alpha = 0.2) +\n  guides(color = FALSE, size = FALSE) +\n  #theme_void() +\n  labs(title = \"Degrees of association to word \\\"poverty\\\"\",\n       subtitle = \"Weak (grey), medium (orange), strong (red)\"#,\n       # caption = \"Source: https://ladal.edu.au/coll.html#Association_Strength\",\n       # y = \"\", \n       # x = \"\"\n       )\n\np_ggnet_poverty %T&gt;% \n  print() %T&gt;%\n  ggsave(., filename = here(\"analysis\", \"output\", \"figures\", \"p_ggnet_poverty.pdf\"),\n         #width = 4, height = 2.25, units = \"in\",\n         device = cairo_pdf) %&gt;% \n  ggsave(., filename = here(\"analysis\", \"output\", \"figures\", \"p_ggnet_poverty.png\"),\n         #width = 4, height = 2.25, units = \"in\", \n         type = \"cairo\", dpi = 300)  \n\n–&gt; (EXE) Biplots\nAn alternative way to display co-occurrence patterns are bi-plots which are used to display the results of Correspondence Analyses. They are useful, in particular, when one is not interested in one particular key term and its collocations but in the overall similarity of many terms. Semantic similarity in this case refers to a shared semantic and this distributional profile. As such, words can be deemed semantically similar if they have a similar co-occurrence profile - i.e. they co-occur with the same elements. Biplots can be used to visualize collocations because collocates co-occur and thus share semantic properties which renders then more similar to each other compared with other terms.\n\n# perform correspondence analysis\nres.ca &lt;- FactoMineR::CA(collocates_redux, graph = FALSE)\n# plot results\nfactoextra::fviz_ca_row(res.ca, repel = TRUE, col.row = \"gray20\")\n\nThe bi-plot shows that poverty and development collocate as they are plotted in close proximity. The advantage of the biplot becomes apparent when we focus on other terms because the biplot also shows other collocates such as issues and growth\n–&gt; (EXE) Determining Significance\nIn order to identify which words occur together significantly more frequently than would be expected by chance, we have to determine if their co-occurrence frequency is statistical significant. This can be done wither for specific key terms or it can be done for the entire data. In this example, we will continue to focus on the key word selection.\nTo determine which terms collocate significantly with the key term (selection), we use multiple (or repeated) Fisher’s Exact tests which require the following information:\n\na = Number of times coocTerm occurs with term j\nb = Number of times coocTerm occurs without term j\nc = Number of times other terms occur with term j\nd = Number of terms that are not coocTerm or term j\n\nIn a first step, we create a table which holds these quantities.\n\n# convert to data frame\ncoocdf &lt;- as.data.frame(as.matrix(collocates))\n# reduce data\ndiag(coocdf) &lt;- 0\ncoocdf &lt;- coocdf[which(rowSums(coocdf) &gt; 10),]\ncoocdf &lt;- coocdf[, which(colSums(coocdf) &gt; 10)]\n# extract stats\ncooctb &lt;- coocdf %&gt;%\n  dplyr::mutate(Term = rownames(coocdf)) %&gt;%\n  tidyr::gather(CoocTerm, TermCoocFreq,\n                colnames(coocdf)[1]:colnames(coocdf)[ncol(coocdf)]) %&gt;%\n  dplyr::mutate(Term = factor(Term),\n                CoocTerm = factor(CoocTerm)) %&gt;%\n  dplyr::mutate(AllFreq = sum(TermCoocFreq)) %&gt;%\n  dplyr::group_by(Term) %&gt;%\n  dplyr::mutate(TermFreq = sum(TermCoocFreq)) %&gt;%\n  dplyr::ungroup(Term) %&gt;%\n  dplyr::group_by(CoocTerm) %&gt;%\n  dplyr::mutate(CoocFreq = sum(TermCoocFreq)) %&gt;%\n  dplyr::arrange(Term) %&gt;%\n  dplyr::mutate(a = TermCoocFreq,\n                b = TermFreq - a,\n                c = CoocFreq - a, \n                d = AllFreq - (a + b + c)) %&gt;%\n  dplyr::mutate(NRows = nrow(coocdf))\n\nWe now select the key term (poverty). If we wanted to find all collocations that are present in the data, we would use the entire data rather than only the subset that contains poverty.\n\ncooctb_redux &lt;- cooctb %&gt;%\n  dplyr::filter(Term == coocTerm2)\n\nNext, we calculate which terms are (significantly) over- and under-proportionately used with poverty. It is important to note that this procedure informs about both: over- and under-use! This is especially crucial when analyzing if specific words are attracted o repelled by certain constructions. Of course, this approach is not restricted to analyses of constructions and it can easily be generalized across domains and has also been used in machine learning applications.\n\ncoocStatz &lt;- cooctb_redux %&gt;%\n  dplyr::rowwise() %&gt;%\n  dplyr::mutate(p = as.vector(unlist(fisher.test(matrix(c(a, b, c, d), \n                                                        ncol = 2, byrow = T))[1]))) %&gt;%\n    dplyr::mutate(x2 = as.vector(unlist(chisq.test(matrix(c(a, b, c, d),                                                           ncol = 2, byrow = T))[1]))) %&gt;%\n  dplyr::mutate(phi = sqrt((x2/(a + b + c + d)))) %&gt;%\n      dplyr::mutate(expected = as.vector(unlist(chisq.test(matrix(c(a, b, c, d), ncol = 2, byrow = T))$expected[1]))) %&gt;%\n  dplyr::mutate(Significance = dplyr::case_when(p &lt;= .001 ~ \"p&lt;.001\",\n                                                p &lt;= .01 ~ \"p&lt;.01\",\n                                                p &lt;= .05 ~ \"p&lt;.05\", \n                                                FALSE ~ \"n.s.\"))\n\nWe now add information to the table and remove superfluous columns s that the table can be more easily parsed.\n\ncoocStatz &lt;- coocStatz %&gt;%\n  dplyr::ungroup() %&gt;%\n  dplyr::arrange(p) %&gt;%\n  dplyr::mutate(j = 1:n()) %&gt;%\n  # perform benjamini-hochberg correction\n  dplyr::mutate(corr05 = ((j/NRows)*0.05)) %&gt;%\n  dplyr::mutate(corr01 = ((j/NRows)*0.01)) %&gt;%\n  dplyr::mutate(corr001 = ((j/NRows)*0.001)) %&gt;%\n  # calculate corrected significance status\n  dplyr::mutate(CorrSignificance = dplyr::case_when(p &lt;= corr001 ~ \"p&lt;.001\",\n                                                    p &lt;= corr01 ~ \"p&lt;.01\",\n                                                    p &lt;= corr05 ~ \"p&lt;.05\", \n                                                    FALSE ~ \"n.s.\")) %&gt;%\n  dplyr::mutate(p = round(p, 6)) %&gt;%\n  dplyr::mutate(x2 = round(x2, 1)) %&gt;%\n  dplyr::mutate(phi = round(phi, 2)) %&gt;%\n  dplyr::arrange(p) %&gt;%\n  dplyr::select(-a, -b, -c, -d, -j, -NRows, -corr05, -corr01, -corr001) %&gt;%\n  dplyr::mutate(Type = ifelse(expected &gt; TermCoocFreq, \"Antitype\", \"Type\"))\n\nThe results show that poverty DOES NOT collocates significantly with anywords.\n\n\n–&gt; (EXE) Changes in Collocation Strength\n–&gt; (EXE) Collostructional Analysis"
  },
  {
    "objectID": "analysis/03_WDR_abstracs_explor.html#iii.vi-sentiment-analysis",
    "href": "analysis/03_WDR_abstracs_explor.html#iii.vi-sentiment-analysis",
    "title": "Process and merge data - WDR abstracts",
    "section": "III.vi) > > Sentiment Analysis",
    "text": "III.vi) &gt; &gt; Sentiment Analysis\nhttps://cfss.uchicago.edu/notes/harry-potter-exercise/"
  },
  {
    "objectID": "analysis/03_WDR_abstracs_explor.html#iii.v-topic-modeling",
    "href": "analysis/03_WDR_abstracs_explor.html#iii.v-topic-modeling",
    "title": "Process and merge data - WDR abstracts",
    "section": "III.v) Topic modeling",
    "text": "III.v) Topic modeling\n\nRobinson, Silge\nhttps://cfss.uchicago.edu/notes/topic-modeling/\nhttps://m-clark.github.io/text-analysis-with-R/topic-modeling.html\nAH: https://datavizf18.classes.andrewheiss.com/class/11-class/#topic-modeling\n\n\n[not sure applicable, they are all same topic here!]\n\nTopic modeling is a method for unsupervised classification of documents (blog post, news articles), similar to clustering on numeric data, which finds natural groups of items even when we’re not sure what we’re looking for.\nMethods:\n\n\nLatent Dirichlet allocation (LDA) is a particularly popular method for fitting a topic model -&gt; It treats each document as a mixture of topics, and each topic as a mixture of words. The basic idea is that we’ll take a whole lot of features and boil them down to a few ‘topics’. In this sense LDA is akin to discrete PCA.\n\n— LDA (Latent Dirichlet allocation) with topicmodels package\n\nNOTE: The topicmodels package takes a Document-Term Matrix as input and produces a model that can be tided by tidytext, such that it can be manipulated and visualized with dplyr and ggplot2.\n\nPrinciples:\n\nimagine that each document may contain words from several topics in particular proportions\nEvery topic is a mixture of words\n\nLDA is a mathematical method for estimating both of these at the same time: finding the mixture of words that is associated with each topic, while also determining the mixture of topics that describes each document.\n— From abstracts tiditext 2 dtm with tidytext\n\n\n# starting from this \npaint(abs_words2)\n\n# cast into a Document-Term Matrix (*)\nabs_words_dtm &lt;- abs_words2 %&gt;%\n  tidytext::cast_dtm(date_issued, word, n)\nabs_words_dtm\n\n# cast into a Term-Document Matrix\nabs_words_tdm &lt;- abs_words2 %&gt;%\n  tidytext::cast_tdm(date_issued, word, n)\nabs_words_tdm\n\n# cast into quanteda's dfm Document-feature matrix\nabs_words_dfm &lt;- abs_words2 %&gt;%\n    cast_dfm(date_issued, word, n)\n\n# cast into a Matrix object\nabs_words_m &lt;-  abs_words2 %&gt;%\n  cast_sparse(date_issued, word, n)\nclass(abs_words_m)\n\n— …from dtm 2 LDA document structure\nhttps://cfss.uchicago.edu/notes/topic-modeling/\n\n# from tidytext format (one-row-per-token)\n# ---- 1/2 cast into a Document-Term Matrix (*)\nabs_words_dtm &lt;- abs_words2 %&gt;%\n  tidytext::cast_dtm(date_issued, word, n)\nabs_words_dtm\n\n\n# # ---- 2/2 using Document-Term Matrix (*)\n# # set a seed so that the output of the model is predictable\n# # k is the number of topic\nabs_lda &lt;- topicmodels::LDA(abs_words_dtm, k = 2, control = list(seed = 1234))\nabs_lda\n\nFitting the model was the “easy part”: the rest of the analysis will involve exploring and interpreting the model using tidying functions from the tidytext package.\n\nNOTE: What if k change? Several different values for may be plausible, but by increasing we sacrifice clarity.\n\n— Word-topic probabilities\nThe tidytext package uses broom::tidy for extracting the per-topic-per-word probabilities, called β (“beta”), from the model.\n\nNOTE: For each combination, the model computes the probability of that term being generated from that topic.\n\n\n# extract per-topic-per-word beta\nabs_topics &lt;- tidytext::tidy(abs_lda, matrix = \"beta\")\nabs_topics # one-topic-per-term-per-row format\n\n#&gt; For example, the term “data” has a 8.33×10−12 probability of being generated from topic 1, but a 1.1×10−3 probability of being generated from topic 2.\n\nWe could use dplyr’s slice_max() to find the 10 terms that are most common within each topic. As a tidy data frame, this lends itself well to a ggplot2 visualization\n\nabs_top_terms &lt;- abs_topics %&gt;%\n  group_by(topic) %&gt;%\n  slice_max(beta, n = 10) %&gt;% \n  ungroup() %&gt;%\n  arrange(topic, -beta)\n\nabs_top_terms %&gt;%\n  mutate(term = reorder_within(term, beta, topic)) %&gt;%\n  ggplot(aes(beta, term, fill = factor(topic))) +\n  geom_col(show.legend = FALSE) +\n  facet_wrap(~ topic, scales = \"free\") +\n  scale_y_reordered()\n\nIn alternative, we could consider the terms that had the greatest difference in \\(\\beta\\) between topic 1 and topic 2. This can be estimated based on the log ratio of the two: \\(\\log_2(\\frac{\\beta_2}{\\beta_1})\\) (a log ratio is useful because it makes the difference symmetrical: \\(\\beta_2\\) being twice as large leads to a log ratio of 1, while \\(\\beta_1\\) being twice as large results in -1). To constrain it to a set of especially relevant words, we can filter for relatively common words, such as those that have a \\(\\beta\\) greater than 1/1000 in at least one topic.\n\nbeta_wide &lt;- abs_topics %&gt;%\n  mutate(topic = paste0(\"topic\", topic)) %&gt;%\n  pivot_wider(names_from = topic, values_from = beta) %&gt;% \n  filter(topic1 &gt; .001 | topic2 &gt; .001) %&gt;%\n  mutate(log_ratio = log2(topic2 / topic1))\nbeta_wide\n\nThe words with the greatest differences between the two topics are visualized in Figure @ref(fig:topiccompare).\n(ref:topiccap) Words with the greatest difference in \\(\\beta\\) between topic 2 and topic 1\n— Document-topic probabilities\nBesides estimating each topic as a mixture of words, LDA also models each document as a mixture of topics. We can examine the per-document-per-topic probabilities, called \\(\\gamma\\) (“gamma”), with the matrix = \"gamma\" argument to tidy().\n\nabs_documents &lt;- tidy(abs_lda, matrix = \"gamma\")\nabs_documents # 44 WDR x 2 topics = 88!\n\nEach of these values is an estimated proportion of words from that document that are generated from that topic. For example, the model estimates that only about percent(abs_documents$gamma[1]) of the words in document 1 were generated from topic 1.\nWe can see that many of these documents were drawn from a mix of the two topics, but that document 2014 was drawn almost entirely from topic 1, having a \\(\\gamma\\) from topic 2 close to zero. To check this answer, we could tidy() the document-term matrix (see Chapter @ref(tidy-dtm)) and check what the most common words in that document were.\n\ntidy(abs_words_dtm) %&gt;%\n  filter(document == 2014) %&gt;%\n  arrange(desc(count))\n\nBased on the most common words, this appears to be an article about the relationship between the American government and Panamanian dictator Manuel Noriega, which means the algorithm was right to place it in topic 2 (as political/national news).\n————– STOP ——————\n— Alternative LDA implementations"
  },
  {
    "objectID": "analysis/03_WDR_abstracs_explor.html#iii.vii-supervised-classification-with-text-data",
    "href": "analysis/03_WDR_abstracs_explor.html#iii.vii-supervised-classification-with-text-data",
    "title": "Process and merge data - WDR abstracts",
    "section": "III.vii) > > Supervised classification with text data",
    "text": "III.vii) &gt; &gt; Supervised classification with text data\nhttps://cfss.uchicago.edu/notes/supervised-text-classification/ we can now use machine learning models to classify text into specific sets of categories. This is known as supervised learning."
  },
  {
    "objectID": "posts/2024-05-29-sviluppo-sostenibile/index.html",
    "href": "posts/2024-05-29-sviluppo-sostenibile/index.html",
    "title": "Sviluppo sostenibile",
    "section": "",
    "text": "«Perfezionamento, miglioramento, progresso verso un superiore livello tecnico o metodologico, qualitativo, estetico o stilistico in un determinato ambito pratico o intellettuale. […] Incremento, potenziamento di un’attività industriale, terziaria, agricola, di una produzione o più genericamente di un sistema economico. […] Processo di crescita verso la piena maturità psicofisica, mentale, sessuale; irrobustimento del fisico, di una parte del corpo. […] Processo di crescita fino alla compiuta formazione di un organo o di un organismo animale o vegetale. […] » (Garbini 2003. p. 80)\n\n\n\n\n\nEntrato nella lingua italiana nel XVIII secolo (forse dall’Inglese o dal Francese), il termine sviluppo trova la sua origine nel latino tardo (X sec.) falŭppa definito “scarti di paglia minutissimi o ramoscelli minuti” incrociatosi con un derivativo del verbo volvere “avviluppare” e successivamente accresciuto dal prefisso estrattivo-durativo (Garbini 2003. p. 80)."
  },
  {
    "objectID": "posts/2024-05-29-sviluppo-sostenibile/index.html#definizione",
    "href": "posts/2024-05-29-sviluppo-sostenibile/index.html#definizione",
    "title": "Sviluppo sostenibile",
    "section": "",
    "text": "«Perfezionamento, miglioramento, progresso verso un superiore livello tecnico o metodologico, qualitativo, estetico o stilistico in un determinato ambito pratico o intellettuale. […] Incremento, potenziamento di un’attività industriale, terziaria, agricola, di una produzione o più genericamente di un sistema economico. […] Processo di crescita verso la piena maturità psicofisica, mentale, sessuale; irrobustimento del fisico, di una parte del corpo. […] Processo di crescita fino alla compiuta formazione di un organo o di un organismo animale o vegetale. […] » (Garbini 2003. p. 80)"
  },
  {
    "objectID": "posts/2024-05-29-sviluppo-sostenibile/index.html#etimologia",
    "href": "posts/2024-05-29-sviluppo-sostenibile/index.html#etimologia",
    "title": "Sviluppo sostenibile",
    "section": "",
    "text": "Entrato nella lingua italiana nel XVIII secolo (forse dall’Inglese o dal Francese), il termine sviluppo trova la sua origine nel latino tardo (X sec.) falŭppa definito “scarti di paglia minutissimi o ramoscelli minuti” incrociatosi con un derivativo del verbo volvere “avviluppare” e successivamente accresciuto dal prefisso estrattivo-durativo (Garbini 2003. p. 80)."
  },
  {
    "objectID": "posts/2024-05-29-sviluppo-sostenibile/index.html#sviluppo-economico",
    "href": "posts/2024-05-29-sviluppo-sostenibile/index.html#sviluppo-economico",
    "title": "Sviluppo sostenibile",
    "section": "SVILUPPO ECONOMICO",
    "text": "SVILUPPO ECONOMICO\n\n«La nozione di sviluppo [economico sostenibile], concetto maggiore e di marca Onu di metà XX secolo, è una parola chiave sulla quale si sono incontrate tutte le vulgate politico-ideologiche dei decenni Cinquanta e Sessanta. Ma è stata veramente pensata? (Garbini 2003. p. 81)."
  },
  {
    "objectID": "posts/2024-05-29-sviluppo-sostenibile/index.html#sviluppo-economico-sostenibile",
    "href": "posts/2024-05-29-sviluppo-sostenibile/index.html#sviluppo-economico-sostenibile",
    "title": "Sviluppo sostenibile",
    "section": "SVILUPPO (ECONOMICO) SOSTENIBILE",
    "text": "SVILUPPO (ECONOMICO) SOSTENIBILE\n\nDa dove viene\nNel 1972, la conferenza ONU sull’ “Ambiente Umano” (Stoccolma, 5-16 giugno 1972) è considerata la prima in cui (almeno i …. paesi presenti) prendono in considerazione il problema della conservazione dell’ambiente e della gestione delle risrse naturali come questione fondamentale. La conferenza promulga un DICHIARAZIONE con 7 proclami e 26 Principi. Qualche stralcio:\n(non ho piu visto questa definizione) &gt; “human environment”\n\nProclama 5: The natural growth of population continuously presents problems for the preservation of the environment, and adequate policies and measures should be adopted, as appropriate, to face this problems. Of all things in teh world, people are the most precious.\n\n\nPronciple 2: The natural resources of the earth, including the air, water, land, flora and fauna and especially representative samples of natural ecosustems, must be safeguarded for the benefit of present and future generations through careful planning or management\n\n(controversa la discussione se la crescita demografica sia un problmea o noma questo finisce nelle raccomandazioni) &gt; Principle 16: Demographic policies which are without prejudcie to basic human rights and which are deemed appropriate by Governments concerned should be applied in those regions where the rate of populatioon growth or excessive population concentration are likely to have adverse effects on the environemnr of the human environment and impede development\n\nRecommendation 12: I. It is recommended that the World Health Organization and other United Nations agencies should provide increased assistance to Governments which so request in the field of family planning programmes without delay. 2. It is further recommended that the World Health Organization should promote and intensify research endeavour in the field of human reproduction, so that the serious consequences of population explosion on human environment can be prevented.\n\n\nConclusions: 48. Many speakers, from both developing and developed countries, agreed that the ruthless pursuit of gross national product, without consideration for other factors, produced conditions of life that were an affront to the dignity of man. The requirements of clean air, water, shelter and health were undeniable needs and rights of man.\n\n\n\n[POPULAZION] Several speakers expressed regret that population problems took so minor a place in the agenda of the Conference. They argued that all strategies for develop- ment and environment would be fatally damaged unless the rate of population increase was reduced. Other speakers said that the population increase was not the problem; the real challenge was the fact that so large a number of the people of the world had such a small expectation for a fruitful, happy and long life. In the opinion of certain delegations there was no incompatibility between population growth and preservation of the environment.\n\n\n\n\nUna definizione vera e propria?\nIl concetto di “sviluppo sostenibile” appare per la prima volta nel 1987, nel cosiddetto rapporto Brundtland del 1987 presentato alla Commissione mondiale per l’ambiente e lo sviluppo denominato anche “Our Common Future” guidata da Gro Harlem Brundtland. Qui si menzionano:\n\nuno sviluppo che deve avere dei limiti\n\n\nHumanity has the ability to make development sustainable to ensure that it meets the needs of the present without compromising the ability of future generations to meet their own needs.\nThe concept of sustainable development does imply limits - not absolute limits but limitations imposed by the present state of technology and social organization on environmental resources and by the ability of the biosphere to absorb the effects of human activities. (Environment and Development. 1987, 16)\n\n\nuno sviluppo che adotta stili di vita diversi in base alla ricchezza del paese\n\n\nSustainable global development requires that those who are more affluent adopt life-styles within the planet’s ecological means - in their use of energy, for example. Further, rapidly growing populations can increase the pressure on resources and slow any rise in livingstandards; thus sustainable development can only be pursued if population size and growth are in harmony with the changing productive potential of the ecosystem. (Environment and Development. 1987, 17)\n\n\nuno sviluppo che considera il futuro\n\n\nsustainable development is not a fixed state of harmony, but rather a process of change in which the exploitation of resources, the direction of investments, the orientation of technological development, and institutional change are made consistent with future as well as present needs. (Environment and Development. 1987, 17)\n\n\nuno sviluppo che considera “poggia” su precisi indirizzi politici\n\n\nWe do not pretend that the process is easy or straightforward. Painful choices have to be made. Thus, in the final analysis, sustainable development must rest on political will. (Environment and Development. 1987, 17)\n\n\nla crescita della popolazione e’ superiore alle risorse disponibili\n\n\nThe issue is not just numbers of people, but how those numbers relate to available resources. Thus the ‘population problem’ must be dealt with in part by efforts to eliminate mass poverty, in order to assure more equitable access to resources, and by education to improve human potential to manage those resources. Urgent steps are needed to limit extreme rates of population growth… providing people with facilities and education that allow them to choose the size of their families is a way of assuring - especially for women - the basic human right of self-determination. (Environment and Development. 1987, 18)\n\n\nla crescita della popolazione e’ un problema IN CERTI LUOGHI\n\n\nOur human world of 5 billion must make room in a finite environment for another human world. The population could stabilize at between 8 and 14 billion sometime next century, according to UN projections. More than 90 per cent of the increase will occur in the poorest countries, and 90 per cent of that growth in already bursting cities.(Environment and Development. 1987, 13)\n\nSince that time, sustainable development has emerged as a core idea of international development theory and policy. However, some experts have criticized certain features of the concept, including:\n\nIts generality or vagueness, which has led to a great deal of debate over which forms or aspects of development qualify as “sustainable”\nIts lack of quantifiable or objectively measurable goals\nIts assumption of the inevitability and desirability of industrialization and economic development\nIts failure to ultimately prioritize human needs or environmental commitments, either of which may reasonably be considered more important in certain circumstances\n\n\n\nQuali assunti contiene\n\npopolazione\nLo stesso Economist diceva che ESG mette insieme 3 concetti disparati e (potenzialmente) in contraddizione (The Economist 2022)\npossono organi consultivi (o di indirizzo politico) dare direttive tecniche (con tanto di deadline) su temi sui quali non sono d’accordo nemmeno gli scienziati specialisti?\nnucleare no ma fotovoltaico si\n\n\n\nQuali principi dovrebbe contenere\n\nLCA ?\nintensita di energia?\n\n\n\nDove arriva\na cose molto concrete dove frasi un po vagheggianti spostano some dollari estremamente sonanti\n\nDirettive\nNel 2001, l’UE ha adottato una strategia a favore dello “sviluppo sostenibile” e successivamente ha introdotto il concetto in altri documenti, incluso l’Articolo (3) del Trattato sull’Unione Europea.\nIn 2015 the United Nations General Assembly adopted the 2030 Agenda for Sustainable Development, which included 17 sweeping goals designed to create a globally equitable society alongside a thriving environment.\n\n\nESG\n\n\nONG l’anello debole e piu ricattabile\nxche on the receiving end of funding\n\n\n\nIndirizzo politico o diktat tecnico?\nLa catena principi - obiettivi - metodi - tecnologie\nAbbiamo i proncipi? e se si li stiamo perdendo di vista?\n\nLe direttive non sono neutrali\nvedi come devi produrre le auto per"
  },
  {
    "objectID": "posts/2024-05-29-sviluppo-sostenibile/index.html#riferimentirinvii-1",
    "href": "posts/2024-05-29-sviluppo-sostenibile/index.html#riferimentirinvii-1",
    "title": "Sviluppo sostenibile",
    "section": "Riferimenti/rinvii",
    "text": "Riferimenti/rinvii"
  },
  {
    "objectID": "posts/2024-05-29-sviluppo-sostenibile/index.html#riduzionismo",
    "href": "posts/2024-05-29-sviluppo-sostenibile/index.html#riduzionismo",
    "title": "Sviluppo sostenibile",
    "section": "Riduzionismo",
    "text": "Riduzionismo"
  },
  {
    "objectID": "intro_NLP.html",
    "href": "intro_NLP.html",
    "title": "Intro to NLP",
    "section": "",
    "text": "Text Mining (or Text Analytics) is the process of deriving meaningful information from unstructured text data, which involves techniques that can identify patterns, trends, and correlations in text data. As a research method, it is versatile in its aplications and can be combined with different disciplines and techniques.\n\n\nText analytics often utilizes NLP techniques to process and analyze text.\nA subfield of artificial intelligence (AI), NLP focuses on enabling computers to understand, interpret, and generate human language.\n\nWhile NLP provides the tools and algorithms (like tokenization, parsing, and entity recognition), text mining applies these tools to extract specific information from large text corpora.\n\n\n\nCorpus Linguistics is a branch of text analysis research applied to linguistics (e.g. the role of frequency and phonotactics in affix ordering in English, study of idiomatic expressions, geographic spread of neologisms, etc.) or where insight from language is sought\n\nSometimes, Text analytics serves as a method to support other research methods.\n\n\n\n\nPopulation (in language) ~ Any (idealized) compendium of words that we are interested in analysing… (most) populations are amorphous moving targets.\n\n\nCorpus (pl. Corpora) ~ A language population is called a corpus, a collection of similar documents | objects that typically contain raw strings annotated with additional metadata and details\n\n\nreference corpora, e.g. the American National Corpus\nspecialized corpora\nparrallel and comparable corpora\n\n\n\nUnstructured Data ~ data which does not have a machine-readable internal structure. This is the case for plain text files (.txt), which are simply a sequence of characters (as opposed to structured data that conforms to tabular format and is machine readable)\n\n\n.json format is somwhere in the middle /~ semi-structured data /~ which reflects the autho rpreferences\n\n\n\nString ~ in computational approaches, a string is a specific type of data that represents text and is often encoded in specific format, e.g., Latin1 or UTF8.\n\nTidy text ~ refers both to the structural (physical) and infor- mational (semantic) organization of the dataset. Physically, a tidy dataset is a tabular data structure, where each row is an observation (e.g. token) and each column is a variable that contains measures of a feature or attribute of each observation.\n\n\nToken ~ is a meaningful unit of text, such as a word, that we are interested in using for analysis\n\nTypes ~ refers to the unique tokens in a term variable (&lt; of tokens if repetition)\n\n\n\nBigrams/n-gram ~ Sequential groupings of characters and words (e.g.sentence, or paragraph)\n\nCollocations ~ words that are attracted to each other (and that co-occur or co-locate together), e.g., Merry Christmas, Good Morning, No worries.\n\n\n\nText normalization ~ standardizing text to convert the text into a uniform format and reduce unwanted variation and noise. (e.g. eliminating missing, redundant, or anoma- lous observations, changing the case of the text, removing punctuation, stan- dardizing forms, etc.)\n\nText Tokenization ~ splitting text into tokens, i.e. adapting the text so it reflects the target lin- guistic unit that will be used in the analysis. (involves expanding or reducing the number of rows depending on the linguistic unit of analysis)\n\nEnrichment transformations ~ to add new attributes to the dataset (e.g. generation, recoding, and integration of observations and/or variables.)\n\nStemming ~ is the process of reducing inflected words to their word stem, base, or root form. E.g.: believe --\\&gt; believ\n\nA stem is the base part of a word to which affixes can be attached for derivatives\n\n\n\nLemmatization ~ is the process of reducing inflected words to their dictionary form, or lemma. E.g.: gone\\|going --\\&gt; go\n\n\nDocument-term matrix (DTM) ~ rows = documents | cols = words | cells = [0,1]/frequencies. A sparse matrix describing a collection (i.e., a corpus) of documents with one row for each document and one column for each term. (WDR_com)\n\nTerm-Document matrix (TDM) ~ rows = words | cols = documents | cells = [0,1]/frequencies. A sparse matrix describing a collection (i.e., a corpus) of documents with one row for each document and one column for each term. (WDR_com)\n\nA part of computer science and AI that deals with human languages, Natural Language Processing (NLP) has evolved significantly over the past few decades, driven by advances in computational power, machine learning, and the availability of large datasets.\nBroadly speaking, these were some key steps in its evolution:\n\nEarly Years (1950s - 1980s) - Rule-based Systems (Early NLP systems were based on rule-based methods, which relied on handcrafted rules for tasks like translation, parsing, and information retrieval).\n1970s - 1980s: Statistical Methods and Linguistic Models (The introduction of the Chomskyan Linguistic Models influenced NLP research, focusing on syntax and grammar, while statistical methods began to emerge, laying the groundwork for more data-driven approaches)\n1990s: Statistical NLP (significant shift towards statistical approaches due to the availability of larger text corpora and more powerful computers, Hidden Markov Models (HMMs) and n-grams became popular for tasks such as part-of-speech tagging, speech recognition, and machine translation)\n2000s: Machine Learning and Data-Driven Methods (rise of machine learning in NLP, particularly supervised learning methods: Support Vector Machines (SVMs), Maximum Entropy models, etc. The development of large annotated corpora and platforms fueled progress in areas such as parsing, word sense disambiguation, and sentiment analysis.)\n2010s: Deep Learning Revolution (Neural networks, particularly recurrent neural networks (RNNs) and later long short-term memory (LSTM) networks, became the standard for many NLP tasks. The introduction of word embeddings allowed words to be represented as continuous vectors in a high-dimensional space, capturing semantic relationships between them. Convolutional Neural Networks (CNNs) were applied to text classification and other tasks, although they were more commonly used in computer vision. The development of sequence-to-sequence (Seq2Seq) models enabled advancements in machine translation, summarization, and other sequence generation tasks. Transformers outperformed RNNs on many tasks and led to the development of large-scale pre-trained language models.)\nLate 2010s - Present: Pre-trained Language Models and NLP at Scale (Pre-trained language models like BERT (2018) by Google and GPT (Generative Pre-trained Transformer) by OpenAI revolutionized NLP by providing powerful, general-purpose models that could be fine-tuned for specific tasks with minimal training data. The concept of transfer learning became central, where models trained on massive datasets could be adapted to specific tasks. ChatGPT, BERT, T5, and FLAN-T5 continue to push the boundaries of what NLP can achieve, leading to increasingly sophisticated and human-like interactions.)\n2020s - Future Directions Multimodal models: Integrating NLP with other forms of data, such as images and audio, to create more comprehensive models. Explainability and interpretability: As models grow in complexity, understanding their decision-making processes becomes more important.\n\n\n\n\nNatural Language Processing (NLP) ~ is an interdisciplinary field in computer science that has specialized on processing natural language data using computational and mathematical methods.\n\nNetwork Analysis ~ the most common way to visualize relationships between entities. Networks, also called graphs, consist of nodes (typically represented as dots) and edges (typically represented as lines) and they can be directed or undirected networks.\n\nText Classification ~ a supervised learning method of learning and predicting the category or the class of a document given its text content.\n\nNamed Entity Recognition ~ NER is the task of classifying words or key phrases of a text into predefined entities of interest.\n\nText Summarization ~ a language generation task of summarizing the input text into a shorter paragraph of text.\n\nEntailment ~ the task of classifying the binary relation between two natural-language texts, text and hypothesis, to determine if the text agrees with the hypothesis or not.\n\nQuestion Answering ~ QA is the task of retrieving or generating a valid answer for a given query in natural language, provided with a passage related to the query.\n\nSentence Similarity ~ the process of computing a similarity score given a pair of text documents.\n\nEmbeddings ~ the process of converting a word or a piece of text to a continuous vector space of real number, usually, in low dimension.\n\nSentiment Analysis ~ Provides an example of train and use Aspect Based Sentiment Analysis with Azure ML and Intel NLP Architect.\n\nSemantic Analysis ~ Allows to analyze the semantic (semantics) fo texts. Such analyses often rely on semantic tagsets that are based on word meaning or meaning families/categories.\n\nPart-of-Speech (PoS) ~ Tagging identifies the word classes of words (e.g., noun, adjective, verb, etc.) in a text and adds part-of-speech tags to each word.\n\nTopic Modeling ~ Topic Modeling is a machine learning method seeks to answer the question: given a collection of documents, can we identify what they are about? Topic model algorithms look for patterns of co-occurrences of words in documents.\n\nThis goes beyond my scope, but just to lay out some important elements ust recall\n\nHermeneutics: from Greek \\(ἑρμηνευτική (τέχνη)\\) - hermeneutikè (téchne), deriving from the verb \\(ἑρμηνεύ\\) (hermēneuō) - is the science and practice of interpretation. Over history, it has been mainly applied to sacred or juridical texts. Two are the main approaches: 1. reconstructing the original intention of the authors 2. adapting the interpretation based on the person who receives the text\nexegesis: from Greek \\(ἐξήγησις\\) - exégesis - deriving from the verb \\(ἐξηγέομαι\\) (exegéomai, ~ bring out) - indicates “studying to explain”, but referring to the maximum level of depth is seeked (hence it is referred to sacred or normaitive texts for which every nuance can be poignant).\nphilology: from Greek \\(ϕιλολογία\\) - philologĭa - composed from \\(φίλος\\)- phìlos and \\(λόγος\\) - lògos - indicates literally interest/love for the word/reason. The term indicates the study of texts and their history, but changed a little with the Latin philologia, and later embraced the sense of ‘love of literature’.\nlinguistics\n\n…\n\n\nAn adage is an ancient saying or maxim, brief and sometimes mysterious, that has become accepted as conventional wisdom. In classical rhetoric, an adage is also known as a rhetorical proverb or paroemia. Often it’s a type of metaphor. It can express the values of a culture. (Nordquist 2018)\n\nExample(s): “The early bird gets the worm”, “Better late than never.”\n\n\nThe English word slogan has a Scottish Gaelic origin and derives from the combination of sluagh (army) + gairm (shout), i.e. a “battle cry”. Nowadays, it signifies a short, memorable and concise phrase used for marketing or political campaigns. Marketing slogans are often called taglines in the United States\n\nExample(s): “…”, “…”\n\n…"
  },
  {
    "objectID": "intro_NLP.html#what-is-text-mining-text-analytics",
    "href": "intro_NLP.html#what-is-text-mining-text-analytics",
    "title": "Intro to NLP",
    "section": "",
    "text": "Text Mining (or Text Analytics) is the process of deriving meaningful information from unstructured text data, which involves techniques that can identify patterns, trends, and correlations in text data. As a research method, it is versatile in its aplications and can be combined with different disciplines and techniques.\n\n\nText analytics often utilizes NLP techniques to process and analyze text.\nA subfield of artificial intelligence (AI), NLP focuses on enabling computers to understand, interpret, and generate human language.\n\nWhile NLP provides the tools and algorithms (like tokenization, parsing, and entity recognition), text mining applies these tools to extract specific information from large text corpora.\n\n\n\nCorpus Linguistics is a branch of text analysis research applied to linguistics (e.g. the role of frequency and phonotactics in affix ordering in English, study of idiomatic expressions, geographic spread of neologisms, etc.) or where insight from language is sought\n\nSometimes, Text analytics serves as a method to support other research methods.\n\n\n\n\nPopulation (in language) ~ Any (idealized) compendium of words that we are interested in analysing… (most) populations are amorphous moving targets.\n\n\nCorpus (pl. Corpora) ~ A language population is called a corpus, a collection of similar documents | objects that typically contain raw strings annotated with additional metadata and details\n\n\nreference corpora, e.g. the American National Corpus\nspecialized corpora\nparrallel and comparable corpora\n\n\n\nUnstructured Data ~ data which does not have a machine-readable internal structure. This is the case for plain text files (.txt), which are simply a sequence of characters (as opposed to structured data that conforms to tabular format and is machine readable)\n\n\n.json format is somwhere in the middle /~ semi-structured data /~ which reflects the autho rpreferences\n\n\n\nString ~ in computational approaches, a string is a specific type of data that represents text and is often encoded in specific format, e.g., Latin1 or UTF8.\n\nTidy text ~ refers both to the structural (physical) and infor- mational (semantic) organization of the dataset. Physically, a tidy dataset is a tabular data structure, where each row is an observation (e.g. token) and each column is a variable that contains measures of a feature or attribute of each observation.\n\n\nToken ~ is a meaningful unit of text, such as a word, that we are interested in using for analysis\n\nTypes ~ refers to the unique tokens in a term variable (&lt; of tokens if repetition)\n\n\n\nBigrams/n-gram ~ Sequential groupings of characters and words (e.g.sentence, or paragraph)\n\nCollocations ~ words that are attracted to each other (and that co-occur or co-locate together), e.g., Merry Christmas, Good Morning, No worries.\n\n\n\nText normalization ~ standardizing text to convert the text into a uniform format and reduce unwanted variation and noise. (e.g. eliminating missing, redundant, or anoma- lous observations, changing the case of the text, removing punctuation, stan- dardizing forms, etc.)\n\nText Tokenization ~ splitting text into tokens, i.e. adapting the text so it reflects the target lin- guistic unit that will be used in the analysis. (involves expanding or reducing the number of rows depending on the linguistic unit of analysis)\n\nEnrichment transformations ~ to add new attributes to the dataset (e.g. generation, recoding, and integration of observations and/or variables.)\n\nStemming ~ is the process of reducing inflected words to their word stem, base, or root form. E.g.: believe --\\&gt; believ\n\nA stem is the base part of a word to which affixes can be attached for derivatives\n\n\n\nLemmatization ~ is the process of reducing inflected words to their dictionary form, or lemma. E.g.: gone\\|going --\\&gt; go\n\n\nDocument-term matrix (DTM) ~ rows = documents | cols = words | cells = [0,1]/frequencies. A sparse matrix describing a collection (i.e., a corpus) of documents with one row for each document and one column for each term. (WDR_com)\n\nTerm-Document matrix (TDM) ~ rows = words | cols = documents | cells = [0,1]/frequencies. A sparse matrix describing a collection (i.e., a corpus) of documents with one row for each document and one column for each term. (WDR_com)"
  },
  {
    "objectID": "intro_NLP.html#what-is-natural-language-processing-nlp",
    "href": "intro_NLP.html#what-is-natural-language-processing-nlp",
    "title": "Intro to NLP",
    "section": "",
    "text": "A part of computer science and AI that deals with human languages, Natural Language Processing (NLP) has evolved significantly over the past few decades, driven by advances in computational power, machine learning, and the availability of large datasets.\nBroadly speaking, these were some key steps in its evolution:\n\nEarly Years (1950s - 1980s) - Rule-based Systems (Early NLP systems were based on rule-based methods, which relied on handcrafted rules for tasks like translation, parsing, and information retrieval).\n1970s - 1980s: Statistical Methods and Linguistic Models (The introduction of the Chomskyan Linguistic Models influenced NLP research, focusing on syntax and grammar, while statistical methods began to emerge, laying the groundwork for more data-driven approaches)\n1990s: Statistical NLP (significant shift towards statistical approaches due to the availability of larger text corpora and more powerful computers, Hidden Markov Models (HMMs) and n-grams became popular for tasks such as part-of-speech tagging, speech recognition, and machine translation)\n2000s: Machine Learning and Data-Driven Methods (rise of machine learning in NLP, particularly supervised learning methods: Support Vector Machines (SVMs), Maximum Entropy models, etc. The development of large annotated corpora and platforms fueled progress in areas such as parsing, word sense disambiguation, and sentiment analysis.)\n2010s: Deep Learning Revolution (Neural networks, particularly recurrent neural networks (RNNs) and later long short-term memory (LSTM) networks, became the standard for many NLP tasks. The introduction of word embeddings allowed words to be represented as continuous vectors in a high-dimensional space, capturing semantic relationships between them. Convolutional Neural Networks (CNNs) were applied to text classification and other tasks, although they were more commonly used in computer vision. The development of sequence-to-sequence (Seq2Seq) models enabled advancements in machine translation, summarization, and other sequence generation tasks. Transformers outperformed RNNs on many tasks and led to the development of large-scale pre-trained language models.)\nLate 2010s - Present: Pre-trained Language Models and NLP at Scale (Pre-trained language models like BERT (2018) by Google and GPT (Generative Pre-trained Transformer) by OpenAI revolutionized NLP by providing powerful, general-purpose models that could be fine-tuned for specific tasks with minimal training data. The concept of transfer learning became central, where models trained on massive datasets could be adapted to specific tasks. ChatGPT, BERT, T5, and FLAN-T5 continue to push the boundaries of what NLP can achieve, leading to increasingly sophisticated and human-like interactions.)\n2020s - Future Directions Multimodal models: Integrating NLP with other forms of data, such as images and audio, to create more comprehensive models. Explainability and interpretability: As models grow in complexity, understanding their decision-making processes becomes more important.\n\n\n\n\nNatural Language Processing (NLP) ~ is an interdisciplinary field in computer science that has specialized on processing natural language data using computational and mathematical methods.\n\nNetwork Analysis ~ the most common way to visualize relationships between entities. Networks, also called graphs, consist of nodes (typically represented as dots) and edges (typically represented as lines) and they can be directed or undirected networks.\n\nText Classification ~ a supervised learning method of learning and predicting the category or the class of a document given its text content.\n\nNamed Entity Recognition ~ NER is the task of classifying words or key phrases of a text into predefined entities of interest.\n\nText Summarization ~ a language generation task of summarizing the input text into a shorter paragraph of text.\n\nEntailment ~ the task of classifying the binary relation between two natural-language texts, text and hypothesis, to determine if the text agrees with the hypothesis or not.\n\nQuestion Answering ~ QA is the task of retrieving or generating a valid answer for a given query in natural language, provided with a passage related to the query.\n\nSentence Similarity ~ the process of computing a similarity score given a pair of text documents.\n\nEmbeddings ~ the process of converting a word or a piece of text to a continuous vector space of real number, usually, in low dimension.\n\nSentiment Analysis ~ Provides an example of train and use Aspect Based Sentiment Analysis with Azure ML and Intel NLP Architect.\n\nSemantic Analysis ~ Allows to analyze the semantic (semantics) fo texts. Such analyses often rely on semantic tagsets that are based on word meaning or meaning families/categories.\n\nPart-of-Speech (PoS) ~ Tagging identifies the word classes of words (e.g., noun, adjective, verb, etc.) in a text and adds part-of-speech tags to each word.\n\nTopic Modeling ~ Topic Modeling is a machine learning method seeks to answer the question: given a collection of documents, can we identify what they are about? Topic model algorithms look for patterns of co-occurrences of words in documents."
  },
  {
    "objectID": "intro_NLP.html#methodological-notes",
    "href": "intro_NLP.html#methodological-notes",
    "title": "Intro to NLP",
    "section": "",
    "text": "This goes beyond my scope, but just to lay out some important elements ust recall\n\nHermeneutics: from Greek \\(ἑρμηνευτική (τέχνη)\\) - hermeneutikè (téchne), deriving from the verb \\(ἑρμηνεύ\\) (hermēneuō) - is the science and practice of interpretation. Over history, it has been mainly applied to sacred or juridical texts. Two are the main approaches: 1. reconstructing the original intention of the authors 2. adapting the interpretation based on the person who receives the text\nexegesis: from Greek \\(ἐξήγησις\\) - exégesis - deriving from the verb \\(ἐξηγέομαι\\) (exegéomai, ~ bring out) - indicates “studying to explain”, but referring to the maximum level of depth is seeked (hence it is referred to sacred or normaitive texts for which every nuance can be poignant).\nphilology: from Greek \\(ϕιλολογία\\) - philologĭa - composed from \\(φίλος\\)- phìlos and \\(λόγος\\) - lògos - indicates literally interest/love for the word/reason. The term indicates the study of texts and their history, but changed a little with the Latin philologia, and later embraced the sense of ‘love of literature’.\nlinguistics\n\n…"
  },
  {
    "objectID": "intro_NLP.html#essential-list-of-rhetorical-devices",
    "href": "intro_NLP.html#essential-list-of-rhetorical-devices",
    "title": "Intro to NLP",
    "section": "",
    "text": "An adage is an ancient saying or maxim, brief and sometimes mysterious, that has become accepted as conventional wisdom. In classical rhetoric, an adage is also known as a rhetorical proverb or paroemia. Often it’s a type of metaphor. It can express the values of a culture. (Nordquist 2018)\n\nExample(s): “The early bird gets the worm”, “Better late than never.”\n\n\nThe English word slogan has a Scottish Gaelic origin and derives from the combination of sluagh (army) + gairm (shout), i.e. a “battle cry”. Nowadays, it signifies a short, memorable and concise phrase used for marketing or political campaigns. Marketing slogans are often called taglines in the United States\n\nExample(s): “…”, “…”\n\n…"
  },
  {
    "objectID": "posts/2023-11-15-revisione-libro-il-sistema-invisibile/index.html",
    "href": "posts/2023-11-15-revisione-libro-il-sistema-invisibile/index.html",
    "title": "Revisione libro: Il Sistema invisibile, di Marcello Foa del 15 Novembre 2023",
    "section": "",
    "text": "Da “4.4. Come disarticolare una società”\n\n\nVladimis Volkoff (Il montaggio) “Quando si tratta di mobilitare le masse in realta non si ha che uno scopo: immobilizzarle” (…) tante volte la nostra rabbia e’ stata indirizzata verso bersagli di facile presa, il politico corrotto che abusa dell’auto blu, mentre i veri sprechi e gli scandali rilevanti erano altri, come nel caso di Autostrade Italiane. Tante, troppe volte l’indignazione delle masse e’ stata indirizzata contro bersagli fittizzi o sproporzionati, che non hanno prodotto altro effetto che generare sfiducia socaile verso le autorita costituite, i partiti e le istituzioni (p. 104, 105)\n\n\n\n\nCitazioneBibTeX@online{m. mimmi2023,\n  author = {M. Mimmi, Luisa and M. Mimmi, Luisa},\n  title = {Revisione libro: Il Sistema invisibile, di Marcello Foa del\n    15 Novembre 2023},\n  date = {2023-11-15},\n  url = {https://lulliter.github.io/slogan/posts/2023-11-15-revisione-libro-il-sistema-invisibile},\n  langid = {it}\n}\nPer favore citare questo lavoro come:\nM. Mimmi, Luisa, and Luisa M. Mimmi. 2023. “Revisione libro: Il\nSistema invisibile, di Marcello Foa del 15 Novembre 2023.”\nNovember 15, 2023. https://lulliter.github.io/slogan/posts/2023-11-15-revisione-libro-il-sistema-invisibile."
  },
  {
    "objectID": "analysis/02_WDR_subjects_explor.html",
    "href": "analysis/02_WDR_subjects_explor.html",
    "title": "Process and merge data - WDR subjects",
    "section": "",
    "text": "Work in progress\n#knitr::opts_chunk$set(include = TRUE, warning = FALSE)\n# Pckgs -------------------------------------\n#if (!require (\"pacman\")) (install.packages(\"pacman\"))\n\n#p_install_gh(\"luisDVA/annotater\")\n#p_install_gh(\"HumanitiesDataAnalysis/hathidy\")\n# devtools::install_github(\"HumanitiesDataAnalysis/HumanitiesDataAnalysis\") \nlibrary(here)\nlibrary(fs)\nlibrary(paint) \nlibrary(tidyverse) \nlibrary(magrittr)\nlibrary(skimr)\nlibrary(scales) \nlibrary(colorspace)\nlibrary(httr)\nlibrary(DT) # an R interface to the JavaScript library DataTables\nlibrary(knitr)\nlibrary(kableExtra) \nlibrary(flextable) \nlibrary(splitstackshape)  #Stack and Reshape Datasets After Splitting Concatenated Values\nlibrary(tm) # Text Mining Package\nlibrary(tidytext) # Text Mining using 'dplyr', 'ggplot2', and Other Tidy Tools\n# this requires pre-requirsites to install : https://github.com/quanteda/quanteda\nlibrary(quanteda)\nlibrary(igraph)\nlibrary(sjmisc) # Data and Variable Transformation Functions\nlibrary(ggraph) # An Implementation of Grammar of Graphics for Graphs and Networks\nlibrary(widyr) # Widen, Process, then Re-Tidy Data\nlibrary(SnowballC) # Snowball Stemmers Based on the C 'libstemmer' UTF-8 Library\n# library(#HumanitiesDataAnalysis, # Data and Code for Teaching Humanities Data Analysis\nlibrary(sentencepiece) # Text Tokenization using Byte Pair Encoding and Unigram Modelling\nlibrary(sysfonts) \nlibrary(ggdendro)\nlibrary(network)\nlibrary(GGally)\n\nlibrary(topicmodels)                #  with dep   FAILED !!!!!!\n\n# extra steo needed to install github version \n#if (!require(\"devtools\")) install.packages(\"devtools\")\n#library(devtools)\n#install_github(\"husson/FactoMineR\")     FAILED !!!!!!\n# library(FactoMineR)\n#library(factoextra)\n\n# Plot Theme(s) -------------------------------------\n#source(here(\"R\", \"ggplot_themes.R\"))\nggplot2::theme_set(theme_minimal())\n# color paletts -----\nmycolors_gradient &lt;- c(\"#ccf6fa\", \"#80e8f3\", \"#33d9eb\", \"#00d0e6\", \"#0092a1\")\nmycolors_contrast &lt;- c(\"#E7B800\", \"#a19100\", \"#0084e6\",\"#005ca1\", \"#e60066\" )\n\n\n# Function(s) -------------------------------------\n\n# Data -------------------------------------\n\n# -------------------- {cut bc made too heavy} -------------------------------------\n# # Tables [AH knit setup when using kbl() ]------------------------------------\n# knit_print.data.frame &lt;- function(x, ...) {\n#   res &lt;- paste(c('', '', kable_styling(kable(x, booktabs = TRUE))), collapse = '\\n')\n#   asis_output(res)\n# }\n# \n# registerS3method(\"knit_print\", \"data.frame\", knit_print.data.frame)\n# registerS3method(\"knit_print\", \"grouped_df\", knit_print.data.frame)"
  },
  {
    "objectID": "analysis/02_WDR_subjects_explor.html#i-pre-processing",
    "href": "analysis/02_WDR_subjects_explor.html#i-pre-processing",
    "title": "Process and merge data - WDR subjects",
    "section": "I) Pre-processing",
    "text": "I) Pre-processing\nI.ii) – Set stopwords [more…]\n\n# --- alt stop words\n# mystopwords &lt;- tibble(word = c(\"eq\", \"co\", \"rc\", \"ac\", \"ak\", \"bn\", \n#                                    \"fig\", \"file\", \"cg\", \"cb\", \"cm\",\n#                                \"ab\", \"_k\", \"_k_\", \"_x\"))\n\n# --- set up stop words\nstop_words &lt;- as_tibble(stop_words) %&gt;% # in the tidytext dataset \n  add_row(word = \"WDR\", lexicon = NA_character_) %&gt;%\n  # add_row(word = \"world\", lexicon = NA_character_) %&gt;%\n  add_row(word = \"report\", lexicon = NA_character_) %&gt;%\n  # add_row(word = \"development\", lexicon = NA_character_) %&gt;%\n  add_row(word = \"1978\", lexicon = NA_character_) %&gt;%\n  add_row(word = \"1979\", lexicon = NA_character_) %&gt;%\n  add_row(word = \"1980\", lexicon = NA_character_) %&gt;%\n  add_row(word = \"1981\", lexicon = NA_character_) %&gt;%\n  add_row(word = \"1982\", lexicon = NA_character_) %&gt;%\n  add_row(word = \"1983\", lexicon = NA_character_) %&gt;%\n  add_row(word = \"1984\", lexicon = NA_character_) %&gt;%\n  add_row(word = \"1985\", lexicon = NA_character_) %&gt;%\n  add_row(word = \"1986\", lexicon = NA_character_) %&gt;%\n  add_row(word = \"1987\", lexicon = NA_character_) %&gt;%\n  add_row(word = \"1988\", lexicon = NA_character_) %&gt;%\n  add_row(word = \"1989\", lexicon = NA_character_) %&gt;%\n  add_row(word = \"1990\", lexicon = NA_character_) %&gt;%\n  add_row(word = \"1991\", lexicon = NA_character_) %&gt;%\n  add_row(word = \"1992\", lexicon = NA_character_) %&gt;%\n  add_row(word = \"1993\", lexicon = NA_character_) %&gt;%\n  add_row(word = \"1994\", lexicon = NA_character_) %&gt;%\n  add_row(word = \"1995\", lexicon = NA_character_) %&gt;%\n  add_row(word = \"1996\", lexicon = NA_character_) %&gt;%\n  add_row(word = \"1997\", lexicon = NA_character_) %&gt;%\n  add_row(word = \"1998\", lexicon = NA_character_) %&gt;%\n  add_row(word = \"1999\", lexicon = NA_character_) %&gt;%\n  add_row(word = \"2000\", lexicon = NA_character_) %&gt;%\n  add_row(word = \"2001\", lexicon = NA_character_) %&gt;%\n  add_row(word = \"2002\", lexicon = NA_character_) %&gt;%\n  add_row(word = \"2003\", lexicon = NA_character_) %&gt;%\n  add_row(word = \"2004\", lexicon = NA_character_) %&gt;%\n  add_row(word = \"2005\", lexicon = NA_character_) %&gt;%\n  add_row(word = \"2006\", lexicon = NA_character_) %&gt;%\n  add_row(word = \"2007\", lexicon = NA_character_) %&gt;%\n  add_row(word = \"2008\", lexicon = NA_character_) %&gt;%\n  add_row(word = \"2009\", lexicon = NA_character_) %&gt;%\n  add_row(word = \"2010\", lexicon = NA_character_) %&gt;%\n  add_row(word = \"2011\", lexicon = NA_character_) %&gt;%\n  add_row(word = \"2012\", lexicon = NA_character_) %&gt;%\n  add_row(word = \"2013\", lexicon = NA_character_) %&gt;%\n  add_row(word = \"2014\", lexicon = NA_character_) %&gt;%\n  add_row(word = \"2015\", lexicon = NA_character_) %&gt;%\n  add_row(word = \"2016\", lexicon = NA_character_) %&gt;%\n  add_row(word = \"2017\", lexicon = NA_character_) %&gt;%\n  add_row(word = \"2018\", lexicon = NA_character_) %&gt;%\n  add_row(word = \"2019\", lexicon = NA_character_) %&gt;%\n  add_row(word = \"2020\", lexicon = NA_character_) %&gt;%\n  add_row(word = \"2021\", lexicon = NA_character_) %&gt;%\n  add_row(word = \"2022\", lexicon = NA_character_) %&gt;% \n  filter (word != \"changes\") %&gt;% \n   filter (word != \"value\") %&gt;% \n   filter (word != \"member\") %&gt;% \n   filter (word != \"part\") %&gt;% \n   filter (word != \"possible\") %&gt;% \n   filter (word != \"point\") %&gt;% \n   filter (word != \"present\") %&gt;% \n   filter (word != \"zero\") %&gt;% \n     filter (word != \"young\") %&gt;% \n     filter (word != \"old\") %&gt;% \n     filter (word != \"trying\") \n\n# --- set up stop words stemmed\nstop_words_stem &lt;- stop_words  %&gt;% \nmutate (word = SnowballC::wordStem(word ))"
  },
  {
    "objectID": "analysis/02_WDR_subjects_explor.html#ii-data-ingestion-loading-cleaning",
    "href": "analysis/02_WDR_subjects_explor.html#ii-data-ingestion-loading-cleaning",
    "title": "Process and merge data - WDR subjects",
    "section": "II) Data (ingestion), loading & cleaning",
    "text": "II) Data (ingestion), loading & cleaning\nIngestion of WDR basic metadata was done in ./_my_stuff/WDR-data-ingestion.Rmd and the result saved as WDR.rds &lt;– (Being somewhat computational intensive, I only did it once.)\n\n\nWDR = tibble [45, 8]\n\ndoc_mt_identifier_1 chr oai:openknowledge.worldbank.org:109~\n\ndoc_mt_identifier_2 chr http://www-wds.worldbank.org/extern~\n\ndoc_mt_title chr Development Economics through the ~\n\ndoc_mt_date  chr 2012-03-19T10:02:25Z 2012-03-19T19:~\n\ndoc_mt_creator  chr Yusuf, Shahid World Bank World Bank~\n\ndoc_mt_subject chr ABSOLUTE POVERTY AGGLOMERATION BENE~\n\ndoc_mt_description chr The World Development Report (WDR) ~\n\ndoc_mt_set_spec chr oai:openknowledge.worldbank.org:109~\n\n\nIngestion of WDR lists of subjects was available among metadata but presented issues (difficulty to extract, many records with repetition,apparently wrong) so I reconstructed them manually in data/raw_data/WDR_subjects_corrected2010_2011.xlsx taking them from site https://elibrary.worldbank.org/ which lists keywords correctly Es 2022 WDR\n\n# # WRD metadata taken with API get (issues) \n# WDR &lt;- readr::read_rds(here::here(\"data\", \"raw_data\", \"WDR.rds\" )) %&gt;% \n#   # Extract only the portion of string AFTER the backslash {/}\n#   mutate(id = as.numeric(stringr::str_extract(doc_mt_identifier_1, \"[^/]+$\"))) %&gt;% \n#   dplyr::relocate(id, .before = doc_mt_identifier_1) %&gt;% \n#   mutate(url_keys = paste0(\"https://openknowledge.worldbank.org/handle/10986/\", id , \"?show=full\"))  %&gt;% \n#  # eliminate NON WDR book\n#   dplyr::filter(id != \"2586\") \n# \n# # WRD subject/date_issued taken by manual review \n# WDR_subjects &lt;- readxl::read_excel(here::here(\"data\", \"raw_data\", \n#                                               \"WDR_subjects_corrected2010_2011.xlsx\")) %&gt;%\n#   drop_na(id) %&gt;% \n#  # eliminate NON WDR book\n#   dplyr::filter(id != \"2586\") \n# \n# # delete empty cols \n# ColNums_NotAllMissing &lt;- function(df){ # helper function\n#   as.vector(which(colSums(is.na(df)) != nrow(df)))\n# }\n# \n# WDR_subjects &lt;- WDR_subjects  %&gt;% \n#   select(ColNums_NotAllMissing(.))\n#  # # convert all columns that start with \"subj_\" to lowercase\n#  # WDR_subjects[3:218] &lt;- sapply(WDR_subjects[3:218], function(x) tolower(x))\n# \n# # join\n# WDR_com &lt;- left_join(WDR, WDR_subjects, by = \"id\") %&gt;% \n#   dplyr::relocate(date_issued, .before = id ) %&gt;% \n#   # drop useles clmns \n#   dplyr::select(#-doc_mt_identifier_1, \n#                 -doc_mt_identifier_2, -doc_mt_date, \n#                 -doc_mt_subject, -doc_mt_creator, -doc_mt_set_spec) %&gt;% \n#   # dplyr::relocate(url_keys, .after = subj_216 ) %&gt;% \n#   dplyr::rename(abstract = doc_mt_description) %&gt;% \n#   # correct titles -&gt; portion after {:}\n#   dplyr::mutate(., title = str_extract(doc_mt_title,\"[^:]+$\")) %&gt;% \n#   dplyr::relocate(title, .after = id)  %&gt;% \n#   dplyr::rename(title_miss = doc_mt_title) %&gt;% \n#   dplyr::mutate(title_miss = case_when(\n#     str_starts(title, \"World Development Report\") ~ \"Y\",\n#     TRUE ~ NA_character_) \n#   ) %&gt;% \n#   dplyr::mutate(subject_miss = if_else(is.na(subj_1), \n#                                        \"Y\", \n#                                        NA_character_)) %&gt;% \n#   dplyr::relocate(subject_miss, .after = title_miss)    %&gt;% \n#   dplyr::relocate(ISBN, .after = id)    \n#   \n# #paint(WDR_com)\n# \n# # convert all columns that start with \"subj_\" to lowercase (maybe redundant)\n# WDR_com[, grep(\"^subj_\", names(WDR_com))] &lt;- sapply(WDR_com[, grep(\"^subj_\", names(WDR_com))], function(x) tolower(x))\n# \n# # combine all `subj_...` vars into a vector separated by comma\n# col_subj &lt;- names(WDR_com[, grep(\"^subj_\", names(WDR_com))] )\n# \n# WDR_com &lt;- WDR_com %&gt;% tidyr::unite(\n#   col = \"all_subj\", \n#   subj_1:subj_46, \n#   sep = \",\",\n#   remove = FALSE,\n#   na.rm = TRUE) %&gt;% \n#   arrange(date_issued)\n# \n# #paint(WDR_com)\n\n\nwdr &lt;- readr::read_rds(here::here(\"data\", \"derived_data\", \"wdr.rds\" ))\npaint(wdr)\n\nI.iii) &gt; &gt; Part of Speech Tagging\nTagging segments of speech for part-of-speech (nouns, verbs, adjectives, etc.) or entity recognition (person, place, company, etc.) https://m-clark.github.io/text-analysis-with-R/part-of-speech-tagging.html\n– tagging with cleanNLP\n\nAH: https://datavizs22.classes.andrewheiss.com/example/13-example/#sentiment-analysis\nHere’s the general process for tagging (or “annotating”) text with the cleanNLP package:\n\nMake a dataset where one column is the id (line number, chapter number, book+chapter, etc.), and another column is the text itself.\nInitialize the NLP tagger. You can use any of these:\n\n\ncnlp_init_udpipe(): Use an R-only tagger that should work without installing anything extra (a little slower than the others, but requires no extra steps!)\n\ncnlp_init_spacy(): Use spaCy (if you’ve installed it on your computer with Python)\n\ncnlp_init_corenlp(): Use Stanford’s NLP library (if you’ve installed it on your computer with Java)\n\n\nFeed the data frame from step 1 into the cnlp_annotate() function and wait.\nSave the tagged data on your computer so you don’t have to re-tag it every time.\n————– [TITLES ?] ——————"
  },
  {
    "objectID": "analysis/02_WDR_subjects_explor.html#iv.i-tokenization",
    "href": "analysis/02_WDR_subjects_explor.html#iv.i-tokenization",
    "title": "Process and merge data - WDR subjects",
    "section": "IV.i) Tokenization",
    "text": "IV.i) Tokenization\nFollowing: http://varianceexplained.org/r/hn-trends/\n\n# unnest titles \ntitle_words &lt;- wdr %&gt;%                           # 44 obs X 5 var \n  mutate (year = date_issued) %&gt;% \n  # isolate necessary \n  dplyr::select(id, year, decade, title, altmetric ) %&gt;% # isolate titles\n  arrange(desc(year)) %&gt;%\n  # (redundant) Select only unique/distinct rows from a data frame \n  # dplyr::distinct(title, .keep_all = TRUE) %&gt;%\n  # ----- tidytext’s unnest_tokens function = {turn titles in individual words}\n  unnest_tokens(output = word, \n                input = title, \n                drop = FALSE, # Whether original input column should get dropped\n                to_lower = T, # (implicit) otherwise cannot match the stop_words\n                strip_punc = TRUE) %&gt;%            # 196 obs \n   # ---- token processing\n  # [Optional] stems words\n  mutate(word = SnowballC::wordStem(word)) %&gt;% # **\n  # [Optional] sometimes needed to graph \n  mutate(title = factor(title, ordered = TRUE))  %&gt;%\n  mutate(year = factor(year, ordered = TRUE)) %&gt;% # 196 obs X 5 var \n  # creates a data frame with one row per word per post!!!\n  # Select only unique/distinct rows from a data frame (if not unique keep first) | keep all vars\n  distinct(id, word, .keep_all = TRUE) %&gt;% # (redundant, no repetition of words in title) \n  # delete stop words (also previously stemmed)\n  anti_join(stop_words_stem, by = \"word\") %&gt;% # ** # 101 obs | 95 if stemmed \n  filter(str_detect(word, \"[^\\\\d]\")) %&gt;%\n  # calculate totals by word across all titles (eg agricultur = in 3 WDR) \n  group_by(word) %&gt;%\n  mutate(word_total = n()) %&gt;%\n  ungroup()  \n\n— Plot/save most common words in ALL 44 TITLES\n\n# this is the same as title_words$word_total, but just the totals NO REPETITION\nword_counts &lt;- title_words %&gt;%\n  # Count observations by group(ed words)\n  count(word, sort = TRUE)\n\n# plot \np_most_common_word_in_title &lt;- word_counts %&gt;%\n   head(30) %&gt;%\n  # filter ( n &gt;1) %&gt;% \n  mutate(word = reorder(word, n)) %&gt;%\n  ggplot(aes(word, n)) +\n  # geom_col() uses stat_identity(): it leaves the data as is.\n  geom_col(fill = \"lightblue\") +\n  scale_y_continuous(labels = comma_format()) +\n  coord_flip() +\n  labs(title = \"50 most common words in 44 World Development Reports' titles\",\n       subtitle = \"[stemmed & stop words removed]\" #, y = \"# of uses\"\n       )\n\np_most_common_word_in_title %T&gt;% \n  print() %T&gt;%\n  ggsave(., filename = here(\"analysis\", \"output\", \"figures\", \"most_common_word_in_title.pdf\"),\n         #width = 4, height = 2.25, units = \"in\",\n         device = cairo_pdf) %&gt;% \n  ggsave(., filename = here(\"analysis\", \"output\", \"figures\", \"most_common_word_in_title.png\"),\n         #width = 4, height = 2.25, units = \"in\", \n         type = \"cairo\", dpi = 300)  \n\nWhat are specific words that get a high altmetric ? https://youtu.be/C69QyycHsgE\n\nalt_title_words &lt;-  title_words %&gt;%\n  # Count observations by group(ed words)\n  add_count(word ) %&gt;% \n  group_by(word) %&gt;% \n  summarise(median_alt = median (altmetric),\n            # compresses the scale and you go up by smaller increments \n            geometric_mean_alt = exp(mean(log(altmetric + 1))) -1,\n            occurrences = n()) %&gt;% \n  arrange(desc(median_alt))  \n  \nalt_title_words\n\n— Plot/save most common words in ALL 44 TITLES - over time\nWhat words and topics have become more frequent, or less frequent, over time? These could give us a sense of the changing focus in dev econ, and let us predict what topics will continue to grow in relevance.\nTo achieve this, we’ll first count the occurrences of words in titles by decades\n\nwdr_decade &lt;- wdr %&gt;% \n  mutate (year = date_issued) %&gt;% \n  # isolate necessary \n  dplyr::select(id, year, decade, title ) %&gt;%  \n  arrange(desc(year))  \n  \n  \n# 1) obtain \"decade_total\"\ntitle_per_decade &lt;- wdr_decade   %&gt;%\n  group_by(decade) %&gt;%\n  summarize(decade_total = n()) %&gt;% \n  ungroup()\n\n# 2) obtain count \"n\" &lt;--  (group BY word*decade) \nword_decade_counts &lt;- title_words %&gt;%\n  # filter(word_total &gt;= 1000) %&gt;%\n  count(word, decade) %&gt;%\n  complete(word, decade, fill = list(n = 0)) %&gt;% \n  # join with 1)  \n  inner_join(title_per_decade, by = \"decade\") %&gt;%\n  mutate(percent = n / decade_total) %&gt;% \n  # weird step to re-attach year \n  inner_join(title_words, by = c(\"word\", \"decade\")) %&gt;% \n  select (-id, -title,  word,  word_total, decade, n, decade_total, percent, year) %&gt;% \n  mutate (year =  as.character(year)) %&gt;% \n  mutate (year =  as.numeric(year))  \n  \n  paint(word_decade_counts)\n\n————– {START: troppo difficile} ——————\n\n# library(broom)\n# \n# mod &lt;- ~ glm(cbind(n, decade_total - n) ~ decade, ., family = \"binomial\")\n# \n# slopes &lt;- word_decade_counts %&gt;%\n#   nest(-word) %&gt;%\n#   mutate(model = map(data, mod)) %&gt;%\n#   unnest(map(model, tidy)) %&gt;%\n#   filter(term == \"year\") %&gt;%\n#   arrange(desc(estimate))\n# \n# slopes\n\ntibble [100, 7] word chr 21st adjust ag agricultur agricultur agric~ word_total int 1 1 1 3 3 3 = [how many times word appear in titles ] decade chr 1990s 1980s 2020s 1980s 1980s 2000s n int 1 1 1 2 2 1 = [how many times word appear in titles ] decade_total int 10 10 3 10 10 9 = [how many doc per decade ] percent dbl 0.1 0.1 0.333333 0.2 0.2 0.111111 = [% of doc per decade mentioning the word] year dbl 1999 1981 2020 1986 1982 2008\nsimple lineover time\n\nword_decade_counts %&gt;%\n filter(word_total &gt; 2) %&gt;% \n  ggplot(aes(year, percent, color = word)) +\n  geom_point() +\n  scale_y_continuous(labels = percent_format()) +\n  labs(x = \"year\",\n       y = \"% of word in title per decade\",\n       color = \"\")\n\n————– {END: troppo difficile} ——————"
  },
  {
    "objectID": "analysis/02_WDR_subjects_explor.html#iv.ii-word-and-document-frequency-tf-idf",
    "href": "analysis/02_WDR_subjects_explor.html#iv.ii-word-and-document-frequency-tf-idf",
    "title": "Process and merge data - WDR subjects",
    "section": "IV.ii) > > Word and document frequency: TF-IDF",
    "text": "IV.ii) &gt; &gt; Word and document frequency: TF-IDF"
  },
  {
    "objectID": "analysis/02_WDR_subjects_explor.html#iv.iii-relationships-bw-words-word-clusters",
    "href": "analysis/02_WDR_subjects_explor.html#iv.iii-relationships-bw-words-word-clusters",
    "title": "Process and merge data - WDR subjects",
    "section": "IV.iii) Relationships b/w words: Word clusters",
    "text": "IV.iii) Relationships b/w words: Word clusters\nI want to consider clusters, but I don’t want to guess them I want to draw them from the data\n\n# hereI want to unstemm the title words \n# unnest titles \ntitle_words2 &lt;- wdr %&gt;%                           # 44 obs X 5 var \n  mutate (year = date_issued) %&gt;% \n  # isolate necessary \n  dplyr::select(id, year, decade, title, altmetric ) %&gt;% # isolate titles\n  arrange(desc(year)) %&gt;%\n  # (redundant) Select only unique/distinct rows from a data frame \n  # dplyr::distinct(title, .keep_all = TRUE) %&gt;%\n  # ----- tidytext’s unnest_tokens function = {turn titles in individual words}\n  unnest_tokens(output = word, \n                input = title, \n                drop = FALSE, # Whether original input column should get dropped\n                to_lower = T, # (implicit) otherwise cannot match the stop_words\n                strip_punc = TRUE) %&gt;%            # 196 obs \n   # ---- token processing\n  # [Optional] stems words\n  # mutate(word = SnowballC::wordStem(word)) %&gt;% # **\n  # [Optional] sometimes needed to graph \n  mutate(title = factor(title, ordered = TRUE))  %&gt;%\n  mutate(year = factor(year, ordered = TRUE)) %&gt;% # 196 obs X 5 var \n  # creates a data frame with one row per word per post!!!\n  # Select only unique/distinct rows from a data frame (if not unique keep first) | keep all vars\n  distinct(id, word, .keep_all = TRUE) %&gt;% # (redundant, no repetition of words in title) \n  # delete stop words (also previously stemmed)\n  anti_join(stop_words_stem, by = \"word\") %&gt;% # ** # 101 obs | 95 if stemmed \n  filter(str_detect(word, \"[^\\\\d]\")) %&gt;%\n  # calculate totals by word across all titles (eg agricultur = in 3 WDR) \n  group_by(word) %&gt;%\n  mutate(word_total = n()) %&gt;%\n  ungroup()  \n\n# I will also make the alt alt_title_words2\n\nalt_title_words2 &lt;-  title_words2 %&gt;%\n  # Count observations by group(ed words)\n  add_count(word ) %&gt;% \n  group_by(word) %&gt;% \n  summarise(median_alt = median (altmetric),\n            # compresses the scale and you go up by smaller increments \n            geometric_mean_alt = exp(mean(log(altmetric + 1))) -1,\n            occurrences = n()) %&gt;% \n  arrange(desc(median_alt))\n\ncorr GRAPHS\n\n# get pairwise correlation with {widyr}\ntop_corr &lt;- title_words2 %&gt;% \n  select (id, word) %&gt;% \n  widyr::pairwise_cor(word, id, sort = TRUE) %&gt;% \n  head(150)\n \n#str(top_corr)\n\nset.seed(2022)\n# graph them \ntop_corr %&gt;% \n  graph_from_data_frame() %&gt;% \n  ggraph() +\n  geom_edge_link() +\n  geom_node_point() +\n  geom_node_text(aes(label = name), repel = TRUE) + theme_void()\n\nNow I want to add some metrics to the graph, so I take alt_title_words which had some calculated things in it\n\nvertices &lt;- alt_title_words2 %&gt;%\n  # filter words that have correlation\n filter(word %in% top_corr$item1 | \n         word %in% top_corr$item2)\n\nset.seed(2022)\n# graph them \n# here I add what clusters get more altmetric than others\ntop_corr %&gt;% \n  graph_from_data_frame(vertices = vertices) %&gt;%  # df !\n  ggraph() +\n  geom_edge_link() +\n  geom_node_point(aes(size = occurrences,\n                      color = geometric_mean_alt)) + # aes !\n  geom_node_text(aes(label = name), repel = TRUE) + \n  scale_color_gradient2(low = \"blue\",\n                        high = \"red\",\n                        midpoint = 1000) +\n  theme_void() + \n  labs(title = \"what's hot in WDR titles?\",\n       subtitle = \"Color shows the geom mean of altmetric score on WDR titles containing this word\",\n       size = \"# of occurrences\",\n       color = \"Altmetric (mean)\")   \n\nPredicting altmetric based on title + topic\n\n# some reshaping \ntitle_word_matrix &lt;- title_words2 %&gt;% \n  distinct(id, word, altmetric) %&gt;% \n  # turn into a sparse matrix \n  cast_sparse(id, word)\n\ndim(title_word_matrix)\n\n# ..."
  },
  {
    "objectID": "analysis/02_WDR_subjects_explor.html#iv.iv-relationships-bw-words-n-grams-and-correlations-word-clusters",
    "href": "analysis/02_WDR_subjects_explor.html#iv.iv-relationships-bw-words-n-grams-and-correlations-word-clusters",
    "title": "Process and merge data - WDR subjects",
    "section": "IV.iv) > > Relationships b/w words: n-grams and correlations Word clusters",
    "text": "IV.iv) &gt; &gt; Relationships b/w words: n-grams and correlations Word clusters"
  },
  {
    "objectID": "analysis/02_WDR_subjects_explor.html#iv.v-topic-modeling",
    "href": "analysis/02_WDR_subjects_explor.html#iv.v-topic-modeling",
    "title": "Process and merge data - WDR subjects",
    "section": "IV.v) > > Topic modeling",
    "text": "IV.v) &gt; &gt; Topic modeling\n————– [SUBJECTS & TOPICS !!!] ——————\nmust spread all_subj so that I have colum = “agric” row equal 0,1 thenn\n\n#noquote(names(wdr))\nwdr_subj &lt;- wdr %&gt;% \n  # delete subj_\n  select (date_issued, decade, title, abstract,\n          altmetric, all_topic, all_subj) \n \n# rownames_to_column(wdr_subj, 'all_subj') %&gt;%\n#         separate_rows(col) %&gt;% \n#         filter(col !=\"\")  %&gt;% \n#         count( all_subj, col) %&gt;%\n#         spread(col, n, fill = 0) %&gt;%\n#         ungroup() %&gt;% \n#         select(-all_subj)\n\n# # base \n# x   &lt;- strsplit(as.character(wdr_subj$all_subj), \",\\\\s?\") # split the strings\n# lvl &lt;- unique(unlist(x))                         # get unique elements\n# x   &lt;- lapply(x, factor, levels = lvl)           # convert to factor\n# subj_df &lt;- as_tibble(t(sapply(x, table)) )      # count elements and transpose \n\n\n# # data.table\n# library(data.table)\n# wdr_subj2 &lt;- setDT(wdr_subj)[, tstrsplit(all_subj, \", |,\")]\n# dcast(melt(wdr_subj2, measure = names(wdr_subj2)), rowid(variable) ~ value, length)\n\nlibrary(splitstackshape)\nwdr_subj2 &lt;- splitstackshape::cSplit_e(wdr_subj, \"all_subj\", \",\", mode = \"binary\", type = \"character\", fill = 0)\n\nwdr_subj3 &lt;- splitstackshape::cSplit_e(wdr_subj, \"all_topic\", \",\", mode = \"binary\", type = \"character\", fill = 0)\n\n— which SUBJ are the most common?\n\nwdr_subj2 %&gt;%\n  # summarise whole bunch of columns with sum\n  summarise_at(vars(starts_with(\"all_subj_\")), sum)\n\n# most popular AFTER RESHAPING \nwdr_subj_gathered &lt;-  wdr_subj2 %&gt;%\n  # summarise whole bunch of columns with sum\n  gather(subj, value,(starts_with(\"all_subj_\"))) %&gt;% \n  mutate(subj = str_remove(subj, \"all_subj_\")) %&gt;% \n  filter (value ==1) \n\nwdr_subj_gathered %&gt;% \n  count(subj, sort = TRUE)\n\nwdr_subj_gathered %&gt;% \n  group_by(decade) %&gt;% \n  count(subj, sort = TRUE) %&gt;% \n  arrange (desc(n) )-&gt; subj_bydecade\n\nsubj_bydecade %&gt;% \n  ggplot(aes(n)) + \n  geom_histogram()   #  scale_x_log10() # when data is very skewed\n\n— which TOPICS are the most common?\n\nwdr_subj3 %&gt;%\n  # summarise whole bunch of columns with sum\n  summarise_at(vars(starts_with(\"all_topic_\")), sum)\n\n\nwdr_subj3 %&gt;% \n  ggplot(aes(altmetric)) +\n  geom_histogram() +\n  scale_x_log10(labels =scales::comma_format())\n  \n \nwdr_subj3 %&gt;%   ggplot( aes(x=altmetric, fill=decade)) +\n    geom_histogram( color=\"#e9ecef\", alpha=0.6, position = 'identity') +\n    #scale_fill_manual(values=c(\"#69b3a2\", \"#404080\")) +\n    #theme_ipsum() +\n    labs(fill=\"\") +\n    facet_wrap(~decade)\n# not super meaningful but is says that over the decades the altmetric have been moving to the right (i.e. getting higher)\n\n# most popular AFTER RESHAPING \nwdr_top_gathered &lt;-  wdr_subj3 %&gt;%\n  # summarise whole bunch of columns with sum\n  gather(top, value,(starts_with(\"all_topic_\"))) %&gt;% \n  mutate(top = str_remove(top, \"all_topic_\")) %&gt;% \n  filter (value ==1) \n\nwdr_top_gathered %&gt;% \n  count(top, sort = TRUE)\n\nwdr_top_gathered %&gt;% \n  group_by(decade) %&gt;% \n  count(top, sort = TRUE) %&gt;% \n  arrange (desc(n) ) -&gt; topic_bydecade\n\n topic_bydecade %&gt;% \n   ggplot(aes(n))\n\n— plot most common TOPICS by decades\n\n# skimr::n_unique(topic_bydecade$top) # 26 \n# skimr::skim(topic_bydecade$n) # 26 \n\n# geom_col \np_topic_over_decades &lt;-  topic_bydecade  %&gt;%\n  # filter ( n &gt;1) %&gt;% \n  # mutate(top = reorder(top, n)) %&gt;%\n  # need reorder here or it won't stay \n  ggplot(aes(x= reorder(top, n), y = n), fill = decade) +\n  # geom_col() uses stat_identity(): it leaves the data as is.\n  geom_col(fill = \"lightblue\") +\n    scale_y_continuous( breaks = seq(1,9,1),\n                        labels = c(seq(1,8,1), \"9+\" )\n                        ) +\n  # more readable\n  coord_flip() +\n  labs(title = \"Most common topics in 44 WDRs over decades\",\n       subtitle = \"[High level topics covered = 26]\",\n       y = \"# of WDRs on topic per decade\", x = \"\"\n       )  +  facet_wrap(~decade)\n\np_topic_over_decades\n\np_topic_over_decades %T&gt;% \n  print() %T&gt;%\n  ggsave(., filename = here(\"analysis\", \"output\", \"figures\", \"p_topic_over_decades.pdf\"),\n         #width = 4, height = 2.25, units = \"in\",\n         device = cairo_pdf) %&gt;% \n  ggsave(., filename = here(\"analysis\", \"output\", \"figures\", \"p_topic_over_decades.png\"),\n         #width = 4, height = 2.25, units = \"in\", \n         type = \"cairo\", dpi = 300)  \n\nneed to create groups “umbrella subjects”\n\n# ggplot(subj_bydecade, aes(n, fill = decade)) +\n#   geom_histogram(binwidth = 1,\n#                  color = \"white\") +\n#   scale_y_continuous(breaks= pretty_breaks()) +\n#   xlim(0, 20) +\n#   labs(#title = ~date_issued, \n#     x = \"frequency\",\n#     y = \"N of words @ that frequency\") + \n#   facet_wrap( ~decade ) #+ # , ncol = 2, scales = \"free_y\")\n#   #guides( fill = \"none\") # way to turn legend off\n# \n\n# geom_col \np_most_common_word_in_subj &lt;- subj_bydecade %&gt;%\n  head(50) %&gt;%\n  # filter ( n &gt;1) %&gt;% \n  mutate(subj = reorder(subj, n)) %&gt;%\n  ggplot(aes(subj, n), fill = decade) +\n  # geom_col() uses stat_identity(): it leaves the data as is.\n  geom_col(fill = \"lightblue\") +\n  scale_y_continuous(labels = comma_format()) +\n  coord_flip() +\n  labs(title = \"50 most common subjects in 44 World Development Reports' titles\",\n       subtitle = \"[ ]\" #, y = \"# of uses\"\n       ) +\n  facet_wrap(~decade)\n\np_most_common_word_in_subj\n\np_most_common_word_in_subj %T&gt;% \n  print() %T&gt;%\n  ggsave(., filename = here(\"analysis\", \"output\", \"figures\", \"most_common_word_in_subj.pdf\"),\n         #width = 4, height = 2.25, units = \"in\",\n         device = cairo_pdf) %&gt;% \n  ggsave(., filename = here(\"analysis\", \"output\", \"figures\", \"most_common_word_in_subj.png\"),\n         #width = 4, height = 2.25, units = \"in\", \n         type = \"cairo\", dpi = 300)  \n\n— [word clouds by decade ???]\n— [CORRELATION GRAPHS ???]\n— PREDICTION OF DOWNLOADS ???"
  },
  {
    "objectID": "analysis/02_WDR_subjects_explor.html#a4.-tokenization-by-n-gram---iteratively",
    "href": "analysis/02_WDR_subjects_explor.html#a4.-tokenization-by-n-gram---iteratively",
    "title": "Process and merge data - WDR subjects",
    "section": "A4. Tokenization by n-gram - ITERATIVELY]",
    "text": "A4. Tokenization by n-gram - ITERATIVELY]\n– using abstract?\n– using subjects?"
  },
  {
    "objectID": "analysis/01_WB_project_data.html",
    "href": "analysis/01_WB_project_data.html",
    "title": "WB Project Data Preprocessing",
    "section": "",
    "text": "Work in progress\n# Pckgs -------------------------------------\nif (!require (\"pacman\")) (install.packages(\"pacman\"))\np_load(tidyverse, \n       janitor, skimr,\n       here, paint,\n       readxl,\n       repurrrsive, # examples of recursive lists\n       listviewer, # provides an interactive method for viewing the structure of a list.\n       httr, jsonlite, XML,xml2, \n       oai, # R client to work with OAI-PMH \n       citr,\n       fs)"
  },
  {
    "objectID": "analysis/01_WB_project_data.html#raw-data",
    "href": "analysis/01_WB_project_data.html#raw-data",
    "title": "WB Project Data Preprocessing",
    "section": "Raw data",
    "text": "Raw data\nI retrieved manually ALL WB projects approved between FY 1973 and 2023 (last FY incomplete) on 09/22/2022 (WDRs go from 1978-2022) using this example url FY 1978/79 and saved individual .xlsx files in data/raw_data/projects\n\n\n\n—————————————————————————"
  },
  {
    "objectID": "analysis/01_WB_project_data.html#ingest-projects-data-via-api",
    "href": "analysis/01_WB_project_data.html#ingest-projects-data-via-api",
    "title": "WB Project Data Preprocessing",
    "section": "{Ingest Projects data (via API)}",
    "text": "{Ingest Projects data (via API)}\n\nDOESN’T WORK!"
  },
  {
    "objectID": "analysis/01_WB_project_data.html#ingest-projects-data-manually-split",
    "href": "analysis/01_WB_project_data.html#ingest-projects-data-manually-split",
    "title": "WB Project Data Preprocessing",
    "section": "1/2 Ingest Projects data (manually split)",
    "text": "1/2 Ingest Projects data (manually split)\nI retrieved manually ALL WB projects approved between FY 1973 and 2023 (last FY incomplete) on 09/22/2022 (WDRs go from 1978-2022) using this example url and saved individual .xlsx files in data/raw_data/project\n\nnote the manual download is limited to # = 500\n\n— Load all .xlsx files separately\n\n— Save objs in folder as .Rds files separately"
  },
  {
    "objectID": "analysis/01_WB_project_data.html#ingest-projects-data-manually-all-together",
    "href": "analysis/01_WB_project_data.html#ingest-projects-data-manually-all-together",
    "title": "WB Project Data Preprocessing",
    "section": "2/2 Ingest Projects data (manually all together)",
    "text": "2/2 Ingest Projects data (manually all together)\nI retrieved manually ALL WB projects approved between FY 1973 and 2023 (last FY incomplete) on 31/08/2024 using simply the Excel button on this page this WBG Projects and saved HUUUGE .xlsx files in data/raw_data/project2/all_projects_as_of29ago2024.xls\n\nall_projects_as_of29ago2024   &lt;- read_excel(\"data/raw_data/project2/all_projects_as_of29ago2024.xls\", \n                         col_names = FALSE,\n                         skip = 1) \n# Nomi delle colonne\ncnames &lt;- read_excel(\"data/raw_data/project2/all_projects_as_of29ago2024.xls\", \n                         col_names = FALSE,\n                         skip = 1,\n                     n_max = 2) \n# file completo\nall_proj &lt;- read_excel(\"data/raw_data/project2/all_projects_as_of29ago2024.xls\", \n                         col_names = TRUE,\n                         skip = 2) \n\nsave(all_proj, file = \"data/raw_data/project2/all_projects_as_of29ago2024.Rdata\")"
  },
  {
    "objectID": "analysis/01_WB_project_data.html#raw-data-1",
    "href": "analysis/01_WB_project_data.html#raw-data-1",
    "title": "WB Project Data Preprocessing",
    "section": "Raw data",
    "text": "Raw data"
  },
  {
    "objectID": "analysis/01_WB_project_data.html#oai-pmh",
    "href": "analysis/01_WB_project_data.html#oai-pmh",
    "title": "WB Project Data Preprocessing",
    "section": "OAI-PMH",
    "text": "OAI-PMH\nOAI-PMH (Open Archives Initiative Protocol for Metadata Harvest-ing) services, a protocol developed by the Open Archives Initiative (https://en.wikipedia.org/wiki/Open_Archives_Initiative). OAI-PMH uses XML data format transported over HTTP.\n\npackg: Package oai is built on xml2 and httr.\npaging: OAI-PMH uses (optionally) resumptionTokens, with an optional expiration date. These tokens can be used to continue on to the next chunk of data, if the first request did not get to the end."
  },
  {
    "objectID": "analysis/01_WB_project_data.html#world-bank-list-of-world-development-reports",
    "href": "analysis/01_WB_project_data.html#world-bank-list-of-world-development-reports",
    "title": "WB Project Data Preprocessing",
    "section": "1. World Bank list of World Development Reports",
    "text": "1. World Bank list of World Development Reports\nGenreal OKR link https://openknowledge.worldbank.org/search?spc.page=1&query=%20&scope=3d9bbbf6-c007-5043-b655-04d8a1cfbfb2\nhttps://openknowledge.worldbank.org/entities/publication/5e5ac9f1-71ee-4734-825e-60966658395f 2023 | key takeaways | https://openknowledge.worldbank.org/server/api/core/bitstreams/54de9b54-dc23-43da-9a88-fe94dd5a3c24/content\nhttps://openknowledge.worldbank.org/server/api/core/bitstreams/e1e22749-80c3-50ea-b7e1-8bc332d0c2ff/content\n\n\nWorld Development Report (WDR);\n\n(in 2022) https://openknowledge.worldbank.org/handle/10986/2124\n(in 2024) https://openknowledge.worldbank.org/collections/3d9bbbf6-c007-5043-b655-04d8a1cfbfb2?spc.sf=dc.date.issued&spc.sd=DESC\n(in 2024) https://openknowledge.worldbank.org/collections/3d9bbbf6-c007-5043-b655-04d8a1cfbfb2?spc.sf=dc.date.issued&spc.sd=DESC&f.supportedlanguage=en,equals&spc.page=1&spc.rpp=100\n\n\n\n—————————————————————————\n\ninstall.packages(\"rvest\")\nlibrary(rvest)\n\n\nlink2023 &lt;- \"https://openknowledge.worldbank.org/entities/publication/5e5ac9f1-71ee-4734-825e-60966658395f/full\"\nWDR2023 &lt;- read_html(link2023)\n\nWDR2023  %&gt;%\n  html_elements(xpath = \"\")"
  },
  {
    "objectID": "analysis/01_WB_project_data.html#world-bank-list-of-world-development-reports-1",
    "href": "analysis/01_WB_project_data.html#world-bank-list-of-world-development-reports-1",
    "title": "WB Project Data Preprocessing",
    "section": "1. World Bank list of World Development Reports",
    "text": "1. World Bank list of World Development Reports\nHere I want to extract documents metadata from WBG OKR repository.\nWhich metadata: + collections: “Books” https://openknowledge.worldbank.org/handle/10986/4 + sub-collections: “Corporate Flagships” https://openknowledge.worldbank.org/handle/10986/2123 + World Development Report (WDR); https://openknowledge.worldbank.org/handle/10986/2124 https://openknowledge.worldbank.org/collections/3d9bbbf6-c007-5043-b655-04d8a1cfbfb2?spc.sf=dc.date.issued&spc.sd=DESC\n  + Global Economic Prospects (GEP), \n  + Doing Business (DB), and \n  + Poverty and Shared Prosperity (PSP).  \n  + ...\nI can search adding a keyword:\n+ World Development Report (WDR); + Keyword = “economic development”\n… the url would become: https://openknowledge.worldbank.org/handle/10986/2124/discover?filtertype=subject&filter_relational_operator=equals&filter=economic+development"
  },
  {
    "objectID": "analysis/01_WB_project_data.html#world-bank-projects-operations",
    "href": "analysis/01_WB_project_data.html#world-bank-projects-operations",
    "title": "WB Project Data Preprocessing",
    "section": "1. World Bank Projects & Operations:",
    "text": "1. World Bank Projects & Operations:\n—- DONT RUN —–"
  },
  {
    "objectID": "analysis/01_WB_project_data.html#tokenization",
    "href": "analysis/01_WB_project_data.html#tokenization",
    "title": "WB Project Data Preprocessing",
    "section": "Tokenization",
    "text": "Tokenization\n\nA token is a meaningful unit of text, such as a word, that we are interested in using for analysis\nbigrams"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Motivation",
    "section": "",
    "text": "I was really struck the first time I realized how the Latin motto I had heard since childhood: “Mens sana in corpore sano”, was actually a partial quotation used in 1861 by the Englishman John Hulley, as a motto for his Liverpool Athletic Club. In fact, it was extrapolated from a longer sentence by Juvenal poet: “Orandum est ut sit mens sana in corpore sano” (Satire X, 356). Taken in its entirety, the statement has quite a different meaning than its widely used shortened version: for one thing, it revolved around prayer and not fitness!\nI have seen this same dynamic (the meaning of words/phrases progressively detaching from the reality or from their originally intended purpose) at work also in the field of public policy, so I decided to dig deeper.\nIn fact, quoting Giovanni Gentile’s “Sommario di pedagogia come scienza filosofica”: “Language is not a garment of thought: it is its own body” (Garbini 2003, p3), i.e. language does not merely communicate our ideas to others, but it is also the tool of our thought, the wings for it to fly.  D’Auria put it in another way (D’Auria 2024), asserting that language is “performative”, in that it colors, deforms, produces reality. For this reason, it is necessarily a divisive instrument because, as soon as it is able to categorize, it is also capable of separating (good from bad, truth from lie, etc.). Knowledge itself, as much as it may scare the proponents of political correctness, lives on categories and categories, including linguistic ones, must precisely distinguish and correctly identify the ontological characteristics of what we are talking about."
  },
  {
    "objectID": "index.html#at-the-core-of-reduction",
    "href": "index.html#at-the-core-of-reduction",
    "title": "Motivation",
    "section": "",
    "text": "I was really struck the first time I realized how the Latin motto I had heard since childhood: “Mens sana in corpore sano”, was actually a partial quotation used in 1861 by the Englishman John Hulley, as a motto for his Liverpool Athletic Club. In fact, it was extrapolated from a longer sentence by Juvenal poet: “Orandum est ut sit mens sana in corpore sano” (Satire X, 356). Taken in its entirety, the statement has quite a different meaning than its widely used shortened version: for one thing, it revolved around prayer and not fitness!\nI have seen this same dynamic (the meaning of words/phrases progressively detaching from the reality or from their originally intended purpose) at work also in the field of public policy, so I decided to dig deeper.\nIn fact, quoting Giovanni Gentile’s “Sommario di pedagogia come scienza filosofica”: “Language is not a garment of thought: it is its own body” (Garbini 2003, p3), i.e. language does not merely communicate our ideas to others, but it is also the tool of our thought, the wings for it to fly.  D’Auria put it in another way (D’Auria 2024), asserting that language is “performative”, in that it colors, deforms, produces reality. For this reason, it is necessarily a divisive instrument because, as soon as it is able to categorize, it is also capable of separating (good from bad, truth from lie, etc.). Knowledge itself, as much as it may scare the proponents of political correctness, lives on categories and categories, including linguistic ones, must precisely distinguish and correctly identify the ontological characteristics of what we are talking about."
  },
  {
    "objectID": "index.html#issues-of-modern-english",
    "href": "index.html#issues-of-modern-english",
    "title": "Motivation",
    "section": "Issues of modern English",
    "text": "Issues of modern English\nBelow is the list of red flags detected in Orwell’s “Politics and the English Language”(Orwell 1946):\n\nDying metaphors, or worn−out metaphors which have lost all evocative power and are merely used because they save people the trouble of inventing phrases for themselves. \nOperators, or verbal false limbs, which save the trouble of picking out appropriate verbs and nouns, and at the same time pad each sentence with extra syllables which give it an appearance of symmetry. Characteristic phrases are: render inoperative, militate against, prove unacceptable, make contact with, be subjected to, give rise to, give grounds for, having the effect of, play a leading part (role) in, make itself felt, take effect, exhibit a tendency to, serve the purpose of, etc.\nPretentious diction (used to dress up simple statements and give an air of scientific impartiality to biased judgments), foreign words and expressions, jargon\nMeaningless words, as in certain kinds of writing, particularly in art criticism and literary criticism, where they not only do not point to any discoverable object, but are hardly even expected to do so by the reader (like romantic, plastic, values, human, dead, sentimental, natural, vitality), it is normal to come across long passages which are almost completely lacking in meaning. Many political words are similarly abused: The words democracy, socialism, freedom, patriotic, realistic, justice, have each of them several different meanings which cannot be reconciled with one another. Words of this kind are often used in a consciously dishonest way. That is, the person who uses them has his own private definition, but allows his hearer to think he means something quite different."
  },
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "Some examples of “domesticated lexicon”",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n\n\n\n\n\n\n\n\n\n\nSviluppo sostenibile\n\n\n\n\n\n\n🇮🇹\n\n\nlexicon\n\n\n\n[lessico addomesticato] \n\n\n\n\n\nMay 29, 2024\n\n\nLuisa M. Mimmi, Luisa M. Mimmi\n\n\n\n\n\n\n\n\n\n\n\n\nRevisione libro: Il Sistema invisibile, di Marcello Foa del 15 Novembre 2023\n\n\n\n\n\n\nspunti\n\n\n🇮🇹\n\n\n\n[libri] \n\n\n\n\n\nNov 15, 2023\n\n\nLuisa M. Mimmi, Luisa M. Mimmi\n\n\n\n\n\n\nNo matching items"
  }
]