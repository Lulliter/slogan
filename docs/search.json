[
  {
    "objectID": "data/raw_data/project2/all_projects_as_of29ago2024descr-stats.html",
    "href": "data/raw_data/project2/all_projects_as_of29ago2024descr-stats.html",
    "title": "Source",
    "section": "",
    "text": "Source\nALL WB projects available as of 31/08/2024 using simply the Excel button on this page this WBG Projects.\n\n\nFormat\n\nSaved HUUUGE .xls file in data/raw_data/project2/all_projects_as_of29ago2024.xls\nalso saved/copied as all_projects_as_of29ago2024.Rdata\n\n\n\nNote\nI tried using the API from link but it was too slow to lead and I couldn‚Äôt ‚Ä¶.\n\n\nData sampling frame\n\nData on WBG projects from boardapprovalFY 1947 to 2026\nColumns\n\n‚Ä¶1 chr Project ID |id ‚Ä¶2 chr Region |regionname ‚Ä¶3 chr Country |countryname ‚Ä¶4 chr Project Status |projectstatusdisplay ‚Ä¶5 chr Last Stage Reached Name |last_stage_reached_name ‚Ä¶6 chr Project Name |project_name ‚Ä¶7 chr Project Development Objective |pdo ‚Ä¶8 chr Implementing Agency |impagency ‚Ä¶9 chr Consultant Services Required |cons_serv_reqd_ind ‚Ä¶10 chr Project URL |url ‚Ä¶11 chr Board Approval Date |boardapprovaldate ‚Ä¶12 chr Project Closing Date |closingdate ‚Ä¶13 chr Financing Type |projectfinancialtype ‚Ä¶14 chr Current Project Cost |curr_project_cost ‚Ä¶15 chr IBRD Commitment |curr_ibrd_commitment ‚Ä¶16 chr IDA Commitment |curr_ida_commitment ‚Ä¶17 chr Total IDA and IBRD Commitment |curr_total_commitme~ ‚Ä¶18 chr Grant Amount |grantamt ‚Ä¶19 chr Borrower |borrower ‚Ä¶20 chr Lending Instrument |lendinginstr ‚Ä¶21 chr Environmental Assessment Category |envassesmentcategorycode ‚Ä¶22 chr Environmental and Social Risk |esrc_ovrl_risk_rate ‚Ä¶23 chr Sector 1 |sector1 ‚Ä¶24 chr Sector 2 |sector2 ‚Ä¶25 chr Sector 3 |sector3 ‚Ä¶26 chr Theme 1 |theme1 ‚Ä¶27 chr Theme 2 |theme2\n\n\nData collection date(s)"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "analysis/hypotheses.html",
    "href": "analysis/hypotheses.html",
    "title": "Research questions",
    "section": "",
    "text": "In general, a research question‚Äôs nature depends on the goal and type of the pursued analysis (See fig 3.9 in Francom 2024):\n\n\nTable¬†1: Overview of analysis types\n\n\n\n\n\n\n\n\n\n\nType\nAims\nApproach\nMethods\nEvaluation\n\n\n\nExploratory\nExplore: gain insight\nInductive, data-driven, and iterative\nDescriptive, pattern detection with machine learning (unsupervised)\nAssociative\n\n\nPredictive\nPredict: validate associations\nSemi-deductive, data-/ theory-driven, and iterative\nPredictive modeling with machine learning (supervised)\nModel performance, feature importance, and associative\n\n\nInferential\nExplain: test hypotheses\nDeductive, theory-driven, and non-iterative\nHypothesis testing with statistical tests\nCausal\n\n\n\n\n\n\n\n\nIs there a pattern in the WBG project document corpus1 that shows non random variation in the incidence of certain policy concepts2 over time?\n\nSince the WBG project document corpus data are very incomplete when it comes to sector and theme tagging: is it possible to overcome the insufficient data completion using TOPIC MODELING?\n\nCould the WDR 3 ‚Äúexplain‚Äù or at least have a correlation to the appearance-prevalence of said concepts?\nFor the moment, the present study‚Äôs research aim (See Table¬†1) is mainly TO EXPLORE (trends over time in concepts use), and possibly to PREDICT (conjecture about WRD traction effect).",
    "crumbs": [
      "Analysis",
      "Research Plan",
      "Research questions"
    ]
  },
  {
    "objectID": "analysis/hypotheses.html#sec-RQ11",
    "href": "analysis/hypotheses.html#sec-RQ11",
    "title": "Research questions",
    "section": "",
    "text": "Is there a pattern in the WBG project document corpus1 that shows non random variation in the incidence of certain policy concepts2 over time?",
    "crumbs": [
      "Analysis",
      "Research Plan",
      "Research questions"
    ]
  },
  {
    "objectID": "analysis/hypotheses.html#sec-RQ12",
    "href": "analysis/hypotheses.html#sec-RQ12",
    "title": "Research questions",
    "section": "",
    "text": "Since the WBG project document corpus data are very incomplete when it comes to sector and theme tagging: is it possible to overcome the insufficient data completion using TOPIC MODELING?",
    "crumbs": [
      "Analysis",
      "Research Plan",
      "Research questions"
    ]
  },
  {
    "objectID": "analysis/hypotheses.html#sec-RQ13",
    "href": "analysis/hypotheses.html#sec-RQ13",
    "title": "Research questions",
    "section": "",
    "text": "Could the WDR 3 ‚Äúexplain‚Äù or at least have a correlation to the appearance-prevalence of said concepts?\nFor the moment, the present study‚Äôs research aim (See Table¬†1) is mainly TO EXPLORE (trends over time in concepts use), and possibly to PREDICT (conjecture about WRD traction effect).",
    "crumbs": [
      "Analysis",
      "Research Plan",
      "Research questions"
    ]
  },
  {
    "objectID": "analysis/hypotheses.html#hyps-1.1",
    "href": "analysis/hypotheses.html#hyps-1.1",
    "title": "Research questions",
    "section": "Hyps 1.1",
    "text": "Hyps 1.1\nThe hypothesis being tested here (Section¬†1.1) is that the WBG project document corpus shows a non-random variation in the incidence of certain policy concepts over time.\nThe launch of a ‚Äúpolicy slogan‚Äù carries intrinsic motivations to shift the PDO in a certain direction.\n\nThis question will be handled in a data-driven way, i.e.¬†starting from the data and not from preconceived ideas‚Ä¶\n\n(i.e.¬†I see that after 2020, the word ‚Äúpandemic‚Äù and ‚Äúvaccine‚Äù peaks within PDOs‚Äô texts, so I will look for a correlation with the COVID-19 pandemic shock, instead of the other way around).",
    "crumbs": [
      "Analysis",
      "Research Plan",
      "Research questions"
    ]
  },
  {
    "objectID": "analysis/hypotheses.html#hyps-1.2",
    "href": "analysis/hypotheses.html#hyps-1.2",
    "title": "Research questions",
    "section": "Hyps 1.2",
    "text": "Hyps 1.2\nThe hypothesis being tested here (Section¬†1.2) is that some ML techniques can help improving the quality of the ‚Äúdocument data collection‚Äù, e.g.¬†the poor and incomplete sector/theme tagging of the WBG project documents.\n\nNote that for this purpose the available dataset (~ 20 FYs worth of project PDOs descriptions) has been splitted into a training + validation + test sets.",
    "crumbs": [
      "Analysis",
      "Research Plan",
      "Research questions"
    ]
  },
  {
    "objectID": "analysis/hypotheses.html#hyps-1.3",
    "href": "analysis/hypotheses.html#hyps-1.3",
    "title": "Research questions",
    "section": "Hyps 1.3",
    "text": "Hyps 1.3\nThe ‚Äúalternative‚Äù hypothesis being tested here (Section¬†1.3) is that the WDR has a ‚Äútraction effect‚Äù on the PDO of the following FYs.",
    "crumbs": [
      "Analysis",
      "Research Plan",
      "Research questions"
    ]
  },
  {
    "objectID": "analysis/hypotheses.html#footnotes",
    "href": "analysis/hypotheses.html#footnotes",
    "title": "Research questions",
    "section": "Footnotes",
    "text": "Footnotes\n\nWBG project document observed in this case are Project Development Objectives (PDO) descriptive short texts.‚Ü©Ô∏é\nconcepts encompasses ‚Äúpolicy focus‚Äù, ‚Äúsector‚Äù, ‚Äústrategy‚Äù or ‚Äúemerging priority‚Äù in the arena of funding for development ‚Ä¶.‚Ü©Ô∏é\nWDRs are the flagship reports of the World Bank group‚Ä¶‚Ü©Ô∏é\nAnnex to the Giuseppe Fioravanti‚Äôs book (Fioravanti 2006)‚Ü©Ô∏é",
    "crumbs": [
      "Analysis",
      "Research Plan",
      "Research questions"
    ]
  },
  {
    "objectID": "analysis/PDO-explor.html",
    "href": "analysis/PDO-explor.html",
    "title": "Motivation",
    "section": "",
    "text": "I have always been fascinated by the idea of analyzing language as data and I finally found some time to study Natural Language Processing (NLP) and Text Analytics techniques.\nIn this project, I explore a dataset of World Bank Projects & Operations, with a focus on the text data contained in the Project Development Objective (PDO) section of the Bank‚Äôs projects. The PDO outlines, in synthetic form, the proposed objectives of operations (loans, grants, technical assistance), as defined in the early stages of the World Bank project cycle. \nNormally, a few objectives are listed in paragraphs that are a couple sentences long. Table¬†1 shows two examples.\n\n\n\nTable¬†1: Illustrative PDOs text in Projects‚Äô documents\n\n\n\n\n\nProject_ID\nProject_Name\nProject_Development_Objective\n\n\n\nP127665\nSecond Economic Recovery Development Policy Loan\nThis development policy loan supports the Government of Croatia's reform efforts with the aim to: (i) enhance fiscal sustainability through expenditure-based consolidation; and (ii) strengthen investment climate.\n\n\nP179010\nTunisia Emergency Food Security Response Project\nTo (a) ensure, in the short-term, the supply of (i) agricultural inputs for farmers to secure the next cropping seasons and for continued dairy production, and (ii) wheat for uninterrupted access to bread and other grain products for poor and vulnerable households; and (b) strengthen Tunisia‚Äôs resilience to food crises by laying the ground for reforms of the grain value chain.\n\n\n\n\n\n\n\n\n\nThe dataset also includes some relevant metadata about the projects, including: country, fiscal year of approval, project status, main sector, main theme, environmental risk category, or lending instrument.\n\n\n\n\n\n\n Nerdy Note\n\n\n\n\n\nI retrieved the data on this page WBG Projects. (I made some attempts to make data ingestion automatic via API calls, but at the moment this is apparently beyond my web scraping ability üò¨).\nSuch data is classified by the World Bank as ‚Äúpublic‚Äù and accessible under a Creative Commons Attribution 4.0 International License.",
    "crumbs": [
      "Analysis",
      "PDO text exploration",
      "Motivation"
    ]
  },
  {
    "objectID": "analysis/PDO-explor.html#peaks-in-sector-related-term-frequency-within-the-pdo-text-data",
    "href": "analysis/PDO-explor.html#peaks-in-sector-related-term-frequency-within-the-pdo-text-data",
    "title": "Motivation",
    "section": "Peaks in sector-related term frequency within the PDO text data",
    "text": "Peaks in sector-related term frequency within the PDO text data\nFor the (broadly defined) HEALTH sector, it is quite clear that Covid-19 is the main driver of the peak in 2020.\nWhat about the other sectors? I was struck by the fact that, observing PDOs over time, the broadly defined ‚Äúsector term‚Äù in the PDO always presents at least one peak and I wonder what could trigger it.\nOne hypothetical explanation is that the PDOs somehow reflect the topics discussed by the World Development Reports (WDR) published annually by the World Bank. The WDR is a flagship publication of the World Bank that provides in-depth analysis of a specific aspect of development.\nIt is important to remark that these publications are not some speculative research endeavor, as they are deeply rooted in the concrete information that the Bank retrieves on the ground from projects and operations as they are supported and evaluated. In turn, the WDRs themselves inform the Bank‚Äôs policy priorities and operational strategies.\nTherefore, it is reasonable to expect some kind of correlation between the topics discussed in the WDRs and the objectives of projects stated in in the PDOs.\n\n\n\n\n\n\nNote\n\n\n\n\n\nThe ‚Äúsector‚Äù term for which frequency is observed in the PDOs is actually an artificial construct. It is based on a logical association of certain words with a sector definition, rooted in the World Bank‚Äôs own classification of sector and sub-sector.\nBelow is how I created a ‚Äúbroad SECTOR‚Äù variable to group the sectors in broader definitions:\n\n\nWAT_SAN = water|wastewater|sanitat|Sewer|sewage|Irrigat|Drainag|river basin|groundwater\n\nTRANSPORT = transport|railway|road|airport|waterway|bus|metropolitan|inter-urban|aviation|highway|transit|bridge|port\n\nURBAN = urban|housing|inter-urban|peri-urban|waste manag|slum|city|megacity|intercity|inter-city|town\n\nENERGY = energ|electri|hydroele|hydropow|renewable|transmis|grid|transmission|electric power|geothermal|solar|wind|thermal|nuclear power|energy generation\n\nHEALTH = health|hospital|medicine|drugs|epidem|pandem|covid-19|vaccin|immuniz|diseas|malaria|HIV|AIDS|TB|maternal|clinic|nutrition\n\nEDUCATION = educat|school|vocat|teach|univers|student|literacy|training|curricul|pedagog\n\nAGR_FOR_FISH (Agriculture, Forestry, Fishing) = Agricultural|Agro|Fish|Forest|Crop|livestock|fishery|land|soil\n\nMINING OIL&GAS = Minin|oil|gas|mineral|quarry|extract|coal|natural gas|mine|petroleum|hydrocarbon\n\nSOCIAL_PROT = Social Protec|social risk|social assistance|living standard|informality|insurance|social choesion|gig economy|human capital|employment|unemploy|productivity|wage lev|intergeneration|lifelong learn|vulnerab|empowerment|sociobehav\n\nFINANCIAL = Bank|finan|Investment|credit|microfinan|loan|financial stability|banking|financial intermed|fintech\n\nICT = Information|Communication|ICT|Internet|telecom|cyber|data|AI|artificial intelligence|blockchain|e-learn|e-commerce|platform|software|hardware|digital\n\nIND_TRADE_SERV = Industry|Trade|Service|manufactur|Tourism|Trade and Services|market|export|import|supply chain|logistic|distribut|e-commerce|retail|wholesale|trade facilitation|trade policy|trade agreement|trade barrier|trade finance|trade promotion|trade integration|trade liberalization|trade balance|trade deficit|trade surplus|trade war|trade dispute|trade negotiation|trade cooperation|trade relation|trade partner|trade route|trade corridor\n\n‚ÄúINSTIT_SUPP‚Äù = Government|Public Admin|Institution|Central Agenc|Sub-national Gov|law|justice|governance|Policy|Regulation|Public Expenditure|Public Investment|Public Procurement\n\n‚ÄúGENDER_EQUAL‚Äù = Gender|Women|Girl|Woman|femal|Gender Equal|gender-base|gender inclus|gender mainstream|gender sensit|gender respons|gender gap|gender-based|gender-sensitive|gender-responsive|gender-transform|gender-equit|gender-balance\n\n‚ÄúCLIMATE‚Äù = Climate|Environment|Sustain|Resilience|Adaptation|Mitigation|Green|Eco|Eco-|carbon|carbon cycle|carbon dioxide|climate change|ecosystem|emission|energy effic|greenhouse|greenhouse gas|temperature anomalies|zero net|green growth|low carbon|climate resilient|climate smart|climate tech|climate variab\n\n\n\n\nThe examples visually presented below validate this hypothesis.",
    "crumbs": [
      "Analysis",
      "PDO text exploration",
      "Motivation"
    ]
  },
  {
    "objectID": "analysis/PDO-explor.html#examples-of-peaks-in-sector-related-term-frequency-within-the-pdo-text-data",
    "href": "analysis/PDO-explor.html#examples-of-peaks-in-sector-related-term-frequency-within-the-pdo-text-data",
    "title": "Motivation",
    "section": "Examples of peaks in sector-related term frequency within the PDO text data",
    "text": "Examples of peaks in sector-related term frequency within the PDO text data\n\n\nFigure¬†2\n\n\n\n\n\nFigure¬†2 shows that ‚Ä¶\n\n\nFigure¬†3\n\n\n\n\n\nFigure¬†3 shows that ‚Ä¶\n\n\nFigure¬†4\n\n\n\n\n\nFigure¬†4 shows that ‚Ä¶\n\n\nFigure¬†5\n\n\n\n\n\nFigure¬†5 shows that ‚Ä¶",
    "crumbs": [
      "Analysis",
      "PDO text exploration",
      "Motivation"
    ]
  },
  {
    "objectID": "analysis/PDO-explor.html#steps-of-prediction",
    "href": "analysis/PDO-explor.html#steps-of-prediction",
    "title": "Motivation",
    "section": "Steps of prediction",
    "text": "Steps of prediction\n\n\nlabel engineering Define what we want to predict (outcome variable, \\(y\\)), and its functional form (binary or multiclass, log form or not if numeric)\n\nDeal with missing value in \\(y\\) (understand if there are systematic reasons for missingness, and if so, how to address them) + Deal with extreme values of \\(y\\) (conservatively is best)\n\n\n\nsample design Select the observations to use .\n\nFor high external validity it will have to be as close as possible to the population of interest (patterns of variables‚Äô distribution etc.)\n\n\n\nfeature engineering Define the input data (predictors, \\(X\\)) and their format (text, numeric, categorical)\n\nDeal with missing values in \\(X\\) (understand, variable by variable, the reasons for missingness, and decide what to do: keep, impute value if numeric, drop the predictor?)\n\nSelect the most relevant predictors (which \\(X\\) to have and in which form). For text predictor data, there are specific NLP transformations that can be applied (e.g.¬†tokenization, lemmatization, etc.)\nIn some cases interaction between predictor variables makes sense.\nAlternative models can be build with less predictors in simpler form to compare with others with more predictors in more complex form‚Ä¶ Here domain knowledge + EDA are key to decide what to include and what to exclude.\n\n\n\nmodel selection It‚Äôs impossible to try all possible models (i.e.¬†all possible choices of \\(X\\) variables to include, their possible functional form, and their possible interactions give too many combinations).\n\n\ncross-validation is similar to training-test method, which basically splits the data into training and test sets, but it does this multiple times (e.g.¬†k-fold cross-validation means \\(k\\) = 10 test sets) and it helps selecting the best model without overfitting.\nHere we do all the work said above (model building and best model selection) in the work set. This will be further divided \\(k-times\\) into \\(k\\) train-test splits, then we use the holdout set to evaluate the prediction itself.\n\n\n\nlast_fit means that, once the best model(s) is/are selected, they are re-run on all of the work set (training data) to evaluate the performance to obtain the final model.\n\npost-prediction diagnostic, lastly, serves to evaluate the model‚Äôs performance on the hold-out sample instead. Here we can\n\nevaluate the fit of the prediction (using, MSE, RMSE, accuracy, ROC etc. to summarize goodness of fit\n\n(for continuous \\(y\\)) we can visualize the prediction interval around the prediction, for discrete \\(y\\) the confusion matrix.\nwe can zoom in on the kinds of observations we care about the most or look at the fit in certain sub-samples of the data (e.g.¬†by sector, by year, etc.)\nfinally we should assess the external validity (hold out set helps but is not representative of all the ‚Äúlive data‚Äù)\n\n\n\n\nLASSO (Least Absolute Shrinkage and Selection Operator) is a sort of ‚Äúadd-on‚Äù to linear regression models which, by adding a penalty system, finds a way to get better predictions from regressions with many predictors, by selecting a subset of the predicting variables that helps to avoid overfitting. The output of the LASSO algorithm is the values of the coefficients of the predictors that are kept in the model. In the formula \\(Œª\\) is the tuning parameter term, which is a parameter that can be tuned to get the best model.",
    "crumbs": [
      "Analysis",
      "PDO text exploration",
      "Motivation"
    ]
  },
  {
    "objectID": "posts/2024-05-29-sviluppo-sostenibile/index.html",
    "href": "posts/2024-05-29-sviluppo-sostenibile/index.html",
    "title": "Sviluppo sostenibile",
    "section": "",
    "text": "¬´Perfezionamento, miglioramento, progresso verso un superiore livello tecnico o metodologico, qualitativo, estetico o stilistico in un determinato ambito pratico o intellettuale. [‚Ä¶] Incremento, potenziamento di un‚ÄôattivitaÃÄ industriale, terziaria, agricola, di una produzione o piuÃÄ genericamente di un sistema economico. [‚Ä¶] Processo di crescita verso la piena maturitaÃÄ psicofisica, mentale, sessuale; irrobustimento del fisico, di una parte del corpo. [‚Ä¶] Processo di crescita fino alla compiuta formazione di un organo o di un organismo animale o vegetale. [‚Ä¶] ¬ª (Garbini 2003. p.¬†80)\n\n\n\n\n\nEntrato nella lingua italiana nel XVIII secolo (forse dall‚ÄôInglese o dal Francese), il termine sviluppo trova la sua origine nel latino tardo (X sec.) faluÃÜppa definito ‚Äúscarti di paglia minutissimi o ramoscelli minuti‚Äù incrociatosi con un derivativo del verbo volvere ‚Äúavviluppare‚Äù e successivamente accresciuto dal prefisso estrattivo-durativo (Garbini 2003. p.¬†80)."
  },
  {
    "objectID": "posts/2024-05-29-sviluppo-sostenibile/index.html#definizione",
    "href": "posts/2024-05-29-sviluppo-sostenibile/index.html#definizione",
    "title": "Sviluppo sostenibile",
    "section": "",
    "text": "¬´Perfezionamento, miglioramento, progresso verso un superiore livello tecnico o metodologico, qualitativo, estetico o stilistico in un determinato ambito pratico o intellettuale. [‚Ä¶] Incremento, potenziamento di un‚ÄôattivitaÃÄ industriale, terziaria, agricola, di una produzione o piuÃÄ genericamente di un sistema economico. [‚Ä¶] Processo di crescita verso la piena maturitaÃÄ psicofisica, mentale, sessuale; irrobustimento del fisico, di una parte del corpo. [‚Ä¶] Processo di crescita fino alla compiuta formazione di un organo o di un organismo animale o vegetale. [‚Ä¶] ¬ª (Garbini 2003. p.¬†80)"
  },
  {
    "objectID": "posts/2024-05-29-sviluppo-sostenibile/index.html#etimologia",
    "href": "posts/2024-05-29-sviluppo-sostenibile/index.html#etimologia",
    "title": "Sviluppo sostenibile",
    "section": "",
    "text": "Entrato nella lingua italiana nel XVIII secolo (forse dall‚ÄôInglese o dal Francese), il termine sviluppo trova la sua origine nel latino tardo (X sec.) faluÃÜppa definito ‚Äúscarti di paglia minutissimi o ramoscelli minuti‚Äù incrociatosi con un derivativo del verbo volvere ‚Äúavviluppare‚Äù e successivamente accresciuto dal prefisso estrattivo-durativo (Garbini 2003. p.¬†80)."
  },
  {
    "objectID": "posts/2024-05-29-sviluppo-sostenibile/index.html#sviluppo-economico",
    "href": "posts/2024-05-29-sviluppo-sostenibile/index.html#sviluppo-economico",
    "title": "Sviluppo sostenibile",
    "section": "SVILUPPO ECONOMICO",
    "text": "SVILUPPO ECONOMICO\n\n¬´La nozione di sviluppo [economico sostenibile], concetto maggiore e di marca Onu di metaÃÄ XX secolo, eÃÄ una parola chiave sulla quale si sono incontrate tutte le vulgate politico-ideologiche dei decenni Cinquanta e Sessanta. Ma eÃÄ stata veramente pensata? (Garbini 2003. p.¬†81)."
  },
  {
    "objectID": "posts/2024-05-29-sviluppo-sostenibile/index.html#sviluppo-economico-sostenibile",
    "href": "posts/2024-05-29-sviluppo-sostenibile/index.html#sviluppo-economico-sostenibile",
    "title": "Sviluppo sostenibile",
    "section": "SVILUPPO (ECONOMICO) SOSTENIBILE",
    "text": "SVILUPPO (ECONOMICO) SOSTENIBILE\n\nDa dove viene\nNel 1972, la conferenza ONU sull‚Äô ‚ÄúAmbiente Umano‚Äù (Stoccolma, 5-16 giugno 1972) √® considerata la prima in cui (almeno i ‚Ä¶. paesi presenti) prendono in considerazione il problema della conservazione dell‚Äôambiente e della gestione delle risrse naturali come questione fondamentale. La conferenza promulga un DICHIARAZIONE con 7 proclami e 26 Principi. Qualche stralcio:\n(non ho piu visto questa definizione) &gt; ‚Äúhuman environment‚Äù\n\nProclama 5: The natural growth of population continuously presents problems for the preservation of the environment, and adequate policies and measures should be adopted, as appropriate, to face this problems. Of all things in teh world, people are the most precious.\n\n\nPronciple 2: The natural resources of the earth, including the air, water, land, flora and fauna and especially representative samples of natural ecosustems, must be safeguarded for the benefit of present and future generations through careful planning or management\n\n(controversa la discussione se la crescita demografica sia un problmea o noma questo finisce nelle raccomandazioni) &gt; Principle 16: Demographic policies which are without prejudcie to basic human rights and which are deemed appropriate by Governments concerned should be applied in those regions where the rate of populatioon growth or excessive population concentration are likely to have adverse effects on the environemnr of the human environment and impede development\n\nRecommendation 12: I. It is recommended that the World Health Organization and other United Nations agencies should provide increased assistance to Governments which so request in the field of family planning programmes without delay. 2. It is further recommended that the World Health Organization should promote and intensify research endeavour in the field of human reproduction, so that the serious consequences of population explosion on human environment can be prevented.\n\n\nConclusions: 48. Many speakers, from both developing and developed countries, agreed that the ruthless pursuit of gross national product, without consideration for other factors, produced conditions of life that were an affront to the dignity of man. The requirements of clean air, water, shelter and health were undeniable needs and rights of man.\n\n\n\n[POPULAZION] Several speakers expressed regret that population problems took so minor a place in the agenda of the Conference. They argued that all strategies for develop- ment and environment would be fatally damaged unless the rate of population increase was reduced. Other speakers said that the population increase was not the problem; the real challenge was the fact that so large a number of the people of the world had such a small expectation for a fruitful, happy and long life. In the opinion of certain delegations there was no incompatibility between population growth and preservation of the environment.\n\n\n\n\nUna definizione vera e propria?\nIl concetto di ‚Äúsviluppo sostenibile‚Äù appare per la prima volta nel 1987, nel cosiddetto rapporto Brundtland del 1987 presentato alla Commissione mondiale per l‚Äôambiente e lo sviluppo denominato anche ‚ÄúOur Common Future‚Äù guidata da Gro Harlem Brundtland. Qui si menzionano:\n\nuno sviluppo che deve avere dei limiti\n\n\nHumanity has the ability to make development sustainable to ensure that it meets the needs of the present without compromising the ability of future generations to meet their own needs.\nThe concept of sustainable development does imply limits - not absolute limits but limitations imposed by the present state of technology and social organization on environmental resources and by the ability of the biosphere to absorb the effects of human activities. (Environment and Development. 1987, 16)\n\n\nuno sviluppo che adotta stili di vita diversi in base alla ricchezza del paese\n\n\nSustainable global development requires that those who are more affluent adopt life-styles within the planet‚Äôs ecological means - in their use of energy, for example. Further, rapidly growing populations can increase the pressure on resources and slow any rise in livingstandards; thus sustainable development can only be pursued if population size and growth are in harmony with the changing productive potential of the ecosystem. (Environment and Development. 1987, 17)\n\n\nuno sviluppo che considera il futuro\n\n\nsustainable development is not a fixed state of harmony, but rather a process of change in which the exploitation of resources, the direction of investments, the orientation of technological development, and institutional change are made consistent with future as well as present needs. (Environment and Development. 1987, 17)\n\n\nuno sviluppo che considera ‚Äúpoggia‚Äù su precisi indirizzi politici\n\n\nWe do not pretend that the process is easy or straightforward. Painful choices have to be made. Thus, in the final analysis, sustainable development must rest on political will. (Environment and Development. 1987, 17)\n\n\nla crescita della popolazione e‚Äô superiore alle risorse disponibili\n\n\nThe issue is not just numbers of people, but how those numbers relate to available resources. Thus the ‚Äòpopulation problem‚Äô must be dealt with in part by efforts to eliminate mass poverty, in order to assure more equitable access to resources, and by education to improve human potential to manage those resources. Urgent steps are needed to limit extreme rates of population growth‚Ä¶ providing people with facilities and education that allow them to choose the size of their families is a way of assuring - especially for women - the basic human right of self-determination. (Environment and Development. 1987, 18)\n\n\nla crescita della popolazione e‚Äô un problema IN CERTI LUOGHI\n\n\nOur human world of 5 billion must make room in a finite environment for another human world. The population could stabilize at between 8 and 14 billion sometime next century, according to UN projections. More than 90 per cent of the increase will occur in the poorest countries, and 90 per cent of that growth in already bursting cities.(Environment and Development. 1987, 13)\n\nSince that time, sustainable development has emerged as a core idea of international development theory and policy. However, some experts have criticized certain features of the concept, including:\n\nIts generality or vagueness, which has led to a great deal of debate over which forms or aspects of development qualify as ‚Äúsustainable‚Äù\nIts lack of quantifiable or objectively measurable goals\nIts assumption of the inevitability and desirability of industrialization and economic development\nIts failure to ultimately prioritize human needs or environmental commitments, either of which may reasonably be considered more important in certain circumstances\n\n\n\nQuali assunti contiene\n\npopolazione\nLo stesso Economist diceva che ESG mette insieme 3 concetti disparati e (potenzialmente) in contraddizione (The Economist 2022)\npossono organi consultivi (o di indirizzo politico) dare direttive tecniche (con tanto di deadline) su temi sui quali non sono d‚Äôaccordo nemmeno gli scienziati specialisti?\nnucleare no ma fotovoltaico si\n\n\n\nQuali principi dovrebbe contenere\n\nLCA ?\nintensita di energia?\n\n\n\nDove arriva\na cose molto concrete dove frasi un po vagheggianti spostano some dollari estremamente sonanti\n\nDirettive\nNel 2001, l‚ÄôUE ha adottato una strategia a favore dello ‚Äúsviluppo sostenibile‚Äù e successivamente ha introdotto il concetto in altri documenti, incluso l‚ÄôArticolo (3) del Trattato sull‚ÄôUnione Europea.\nIn 2015 the United Nations General Assembly adopted the 2030 Agenda for Sustainable Development, which included 17 sweeping goals designed to create a globally equitable society alongside a thriving environment.\n\n\nESG\n\n\nONG l‚Äôanello debole e piu ricattabile\nxche on the receiving end of funding\n\n\n\nIndirizzo politico o diktat tecnico?\nLa catena principi - obiettivi - metodi - tecnologie\nAbbiamo i proncipi? e se si li stiamo perdendo di vista?\n\nLe direttive non sono neutrali\nvedi come devi produrre le auto per"
  },
  {
    "objectID": "posts/2024-05-29-sviluppo-sostenibile/index.html#riferimentirinvii-1",
    "href": "posts/2024-05-29-sviluppo-sostenibile/index.html#riferimentirinvii-1",
    "title": "Sviluppo sostenibile",
    "section": "Riferimenti/rinvii",
    "text": "Riferimenti/rinvii"
  },
  {
    "objectID": "posts/2024-05-29-sviluppo-sostenibile/index.html#riduzionismo",
    "href": "posts/2024-05-29-sviluppo-sostenibile/index.html#riduzionismo",
    "title": "Sviluppo sostenibile",
    "section": "Riduzionismo",
    "text": "Riduzionismo"
  },
  {
    "objectID": "intro_NLP.html",
    "href": "intro_NLP.html",
    "title": "Intro to NLP",
    "section": "",
    "text": "Text Mining (or Text Analytics) is the process of deriving meaningful information from unstructured text data, which involves techniques that can identify patterns, trends, and correlations in text data. As a research method, it is versatile in its aplications and can be combined with different disciplines and techniques.\n\n\nText analytics often utilizes NLP techniques to process and analyze text.\nA subfield of artificial intelligence (AI), NLP focuses on enabling computers to understand, interpret, and generate human language.\n\nWhile NLP provides the tools and algorithms (like tokenization, parsing, and entity recognition), text mining applies these tools to extract specific information from large text corpora.\n\n\n\nCorpus Linguistics is a branch of text analysis research applied to linguistics (e.g.¬†the role of frequency and phonotactics in affix ordering in English, study of idiomatic expressions, geographic spread of neologisms, etc.) or where insight from language is sought\n\nSometimes, Text analytics serves as a method to support other research methods.\n\n\n\n\nPopulation (in language) ~ Any (idealized) compendium of words that we are interested in analysing‚Ä¶ (most) populations are amorphous moving targets.\n\n\nCorpus (pl. Corpora) ~ A language population is called a corpus, a collection of similar documents | objects that typically contain raw strings annotated with additional metadata and details\n\n\nreference corpora, e.g.¬†the American National Corpus\nspecialized corpora\nparrallel and comparable corpora\n\n\n\nUnstructured Data ~ data which does not have a machine-readable internal structure. This is the case for plain text files (.txt), which are simply a sequence of characters (as opposed to structured data that conforms to tabular format and is machine readable)\n\n\n.json format is somwhere in the middle /~ semi-structured data /~ which reflects the autho rpreferences\n\n\n\nString ~ in computational approaches, a string is a specific type of data that represents text and is often encoded in specific format, e.g., Latin1 or UTF8.\n\nTidy text ~ refers both to the structural (physical) and infor- mational (semantic) organization of the dataset. Physically, a tidy dataset is a tabular data structure, where each row is an observation (e.g.¬†token) and each column is a variable that contains measures of a feature or attribute of each observation.\n\n\nToken ~ is a meaningful unit of text, such as a word, that we are interested in using for analysis\n\nTypes ~ refers to the unique tokens in a term variable (&lt; of tokens if repetition)\n\n\n\nBigrams/n-gram ~ Sequential groupings of characters and words (e.g.sentence, or paragraph)\n\nCollocations ~ words that are attracted to each other (and that co-occur or co-locate together), e.g., Merry Christmas, Good Morning, No worries.\n\n\n\nText normalization ~ standardizing text to convert the text into a uniform format and reduce unwanted variation and noise. (e.g.¬†eliminating missing, redundant, or anoma- lous observations, changing the case of the text, removing punctuation, stan- dardizing forms, etc.)\n\nText Tokenization ~ splitting text into tokens, i.e.¬†adapting the text so it reflects the target lin- guistic unit that will be used in the analysis. (involves expanding or reducing the number of rows depending on the linguistic unit of analysis)\n\nEnrichment transformations ~ to add new attributes to the dataset (e.g.¬†generation, recoding, and integration of observations and/or variables.)\n\nStemming ~ is the process of reducing inflected words to their word stem, base, or root form. E.g.: believe --\\&gt; believ\n\nA stem is the base part of a word to which affixes can be attached for derivatives\n\n\n\nLemmatization ~ is the process of reducing inflected words to their dictionary form, or lemma. E.g.: gone\\|going --\\&gt; go\n\n\nDocument-term matrix (DTM) ~ rows = documents | cols = words | cells = [0,1]/frequencies. A sparse matrix describing a collection (i.e., a corpus) of documents with one row for each document and one column for each term. (WDR_com)\n\nTerm-Document matrix (TDM) ~ rows = words | cols = documents | cells = [0,1]/frequencies. A sparse matrix describing a collection (i.e., a corpus) of documents with one row for each document and one column for each term. (WDR_com)\n\nA part of computer science and AI that deals with human languages, Natural Language Processing (NLP) has evolved significantly over the past few decades, driven by advances in computational power, machine learning, and the availability of large datasets.\nBroadly speaking, these were some key steps in its evolution:\n\nEarly Years (1950s - 1980s) - Rule-based Systems (Early NLP systems were based on rule-based methods, which relied on handcrafted rules for tasks like translation, parsing, and information retrieval).\n1970s - 1980s: Statistical Methods and Linguistic Models (The introduction of the Chomskyan Linguistic Models influenced NLP research, focusing on syntax and grammar, while statistical methods began to emerge, laying the groundwork for more data-driven approaches)\n1990s: Statistical NLP (significant shift towards statistical approaches due to the availability of larger text corpora and more powerful computers, Hidden Markov Models (HMMs) and n-grams became popular for tasks such as part-of-speech tagging, speech recognition, and machine translation)\n2000s: Machine Learning and Data-Driven Methods (rise of machine learning in NLP, particularly supervised learning methods: Support Vector Machines (SVMs), Maximum Entropy models, etc. The development of large annotated corpora and platforms fueled progress in areas such as parsing, word sense disambiguation, and sentiment analysis.)\n2010s: Deep Learning Revolution (Neural networks, particularly recurrent neural networks (RNNs) and later long short-term memory (LSTM) networks, became the standard for many NLP tasks. The introduction of word embeddings allowed words to be represented as continuous vectors in a high-dimensional space, capturing semantic relationships between them. Convolutional Neural Networks (CNNs) were applied to text classification and other tasks, although they were more commonly used in computer vision. The development of sequence-to-sequence (Seq2Seq) models enabled advancements in machine translation, summarization, and other sequence generation tasks. Transformers outperformed RNNs on many tasks and led to the development of large-scale pre-trained language models.)\nLate 2010s - Present: Pre-trained Language Models and NLP at Scale (Pre-trained language models like BERT (2018) by Google and GPT (Generative Pre-trained Transformer) by OpenAI revolutionized NLP by providing powerful, general-purpose models that could be fine-tuned for specific tasks with minimal training data. The concept of transfer learning became central, where models trained on massive datasets could be adapted to specific tasks. ChatGPT, BERT, T5, and FLAN-T5 continue to push the boundaries of what NLP can achieve, leading to increasingly sophisticated and human-like interactions.)\n2020s - Future Directions Multimodal models: Integrating NLP with other forms of data, such as images and audio, to create more comprehensive models. Explainability and interpretability: As models grow in complexity, understanding their decision-making processes becomes more important.\n\n\n\n\nNatural Language Processing (NLP) ~ is an interdisciplinary field in computer science that has specialized on processing natural language data using computational and mathematical methods.\n\nNetwork Analysis ~ the most common way to visualize relationships between entities. Networks, also called graphs, consist of nodes (typically represented as dots) and edges (typically represented as lines) and they can be directed or undirected networks.\n\nText Classification ~ a supervised learning method of learning and predicting the category or the class of a document given its text content.\n\nNamed Entity Recognition ~ NER is the task of classifying words or key phrases of a text into predefined entities of interest.\n\nText Summarization ~ a language generation task of summarizing the input text into a shorter paragraph of text.\n\nEntailment ~ the task of classifying the binary relation between two natural-language texts, text and hypothesis, to determine if the text agrees with the hypothesis or not.\n\nQuestion Answering ~ QA is the task of retrieving or generating a valid answer for a given query in natural language, provided with a passage related to the query.\n\nSentence Similarity ~ the process of computing a similarity score given a pair of text documents.\n\nEmbeddings ~ the process of converting a word or a piece of text to a continuous vector space of real number, usually, in low dimension.\n\nSentiment Analysis ~ Provides an example of train and use Aspect Based Sentiment Analysis with Azure ML and Intel NLP Architect.\n\nSemantic Analysis ~ Allows to analyze the semantic (semantics) fo texts. Such analyses often rely on semantic tagsets that are based on word meaning or meaning families/categories.\n\nPart-of-Speech (PoS) ~ Tagging identifies the word classes of words (e.g., noun, adjective, verb, etc.) in a text and adds part-of-speech tags to each word.\n\nTopic Modeling ~ Topic Modeling is a machine learning method seeks to answer the question: given a collection of documents, can we identify what they are about? Topic model algorithms look for patterns of co-occurrences of words in documents.\n\nThis goes beyond my scope, but just to lay out some important elements ust recall\n\nHermeneutics: from Greek \\(·ºëœÅŒºŒ∑ŒΩŒµœÖœÑŒπŒ∫ŒÆ (œÑŒ≠œáŒΩŒ∑)\\) - hermeneutik√® (t√©chne), deriving from the verb \\(·ºëœÅŒºŒ∑ŒΩŒµœç\\) (hermƒìneu≈ç) - is the science and practice of interpretation. Over history, it has been mainly applied to sacred or juridical texts. Two are the main approaches: 1. reconstructing the original intention of the authors 2. adapting the interpretation based on the person who receives the text\nexegesis: from Greek \\(·ºêŒæŒÆŒ≥Œ∑œÉŒπœÇ\\) - ex√©gesis - deriving from the verb \\(·ºêŒæŒ∑Œ≥Œ≠ŒøŒºŒ±Œπ\\) (exeg√©omai, ~ bring out) - indicates ‚Äústudying to explain‚Äù, but referring to the maximum level of depth is seeked (hence it is referred to sacred or normaitive texts for which every nuance can be poignant).\nphilology: from Greek \\(œïŒπŒªŒøŒªŒøŒ≥ŒØŒ±\\) - philologƒ≠a - composed from \\(œÜŒØŒªŒøœÇ\\)- ph√¨los and \\(ŒªœåŒ≥ŒøœÇ\\) - l√≤gos - indicates literally interest/love for the word/reason. The term indicates the study of texts and their history, but changed a little with the Latin philologia, and later embraced the sense of ‚Äòlove of literature‚Äô.\nlinguistics\n\n‚Ä¶\n\n\nAn adage is an ancient saying or maxim, brief and sometimes mysterious, that has become accepted as conventional wisdom. In classical rhetoric, an adage is also known as a rhetorical proverb or paroemia. Often it‚Äôs a type of metaphor. It can express the values of a culture. (Nordquist 2018)\n\nExample(s): ‚ÄúThe early bird gets the worm‚Äù, ‚ÄúBetter late than never.‚Äù\n\n\nThe English word slogan has a Scottish Gaelic origin and derives from the combination of sluagh (army) + gairm (shout), i.e.¬†a ‚Äúbattle cry‚Äù. Nowadays, it signifies a short, memorable and concise phrase used for marketing or political campaigns. Marketing slogans are often called taglines in the United States\n\nExample(s): ‚Äú‚Ä¶‚Äù, ‚Äú‚Ä¶‚Äù\n\n‚Ä¶"
  },
  {
    "objectID": "intro_NLP.html#what-is-text-mining-text-analytics",
    "href": "intro_NLP.html#what-is-text-mining-text-analytics",
    "title": "Intro to NLP",
    "section": "",
    "text": "Text Mining (or Text Analytics) is the process of deriving meaningful information from unstructured text data, which involves techniques that can identify patterns, trends, and correlations in text data. As a research method, it is versatile in its aplications and can be combined with different disciplines and techniques.\n\n\nText analytics often utilizes NLP techniques to process and analyze text.\nA subfield of artificial intelligence (AI), NLP focuses on enabling computers to understand, interpret, and generate human language.\n\nWhile NLP provides the tools and algorithms (like tokenization, parsing, and entity recognition), text mining applies these tools to extract specific information from large text corpora.\n\n\n\nCorpus Linguistics is a branch of text analysis research applied to linguistics (e.g.¬†the role of frequency and phonotactics in affix ordering in English, study of idiomatic expressions, geographic spread of neologisms, etc.) or where insight from language is sought\n\nSometimes, Text analytics serves as a method to support other research methods.\n\n\n\n\nPopulation (in language) ~ Any (idealized) compendium of words that we are interested in analysing‚Ä¶ (most) populations are amorphous moving targets.\n\n\nCorpus (pl. Corpora) ~ A language population is called a corpus, a collection of similar documents | objects that typically contain raw strings annotated with additional metadata and details\n\n\nreference corpora, e.g.¬†the American National Corpus\nspecialized corpora\nparrallel and comparable corpora\n\n\n\nUnstructured Data ~ data which does not have a machine-readable internal structure. This is the case for plain text files (.txt), which are simply a sequence of characters (as opposed to structured data that conforms to tabular format and is machine readable)\n\n\n.json format is somwhere in the middle /~ semi-structured data /~ which reflects the autho rpreferences\n\n\n\nString ~ in computational approaches, a string is a specific type of data that represents text and is often encoded in specific format, e.g., Latin1 or UTF8.\n\nTidy text ~ refers both to the structural (physical) and infor- mational (semantic) organization of the dataset. Physically, a tidy dataset is a tabular data structure, where each row is an observation (e.g.¬†token) and each column is a variable that contains measures of a feature or attribute of each observation.\n\n\nToken ~ is a meaningful unit of text, such as a word, that we are interested in using for analysis\n\nTypes ~ refers to the unique tokens in a term variable (&lt; of tokens if repetition)\n\n\n\nBigrams/n-gram ~ Sequential groupings of characters and words (e.g.sentence, or paragraph)\n\nCollocations ~ words that are attracted to each other (and that co-occur or co-locate together), e.g., Merry Christmas, Good Morning, No worries.\n\n\n\nText normalization ~ standardizing text to convert the text into a uniform format and reduce unwanted variation and noise. (e.g.¬†eliminating missing, redundant, or anoma- lous observations, changing the case of the text, removing punctuation, stan- dardizing forms, etc.)\n\nText Tokenization ~ splitting text into tokens, i.e.¬†adapting the text so it reflects the target lin- guistic unit that will be used in the analysis. (involves expanding or reducing the number of rows depending on the linguistic unit of analysis)\n\nEnrichment transformations ~ to add new attributes to the dataset (e.g.¬†generation, recoding, and integration of observations and/or variables.)\n\nStemming ~ is the process of reducing inflected words to their word stem, base, or root form. E.g.: believe --\\&gt; believ\n\nA stem is the base part of a word to which affixes can be attached for derivatives\n\n\n\nLemmatization ~ is the process of reducing inflected words to their dictionary form, or lemma. E.g.: gone\\|going --\\&gt; go\n\n\nDocument-term matrix (DTM) ~ rows = documents | cols = words | cells = [0,1]/frequencies. A sparse matrix describing a collection (i.e., a corpus) of documents with one row for each document and one column for each term. (WDR_com)\n\nTerm-Document matrix (TDM) ~ rows = words | cols = documents | cells = [0,1]/frequencies. A sparse matrix describing a collection (i.e., a corpus) of documents with one row for each document and one column for each term. (WDR_com)"
  },
  {
    "objectID": "intro_NLP.html#what-is-natural-language-processing-nlp",
    "href": "intro_NLP.html#what-is-natural-language-processing-nlp",
    "title": "Intro to NLP",
    "section": "",
    "text": "A part of computer science and AI that deals with human languages, Natural Language Processing (NLP) has evolved significantly over the past few decades, driven by advances in computational power, machine learning, and the availability of large datasets.\nBroadly speaking, these were some key steps in its evolution:\n\nEarly Years (1950s - 1980s) - Rule-based Systems (Early NLP systems were based on rule-based methods, which relied on handcrafted rules for tasks like translation, parsing, and information retrieval).\n1970s - 1980s: Statistical Methods and Linguistic Models (The introduction of the Chomskyan Linguistic Models influenced NLP research, focusing on syntax and grammar, while statistical methods began to emerge, laying the groundwork for more data-driven approaches)\n1990s: Statistical NLP (significant shift towards statistical approaches due to the availability of larger text corpora and more powerful computers, Hidden Markov Models (HMMs) and n-grams became popular for tasks such as part-of-speech tagging, speech recognition, and machine translation)\n2000s: Machine Learning and Data-Driven Methods (rise of machine learning in NLP, particularly supervised learning methods: Support Vector Machines (SVMs), Maximum Entropy models, etc. The development of large annotated corpora and platforms fueled progress in areas such as parsing, word sense disambiguation, and sentiment analysis.)\n2010s: Deep Learning Revolution (Neural networks, particularly recurrent neural networks (RNNs) and later long short-term memory (LSTM) networks, became the standard for many NLP tasks. The introduction of word embeddings allowed words to be represented as continuous vectors in a high-dimensional space, capturing semantic relationships between them. Convolutional Neural Networks (CNNs) were applied to text classification and other tasks, although they were more commonly used in computer vision. The development of sequence-to-sequence (Seq2Seq) models enabled advancements in machine translation, summarization, and other sequence generation tasks. Transformers outperformed RNNs on many tasks and led to the development of large-scale pre-trained language models.)\nLate 2010s - Present: Pre-trained Language Models and NLP at Scale (Pre-trained language models like BERT (2018) by Google and GPT (Generative Pre-trained Transformer) by OpenAI revolutionized NLP by providing powerful, general-purpose models that could be fine-tuned for specific tasks with minimal training data. The concept of transfer learning became central, where models trained on massive datasets could be adapted to specific tasks. ChatGPT, BERT, T5, and FLAN-T5 continue to push the boundaries of what NLP can achieve, leading to increasingly sophisticated and human-like interactions.)\n2020s - Future Directions Multimodal models: Integrating NLP with other forms of data, such as images and audio, to create more comprehensive models. Explainability and interpretability: As models grow in complexity, understanding their decision-making processes becomes more important.\n\n\n\n\nNatural Language Processing (NLP) ~ is an interdisciplinary field in computer science that has specialized on processing natural language data using computational and mathematical methods.\n\nNetwork Analysis ~ the most common way to visualize relationships between entities. Networks, also called graphs, consist of nodes (typically represented as dots) and edges (typically represented as lines) and they can be directed or undirected networks.\n\nText Classification ~ a supervised learning method of learning and predicting the category or the class of a document given its text content.\n\nNamed Entity Recognition ~ NER is the task of classifying words or key phrases of a text into predefined entities of interest.\n\nText Summarization ~ a language generation task of summarizing the input text into a shorter paragraph of text.\n\nEntailment ~ the task of classifying the binary relation between two natural-language texts, text and hypothesis, to determine if the text agrees with the hypothesis or not.\n\nQuestion Answering ~ QA is the task of retrieving or generating a valid answer for a given query in natural language, provided with a passage related to the query.\n\nSentence Similarity ~ the process of computing a similarity score given a pair of text documents.\n\nEmbeddings ~ the process of converting a word or a piece of text to a continuous vector space of real number, usually, in low dimension.\n\nSentiment Analysis ~ Provides an example of train and use Aspect Based Sentiment Analysis with Azure ML and Intel NLP Architect.\n\nSemantic Analysis ~ Allows to analyze the semantic (semantics) fo texts. Such analyses often rely on semantic tagsets that are based on word meaning or meaning families/categories.\n\nPart-of-Speech (PoS) ~ Tagging identifies the word classes of words (e.g., noun, adjective, verb, etc.) in a text and adds part-of-speech tags to each word.\n\nTopic Modeling ~ Topic Modeling is a machine learning method seeks to answer the question: given a collection of documents, can we identify what they are about? Topic model algorithms look for patterns of co-occurrences of words in documents."
  },
  {
    "objectID": "intro_NLP.html#methodological-notes",
    "href": "intro_NLP.html#methodological-notes",
    "title": "Intro to NLP",
    "section": "",
    "text": "This goes beyond my scope, but just to lay out some important elements ust recall\n\nHermeneutics: from Greek \\(·ºëœÅŒºŒ∑ŒΩŒµœÖœÑŒπŒ∫ŒÆ (œÑŒ≠œáŒΩŒ∑)\\) - hermeneutik√® (t√©chne), deriving from the verb \\(·ºëœÅŒºŒ∑ŒΩŒµœç\\) (hermƒìneu≈ç) - is the science and practice of interpretation. Over history, it has been mainly applied to sacred or juridical texts. Two are the main approaches: 1. reconstructing the original intention of the authors 2. adapting the interpretation based on the person who receives the text\nexegesis: from Greek \\(·ºêŒæŒÆŒ≥Œ∑œÉŒπœÇ\\) - ex√©gesis - deriving from the verb \\(·ºêŒæŒ∑Œ≥Œ≠ŒøŒºŒ±Œπ\\) (exeg√©omai, ~ bring out) - indicates ‚Äústudying to explain‚Äù, but referring to the maximum level of depth is seeked (hence it is referred to sacred or normaitive texts for which every nuance can be poignant).\nphilology: from Greek \\(œïŒπŒªŒøŒªŒøŒ≥ŒØŒ±\\) - philologƒ≠a - composed from \\(œÜŒØŒªŒøœÇ\\)- ph√¨los and \\(ŒªœåŒ≥ŒøœÇ\\) - l√≤gos - indicates literally interest/love for the word/reason. The term indicates the study of texts and their history, but changed a little with the Latin philologia, and later embraced the sense of ‚Äòlove of literature‚Äô.\nlinguistics\n\n‚Ä¶"
  },
  {
    "objectID": "intro_NLP.html#essential-list-of-rhetorical-devices",
    "href": "intro_NLP.html#essential-list-of-rhetorical-devices",
    "title": "Intro to NLP",
    "section": "",
    "text": "An adage is an ancient saying or maxim, brief and sometimes mysterious, that has become accepted as conventional wisdom. In classical rhetoric, an adage is also known as a rhetorical proverb or paroemia. Often it‚Äôs a type of metaphor. It can express the values of a culture. (Nordquist 2018)\n\nExample(s): ‚ÄúThe early bird gets the worm‚Äù, ‚ÄúBetter late than never.‚Äù\n\n\nThe English word slogan has a Scottish Gaelic origin and derives from the combination of sluagh (army) + gairm (shout), i.e.¬†a ‚Äúbattle cry‚Äù. Nowadays, it signifies a short, memorable and concise phrase used for marketing or political campaigns. Marketing slogans are often called taglines in the United States\n\nExample(s): ‚Äú‚Ä¶‚Äù, ‚Äú‚Ä¶‚Äù\n\n‚Ä¶"
  },
  {
    "objectID": "posts/2023-11-15-revisione-libro-il-sistema-invisibile/index.html",
    "href": "posts/2023-11-15-revisione-libro-il-sistema-invisibile/index.html",
    "title": "Spunti dal libro: Il Sistema invisibile, di Marcello Foa del 15 Novembre 2023",
    "section": "",
    "text": "Da ‚Äú4.4. Come disarticolare una societ√†‚Äù\n\n\nVladimis Volkoff (Il montaggio) ‚ÄúQuando si tratta di mobilitare le masse in realta non si ha che uno scopo: immobilizzarle‚Äù (‚Ä¶) tante volte la nostra rabbia √® stata indirizzata verso bersagli di facile presa, il politico corrotto che abusa dell‚Äôauto blu, mentre i veri sprechi e gli scandali rilevanti erano altri, come nel caso di Autostrade Italiane. Tante, troppe volte l‚Äôindignazione delle masse √® stata indirizzata contro bersagli fittizzi o sproporzionati, che non hanno prodotto altro effetto che generare sfiducia sociale verso le autorita costituite, i partiti e le istituzioni (p.¬†104, 105)\n\n\n\n\nCitazioneBibTeX@online{m. mimmi2023,\n  author = {M. Mimmi, Luisa and M. Mimmi, Luisa},\n  title = {Spunti dal libro: Il Sistema invisibile, di Marcello Foa del\n    15 Novembre 2023},\n  date = {2023-11-15},\n  url = {https://lulliter.github.io/slogan/posts/2023-11-15-revisione-libro-il-sistema-invisibile},\n  langid = {it}\n}\nPer favore citare questo lavoro come:\nM. Mimmi, Luisa, and Luisa M. Mimmi. 2023. ‚ÄúSpunti dal libro: Il\nSistema invisibile, di Marcello Foa del 15 Novembre 2023.‚Äù\nNovember 15, 2023. https://lulliter.github.io/slogan/posts/2023-11-15-revisione-libro-il-sistema-invisibile."
  },
  {
    "objectID": "analysis/01b_WB_project_pdo_EDA.html",
    "href": "analysis/01b_WB_project_pdo_EDA.html",
    "title": "WB Project PDO text EDA",
    "section": "",
    "text": "Work in progress"
  },
  {
    "objectID": "analysis/01b_WB_project_pdo_EDA.html#tbl-illustrative-pdos-text-in-projects-documents",
    "href": "analysis/01b_WB_project_pdo_EDA.html#tbl-illustrative-pdos-text-in-projects-documents",
    "title": "WB Project PDO text EDA",
    "section": "[TBL] Illustrative PDOs text in Projects‚Äô documents",
    "text": "[TBL] Illustrative PDOs text in Projects‚Äô documents\n\n\n\n\n\nProject_ID\nProject_Name\nProject_Development_Objective\n\n\n\nP127665\nSecond Economic Recovery Development Policy Loan\nThis development policy loan supports the Government of Croatia's reform efforts with the aim to: (i) enhance fiscal sustainability through expenditure-based consolidation; and (ii) strengthen investment climate.\n\n\nP069934\nPERNAMBUCO INTEGRATED DEVELOPMENT: EDUCATION QUALITY IMPROVEMENT PROJECT\nThe development objectives of the Pernambuco Integrated Development: Education Quality Improvement Project are to (a) improve the quality, efficiency, and inclusiveness of the public education system; (b) modernize and strengthen the managerial, financial, and administrative capacity of the Secretariat of Education to set policies and guidelines for the sector and deliver public education efficiently; and (c) support the overall state modernization effort through interventions to be carried out in the Secretariat of Education and to be replicated in other state institutions."
  },
  {
    "objectID": "analysis/01b_WB_project_pdo_EDA.html#saved-file-projs_train_t-pdo_train_t",
    "href": "analysis/01b_WB_project_pdo_EDA.html#saved-file-projs_train_t-pdo_train_t",
    "title": "WB Project PDO text EDA",
    "section": "[Saved file projs_train_t & pdo_train_t]",
    "text": "[Saved file projs_train_t & pdo_train_t]\n\n# Load Proj train dataset `projs_train_t`\nprojs_train &lt;- readRDS(\"~/Github/slogan/data/derived_data/projs_train.rds\")  \n\n# Load clean tokenized-PDO dataset `pdo_train_t`\npdo_train_t &lt;- readRDS(here::here(\"data\" , \"derived_data\", \"pdo_train_t.rds\"))"
  },
  {
    "objectID": "analysis/01b_WB_project_pdo_EDA.html#previous-tokenization-and-pos-tagging",
    "href": "analysis/01b_WB_project_pdo_EDA.html#previous-tokenization-and-pos-tagging",
    "title": "WB Project PDO text EDA",
    "section": "Previous Tokenization and PoS Tagging",
    "text": "Previous Tokenization and PoS Tagging\nTypically, one of the first steps in this transformation from natural language to feature, or any of kind of text analysis, is tokenization.\ni) Explain Tokenization\nBreaking units of language into components relevant for the research question is called ‚Äútokenization‚Äù. Components can be words, n-grams, sentences, etc. or combining smaller units into larger units.\n\nTokenization is a row-wise operation: it changes the number of rows in the dataset.\nThe choices of tokenization\n\nShould words be lower cased?\nShould punctuation be removed?\n\nShould numbers be replaced by some placeholder?\nShould words be stemmed (also called lemmatization)? ‚òëÔ∏è\nShould bigrams/multi-word phrase be used instead of single word phrases? ‚òëÔ∏è\nShould stopwords (the most common words) be removed? ‚òëÔ∏è\nShould rare words be removed? ‚ùå\nShould hyphenated words be split into two words? ‚ùå\n\n\nfor the moment I keep all as conservatively as possible\n\nii) Explain Pos Tagging\nLinguistic annotation is a common for of enriching text data, i.e.¬†adding information about the text that is not directly present in the text itself.\nUpon this, e.g.¬†classifying noun, verb, adjective, etc., one can discover intent or action in a sentence, or scanning ‚Äúverb-noun‚Äù patterns.\nHere I have a training dataset file with:\n\n\n\n\n\nVariable\nType\nProvenance\nDescription\nExample\n\n\n\nproj_id\nchr\noriginal PDO data\n\n\n\n\npdo\nchr\noriginal PDO data\n\n\n\n\nword\nchr\noriginal PDO data\n\nGovernments\n\n\nsid\nint\noutput cleanNLP\nsentence ID\n\n\n\ntid\nchr\noutput cleanNLP\ntoken ID within sentence\n\n\n\ntoken\nchr\noutput cleanNLP\nTokenized form of the token.\ngovernment\n\n\ntoken_with_ws\nchr\noutput cleanNLP\nToken with trailing whitespace\ngovernment\n\n\nlemma\nchr\noutput cleanNLP\nThe base form of the token\ngovernment\n\n\nstem\nchr\noutput SnowballC\nThe base form of the token\ngovern\n\n\nupos\nchr\noutput cleanNLP\nUniversal part-of-speech tag (e.g., NOUN, VERB, ADJ).\n\n\n\nxpos\nchr\noutput cleanNLP\nLanguage-specific part-of-speech tags.\n\n\n\nfeats\nchr\noutput cleanNLP\nMorphological features of the token\n\n\n\ntid_source\nchr\noutput cleanNLP\nToken ID in the source document\n\n\n\nrelation\nchr\noutput cleanNLP\nDependency relation between the token and its head token\n\n\n\npr_name\nchr\noutput cleanNLP\nName of the parent token\n\n\n\nFY_appr\ndbl\noriginal PDO data\n\n\n\n\nFY_clos\ndbl\noriginal PDO data\n\n\n\n\nstatus\nchr\noriginal PDO data\n\n\n\n\nregionname\nchr\noriginal PDO data\n\n\n\n\ncountryname\nchr\noriginal PDO data\n\n\n\n\nsector1\nchr\noriginal PDO data\n\n\n\n\ntheme1\nchr\noriginal PDO data\n\n\n\n\nlendinginstr\nchr\noriginal PDO data\n\n\n\n\nenv_cat\nchr\noriginal PDO data\n\n\n\n\nESrisk\nchr\noriginal PDO data\n\n\n\n\ncurr_total_commitment\ndbl\noriginal PDO data\n\n\n\n\n\n\n\n\n‚Äî PoS Tagging: upos (Universal Part-of-Speech)\n\n\n\n\n\nupos\nn\npercent\nexplan\n\n\n\nADJ\n21261\n0.0852623\nAdjective\n\n\nADP\n27050\n0.1084777\nAdposition\n\n\nADV\n2950\n0.0118303\nAdverb\n\n\nAUX\n3588\n0.0143888\nAuxiliary\n\n\nCCONJ\n14236\n0.0570902\nCoordinating conjunction\n\n\nDET\n21505\n0.0862408\nDeterminer\n\n\nINTJ\n57\n0.0002286\nInterjection\n\n\nNOUN\n70752\n0.2837344\nNoun\n\n\nNUM\n2190\n0.0087825\nNumeral\n\n\nPART\n8691\n0.0348532\nParticle\n\n\nPRON\n2330\n0.0093439\nPronoun\n\n\nPROPN\n14856\n0.0595765\nProper noun\n\n\nPUNCT\n28393\n0.1138635\nPunctuation\n\n\nSCONJ\n2160\n0.0086622\nSubordinating conjunction\n\n\nSYM\n316\n0.0012672\nSymbol\n\n\nVERB\n25806\n0.1034889\nVerb\n\n\nX\n3219\n0.0129090\nOther\n\n\n\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\n\n\nOn random visual check, these are not always correct, but they are a good starting point for now.\n\n\n\niii) Custom Stopwords\nRemove stop words, which are the most common words in a language.\n\nbut I don‚Äôt want to remove any meaningful word for now\n\n\n# Custom list of articles, prepositions, and pronouns\ncustom_stop_words &lt;- c(\n   # Articles\n   \"the\", \"a\", \"an\",   \n   \"and\", \"but\", \"or\", \"yet\", \"so\", \"for\", \"nor\", \"as\", \"at\", \"by\", \"per\",  \n   # Prepositions\n   \"of\", \"in\", \"on\", \"at\", \"by\", \"with\", \"about\", \"against\", \"between\", \"into\", \"through\", \n   \"during\", \"before\", \"after\", \"above\", \"below\", \"to\", \"from\", \"up\", \"down\", \"under\",\n   \"over\", \"again\", \"further\", \"then\", \"once\",  \n   # Pronouns\n   \"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \"you\", \"your\",\n   \"yours\", \"yourself\", \"yourselves\", \"he\", \"him\", \"his\", \"himself\", \"she\", \"her\", \n   \"hers\", \"herself\", \"it\", \"its\", \"itself\", \"they\", \"them\", \"their\", \"theirs\", \"themselves\" ,\n   \"this\", \"that\", \"these\", \"those\", \"which\", \"who\", \"whom\", \"whose\", \"what\", \"where\",\n   \"when\", \"why\", \"how\", \"all\", \"any\", \"both\", \"each\", \"few\", \"more\", \"most\", \"other\",\n   # \"some\", \"such\", \"no\",  \"not\", \n   # \"too\", \"very\",   \n   # verbs\n   \"is\", \"are\", \"would\", \"could\", \"will\", \"be\", \"e.g\", \"e.g.\", \"i.e.\",\n   \"i\", \"ii\", \"iii\", \"iv\", \"v\",\n   # because tautology\n   \"pdo\"\n)\n\n# Convert to a data frame if needed for consistency with tidytext\ncustom_stop_words_df &lt;- tibble(word = custom_stop_words)\n\n\nsaveRDS(custom_stop_words, here(\"data\" , \"derived_data\", \"custom_stop_words.rds\"))\nsaveRDS(custom_stop_words_df, here(\"data\" , \"derived_data\", \"custom_stop_words_df.rds\"))\n\niv) Stemming\nOften documents contain different versions of one base word, often called a stem. Stemming is the process of reducing words to their base or root form.\nSnowball is one framework released in 1980 with an open-source license that can be found in R package SnowballC.\n\n# Using `SnowballC::wordStem` to stem the words. e.g.\npdo_train_t &lt;- pdo_train_t %&gt;% \n   mutate(stem = SnowballC::wordStem(token_l)) %&gt;%\n   relocate(stem, .after = lemma)\n\nWhy Stemming?: For example, in topic modeling, stemming reduces noise by making it easier for the model to identify core topics without being distracted by grammatical variations. (Lemmatization is more computationally intensive as it requires linguistic context and dictionaries, making it slower, especially on large datasets)\n\n\nToken\nLemma\nStem\n\n\n\ndevelopment\ndevelopment\ndevelop\n\n\nquality\nquality\nqualiti\n\n\nhigh-quality\nhigh-quality\nhigh-qual\n\n\ninclude\ninclude\ninclud\n\n\nlogistics\nlogistic\nlogist\n\n\ngovernment/governance\nGovernemnt/government/governance\ngovern\n\n\n\n\nNOTE: Among word / stems encountered in PDOs, there are a lot of acronyms which may refer to World Bank lingo, or local agencies, etc‚Ä¶ Especially when looked at in low case form they don‚Äôt make much sense‚Ä¶\n\nNotes on sparsity\nSparsity in the context of a document-term matrix refers to the proportion of cells in the matrix that contain zeros. High sparsity means that most terms do not appear in most documents.\n\nremoving stopwords before stemming can reduce sparsity\n\ntidytext::cast_tdm turns a ‚Äútidy‚Äù one-term-per-document-per-row data frame into a Document-Term Matrix (DTM) from the tm package.\n\nthis dataset contains 4403 documents (each of them a PDO) and 11029 terms (distinct words). Notice that this DTM is 100% sparse (100% of document-word pairings are zero, bc most pairings of document and term do not occur (they have the value zero).\n\n\n\n\n# create document-word matrix\nDTM &lt;- pdo_train_t %&gt;% \n   anti_join(custom_stop_words_df, by = c(\"token_l\" = \"word\")) %&gt;% \n   count(proj_id, token_l) %&gt;%\n   tidytext::cast_dtm(proj_id, token_l, n) # HIGH!!!\n\nDTM\n# &lt;&lt;DocumentTermMatrix (documents: 4403, terms: 11029)&gt;&gt;\n# Non-/sparse entries: 129940/48430747\n# Sparsity           : 100%\n# Maximal term length: 34\n# Weighting          : term frequency (tf)\n\nv) Document-term matrix or TF-IDF\n\nThe tf-idf is the product of the term frequency and the inverse document frequency::\n\n\\[\n\\begin{aligned}\ntf(\\text{term}) &= \\frac{n_{\\text{term}}}{n_{\\text{terms in document}}} \\\\\nidf(\\text{term}) &= \\ln{\\left(\\frac{n_{\\text{documents}}}{n_{\\text{documents containing term}}}\\right)} \\\\\ntf\\text{-}idf(\\text{term}) &= tf(\\text{term}) \\times idf(\\text{term})\n\\end{aligned}\n\\]\n‚Äî TF-IDF matrix on train pdo\n\n# reduce size \n\npdo_train_4_tf_idf &lt;- pdo_train_t %&gt;% # 255964\n   # Keep only content words [very restrictive for now]\n   # normally c(\"NOUN\", \"VERB\", \"ADJ\", \"ADV\")\n   filter(upos %in% c(\"NOUN\")) %&gt;% #    72,668 \n   filter(!token_l %in% c(\"development\", \"objective\", \"project\")) %&gt;%   #  66,741\n   # get rid of stop words (from default list)   \n   filter(!token_l %in% custom_stop_words_df$word) %&gt;%   #  66,704\n   # Optional: Remove lemmas of length 1 or shorter\n   filter(nchar(lemma) &gt; 1)  #  66,350\n\nNow, count the occurrences of each lemma for each document. (This is the term frequency or tf)\n\n# This is the term frequency or `tf`\n\n# Count lemmas per document\nlemma_counts &lt;- pdo_train_4_tf_idf %&gt;%\n  count(proj_id, lemma, sort = TRUE)\n# Preview the result\nhead(lemma_counts) \n\nWith the lemma counts prepared, the bind_tf_idf() function from the tidytext package computes the TF-IDF scores.\n\n# Compute the TF-IDF scores\nlemma_tf_idf &lt;- lemma_counts %&gt;%\n  bind_tf_idf(lemma, proj_id, n) %&gt;%\n  arrange(desc(tf_idf))\n\nhead(lemma_tf_idf)\n\n\n\n\n\n\n\nNote\n\n\n\n\n\nWhat to use: token, lemma, or stem?\nGeneral Preference in Real-World NLP:\n\n\nTokens for analyses where word forms matter or for sentiment analysis.\n\nLemmas (*) for most general-purpose NLP tasks where you want to reduce dimensionality while maintaining accuracy and clarity of meaning.\n\nStems for very large datasets, search engines, and applications where speed and simplicity are more important than linguistic precision.\n\n(*) I use lemma, after ‚Äúaggressively‚Äù reducing the number of words to consider, and removing stop words (at least for now)."
  },
  {
    "objectID": "analysis/01b_WB_project_pdo_EDA.html#term-frequency",
    "href": "analysis/01b_WB_project_pdo_EDA.html#term-frequency",
    "title": "WB Project PDO text EDA",
    "section": "Term frequency",
    "text": "Term frequency\nNote: normally, the most frequent words are function words (e.g.¬†determiners, prepositions, pronouns, and auxiliary verbs), which are not very informative. Moreover, even content words (e.g.¬†nouns, verbs, adjectives, and adverbs) can often be quite generic semantically speaking (e.g.¬†‚Äúgood‚Äù may be used for many different things).\nHowever, in this analysis, I do not use the STOPWORD approach, but use the POS tags to reduce ‚Äì in a more controlled way ‚Äì the dataset, filtering the content words such as nouns, verbs, adjectives, and adverbs.\n[FUNC] save plots\n[FIG] Overall token freq ggplot\n\nExcluding ‚Äúproject‚Äù ‚Äúdevelop‚Äù,‚Äúobjective‚Äù\n\nIncluding only ‚Äúcontent words‚Äù (NOUN, VERB, ADJ, ADV)\n\n\n# Evaluate the title with glue first\ntitle_text &lt;- glue::glue(\"Most frequent TOKEN in {n_distinct(pdo_train_t$proj_id)} PDOs from projects approved between FY {min(pdo_train_t$FY_appr)}-{max(pdo_train_t$FY_appr)}\") \n\npdo_wrd_freq &lt;- pdo_train_t %&gt;%   # 123,927\n   # include only content words\n   filter(upos %in% c(\"NOUN\", \"VERB\", \"ADJ\", \"ADV\")) %&gt;%\n   #filter (!(upos %in% c(\"AUX\",\"CCONJ\", \"INTJ\", \"DET\", \"PART\",\"ADP\", \"SCONJ\", \"SYM\", \"PART\", \"PUNCT\"))) %&gt;%\n   filter (!(relation %in% c(\"nummod\" ))) %&gt;% # 173,686 \n filter (!(token_l %in% c(\"pdo\",\"project\", \"development\", \"objective\",\"objectives\", \"i\", \"ii\", \"iii\",\n                          \"is\"))) %&gt;% # whne it is VERB\n   count(token_l) %&gt;% \n   filter(n &gt; 800) %&gt;% \n   mutate(token_l = reorder(token_l, n))   # reorder values by frequency\n\n# plot \npdo_wrd_freq_p &lt;- pdo_wrd_freq %&gt;% \n   ggplot(aes(token_l, n)) +\n   geom_col(fill = \"#d7b77b\") +\n   scale_y_continuous(breaks = seq(0, max(pdo_wrd_freq$n), by = 400)) + # directly use 'n' instead of .data$n\n   coord_flip() + # flip x and y coordinates so we can read the words better\n   labs(#title = title_text,\n      subtitle = \"[TOKEN with count &gt; 800]\", y = \"\", x = \"\")+\n   geom_hline(yintercept = 800, linetype = \"dashed\", color = \"#873c4a\") +\n     my_pretty_theme\n\n[FIG] Overall stem freq ggplot\n\nWithout ‚Äúproject‚Äù ‚Äúdevelop‚Äù,‚Äúobjective‚Äù\n\nIncluding only ‚Äúcontent words‚Äù (NOUN, VERB, ADJ, ADV)\n\n\n# Evaluate the title with glue first\ntitle_text &lt;- glue::glue(\"Most frequent STEM in {n_distinct(pdo_train_t$proj_id)} PDOs from projects approved between FY {min(pdo_train_t$FY_appr)}-{max(pdo_train_t$FY_appr)}\") \n# Plot\npdo_stem_freq &lt;- pdo_train_t %&gt;%   # 256,632\n   # include only content words\n   filter(upos %in% c(\"NOUN\", \"VERB\", \"ADJ\", \"ADV\")) %&gt;%\n   filter (!(relation %in% c(\"nummod\" ))) %&gt;% # 173,686 \n   filter (!(stem %in% c(\"pdo\", \"project\", \"develop\", \"object\", \"i\", \"ii\", \"iii\"))) %&gt;%\n   count(stem) %&gt;% \n   filter(n &gt; 800) %&gt;%\n   mutate(stem = reorder(stem, n))    # reorder values by frequency\n   \n   # plot \npdo_stem_freq_p &lt;-    pdo_stem_freq %&gt;% \n      ggplot(aes(stem, n)) +\n      geom_col(fill = \"#d7b77b\") +\n      scale_y_continuous(breaks = seq(0, max(pdo_stem_freq$n), by = 400)) + # directly use 'n' instead of .data$n\n      coord_flip() + # flip x and y coordinates so we can read the words better\n      labs(#title = title_text,\n         subtitle = \"[STEM with count &gt; 800]\", y = \"\", x = \"\") +\n      geom_hline(yintercept = 800, linetype = \"dashed\", color = \"#873c4a\") +\n     my_pretty_theme\n\n\nEvidently, after stemming, more words (or stems) reach the threshold frequency count of 800 (they have been combined by root).\n\n[FIG] token + stem freq ggplot\n\ntitle2_text &lt;- glue::glue(\"Most frequent TOKEN & STEM in {n_distinct(pdo_train_t$proj_id)} PDOs\") \nsubtitle2_text &lt;- glue::glue(\"From projects approved between FY {min(pdo_train_t$FY_appr)}-{max(pdo_train_t$FY_appr)}\") \n\ncombo_freq &lt;-  pdo_wrd_freq_p + pdo_stem_freq_p + \n   plot_annotation(title = title2_text,\n                    subtitle = subtitle2_text,\n                   # caption = \"Source: World Bank Project Documents\",\n                   theme = theme(plot.title = element_text(size = 12, face = \"bold\"),\n                                 plot.subtitle = element_text(size = 10, face = \"italic\"),\n                                 plot.caption = element_text(size = 10, face = \"italic\"))\n                   )\ncombo_freq\n\n\n\n\n\n\n\n\nf_save_plot(\"combo_freq\", combo_freq)"
  },
  {
    "objectID": "analysis/01b_WB_project_pdo_EDA.html#sector-related-term-frequency",
    "href": "analysis/01b_WB_project_pdo_EDA.html#sector-related-term-frequency",
    "title": "WB Project PDO text EDA",
    "section": "SECTOR-related term frequency",
    "text": "SECTOR-related term frequency\nIsolate SECTOR words and see frequency over years\nTo try and make it a bit more meaningful, let‚Äôs focus on the frequency of the most common words related to SECTORS.\nFrom token_l, I created a ‚Äúbroad SECTOR‚Äù variable to group the sectors in broader definitions:\n\n\nWAT_SAN = water|wastewater|sanitat|Sewer|sewage|Irrigat|Drainag|river basin|groundwater\n\nTRANSPORT = transport|railway|road|airport|waterway|bus|metropolitan|inter-urban|aviation|highway|transit|bridge|port\n\nURBAN = urban|housing|inter-urban|peri-urban|waste manag|slum|city|megacity|intercity|inter-city|town\n\nENERGY = energ|electri|hydroele|hydropow|renewable|transmis|grid|transmission|electric power|geothermal|solar|wind|thermal|nuclear power|energy generation\n\nHEALTH = health|hospital|medicine|drugs|epidem|pandem|covid-19|vaccin|immuniz|diseas|malaria|HIV|AIDS|TB|maternal|clinic|nutrition\n\nEDUCATION = educat|school|vocat|teach|univers|student|literacy|training|curricul|pedagog\n\nAGR_FOR_FISH (Agriculture, Forestry, Fishing) = Agricultural|Agro|Fish|Forest|Crop|livestock|fishery|land|soil\n\nMINING OIL&GAS = Minin|oil|gas|mineral|quarry|extract|coal|natural gas|mine|petroleum|hydrocarbon\n\nSOCIAL_PROT = Social Protec|social risk|social assistance|living standard|informality|insurance|social choesion|gig economy|human capital|employment|unemploy|productivity|wage lev|intergeneration|lifelong learn|vulnerab|empowerment|sociobehav\n\nFINANCIAL = Bank|finan|Investment|credit|microfinan|loan|financial stability|banking|financial intermed|fintech\n\nICT = Information|Communication|ICT|Internet|telecom|cyber|data|AI|artificial intelligence|blockchain|e-learn|e-commerce|platform|software|hardware|digital\n\nIND_TRADE_SERV = Industry|Trade|Service|manufactur|Tourism|Trade and Services|market|export|import|supply chain|logistic|distribut|e-commerce|retail|wholesale|trade facilitation|trade policy|trade agreement|trade barrier|trade finance|trade promotion|trade integration|trade liberalization|trade balance|trade deficit|trade surplus|trade war|trade dispute|trade negotiation|trade cooperation|trade relation|trade partner|trade route|trade corridor\n\n‚ÄúINSTIT_SUPP‚Äù = Government|Public Admin|Institution|Central Agenc|Sub-national Gov|law|justice|governance|Policy|Regulation|Public Expenditure|Public Investment|Public Procurement\n\n‚ÄúGENDER_EQUAL‚Äù = Gender|Women|Girl|Woman|femal|Gender Equal|gender-base|gender inclus|gender mainstream|gender sensit|gender respons|gender gap|gender-based|gender-sensitive|gender-responsive|gender-transform|gender-equit|gender-balance\n\n‚ÄúCLIMATE‚Äù = Climate|Environment|Sustain|Resilience|Adaptation|Mitigation|Green|Eco|Eco-|carbon|carbon cycle|carbon dioxide|climate change|ecosystem|emission|energy effic|greenhouse|greenhouse gas|temperature anomalies|zero net|green growth|low carbon|climate resilient|climate smart|climate tech|climate variab\n\n\npdo_train_t &lt;- pdo_train_t %&gt;%\n   # dealing with water/watershed/waterway\n   mutate(tok_sector_broad = case_when(\n      str_detect(token_l, regex(\"water|wastewater|sanitat|Sewer|sewage|Irrigat|Drainag|river basin|groundwater\", ignore_case = T)) ~ \"WAT_SAN\",\n      str_detect(token_l, regex(\"transport|railway|road|airport|waterway|bus|metropolitan|inter-urban|aviation|highway|transit|bridge|port\", ignore_case = T)) ~ \"TRANSPORT\",\n      str_detect(token_l, regex(\"urban|housing|inter-urban|peri-urban|waste manag|slum|city|megacity|intercity|inter-city|town\", ignore_case = T)) ~ \"URBAN\",\n      str_detect(token_l, regex(\"energ|electri|hydroele|hydropow|renewable|transmis|grid|transmission|electric power|geothermal|solar|wind|thermal|nuclear power|energy generation\", ignore_case = T)) ~ \"ENERGY\",   \n      str_detect(token_l, regex(\"health|hospital|medicine|drugs|epidem|pandem|covid-19|vaccin|immuniz|diseas|malaria|HIV|AIDS|TB|maternal|clinic|nutrition\", ignore_case = T))  ~ \"HEALTH\",\n      str_detect(token_l, regex(\"educat|school|vocat|teach|univers|student|literacy|training|curricul|pedagog\", ignore_case = T)) ~ \"EDUCATION\",\n      # not infra \n      str_detect(token_l, regex(\"Agricultural|Agro|Fish|Forest|Crop|livestock|fishery|land|soil\", ignore_case = T)) ~ \"AGR_FOR_FISH\",\n      str_detect(token_l, regex(\"Minin|oil|gas|mineral|quarry|extract|coal|natural gas|mine|petroleum|hydrocarbon\", ignore_case = T)) ~ \"MINING OIL&GAS\",\n      str_detect(token_l, regex(\"Social Protec|social risk|social assistance|living standard|informality|insurance|social choes|gig economy|human capital|employment|unemploy|productivity|wage lev|intergeneration|lifelong learn|vulnerab|empowerment|sociobehav\", ignore_case = T)) ~ \"SOCIAL_PROT\",\n      str_detect(token_l, regex(\"Bank|finan|Investment|credit|microfinan|loan|financial stability|banking|financial intermed|fintech\", ignore_case = T)) ~ \"FINANCIAL\",\n      str_detect(token_l, regex(\"Information|Communication|ICT|Internet|telecom|cyber|data|AI|artificial intelligence|blockchain|e-learn|platform|software|hardware|digital\", ignore_case = T)) ~ \"ICT\",\n      str_detect(token_l, regex(\"Industry|Trade|Service|manufactur|Tourism|Trade and Services|market|export|import|supply chain|logistic|distribut|e-commerce|retail|wholesale|trade facilitation|trade policy|trade agreement|trade barrier|trade finance|trade promotion|trade integration|trade liberalization|trade balance|trade deficit|trade surplus|trade war|trade dispute|trade negotiation|trade cooperation|trade relation|trade partner|trade route|trade corridor\", ignore_case = T)) ~ \"IND_TRADE_SERV\",\n      str_detect(token_l, regex(\"Government|Public Admin|Institution|Central Agenc|Sub-national Gov|law|justice|governance|Policy|Regulation|Public Expenditure|Public Investment|Public Procurement\", ignore_case = T)) ~ \"INSTIT_SUPP\",\n      str_detect(token_l, regex(\"Gender|Women|Girl|Woman|femal|Gender Equal|gender-base|gender inclus|gender mainstream|gender sensit|gender respons|gender gap|gender-based|gender-sensitive|gender-responsive|gender-transform|gender-equit|gender-balance\", ignore_case = T)) ~  \"GENDER_EQUAL\" ,\n            str_detect(token_l, regex(\"Climate|Environment|Sustain|Resilience|Adaptation|Mitigation|Green|Eco|Eco-|carbon|carbon cycle|carbon dioxide|climate change|ecosystem|emission|energy effic|greenhouse|greenhouse gas|temperature anomalies|zero net|green growth|low carbon|climate resilient|climate smart|climate tech|climate variab\", ignore_case = T)) ~ \"CLIMATE\" ,\n      TRUE ~ NA_character_)) %&gt;% \n   relocate(tok_sector_broad, .after = token_l) # move the new column to the right of token_l\n\nData prep for sector plots\n\ntabyl(pdo_train_t$tok_sector_broad)\n\n# Create a custom color list for each sector\nsector_colors &lt;- c(\n   \"WAT_SAN\" = \"#26BDE2\",  # SDG 6\n   \"ENERGY\" = \"#FCC30B\", # 7SDG \n   \"MINING OIL&GAS\" = \"#23399b\", # no SDG! \n   \"URBAN\" = \"#FD9D24\", # SDG 11\n   \"ICT\" = \"#0f7184\",    # no SDG!\n   \"HEALTH\" = \"#4C9F38\", # SDG 3\n   \"EDUCATION\" = \"#C5192D\", # SDG 4\n   # SDGS \n   \"POVERTY\" = \"#E5243B\", # SDG 1\n   \"ZERO_HUNGER\" = \"#DDA63A\", # SDG 2\n   \"GENDER_EQUAL\" = \"#FF3A21\", # SDG 5\n   \"WORK\" = \"#A21942\", # SDG 8\n   \"INDUSTRY\" = \"#FD9D24\", # SDG 9\n   \"INEQUALITY\" =  \"#DD1367\", # SDG 10\n   \"CONSUMPTION\" = \"#BF8B2E\", # SDG 12\n   \"CLIMATE\" = \"#3F7E44\", # SDG 13\n   \"OCEANS\" = \"#0A97D9\", # SDG 14\n   \"BIODIVERSITY\" = \"#56C02B\", # SDG 15\n   \"PEACE\" = \"#00689D\", # SDG 16\n   \"PARTNERSHIP\" = \"#19486A\", # SDG 17\n   # MINE\n   \"TRANSPORT\" =  \"#A6A6A6\", \n   \"AGR_FOR_FISH\" = \"#56C02B\" , \n   \"SOCIAL_PROT\" = \"#e28293\", \n   \"FINANCIAL\" =  \"#e60066\",\n   \"IND_TRADE_SERV\" = \"#85239b\",\n   \"INSTIT_SUPP\" = \"#49239b\"\n   )\n\n# prepare data for plotting (count)\nsector_broad &lt;- pdo_train_t %&gt;% \n   filter(!is.na(tok_sector_broad)) %&gt;% \n   filter(tok_sector_broad %in% c(\"WAT_SAN\", \"ENERGY\", \"TRANSPORT\", \"URBAN\", \"MINING OIL&GAS\", \"ICT\", \"HEALTH\", \"EDUCATION\", \n                                  # not infrastructure\n                                  \"AGR_FOR_FISH\", \"GENDER_EQUAL\", \"CLIMATE\",  \"SOCIAL_PROT\", \"FINANCIAL\", \"IND_TRADE_SERV\", \"INSTIT_SUPP\" )) %&gt;%\n   count(FY_appr, tok_sector_broad) %&gt;% \n   filter(n &gt; 0) %&gt;% \n   mutate(tok_sector_broad = factor(tok_sector_broad, levels = c(\n      \"WAT_SAN\", \"ENERGY\", \"TRANSPORT\",\"URBAN\",\"MINING OIL&GAS\",\"ICT\", \"HEALTH\", \"EDUCATION\",\n      # not infrastructure\n       \"AGR_FOR_FISH\", \"GENDER_EQUAL\", \"CLIMATE\",  \"SOCIAL_PROT\", \"FINANCIAL\", \"IND_TRADE_SERV\", \"INSTIT_SUPP\"))) # reorder values by frequency\n#df$FY\n\n[FIG] faceted sector (tok_sector_broad) freq ggplot\n\n# Evaluate the title with glue first\ntitle_text &lt;- glue::glue(\"Sector words frequency in PDO over FY {min(pdo_train_t$FY_appr)}-{max(pdo_train_t$FY_appr)}\") \n\n# Plot\npdo_sect_broad_freq &lt;- sector_broad %&gt;% \n   # only the \"original group\n  filter(tok_sector_broad %in% c(\"WAT_SAN\", \"ENERGY\", \"TRANSPORT\", \"URBAN\", \"MINING OIL&GAS\", \"ICT\", \"HEALTH\", \"EDUCATION\" )) %&gt;% \n   ggplot(.,\n          aes(x = FY_appr, y = n, \n              group = tok_sector_broad, color = tok_sector_broad)) +\n   geom_line(linetype = \"dotted\", alpha = 0.5, size = 1) +\n   geom_point(size = 2) +\n   scale_x_continuous(breaks =  seq(2001, 2023, by=  1)) +\n   scale_y_continuous(breaks =  seq(0,150, by=  25)) +\n   # ~ SDG colors \n    # scale_color_viridis_d(option = \"magma\", end = 0.9) + \n   scale_color_manual(values = sector_colors) +\n   facet_wrap(~tok_sector_broad, ncol = 3, scales = \"free\")+ \n   guides(color = FALSE) +\n   my_pretty_theme +\n   theme(# Adjust angle and alignment of x labels\n      axis.text.x = element_text(angle = 45, hjust = 1)) + \n   labs(title = \"Sector words frequency in PDOs by fiscal years of approval\",\n        subtitle = \"[Using \\\"custom\\\" broad sector definition]\",\n        x =  \"\",# \"Board approval FY\", \n        y = \"\"#\"Counts of 'sector' word (tok_sector_broad)\"\n   ) + \n   # Add the reference line at y = 50, red, dashed, and transparent (50% opacity)\n   geom_hline(yintercept = 50, linetype = \"longdash\", color = \"#d02e4c\", alpha = 0.40)   \n   # geom_vline(data = subset(sector_broad, tok_sector_broad == \"HEALTH\"), \n   #            aes(xintercept = 2020), \n   #            linetype = \"solid\", color = \"#9b6723\",alpha = 0.35) +\n   # geom_text(data = subset(sector_broad, tok_sector_broad == \"HEALTH\"), \n   #           aes(x = 2020, y = max(sector_broad$n)*0.65, label = \"Covid\"), \n   #           angle = 90, vjust = -0.5, color = \"#9b6723\") \n\npdo_sect_broad_freq\n\n\n\n\n\n\n\n[FUN fig] split sector (tok_sector_broad) freq ggplot\n\n# --- Get a LIST of unique sectors (facets) and split the data\nsector_list &lt;- base::split(x = sector_broad, f = sector_broad$tok_sector_broad)\n\n# --- Create a function to plot for each sector with custom colors\nf_plot_sector &lt;- function(data) {\n   # Get the sector name\n   sector &lt;- unique(data$tok_sector_broad)\n   # Create the plot\n   p &lt;-ggplot(data = data, \n              aes(x = FY_appr, y = n, \n                  group = tok_sector_broad, color = tok_sector_broad)) +\n      # By sector ... \n      geom_line(color = sector_colors[sector], linetype = \"dotted\", alpha = 0.5, size = 1) +   \n      geom_point(color = sector_colors[sector], size = 3) +               \n      scale_x_continuous(breaks = seq(2001, 2023, by = 1)) +\n      scale_y_continuous(breaks = seq(0, max(data$n), by = 25)) +\n      # custom\n      my_pretty_theme + \n      theme(axis.text.x = element_text(angle = 45, hjust = 1)) +\n      labs(\n         title = paste(\"\\\"\",sector,\"\\\" in PDOs by fiscal years of approval\"),  # Use facet-specific title\n         subtitle = \"[Using a \\\"custom\\\" broad sector definition &\\nshowing relevant WDR publication(s)]\",\n         x = \"\", \n         y = \"\"  # Remove y-axis label\n      ) +\n      # Ensure y-axis limit includes 50\n      expand_limits(y = 50) + \n      # Add the reference line at y = 50, red, dashed, and transparent (50% opacity)\n      geom_hline(yintercept = 50, linetype = \"longdash\", color = \"#d02e4c\", alpha = 0.75)\n   # # Add vline and text annotation only for the HEALTH sector\n   if (sector == \"HEALTH\") {\n      p &lt;- p +\n         geom_vline(aes(xintercept = 2020), linetype = \"solid\", color = \"#9b6723\",alpha = 0.35) +\n         geom_text(aes(x = 2020, y = max(n) * 0.75, label = \"Covid\"),\n                   angle = 90, vjust = -0.5, color = \"#9b6723\")\n   }\n\n   return(p)\n}\n\n# --- Use purrr::map to create a LIST of plots, one for each sector\nsector_plots &lt;- map(sector_list, f_plot_sector)\n\n# --- (exe) Extract the first plot to display\nsector_plots$HEALTH\n\n\n# Optionally print each plot to the console\nwalk(sector_plots, print)\n\n\n# Define the output directory using the 'here' function\noutput_dir &lt;- here(\"analysis\", \"output\", \"figures\")\n\n# Save each plot to a file in the specified directory\nwalk2(sector_plots, names(sector_list), \n      ~ggsave(filename = file.path(output_dir, paste0(.y, \"_sector_plot.png\")), plot = .x))"
  },
  {
    "objectID": "analysis/01b_WB_project_pdo_EDA.html#sector-in-pdo-v.-wdr-publications",
    "href": "analysis/01b_WB_project_pdo_EDA.html#sector-in-pdo-v.-wdr-publications",
    "title": "WB Project PDO text EDA",
    "section": "SECTOR in PDO v. WDR publications",
    "text": "SECTOR in PDO v. WDR publications\nFor the (broadly defined) HEALTH sector, it is quite clear that Covid-19 is the main driver of the peak in 2020.\nWhat about the other sectors? I was struck by the fact that, observing PDOs over time, the broadly defined ‚Äúsector term‚Äù in the PDO always presents at least one peak and I wonder what could trigger it.\nOne possible explanation is that the PDOs somehow reflect the topics discussed by the World Development Reports (WDR) published annually by the World Bank. The WDR is a flagship publication of the World Bank that provides in-depth analysis of a specific aspect of development.\nIt is important to remark that these publications are not some speculative research endeavor, as they are deeply rooted in the concrete information that the Bank retrieves on the ground from projects and operations as they are supported and evaluated. In turn, the WDRs themselves inform the Bank‚Äôs policy priorities and operational strategies.\nTherefore, it is reasonable to expect some kind of correlation between the topics discussed in the WDRs and the objectives of projects stated in in the PDOs."
  },
  {
    "objectID": "analysis/01b_WB_project_pdo_EDA.html#ingest-wdr-data",
    "href": "analysis/01b_WB_project_pdo_EDA.html#ingest-wdr-data",
    "title": "WB Project PDO text EDA",
    "section": "Ingest WDR data",
    "text": "Ingest WDR data\nPreviously created, as explained in data/derived_data/_provenance.md\n\n# Read the WDR data\nwdr &lt;- readRDS(here(\"data\",\"derived_data\", \"wdr.rds\"))\n\n‚Äî Manually add WDR 2023\nOKR Full item\n\nlibrary(tibble)\n\n# Create a named list of NA values for subj_11 to subj_46\nna_values &lt;- setNames(rep(NA, 35), paste0(\"subj_\", 12:46))\n\n# Add a new row with the existing columns and NA for subj_11 to subj_46\nwdr &lt;- wdr %&gt;%\n  add_row(\n    date_issued = 2023,\n    decade = \"2020s\",\n    id = NA, # ? \n    ISBN = \"978-1-4648-1941-4\",\n    title = \"Migrants, Refugees, and Societies\",\n    doc_mt_identifier_1 = \"oai:openknowledge.worldbank.org:10986/39696\", #? \n    subject_miss = NA,\n    abstract = \"Migration is a development challenge. About 184 million people-2.3 percent of the world‚Äôs population-live outside of their country of nationality. Almost half of them are in low- and middle-income countries. But what lies ahead? As the world struggles to cope with global economic imbalances, diverging demographic trends, and climate change, migration will become a necessity in the decades to come for countries at all levels of income. If managed well, migration can be a force for prosperity and can help achieve the United Nations‚Äô Sustainable Development Goals. World Development Report 2023 proposes an innovative approach to maximize the development impacts of cross-border movements on both destination and origin countries and on migrants and refugees themselves. The framework it offers, drawn from labor economics and international law, rests on a ‚ÄúMatch and Motive Matrix‚Äù that focuses on two factors: how closely migrants‚Äô skills and attributes match the needs of destination countries and what motives underlie their movements. This approach enables policy makers to distinguish between different types of movements and to design migration policies for each. International cooperation will be critical to the effective management of migration.\",\n    url_keys = \"https://openknowledge.worldbank.org/handle/10986/39696\",\n    altmetric = 150,\n    all_topic = \"Poverty Reduction,Social Development,Conflict and Development\",\n    all_subj = \"migration,migrants,refugees,force displacement,crss-border mobility,remittances,origin country,international protection,refugee-hosting country,irregular migration,international cooperation\",\n    subj_1 = \"migration\",\n    subj_2 = \"migrants\",\n    subj_3 = \"refugees\",\n    subj_4 = \"force displacement\",\n    subj_5 = \"crss-border mobility\",\n    subj_6 = \"remittances\",\n    subj_7 = \"origin country\",\n    subj_8 = \"international protection\",\n    subj_9 = \"refugee-hosting country\",\n    subj_10 = \"irregular migration\",\n    subj_11 = \"international cooperation\",\n    !!!na_values  # Unpack the NA values for subj_12 to subj_46\n  )\n\n‚Äî Manually add WDR 2024\nOKR Full item\n\nlibrary(tibble)\n\n# Create a named list of NA values for subj_11 to subj_46\nna_values &lt;- setNames(rep(NA, 35), paste0(\"subj_\", 12:46))\n# https://documents.worldbank.org/en/publication/documents-reports/documentdetail/099042523192514880/p17826903573340450b2d00e8cfd3baf7ac\n# https://openknowledge.worldbank.org/entities/publication/5e5ac9f1-71ee-4734-825e-60966658395f/full\n\n# Add a new row with the existing columns and NA for subj_11 to subj_46\nwdr &lt;- wdr %&gt;%\n  add_row(\n    date_issued = 2024,\n    decade = \"2020s\",\n    id = NA, # ? \n    ISBN = \"978-1-4648-2078-6\",\n    title = \"The Middle-Income Trap\",\n    doc_mt_identifier_1 = \"oai:openknowledge.worldbank.org:10986/41919\", #? \n    subject_miss = NA,\n    abstract = \"Middle-income countries are in a race against time. Many of them have done well since the 1990s to escape low-income levels and eradicate extreme poverty, leading to the perception that the last three decades have been great for development. But the ambition of the more than 100 economies with incomes per capita between US$1,100 and US$14,000 is to reach high-income status within the next generation. When assessed against this goal, their record is discouraging. Since the 1970s, income per capita in the median middle-income country has stagnated at less than a tenth of the US level. With aging populations, growing protectionism, and escalating pressures to speed up the energy transition, today‚Äôs middle-income economies face ever more daunting odds. To become advanced economies despite the growing headwinds, they will have to make miracles. Drawing on the development experience and advances in economic analysis since the 1950s, World Development Report 2024 identifies pathways for developing economies to avoid the ‚Äúmiddle-income trap.‚Äù It points to the need for not one but two transitions for those at the middle-income level: the first from investment to infusion and the second from infusion to innovation. Governments in lower-middle-income countries must drop the habit of repeating the same investment-driven strategies and work instead to infuse modern technologies and successful business processes from around the world into their economies. This requires reshaping large swaths of those economies into globally competitive suppliers of goods and services. Upper-middle-income countries that have mastered infusion can accelerate the shift to innovation‚Äînot just borrowing ideas from the global frontiers of technology but also beginning to push the frontiers outward. This requires restructuring enterprise, work, and energy use once again, with an even greater emphasis on economic freedom, social mobility, and political contestability. Neither transition is automatic. The handful of economies that made speedy transitions from middle- to high-income status have encouraged enterprise by disciplining powerful incumbents, developed talent by rewarding merit, and capitalized on crises to alter policies and institutions that no longer suit the purposes they were once designed to serve. Today‚Äôs middle-income countries will have to do the same.\",\n    url_keys = \"https://openknowledge.worldbank.org/handle/10986/41919\",\n    altmetric = 13,\n   all_topic = \"Macroeconomics,Economic Growth,Business Cycles and Stabilization Policies,Poverty Reduction,Achieving Shared Growth,Science and Technology Development,Innovation\",\n    all_subj = \"middle-income trap,investment,infusion,innovation,technologies,competitive suppliers,economic freedom\",\n    subj_1   = \"middle-income trap\",\n    subj_2   = \"investment\",\n    subj_3   = \"infusion\",\n    subj_4   = \"innovation\",\n    subj_5   = \"technologies\",\n    subj_6   = \"competitive suppliers\",\n    subj_7   = \"economic freedom\",\n    subj_8   = NA,\n    subj_9   = NA,\n    subj_10  = NA,\n    subj_11  = NA,\n    !!!na_values  # Unpack the NA values for subj_12 to subj_46\n  )\n\n‚Äî Manually correct WDR 2011\n\n\nwdr$url_keys [wdr$id == \"4389\"] &lt;- \"https://openknowledge.worldbank.org/handle/10986/4389\"\n\nwdr$altmetric [wdr$id == \"4389\"] &lt;- \"210\"\n\nwdr$abstract [wdr$id == \"4389\"] &lt;- \"The 2011 World development report looks across disciplines and experiences drawn from around the world to offer some ideas and practical recommendations on how to move beyond conflict and fragility and secure development. The key messages are important for all countries-low, middle, and high income-as well as for regional and global institutions: first, institutional legitimacy is the key to stability. When state institutions do not adequately protect citizens, guard against corruption, or provide access to justice; when markets do not provide job opportunities; or when communities have lost social cohesion-the likelihood of violent conflict increases. Second, investing in citizen security, justice, and jobs is essential to reducing violence. But there are major structural gaps in our collective capabilities to support these areas. Third, confronting this challenge effectively means that institutions need to change. International agencies and partners from other countries must adapt procedures so they can respond with agility and speed, a longer-term perspective, and greater staying power. Fourth, need to adopt a layered approach. Some problems can be addressed at the country level, but others need to be addressed at a regional level, such as developing markets that integrate insecure areas and pooling resources for building capacity Fifth, in adopting these approaches, need to be aware that the global landscape is changing. Regional institutions and middle income countries are playing a larger role. This means should pay more attention to south-south and south-north exchanges, and to the recent transition experiences of middle income countries.\"\n\nwdr$all_topic [wdr$id == \"4389\"] &lt;- tolower(\"Justice,Jobs,Political Violence and Civil War,Political Violence and War,Organized Crime,Fragility,Conflict and Violence,Crime,Social Cohesion,Public Sector Management,Social Development,Law and Development, Social Protections and Labor,Conflict and Development,Water Supply and Sanitation,Judicial System Reform, Labor Markets,Armed Conflict,Urban Solid Waste Management\") \n\n# Define the subjects to be added for the specific row\nsubjects &lt;- c(\n  \"Armed Conflict\",\n  \"Civil Wars\",\n  \"Conflict Prevention\",\n  \"Conflict Resolution\",\n  \"Development Policy\",\n  \"Fragile States\",\n  \"International Development\",\n  \"Peacebuilding\",\n  \"Political Instability\",\n  \"Post-Conflict Reconstruction\",\n  \"Security and Development\"\n) %&gt;% tolower()  # Convert subjects to lowercase\n\n# Ensure id is handled as character and enforce lowercase comparison\nwdr &lt;- wdr %&gt;%\n   mutate(across(starts_with(\"subj_\"),\n                 ~ ifelse(id == \"4389\", \n                          subjects[as.numeric(sub(\"^subj_\", \"\", cur_column()))], \n                          NA_character_))) %&gt;% \n   mutate (all_subj = if_else(id == \"4389\", paste0(subjects, collapse = \",\"), all_subj)) \n\n# Check the result for the row with id == \"4389\"\nwdr %&gt;% filter(id == \"4389\") %&gt;% select(starts_with(\"subj_\"))  # Display the updated subject columns\n\n\n# check &lt;- wdr[wdr$id == \"4389\",] \n\n‚Äî Remove extra space in title column\n\n# Check and remove leading space in the 'title' column\nwdr &lt;- wdr %&gt;%\n  mutate(title = str_trim(title, side = \"left\"))\n\n‚Äî re-save upon correction wrd2.rds\n\n\nwdr2 &lt;-  wdr\nwrite_rds(x = wdr2, file = here::here(\"data\", \"derived_data\",\"wdr2.rds\"))\n\n[TBL] World Develompent Reports 2000-2024\nBelow are the titles of the World Development Reports from 2000 to 2024.\n\n\n\n\n\ndate_issued\ntitle\nurl_keys\n\n\n\n2001\nAttacking Poverty\nhttps://openknowledge.worldbank.org/handle/10986/11856?show=full\n\n\n2002\nBuilding Institutions for Markets\nhttps://openknowledge.worldbank.org/handle/10986/5984?show=full\n\n\n2003\nSustainable Development in a Dynamic World--Transforming Institutions, Growth, and Quality of Life\nhttps://openknowledge.worldbank.org/handle/10986/5985?show=full\n\n\n2004\nMaking Services Work for Poor People\nhttps://openknowledge.worldbank.org/handle/10986/5986?show=full\n\n\n2005\nA Better Investment Climate for Everyone\nhttps://openknowledge.worldbank.org/handle/10986/5987?show=full\n\n\n2006\nEquity and Development\nhttps://openknowledge.worldbank.org/handle/10986/5988?show=full\n\n\n2007\nDevelopment and the Next Generation\nhttps://openknowledge.worldbank.org/handle/10986/5989?show=full\n\n\n2008\nAgriculture for Development\nhttps://openknowledge.worldbank.org/handle/10986/5990?show=full\n\n\n2009\nReshaping Economic Geography\nhttps://openknowledge.worldbank.org/handle/10986/5991?show=full\n\n\n2010\nDevelopment and Climate Change\nhttps://openknowledge.worldbank.org/handle/10986/4387?show=full\n\n\n2011\nConflict, Security, and Development\nhttps://openknowledge.worldbank.org/handle/10986/4389\n\n\n2012\nGender Equality and Development\nhttps://openknowledge.worldbank.org/handle/10986/4391?show=full\n\n\n2013\nJobs\nhttps://openknowledge.worldbank.org/handle/10986/11843?show=full\n\n\n2014\nRisk and Opportunity‚ÄîManaging Risk for Development\nhttps://openknowledge.worldbank.org/handle/10986/16092?show=full\n\n\n2015\nMind, Society, and Behavior\nhttps://openknowledge.worldbank.org/handle/10986/20597?show=full\n\n\n2016\nDigital Dividends\nhttps://openknowledge.worldbank.org/handle/10986/23347?show=full\n\n\n2017\nGovernance and the Law\nhttps://openknowledge.worldbank.org/handle/10986/25880?show=full\n\n\n2018\nLearning to Realize Education's Promise\nhttps://openknowledge.worldbank.org/handle/10986/28340?show=full\n\n\n2019\nThe Changing Nature of Work\nhttps://openknowledge.worldbank.org/handle/10986/30435?show=full\n\n\n2020\nTrading for Development in the Age of Global Value Chains\nhttps://openknowledge.worldbank.org/handle/10986/32437?show=full\n\n\n2021\nData for Better Lives\nhttps://openknowledge.worldbank.org/handle/10986/35218?show=full\n\n\n2022\nFinance for an Equitable Recovery\nhttps://openknowledge.worldbank.org/handle/10986/36883?show=full\n\n\n2023\nMigrants, Refugees, and Societies\nhttps://openknowledge.worldbank.org/handle/10986/39696\n\n\n2024\nThe Middle-Income Trap\nhttps://openknowledge.worldbank.org/handle/10986/41919\n\n\n\n\n\n\nQualify: peak or trend (by sector)"
  },
  {
    "objectID": "analysis/01b_WB_project_pdo_EDA.html#add-geomvline-to-sector-plots-v-wdr-title-cmpl",
    "href": "analysis/01b_WB_project_pdo_EDA.html#add-geomvline-to-sector-plots-v-wdr-title-cmpl",
    "title": "WB Project PDO text EDA",
    "section": "Add geomvline to sector plots v WDR title [CMPL üü†]",
    "text": "Add geomvline to sector plots v WDR title [CMPL üü†]\n\ntabyl(pdo_train_t$tok_sector_broad)\n# pdo_train_t$tok_sector_broad      n  WDR \n\n#                  AGR_FOR_FISH    665 WDR 2008  Agriculture for Development\n#                     EDUCATION   1180 WDR 2004 Making Services Work for Poor People\n#                        ENERGY    886 WDR \n#                     FINANCIAL   1843 WDR \n#                  GENDER_EQUAL    213 WDR 2012  Gender Equality and Development\n#                        HEALTH    946 WDR \n#                           ICT    548 WDR \n#                IND TRADE SERV     60 WDR \n#           INSTITUTIONAL SUPP.   2171 WDR \n#                MINING OIL&GAS    299 WDR \n#                     TRANSPORT   1371 WDR \n#                         URBAN    553 WDR \n#                       WAT_SAN   1069 WDR \n\n‚Äî ‚úÖ AGR_FOR_FISH ( Agriculture, forestry, and fishing)\nThe WDR of 2008 was titled ‚Äù Agriculture for Development‚Äù, link\n\n# --- Get a LIST of unique sectors (facets) and split the data\nsector_list &lt;- base::split(x = sector_broad, f = sector_broad$tok_sector_broad)\n# Specific split df \n#sector_list$'AGR_FOR_FISH'\n\n# Specific plot \npdo_agr_plot &lt;- sector_plots$'AGR_FOR_FISH' +  \n  geom_vline(xintercept = 2012, linetype = \"solid\", color = \"#9b6723\",alpha = 0.35) +\n  geom_text(aes(x = 2012, y = max(n) * 0.5, label = \"WDR Agric\"), \n                   angle = 90, vjust = -0.5, color = \"#9b6723\")\npdo_agr_plot\n\n\n\n\n\n\n\n\nf_save_plot(\"pdo_agr_plot\", pdo_agr_plot)\n\n‚Äî ‚úÖ EDUCATION\nWDR 2007 was titled ‚Äù Development and the Next Generation‚Äù WDR 2018 was titled ‚Äù Learning to Realize Education‚Äôs Promise‚Äù\n\n# Specific split df \n#sector_list$EDUCATION\n\n# Specific plot \npdo_edu_plot &lt;- sector_plots$EDUCATION + \n  geom_vline(xintercept = 2007, linetype = \"solid\", color = \"#9b6723\",alpha = 0.35) +\n  geom_text(aes(x = 2007, y = max(n) * 0.45, label = \"WDR Youth\"), \n                   angle = 90, vjust = -0.5, color = \"#9b6723\") +\n  geom_vline(xintercept = 2018, linetype = \"solid\", color = \"#9b6723\",alpha = 0.35) +\n  geom_text(aes(x = 2018, y = max(n) * 0.30, label = \"WDR Educ\"), \n                   angle = 90, vjust = -0.5, color = \"#9b6723\")\n\npdo_edu_plot\n\n\n\n\n\n\n\n\nf_save_plot(\"pdo_edu_plot\", pdo_edu_plot)\n\n‚Äî ‚úÖ CLIMATE (climate change)\n\nThe WDR of 2010 was titled ‚Äù Development and Climate Change‚Äù, link\n\n# --- Get a LIST of unique sectors (facets) and split the data\nsector_list &lt;- base::split(x = sector_broad, f = sector_broad$tok_sector_broad)\n# GENDER split df \n#sector_list$'CLIMATE'\n\n# Specific plot \npdo_clim_plot &lt;- sector_plots$'CLIMATE' + \n   # geom_vline(xintercept = 2003, linetype = \"solid\", color = \"#9b6723\",alpha = 0.35) +\n   # geom_text(aes(x = 2003, y = max(n) * 0.5, label = \"WDR Sust Dev\"), \n   #           angle = 90, vjust = -0.5, color = \"#9b6723\")+ \n   geom_vline(xintercept = 2010, linetype = \"solid\", color = \"#9b6723\",alpha = 0.35) +\n   geom_text(aes(x = 2010, y = max(n) * 0.2, label = \"WDR Climate change\"), \n             angle = 90, vjust = -0.5, color = \"#9b6723\")\n\npdo_clim_plot\n\n\n\n\n\n\n\n\nf_save_plot(\"pdo_clim_plot\", pdo_clim_plot)\n\n‚Äî ‚úÖ GENDER EQUALITY\nthe WDR of 2012 was titled ‚ÄúGender Equality and Development‚Äù, link\n\n# --- Get a LIST of unique sectors (facets) and split the data\nsector_list &lt;- base::split(x = sector_broad, f = sector_broad$tok_sector_broad)\n# GENDER split df \n#sector_list$GENDER_EQUAL\n\n# Specific plot \npdo_gen_plot &lt;- sector_plots$GENDER_EQUAL +\n  geom_vline(xintercept = 2012, linetype = \"solid\", color = \"#9b6723\",alpha = 0.35) +\n  geom_text(aes(x = 2012, y = max(n) * 0.75, label = \"WDR Gender equal\"), \n                   angle = 90, vjust = -0.5, color = \"#9b6723\")\n\npdo_gen_plot\n\n\n\n\n\n\n\n\nf_save_plot(\"pdo_gen_plot\", pdo_gen_plot)\n\n‚Äî SOCIAL PROTECTION\nWDR 2004 ‚Äù Making Services Work for Poor People‚Äù\n\n# Specific split df \n#sector_list$SOCIAL_PROT\n\n# Specific plot \npdo_soc_plot &lt;- sector_plots$SOCIAL_PROT + \n  geom_vline(xintercept = 2004, linetype = \"solid\", color = \"#9b6723\",alpha = 0.35) +\n  geom_text(aes(x = 2004, y = max(n) * 0.75, label = \"WDR Services\"), \n                   angle = 90, vjust = -0.5, color = \"#9b6723\")\n\npdo_soc_plot\n\n\n\n\n\n\n\n\nf_save_plot(\"pdo_soc_plot\", pdo_soc_plot)\n\n‚Äî INSTITUTIONAL SUPPORT\nWDR 2002 ‚Äù Building Institutions for Markets‚Äù WDR 2007 ‚Äù Governance and the Law‚Äù\n\n# Specific split df \n# sector_list$INSTIT_SUP\n\n# Specific plot \npdo_inst_plot &lt;- sector_plots$INSTIT_SUPP + \n   geom_vline(xintercept = 2002, linetype = \"solid\", color = \"#9b6723\",alpha = 0.35) +\n   geom_text(aes(x = 2002, y = max(n) * 0.75, label = \"WDR Institutions\"), \n             angle = 90, vjust = -0.5, color = \"#9b6723\") + \n   geom_vline(xintercept = 2007, linetype = \"solid\", color = \"#9b6723\",alpha = 0.35) +\n   geom_text(aes(x = 2007, y = max(n) * 0.75, label = \"WDR Governance\"), \n             angle = 90, vjust = -0.5, color = \"#9b6723\")\n\npdo_inst_plot\n\n\nf_save_plot(\"pdo_inst_plot\", pdo_inst_plot)\n\n‚Äî ICT\nWDR 2016 ‚Äù Digital Dividends‚Äù WDR 2021 ‚Äù Data for Better Lives‚Äù\n\n# Specific split df \n# sector_list$ICT\n\n# Specific plot \npdo_ict_plot &lt;- sector_plots$ICT + \n   geom_vline(xintercept = 2016, linetype = \"solid\", color = \"#9b6723\",alpha = 0.35) +\n   geom_text(aes(x = 2016, y = max(n) * 0.75, label = \"WDR Digital Div\"), \n             angle = 90, vjust = -0.5, color = \"#9b6723\") + \n   geom_vline(xintercept = 2021, linetype = \"solid\", color = \"#9b6723\",alpha = 0.35) +\n   geom_text(aes(x = 2021, y = max(n) * 0.75, label = \"WDR Data\"), \n             angle = 90, vjust = -0.5, color = \"#9b6723\")\n\npdo_ict_plot\n\n\nf_save_plot(\"pdo_ict_plot\", pdo_ict_plot)\n\n‚Äî FINANCIAL\nWDR 2005 ‚Äù A Better Investment Climate for Everyone‚Äù WDR 2022 ‚Äù Finance for an Equitable Recovery‚Äù\n\n# Specific split df \n#sector_list$FINANCIAL\n\n# Specific plot \npdo_fin_plot &lt;- sector_plots$FIN + \n  geom_vline(xintercept = 2005, linetype = \"solid\", color = \"#9b6723\",alpha = 0.35) +\n  geom_text(aes(x = 2005, y = max(n) * 0.80, label = \"WDR Inv Clim\"), \n                   angle = 90, vjust = -0.5, color = \"#9b6723\") +\n  geom_vline(xintercept = 2022, linetype = \"solid\", color = \"#9b6723\",alpha = 0.35) +\n  geom_text(aes(x = 2022, y = max(n) * 0.75, label = \"WDR Finance\"), \n                   angle = 90, vjust = -0.5, color = \"#9b6723\")\npdo_fin_plot\n\n\nf_save_plot(\"pdo_fin_plot\", pdo_fin_plot)"
  },
  {
    "objectID": "analysis/01b_WB_project_pdo_EDA.html#bigrams",
    "href": "analysis/01b_WB_project_pdo_EDA.html#bigrams",
    "title": "WB Project PDO text EDA",
    "section": "BIGRAMS",
    "text": "BIGRAMS"
  },
  {
    "objectID": "analysis/01b_WB_project_pdo_EDA.html#clean-bigrams",
    "href": "analysis/01b_WB_project_pdo_EDA.html#clean-bigrams",
    "title": "WB Project PDO text EDA",
    "section": "Clean bigrams",
    "text": "Clean bigrams\nThe challenge is to clean but without separating consecutive words‚Ä¶ so I do this split-reunite process to remove stopwords and punctuation. Basically only keeping bigrams made of 2 nouns or ADJ+noun.\n\n# Separate the bigram column into two words\nbigrams_cleaned &lt;- bigrams %&gt;%\n  tidyr::separate(bigram, into = c(\"word1\", \"word2\"), sep = \" \")\n\n# Remove stopwords and bigrams in EACH component word containing punctuation\nbigrams_cleaned &lt;- bigrams_cleaned %&gt;%\n   # custom stop words\n   filter(!word1 %in% custom_stop_words_df$word, !word2 %in% custom_stop_words_df$word) %&gt;% \n   # Remove punctuation   \n   filter(!str_detect(word1, \"[[:punct:]]\"), !str_detect(word2, \"[[:punct:]]\"))  \n\n# Reunite the component cleaned words into the bigram column\nbigrams_cleaned &lt;- bigrams_cleaned %&gt;%\n   unite(bigram, word1, word2, sep = \" \") %&gt;% \n   # Remove too obvious bigrams \n   filter(!bigram %in% c(\"development objective\", \"development objectives\", \n                         \"proposed project\", \"project development\", \"program development\"))\n\n# View the cleaned dataframe\nbigrams_cleaned\n\n# Count the frequency of each bigram\nbigram_freq &lt;- bigrams_cleaned %&gt;%\n  count(bigram, sort = TRUE)\n\n[FIG] most frequent bigrams in PDOs\n\nExcluding bigrams where 1 word is among stopwords or a punctuation sign\nExcluding ‚Äúdevelopment objective/s‚Äù, ‚Äúproposed project‚Äù, ‚Äúprogram development‚Äù because not very informative\n\n\n# ---- Prepare data for plotting\n# Evaluate the title with glue first\ntitle_text &lt;- glue::glue(\"Frequency of bigrams in PDOs over FY {min(pdo_train_t$FY_appr)}-{max(pdo_train_t$FY_appr)}\") \n# Define the bigrams you want to highlight\nbigrams_to_highlight &lt;- c(\"public sector\", \"private sector\", \"eligible crisis\",\n                          \"health care\", \"health services\", \"public health\")   \n\n \n# ---- Plot the most frequent bigrams\npdo_bigr_freq &lt;- bigram_freq %&gt;%\n   slice_max(n, n = 25) %&gt;%\n   ggplot(aes(x = reorder(bigram, n), y = n,\n              fill = ifelse(bigram %in% bigrams_to_highlight, bigram, \"Other\"))) +\n   geom_col() +\n   # coord flipped so n is Y axis\n   scale_y_continuous(breaks = seq(min(bigram_freq$n)-1, max(bigram_freq$n), by = 50)) +\n   scale_fill_manual(values = c(\"public sector\" = \"#005ca1\", \n                                \"private sector\" = \"#9b2339\", \n                                \"eligible crisis\"= \"#8e550a\", \n                                \"health care\"= \"#245048\",\n                                \"health services\"= \"#245048\",\n                                \"public health\"= \"#245048\", \n                                \"Other\" = \"grey\")) +\n   guides(fill = \"none\") +\n   coord_flip() +\n   labs(title = title_text, subtitle = \"(ranking top 25 bigrams)\",\n        x = \"\", y = \"\") +\n   theme(axis.text.y = element_text(\n            # obtain vector of colors 2 match x axis labels color to fill\n            color = bigram_freq %&gt;%\n               slice_max(n, n = 25) %&gt;%\n               # mutate(color = ifelse(bigram %in% bigrams_to_highlight,\n               #                       ifelse(bigram == \"public sector\", \"#005ca1\",\n               #                              ifelse(bigram == \"private sector\", \"#9b2339\", \"#8e550a\")),\n               #                       \"#4c4c4c\")) \n               mutate(color = case_when (\n                  bigram == \"public sector\" ~ \"#005ca1\",\n                  bigram == \"private sector\" ~ \"#9b2339\",\n                  bigram == \"eligible crisis\" ~ \"#8e550a\",\n                  bigram %in% c(\"health care\", \"health services\", \"public health\") ~ \"#245048\",\n                  TRUE ~ \"#4c4c4c\")) %&gt;%\n               # Ensure the order matches the reordered bigrams (AS BINS)\n               arrange(reorder(bigram, n)) %&gt;%  \n               # Extract the color column in bin order as vector to be passed to element_text()\n               pull(color)\n            )\n         ) + my_pretty_theme\n\npdo_bigr_freq\n\n\n\n\n\n\n\nResults are not surprising in terms of frequent bigram recurrence:\n\nSee for example ‚Äúincrease access‚Äù, ‚Äúservice delivery‚Äù ,‚Äúinstitutional capacity‚Äù, ‚Äúpoverty reduction‚Äù etc, at the top.\nAlthough, while ‚Äúhealth‚Äù recurred in several bigrams (e.g.¬†‚Äúhealth services‚Äù, ‚Äúpublic health‚Äù, ‚Äúhealth care‚Äù) among the top 25, ‚Äúeducation‚Äù did not appear at all.\nA bit mysterious is perhaps ‚Äúeligible crisis‚Äù (&gt; 100 mentions)?! (coming back to this later)\n[FIG] Changes over time BY 1FY\nBesides huge, counter intuitive, difference between ‚Äúhealth‚Äù and ‚Äúeducation‚Äù, ‚Äúclimate change‚Äù appears in the top 25 (ranking above ‚Äúfinancial sector‚Äù and ‚Äúcapacity building‚Äù) which begs the question: Has the frequency of these bigrams has changed over time?\n\n## too busy to be useful\n\n# Step 1: Count the frequency of each bigram by year\ntop_bigrams_1FY &lt;- bigrams_cleaned %&gt;%\n   group_by(FY_appr, bigram) %&gt;%\n   summarise(count = n(), .groups = 'drop') %&gt;%\n   arrange(FY_appr, desc(count)) %&gt;%\n   # ---  +/- top 10  \n   group_by(FY_appr) %&gt;%\n   top_n(10, count) %&gt;%\n   ungroup()\n   # # ---  STRICT  top 10  \n   # mutate(rank = dense_rank(desc(count))) %&gt;%  # Rank bigrams by frequency\n   # filter(rank &lt;= 10) %&gt;%  # Keep only the top 10 by rank\n   # ungroup()\n\n  \n# Add specific bigrams to highlight, if any\nbigrams_to_highlight &lt;- c(\"climate change\",  \"climate resilience\", \"public sector\", \"private sector\")\n\n# Step 2: Plot the top bigrams by frequency over time   \npdo_bigr_FY_freq  &lt;-  top_bigrams_1FY %&gt;% \n ggplot(aes(x = reorder(bigram, count), \n             y = count,\n             fill = ifelse(bigram %in% bigrams_to_highlight, bigram, \"Other\"))) +\n  geom_col() +\n  scale_fill_manual(values = c(\"public sector\" = \"#005ca1\", \"private sector\" = \"#e60066\", \n                               \"climate change\" = \"#399B23\", \"climate resilience\" = \"#d8e600\",\n                               \"Other\" = \"grey\")) +\n  guides(fill = \"none\") +\n  coord_flip() +\n  facet_wrap(~ FY_appr, scales = \"free_y\") +\n  labs(title = \"Top 10 Bigrams by Frequency Over Time\",\n       subtitle = \"(Faceted by Fiscal Year Approval)\",\n       x = \"Bigrams\",\n       y = \"Count\") +\n  theme_minimal() +\n  theme(plot.title.position = \"plot\",\n        axis.text.x = element_text(angle = 45, hjust = 1))+\n     my_pretty_theme\n\npdo_bigr_FY_freq\n\n[FIG] Changes over time BY 3FY\nTo reduce the noise and make the plot more readable, we can group the data by 3 fiscal years (FY) intervals.\n\n# generate FY group \nf_generate_year_groups &lt;- function(years, interval) {\n  breaks &lt;- seq(floor(min(years, na.rm = TRUE) / interval) * interval, \n                ceiling(max(years, na.rm = TRUE) / interval) * interval, \n                by = interval)\n  \n  labels &lt;- paste(breaks[-length(breaks)], \"-\", breaks[-1] - 1)\n  \n  return(list(breaks = breaks, labels = labels))\n}\n\n\n# --- Step 1: Create n-year groups (using `f_generate_year_groups`)\ninterval_i = 3 # decide the interval\nyear_groups &lt;- f_generate_year_groups(bigrams_cleaned$FY_appr, interval = interval_i)\ntop_n_i = 12 # decide the top n bigrams to show\n\n# --- Step 2: Add the generated FY breaks and labels to data frame\ntop_bigrams_FYper &lt;- bigrams_cleaned %&gt;%\n   # cut divides the range of x into intervals\n   mutate(FY_group = base::cut(FY_appr, \n                               breaks = year_groups$breaks, \n                               include.lowest = TRUE, \n                               right = FALSE, \n                               labels = year_groups$labels)) %&gt;% \n   # Count the frequency of each bigram by n-year groups\n   group_by(FY_group, bigram) %&gt;%\n   summarise(count = n(), .groups = 'drop') %&gt;%\n   arrange(FY_group, desc(count)) %&gt;%\n   # Top ? bigrams for each n-year period\n   group_by(FY_group) %&gt;%\n   top_n(top_n_i, count) %&gt;%\n   ungroup()\n\n# --- Step 3: Add specific bigrams to highlight, if any\nbigrams_to_highlight &lt;- c(\"climate change\",  \"climate resilience\", \n                          \"eligible crisis\",  \n                          \"public sector\", \"private sector\",\n                          \"water supply\", \"sanitation services\",\n                          \"health care\", \"health services\", \"public health\", \"health preparedness\"\n                          )\n\n# --- Step 4: Plot the top bigrams by frequency over n-year periods\npdo_bigr_FY_freq  &lt;-  top_bigrams_FYper %&gt;% \n ggplot(aes(x = reorder(bigram, count), \n             y = count,\n             fill = ifelse(bigram %in% bigrams_to_highlight, bigram, \"Other\"))) +\n  geom_col() +\n  scale_fill_manual(values = c(\n     # \"public sector\" = \"#005ca1\", \n     # \"private sector\" = \"#e60066\", \n     \"water supply\" = \"#26BDE2\",\n      \"sanitation services\" = \"#26BDE2\",\n     \"climate change\" = \"#3F7E44\", \n     \"climate resilience\" = \"#a6bd23\",\n     \"eligible crisis\" = \"#e68000\",  \n     \"health care\" = \"#E5243B\",\n     \"health services\" = \"#E5243B\",\n     \"public health\" = \"#E5243B\",\n     \"Other\" = \"grey\")) +\n  guides(fill = \"none\") +\n  coord_flip() +\n  facet_wrap(~ FY_group, ncol = 2 , scales = \"free_y\" )+ \n              #strip.position = \"top\") +  # Facet wrap with columns\n  labs(title = glue::glue(\"Top 12 Bigrams by Frequency Over {interval_i}-Year Periods\"),\n       subtitle =  \"(Some sectors highlighted)\",\n       x = \"\",\n       y = \"\") +\n     my_pretty_theme\n\n\n# print the plot\npdo_bigr_FY_freq\n\n\n\n\n\n\n\n\nFrequency observed over FY intervals is very revealing.\n\n\nInteresting to see the trend of ‚Äúwater supply‚Äù and ‚Äúsanitation services‚Äù bigrams, which are quite stable over time.\nThe bigram ‚Äúhealth care‚Äù and ‚Äúhealth services‚Äù are also quite stable, while ‚Äúpublic health‚Äù obviously gained relevance since the 2019-2021 FY period.\nConversely, ‚Äúprivate sector‚Äù and ‚Äúpublic sector‚Äù loose importance over time (around mid 2010s), while ‚Äúclimate change‚Äù and ‚Äúclimate resilience‚Äù gain relevance from the same point on.\nStill quite surprising the bigram ‚Äúeligible crisis‚Äù, which actually appears in the top 12 bigrams starting in FY 2016-2018!\nü§î Which are the most frequent and persistent Bigrams Over Time?\n\nFor this, I am looking for a ranking that considers Mean frequency across periods arrange(desc(mean_count)) + Stability (low standard deviation) across periods [this is hard bc of NAs], and NOT total count overall‚Ä¶\n\n\nUsing top_bigrams_FYper which had breaks of 3FY\n\n\n# ------------------------------[REPEATED just to see the table]\n\n# --- Step 1: Create n-year groups (using `f_generate_year_groups`)\ninterval_i = 3 # decide the interval\nyear_groups &lt;- f_generate_year_groups(bigrams_cleaned$FY_appr, interval = interval_i)\ntop_n_i = 12 # decide the top n bigrams to show\n\n# --- Step 2: Add the generated FY breaks and labels to data frame\ntop_bigrams_FYper &lt;- bigrams_cleaned %&gt;%\n   # cut divides the range of x into intervals\n   mutate(FY_group = base::cut(FY_appr, \n                               breaks = year_groups$breaks, \n                               include.lowest = TRUE, \n                               right = FALSE, \n                               labels = year_groups$labels)) %&gt;% \n   # Count the frequency of each bigram by n-year groups\n   group_by(FY_group, bigram) %&gt;%\n   summarise(count = n(), .groups = 'drop') %&gt;%\n   arrange(FY_group, desc(count)) %&gt;%\n   # Top ? bigrams for each n-year period\n   group_by(FY_group) %&gt;%\n   top_n(top_n_i, count) %&gt;%\n   ungroup()\n\n\nsd() returns NA for bigrams that are not present in any periods (or are present in just 1 period).\n\n\n# Calculate the mean frequency and standard deviation of the counts for each bigram across periods\nstable_and_frequent_bigrams_per &lt;- top_bigrams_FYper %&gt;%\n   group_by(bigram) %&gt;%\n   summarise(mean_count = mean(count, na.rm = TRUE),     # Mean frequency across periods\n             sd_count = sd(count, na.rm = TRUE),         # Stability (lower sd = more stable)\n             count_non_na = sum(!is.na(count)),  # Count non-NA values\n             sd_count2 = if_else(count_non_na &gt;= 1, sd(count, na.rm = TRUE), NA_real_),  # Only calculate sd if &gt;= 3 non-NA\n             total_count = sum(count)) %&gt;%               # Total count across all periods (optional)\n   arrange(desc(mean_count)) %&gt;%                      # Sort by frequency and then stability\n   # Filter out bigrams with low mean frequency or high instability (you can adjust thresholds)\n   # Focus on the top 25% most frequent bigrams\n   filter(mean_count &gt; quantile(mean_count, 0.70, na.rm = TRUE)) #%&gt;% \n   # Focus on the most stable 50% (lower sd) ---&gt; NO bc NA values\n   #filter( sd_count &lt; quantile(sd_count, 0.5, na.rm = TRUE))\n\n[TBL] Bigrams Over Time [3FY]\n\n# View the most frequent and stable bigrams\nstable_and_frequent_bigrams_per %&gt;% \n   slice_head(n = 15)  %&gt;% kableExtra::kable()\n\n\n\n\nbigram\nmean_count\nsd_count\ncount_non_na\nsd_count2\ntotal_count\n\n\n\nincrease access\n39.83333\n6.080022\n6\n6.080022\n239\n\n\neligible crisis\n37.33333\n1.527525\n3\n1.527525\n112\n\n\nthreat posed\n33.00000\nNA\n1\nNA\n33\n\n\nprivate sector\n31.20000\n10.917875\n5\n10.917875\n156\n\n\nhealth preparedness\n31.00000\nNA\n1\nNA\n31\n\n\nstrengthen national\n28.00000\nNA\n1\nNA\n28\n\n\nservice delivery\n27.71429\n5.313953\n7\n5.313953\n194\n\n\nclimate change\n27.00000\n2.828427\n2\n2.828427\n54\n\n\npoverty reduction\n27.00000\n14.514361\n4\n14.514361\n108\n\n\npublic health\n25.50000\n16.263456\n2\n16.263456\n51\n\n\npublic sector\n25.25000\n8.301606\n4\n8.301606\n101\n\n\ninstitutional capacity\n24.87500\n6.577831\n8\n6.577831\n199\n\n\nimprove access\n24.57143\n8.521681\n7\n8.521681\n172\n\n\nnational systems\n24.00000\nNA\n1\nNA\n24\n\n\n\n\n\n\n\nUsing top_bigrams_1FY which had breaks of 1FY\n\n\n# --- Step 1: Create n-year groups (using `f_generate_year_groups`)\ninterval_i = 1 # decide the interval\nyear_groups &lt;- f_generate_year_groups(bigrams_cleaned$FY_appr, interval = interval_i)\ntop_n_i = 12 # decide the top n bigrams to show\n\n# --- Step 2: Add the generated FY breaks and labels to data frame\ntop_bigrams_1FY &lt;- bigrams_cleaned %&gt;%\n   # cut divides the range of x into intervals\n   mutate(FY_group = base::cut(FY_appr, \n                               breaks = year_groups$breaks, \n                               include.lowest = TRUE, \n                               right = FALSE, \n                               labels = year_groups$labels)) %&gt;% \n   # Count the frequency of each bigram by n-year groups\n   group_by(FY_group, bigram) %&gt;%\n   summarise(count = n(), .groups = 'drop') %&gt;%\n   arrange(FY_group, desc(count)) %&gt;%\n   # Top ? bigrams for each n-year period\n   group_by(FY_group) %&gt;%\n   top_n(top_n_i, count) %&gt;%\n   ungroup()\n\n\n# Calculate the mean frequency and standard deviation of the counts for each bigram across periods\nstable_and_frequent_bigrams_1FY &lt;- top_bigrams_1FY %&gt;%\n   group_by( bigram) %&gt;%\n   summarise(mean_count = mean(count, na.rm = TRUE),     # Mean frequency across periods\n             sd_count = sd(count, na.rm = TRUE),         # Stability (lower sd = more stable)\n             total_count = sum(count)) %&gt;%               # Total count across all periods (optional)\n   arrange(desc(mean_count)) %&gt;%                      # Sort by frequency and then stability\n   # Filter out bigrams with low mean frequency or high instability (you can adjust thresholds)\n   # Focus on the top 25% most frequent bigrams\n   filter(mean_count &gt; quantile(mean_count, 0.70, na.rm = TRUE)) #%&gt;% \n   # Focus on the most stable 50% (lower sd) ---&gt; NO bc NA values\n   #filter( sd_count &lt; quantile(sd_count, 0.5, na.rm = TRUE))\n\n[TBL] Bigrams Over Time [1FY]\n\n# View the most frequent and stable bigrams\nstable_and_frequent_bigrams_1FY %&gt;% \n   slice_head(n = 15)   %&gt;% kableExtra::kable()\n\n\n\n\nbigram\nmean_count\nsd_count\ntotal_count\n\n\n\nmobile applications\n21.00000\nNA\n21\n\n\npublic health\n16.66667\n3.0550505\n50\n\n\nthreat posed\n16.50000\n2.1213203\n33\n\n\nhealth preparedness\n15.50000\n0.7071068\n31\n\n\nincrease access\n14.64706\n5.1713293\n249\n\n\neligible crisis\n14.62500\n10.1971635\n117\n\n\nstrengthen national\n14.00000\n2.8284271\n28\n\n\nvulnerable households\n13.00000\nNA\n13\n\n\nrespond promptly\n12.50000\n10.6066017\n25\n\n\naction plan\n12.00000\nNA\n12\n\n\ndisaster risk\n12.00000\nNA\n12\n\n\nlocal governments\n12.00000\nNA\n12\n\n\nnational systems\n12.00000\n1.4142136\n24\n\n\nworld bank\n12.00000\nNA\n12\n\n\nclimate resilience\n11.66667\n4.5092498\n35"
  },
  {
    "objectID": "analysis/01b_WB_project_pdo_EDA.html#explore-specific-bigrams",
    "href": "analysis/01b_WB_project_pdo_EDA.html#explore-specific-bigrams",
    "title": "WB Project PDO text EDA",
    "section": "Explore specific bigrams",
    "text": "Explore specific bigrams\n‚Äî Public/Private ~ compare frequency over FY\nA case in which looking at bigrams may be better than tokens is the question whether WB project are more focused on public or private sector. It is not easy to capture this information from the text, because:\n\n‚Äúgovernment‚Äù may be referred to the subject/counterpart of the project (e.g.¬†‚Äúgovernment of Mozambique‚Äù)\n‚Äúprivate‚Äù is not necessarily referred to the ‚Äúprivate sector‚Äù (e.g.¬†‚Äúprivate households‚Äù)\n‚Äúpublic‚Äù is not necessarily referred to the ‚Äúpublic sector‚Äù (e.g.¬†‚Äúpublic health‚Äù)\n\nSo, I narrow down to consecutive bigrams ‚Äúpublic sector‚Äù and ‚Äúprivate sector‚Äù to get an indicative frequency of these terms.\n[FIG] Bigrams (‚Äúpublic sector‚Äù, ‚Äúprivate sector‚Äù) freq plot\n\n# Filter for the specific bigrams \"public sector\" and \"private sector\"\nbigrams_pub_priv_sec &lt;- bigrams %&gt;%\n   filter(bigram %in% c(\"public sector\", \"private sector\"))\n\n# Display the result\n#bigrams_pub_priv_sec\n\n# prepare data for plotting (count)\nsector_bigr_df &lt;- bigrams_pub_priv_sec %&gt;% \n   count(FY_appr, bigram) %&gt;% \n   # reorder values by frequency\n   mutate(bigram = factor(bigram, levels = c(\"public sector\", \"private sector\")))\n\n\n# ---- Prepare data for plotting\n# Evaluate the title with glue first\ntitle_text &lt;- glue::glue(\"Frequency of bigrams \\\"public sector\\\" and \\\"private sector\\\" in PDOs over FY {min(sector_bigr_df$FY_appr)}-{max(sector_bigr_df$FY_appr)}\") \n\ntwo_col_contrast &lt;- c( \"#005ca1\",  \"#e60066\" )\n\n# Create a named vector for the legend labels with totals in a single pipeline\nlegend_labels &lt;- sector_bigr_df %&gt;%\n   group_by(bigram) %&gt;%\n   # Calculate total counts for each bigram\n   summarize(total_n = sum(n)) %&gt;% \n   # Append totals to bigram names\n   mutate(label = paste0(bigram, \" (\", total_n, \")\")) %&gt;%  \n   # Create a named vector with bigram as names and labels as values\n   {setNames(.$label, .$bigram)} # curly braces {} in a dplyr pipeline using . as ouptu from previous..\n\n# ---- Plot\npdo_pub_pri_bigr &lt;- ggplot(data = sector_bigr_df, aes(x = FY_appr, y = n, group = bigram, color = bigram)) +\n   geom_line(linetype = \"dotted\", alpha = 0.75, size = 1) +\n   geom_point(size = 3) +\n   scale_x_continuous(breaks = seq(2001, 2023, by = 1)) +\n   scale_color_manual(values = two_col_contrast, \n                      labels = legend_labels) +  # Use modified labels\n   my_pretty_theme +\n   theme(axis.text.x = element_text(angle = 45, hjust = 1)) +\n   labs(title = title_text, \n        x = \"\", \n        y = \"\", \n        color = \"\") \n   \n\npdo_pub_pri_bigr\n\n\n\n\n\n\n\n\n# Save the plot\nf_save_plot(\"pdo_pub_pri_bigr\", pdo_pub_pri_bigr)\n\n\nNote:\n\n\nthese are much less common than the single words.\nWhat happens in FY 2014-2016 that makes these bigram drop in frequency of mention?\n‚Äî climate change ~ notable bigrams over FY [CMPL üü†]\n‚Äî eligible crisis ~ notable bigrams over FY\n\n# reduce back to the original data\npdo_t &lt;- pdo_train_t %&gt;% \n   select(proj_id, pdo,pr_name, FY_appr, FY_clos, status, regionname, countryname, sector1, theme1,lendinginstr, env_cat, ESrisk, curr_total_commitment) %&gt;%\n   group_by(proj_id) %&gt;% \n   slice(1)\n\nFirst of all, let‚Äôs see what are the sentence that contain the bigram ‚Äúeligible crisis‚Äù in the PDOs.\n\n# ---- Define the bigram you want to find\ntarget_bigram &lt;- \"eligible crisis\"\n\n# Tokenize the text data into sentences\nsentences &lt;- pdo_t %&gt;%\n   unnest_tokens(sentence, pdo, token = \"sentences\")\n\n# Count the number of sentences in each document\nsentence_count &lt;- sentences %&gt;%\n   group_by(proj_id) %&gt;%\n   summarise(num_sentences = n())\n\nn_distinct(sentence_count$proj_id)  # number of projects\nsum(sentence_count$num_sentences)   # total number of sentences\n\n# Filter sentences that contain the specific bigram\nsentences_with_targ &lt;- sentences %&gt;%\n   filter(str_detect(sentence, target_bigram))\n\n# Define how many characters before and after the bigram to extract\nchars_before &lt;- 60  # Number of characters before the bigram\nchars_after &lt;- 60   # Number of characters after the bigram\n\n# Add the extracted bigram and surrounding characters to the same dataframe\nsentences_with_eligcris &lt;- sentences_with_targ %&gt;%\n   mutate(closest_text = str_extract(sentence, paste0(\".{0,\", chars_before, \"}\", target_bigram, \".{0,\", chars_after, \"}\"))) %&gt;% \n   # View the updated dataframe with the closest_text column\n   select(proj_id, #sentence, \n          closest_text)\n\n[TBL] Close phrase around bigram ‚Äúeligible crisis‚Äù\nI still don‚Äôt know what ‚Äúeligible‚Äù crisis means, but it appears that the following is like a commonly used phrase in the PDOs: ‚Äúto respond promptly and effectively‚Äù as well as ‚Äúprovide immediate and effective response to‚Äù seem to often accompany the text ‚Äúeligible crisis or emergency‚Äù. Presumably, a standard sentence indicating eligibility for ODA funding.\n\n# Define the phrase you want to search for in the vicinity of the target bigram\nphrase_to_search &lt;- \"respond promptly and effectively\"\n\n# Count how often the phrase appears in the vicinity of the target bigram\nphrase_count &lt;- sentences_with_eligcris %&gt;%\n  mutate(contains_phrase = str_detect(closest_text, phrase_to_search)) %&gt;%  # Check if the phrase is present\n  summarise(count = sum(contains_phrase))  # Count how many times the phrase is found\n\n# View the result\ntabyl(phrase_count$count)\n\nHere are a few examples of the sentences containing the bigram ‚Äúeligible crisis‚Äù and the phrase ‚Äúrespond promptly and effectively‚Äù:\n\n# Filter the sentences that contain the phrase\nsample_with_eligcris &lt;-  sentences_with_eligcris %&gt;% \n   ungroup() %&gt;% \n   # take a random sample of 5 sentences\n   sample_n(8 ) %&gt;%\n   select(proj_id, closest_text) %&gt;% \n   mutate (closest_text =  paste0(\"(...) \", closest_text),\n           # Make \"eligible crisis\" bold by adding &lt;b&gt; tags\n           closest_text = gsub(\"eligible crisis\", \"&lt;b&gt;eligible crisis&lt;/b&gt;\", closest_text)\n   )\n\n# print out sample in a kable \nkable(sample_with_eligcris, format = \"html\", \n      # Display the table with bold formatting\n       escape = FALSE,\n      col.names = c(\"WB Project ID\",\"excerpt of PDO sentences with 'eligible crisis'\")) %&gt;% \n   kable_styling(full_width = FALSE)   \n\n\n\n\nWB Project ID\nexcerpt of PDO sentences with 'eligible crisis'\n\n\n\nP178816\n(...) lactating women in the project regions and to respond to an eligible crisisor emergency.\n\n\nP171040\n(...) ; and (c) to provide immediate and effective response to an eligible crisis or emergency\n\n\nP179592\n(...) project target areas in timor-leste, and (b) in case of an eligible crisis or emergency, respond promptly and effectively to it.\n\n\nP174077\n(...) of the recipient's transport sector, and in the event of an eligible crisis or emergency, to providean immediate response to the eligib\n\n\nP173783\n(...) reparedness in the recipient‚Äôs territory, and in case of an eligible crisis or emergency, respond promptly and effectively to it.\n\n\nP178419\n(...) and (iii) to provide immediate and effective response to an eligible crisis or emergency.\n\n\nP176152\n(...) reparedness in the recipient‚Äôs territory, and in case of an eligible crisis or emergency, respond promptly and effectively to it.\n\n\nP166991\n(...) cture of the recipient‚Äôs highway network, and in case of an eligible crisis or emergency, respond promptly and effectively to it."
  },
  {
    "objectID": "analysis/01b_WB_project_pdo_EDA.html#analyzing-bigrams-tf-idf",
    "href": "analysis/01b_WB_project_pdo_EDA.html#analyzing-bigrams-tf-idf",
    "title": "WB Project PDO text EDA",
    "section": "Analyzing bigrams: tf-idf",
    "text": "Analyzing bigrams: tf-idf\n\nThere are advantages and disadvantages to examining the tf-idf of bigrams rather than individual words. Pairs of consecutive words might capture structure that isn‚Äôt present when one is just counting single words, and may provide context that makes tokens more understandable. However, the per-bigram counts are also sparser (a typical two-word pair is rarer than either of its component words).\n\n\nbigram_tf_idf &lt;- bigrams_cleaned %&gt;% \n # then on that calculate tf-idf\n count(FY_appr, bigram) %&gt;%\n  bind_tf_idf(bigram, FY_appr, n) %&gt;%\n  arrange(FY_appr, desc(tf_idf))\n\nbigram_tf_idf_top &lt;- slice_head(bigram_tf_idf, n =  5, by = FY_appr ) %&gt;% \n      arrange(FY_appr, desc(tf_idf))"
  },
  {
    "objectID": "analysis/01b_WB_project_pdo_EDA.html#concordances",
    "href": "analysis/01b_WB_project_pdo_EDA.html#concordances",
    "title": "WB Project PDO text EDA",
    "section": "‚Äî Concordances",
    "text": "‚Äî Concordances\n\n\nhttps://ladal.edu.au/textanalysis.html#Concordancing\nhttps://www.quantumjitter.com/project/deal/\n\n\nUsing quanteda\n\nfile:///Users/luisamimmi/Github/slogan_old/docs/01b_WDR_data-exploration_abstracts.html\n\n# I use again data = pdo_words\npdo_q_corpus &lt;- quanteda::corpus(as.data.frame(projs_train), \n                               docid_field = \"id\",\n                               text_field = \"pdo\",\n                               meta = list(\"pr_name\", \"boardApprovalFY\")\n)\n \n# --- example with individual keyword \n# Step 1) tokens\npdo_q_tokens &lt;- quanteda::tokens(x = pdo_q_corpus,\n                       remove_punct = TRUE,\n                       remove_symbols = TRUE#,remove_numbers = TRUE\n ) %&gt;% \n  quanteda::tokens_tolower() #%&gt;%\n #quanteda::tokens_remove(pattern = custom_stop_words) %&gt;%\n #quanteda::tokens_remove(pattern = c(\"project\", \"development\", \"bank\", \"world\", \"project\", \"projects\"))\n                                      \n# #______ Step 2) kwic (individual exe )\n# kwic_pdo_data &lt;- quanteda::kwic(x = pdo_q_tokens, # define text(s)\n#                                  # define pattern\n#                                  pattern = quanteda::phrase(c(\"gender\", \"climate\", \"sustainab*\")),\n#                                  # define window size\n#                                  window = 5) %&gt;%\n#     # convert into a data frame\n#     as_tibble() %&gt;%\n#     left_join(projs_train, by = c(\"docname\" =  \"id\")) %&gt;%\n#     # remove superfluous columns\n#      dplyr::select( 'Year' = boardapprovalFY, 'Prj title' = pr_name, pre, keyword, post) %&gt;%\n#   #  slice_sample( n = 50) %&gt;%\n#    kbl(align = \"c\") # %&gt;% kable_styling()\n \n# ____ Step 2) kwic (on vector)\n# Iterate `quanteda::kwic` over a vector of tokens | regex-modified-keywords\nkeywords &lt;- c(\"gender\", \"climate\", \"sustainab*\", \"conditional*\" )\n\n# apply iteratively kwic over a vector of keywords\noutputs_key &lt;-  map(keywords, \n      ~quanteda::kwic(pdo_q_tokens,\n                      pattern =  .x,\n                      window = 5) %&gt;% \n        as_tibble() %&gt;%\n        left_join(projs_train, by = c(\"docname\" =  \"id\")) %&gt;%  \n        # remove superfluous columns\n       dplyr::select( 'Year' = boardapprovalFY, 'Prj title' = pr_name, pre, keyword, post)\n  )\n\n# # all togetha 3\nn = length(keywords)\n\n# check the first element  \noutputs_key[[1]] %&gt;%\n   kbl(align = \"c\")\noutputs_key[[2]] %&gt;%\n   kbl(align = \"c\")\n\n# this list  has no element names \nnames(outputs_key)\n\n‚Äî create kwic with phrases | purrr + print + save png\n\n# Iterate `quanteda::kwic` over a vector of phrases/bigrams \nkeywords_phrase &lt;- c(\"climate change\", \"investment climate\", \"pro-poor\", \n                     \"gender equality\", \"maximizing finance\", \"digital revolution\")\n\n# Step 1) tokens\n# (done above) -&gt; abs_q_tokens\n\n# Step 2) kwic \n# apply iteratively kwic over a vector of bigrams\noutputs_bigrams &lt;- map(keywords_phrase,\n                       ~quanteda::kwic(x = pdo_q_tokens, # define text(s) \n                                       # define pattern\n                                       pattern = quanteda::phrase(.x),\n                                       # define window size\n                                       window = 5) %&gt;%\n                          # convert into a data frame\n                          as_tibble() %&gt;%\n                          left_join(projs_train, by = c(\"docname\" =  \"id\")) %&gt;%  \n                          ## remove superfluous columns\n                          dplyr::select( 'Year' = boardapprovalFY, 'Prj title' = pr_name, pre, keyword, post)\n)  \n\n#  number ofo cbigrams \nn_bi = length(keywords_phrase)\nn_bi # 7\n# name this list's elements \noutputs_bigrams &lt;- outputs_bigrams %&gt;% \n  set_names(paste0(\"kwic_\", keywords_phrase))  \n\n# get rid of empty output dfs in list  \noutputs_bigrams2 &lt;- outputs_bigrams[sapply(\n  outputs_bigrams, function(x) dim(x)[1]) &gt; 0] # 4 left!\n \n#or \noutputs_bigrams3 &lt;- purrr::keep(outputs_bigrams, ~nrow(.) &gt; 0)  # 4 left!\n\n# -------------- print all \n#  walk + print -\nwalk(.x = outputs_bigrams2, .f = print)  \n\n\n# -------------- save  all -&gt; create multiple tables from a single dataframe and save them as images\n# https://stackoverflow.com/questions/69323569/how-to-save-multiple-tables-as-images-using-kable-and-map/69323893#69323893\noutputs_bigrams2  %&gt;%\n  imap(~save_kable(file = paste0('analysis/output/tables/', 'pdo_', .y, '_.png'),\n                   # bs_theme = 'journal', \n                   self_contained = T, \n                   x = kbl(.x, booktabs = T, align = c('l','l', 'c')) %&gt;%\n                     kable_styling() \n  )\n  )"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Motivation",
    "section": "",
    "text": "Through my professional journey as a consultant in multilateral and governmental institutions, I have been exposed to the phenomenon of buzzwords1, catchphrases2 and slogans3 that suddenly emerge in policy discussions, and‚Äîat least for some time‚Äîtake center stage in strategic planning.\nNotably, in the organizations I worked with (including World Bank, Inter-American Development Bank, G20, EU, and the Italian Government) these catchphrases and slogans often turn into significant funding allocations, thus influencing which policies and programs are prioritized.\n\n\nDrawing from my own professional experience in development economics, I recall the ebb and flow of trends in the popularity of phrases like the following (grouped by macro theme):\nOFFICIAL DEVELOPMENT AID\n\nBoost shared prosperity\nMFD (Maximizing Finance for Development)\nPforR (Program for Results)\nRBF (Results-Based Financing)\nRBA (Results-Based Aid)\nEvidence-based policy    \n\nINCLUSION\n\nSouth-South Cooperation\nGender Mainstreaming\nLeave No One Behind   \nThe Bottom Billion\n\nSUSTAINABILITY, ENVIRONMENT\n\nSustainable Development\nSDGs (Sustainable Development Goals)\nESG (Environmental, social, and governance) criteria\nMainstreaming climate action\n3 zeros (0 poverty, 0 net carbon emissions, 0 net loss of natural capital)\nGRID (Green, Resilient, and Inclusive Development)\nBuild Back Better     \n\nI‚Äôve observed how similar formulaic expressions often make their way into project documentation and policy papers, quickly gaining traction‚Äîat times becoming pervasive‚Äîin both academic and professional discourse.\nThis has led me to wonder how much consideration is given to the deliberate selection of such terms, or if they may simply be ‚Äòin vogue‚Äô and become a convenient shorthand for more complex ideas. Something that isn‚Äôt so surprising when we consider the paradoxical effect of social media and their recommendation algorithms on the selection of news, posts and articles that we access. Rather than broadening broadening our exposure, these algorithms tend to restrict it, reinforcing the echo chamber effect.\n\n\n\nAn element that makes this investigation even more intriguing is that, often, the meaning of words and phrases progressively detaches from reality and/or from their originally intended meaning, while some revised meaning (more or less explicit) starts to live on a journey of its own.\nFor example, I was really struck as I realized how the Latin motto I had heard since childhood: ‚ÄúMens sana in corpore sano‚Äù, was actually a partial quotation adopted in 1861 by the Englishman John Hulley as a motto for his Liverpool Athletic Club. In fact, it was extrapolated from a longer sentence by Juvenal poet: ‚ÄúOrandum est ut sit mens sana in corpore sano‚Äù (Satire X, 356). Taken in its entirety, the original statement has quite a different meaning than its more famous shortened version: for one thing, it is about prayer, not fitness!\nA similar reduction is clearly relevant and consequence-ridden if, as Giovanni Gentile pointed out in ‚ÄúSommario di pedagogia come scienza filosofica‚Äù: ‚ÄúLanguage is not a garment of thought: it is thought‚Äôs own body‚Äù, i.e.¬†language does not merely communicate our ideas to others, but it is also the tool of our thought, it provides wings for it to fly (Garbini 2003, p3).  Similarly, Luca D‚ÄôAuria asserts that ‚Äúlanguage is performative‚Äù, in that it colors, deforms, produces reality (D‚ÄôAuria 2024). For this reason, it is necessarily a divisive instrument because, as soon as it is able to categorize, it is also capable of separating (good from bad, truth from lie, etc.). Knowledge itself, much to the dismay of the proponents of political correctness, lives on categories and categories, including linguistic ones, must precisely distinguish and correctly identify the ontological characteristics of what we are talking about.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFor a while now, I have wondered why and how certain catchphrases/slogans/buzzwords become so prevalent in a given public policy context at some specific point in time.\nThe problem is that if you are (as I was) just a small cog in very complex institutional machine, it is difficult to capture the (often intangible) dynamics of this process.\nTherefore, I decided to investigate this phenomenon with a ‚Äúreverse engineering‚Äù approach, starting from the corpora of documents themselves to see if I could identify some patterns or trends over time. Head over to this page to see my specific research questions.\n\n\n\nThe World Bank simply presented, from my point of view, a convenient ‚Äústudy case‚Äù because:\n\nIts documents (related to lending projects and other publications) are available and openly accessible.\nThe Bank has a long history and a vast amount of digitized documents, which allows for a longitudinal analysis.\nThe Bank‚Äôs sectorial and thematic coverage is very broad, which allows for an interesting analysis.\nLast but not least, I am familiar with the Bank‚Äôs documents‚Äô nature, structure and jargon, having worked there for several years."
  },
  {
    "objectID": "index.html#examples-in-the-development-economics-arena",
    "href": "index.html#examples-in-the-development-economics-arena",
    "title": "Motivation",
    "section": "",
    "text": "Drawing from my own professional experience in development economics, I recall the ebb and flow of trends in the popularity of phrases like the following (grouped by macro theme):\nOFFICIAL DEVELOPMENT AID\n\nBoost shared prosperity\nMFD (Maximizing Finance for Development)\nPforR (Program for Results)\nRBF (Results-Based Financing)\nRBA (Results-Based Aid)\nEvidence-based policy    \n\nINCLUSION\n\nSouth-South Cooperation\nGender Mainstreaming\nLeave No One Behind   \nThe Bottom Billion\n\nSUSTAINABILITY, ENVIRONMENT\n\nSustainable Development\nSDGs (Sustainable Development Goals)\nESG (Environmental, social, and governance) criteria\nMainstreaming climate action\n3 zeros (0 poverty, 0 net carbon emissions, 0 net loss of natural capital)\nGRID (Green, Resilient, and Inclusive Development)\nBuild Back Better     \n\nI‚Äôve observed how similar formulaic expressions often make their way into project documentation and policy papers, quickly gaining traction‚Äîat times becoming pervasive‚Äîin both academic and professional discourse.\nThis has led me to wonder how much consideration is given to the deliberate selection of such terms, or if they may simply be ‚Äòin vogue‚Äô and become a convenient shorthand for more complex ideas. Something that isn‚Äôt so surprising when we consider the paradoxical effect of social media and their recommendation algorithms on the selection of news, posts and articles that we access. Rather than broadening broadening our exposure, these algorithms tend to restrict it, reinforcing the echo chamber effect."
  },
  {
    "objectID": "index.html#the-curious-case-of-lexical-reduction-modification-or-domestication",
    "href": "index.html#the-curious-case-of-lexical-reduction-modification-or-domestication",
    "title": "Motivation",
    "section": "",
    "text": "An element that makes this investigation even more intriguing is that, often, the meaning of words and phrases progressively detaches from reality and/or from their originally intended meaning, while some revised meaning (more or less explicit) starts to live on a journey of its own.\nFor example, I was really struck as I realized how the Latin motto I had heard since childhood: ‚ÄúMens sana in corpore sano‚Äù, was actually a partial quotation adopted in 1861 by the Englishman John Hulley as a motto for his Liverpool Athletic Club. In fact, it was extrapolated from a longer sentence by Juvenal poet: ‚ÄúOrandum est ut sit mens sana in corpore sano‚Äù (Satire X, 356). Taken in its entirety, the original statement has quite a different meaning than its more famous shortened version: for one thing, it is about prayer, not fitness!\nA similar reduction is clearly relevant and consequence-ridden if, as Giovanni Gentile pointed out in ‚ÄúSommario di pedagogia come scienza filosofica‚Äù: ‚ÄúLanguage is not a garment of thought: it is thought‚Äôs own body‚Äù, i.e.¬†language does not merely communicate our ideas to others, but it is also the tool of our thought, it provides wings for it to fly (Garbini 2003, p3).  Similarly, Luca D‚ÄôAuria asserts that ‚Äúlanguage is performative‚Äù, in that it colors, deforms, produces reality (D‚ÄôAuria 2024). For this reason, it is necessarily a divisive instrument because, as soon as it is able to categorize, it is also capable of separating (good from bad, truth from lie, etc.). Knowledge itself, much to the dismay of the proponents of political correctness, lives on categories and categories, including linguistic ones, must precisely distinguish and correctly identify the ontological characteristics of what we are talking about."
  },
  {
    "objectID": "index.html#so-what",
    "href": "index.html#so-what",
    "title": "Motivation",
    "section": "",
    "text": "For a while now, I have wondered why and how certain catchphrases/slogans/buzzwords become so prevalent in a given public policy context at some specific point in time.\nThe problem is that if you are (as I was) just a small cog in very complex institutional machine, it is difficult to capture the (often intangible) dynamics of this process.\nTherefore, I decided to investigate this phenomenon with a ‚Äúreverse engineering‚Äù approach, starting from the corpora of documents themselves to see if I could identify some patterns or trends over time. Head over to this page to see my specific research questions."
  },
  {
    "objectID": "index.html#why-focus-on-the-world-bank",
    "href": "index.html#why-focus-on-the-world-bank",
    "title": "Motivation",
    "section": "",
    "text": "The World Bank simply presented, from my point of view, a convenient ‚Äústudy case‚Äù because:\n\nIts documents (related to lending projects and other publications) are available and openly accessible.\nThe Bank has a long history and a vast amount of digitized documents, which allows for a longitudinal analysis.\nThe Bank‚Äôs sectorial and thematic coverage is very broad, which allows for an interesting analysis.\nLast but not least, I am familiar with the Bank‚Äôs documents‚Äô nature, structure and jargon, having worked there for several years."
  },
  {
    "objectID": "index.html#footnotes",
    "href": "index.html#footnotes",
    "title": "Motivation",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nBuzzword: The term probably originated in the 1940s as jargon used in business and marketing circles. More recently, it has taken on the meaning ‚Äúfashionable jargon‚Äù, especially common in business, technology, and politics, where they often signal innovation or forward-thinking but may lack clear, practical applications.‚Ü©Ô∏é\nCatchphrase: The notion is of a short phrase that will ‚Äúcatch‚Äù in the mind of the hearer. The word was used in the 1840s of a line of music that was easy to catch and remember. It describes, in the modern sense, a memorable, often simplistic phrase or expression that becomes widely recognized and associated with a particular individual, group, or concept. It is designed to stick in people‚Äôs minds and to evoke a specific idea or emotion, usually through repetition in media, advertising, or popular culture.‚Ü©Ô∏é\nSlogan: from Gaelic sluagh-ghairm, i.e.¬†‚Äúbattle cry‚Äù, used by Scottish Highland or Irish clans. The metaphoric sense of ‚Äúdistinctive word or phrase used by a political or other group‚Äù is attested from 1704. It still carries the connotation of a rallying cry, a motto, or a watchword, deliberately crafted to be persuasive, promote a cause or sell an idea.‚Ü©Ô∏é"
  },
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "Some examples of ‚Äúdomesticated lexicon‚Äù",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n\n\n\n\n\n\n\n\n\n\nSviluppo sostenibile\n\n\n\n\n\n\nüáÆüáπ\n\n\nlexicon\n\n\n\n[lessico addomesticato] \n\n\n\n\n\nMay 29, 2024\n\n\nLuisa M. Mimmi, Luisa M. Mimmi\n\n\n\n\n\n\n\n\n\n\n\n\nSpunti dal libro: Il Sistema invisibile, di Marcello Foa del 15 Novembre 2023\n\n\n\n\n\n\nspunti\n\n\nüáÆüáπ\n\n\n\n[libri] \n\n\n\n\n\nNov 15, 2023\n\n\nLuisa M. Mimmi, Luisa M. Mimmi\n\n\n\n\n\n\nNo matching items"
  }
]