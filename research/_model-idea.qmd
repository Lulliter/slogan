---
title: "Hypothetical model"
author: "Luisa M. Mimmi"
date: "Last run: `r format(Sys.time(), '%F')`"
lang: en
editor: source
engine: knitr
## ------  general Output Options
execute:     
  eval: false    # actually run? 
  echo: true     #  include source code in output
  warning: false  #  include warning code in output
  error: false    #  include error code in output
  output: false   # include output code in output (CHG in BLOCKS)
  # include: false   # R still runs but code and results DON"T appear in output  
  cache: false # normalmnte false
toc: true
fig-cap-location: top
tbl-cap-location: top
format:
  html:
    # theme: flatly #spacelab
    code-fold: false # redundant bc echo false 
    toc-depth: 2
    toc_float: true
    toc-location: left
    toc-title: Outline
    embed-resources: true # external dependencies embedded (Not in ..._files/)
  # pdf:
  #   toc-depth: 2
  #   toc-title: Indice
  #   highlight-style: github
  #   #lang: it
  #   embed-resources: true # external dependencies embedded (Not in ..._files/)
format-links: false
bibliography: ../bib/slogan.bib
---

# 2 READ 

- See [Anestis Kousis et al. article](https://www.researchgate.net/publication/376801745_Investigating_the_Key_Aspects_of_a_Smart_City_through_Topic_Modeling_and_Thematic_Analysis) on `Interesting topic modeling project on smart city` = This study aims to investigate the underlying structure of the **smart city paradigm** by employing topic modeling and thematic analysis | ... by identifying significant themes as well as subthemes | ... establishment of a semi-automated novel methodology for for the thematic analysis of large corpora
   + 3 distinct sources/ corpora 

- 🟩 See [Ibai Guillén-Pacho et al. article](https://scispace.com/pdf/dynamic-topic-modelling-for-exploring-the-scientific-6o59ybpk.pdf)
Several `time-dependent approaches based on topic models` were compared to analyse the annual evolution of *latent* concepts in the CORD-19 corpus:

      1. Dynamic Topic Model (DTM), 
      2. Dynamic Embedded Topic Model (DETM), 
      3. BERTopic

- See [Malvika Mishra et al. article](https://link.springer.com/article/10.1007/s41060-024-00596-9) comprehensive investigation into the thematic evolution within computational economics over the past two decades, leveraging advanced topic modeling techniques. Utilizing latent semantic analysis, latent dirichlet allocation (LDA), and BERTopic models, 

- See [Wang et al. article](https://www.mdpi.com/2071-1050/16/19/8293/review_report) Dynamic topic modeling (DTM) was utilized to analyze 1872 energy policy documents, revealing shifts in core topics over time, such as the transition from infrastructure development to new energy focus, thereby assessing the evolution and impact of policy ideas.

# 2 Question

> Before, I was using supervised ML, now I am jumping into unsupervised ML.  

Next, I could go some different ways: 
1. dimension of `time` to appreciate change (e.g. in a collection on COVID papers, the time  factor is important)
   + bellissima idea in [@guillen-pacho_dynamic_2023] to distinguish (i.e. labeling) `topic top words` and `trend word` / `fashion word` and `cross-domain words`, `topic interpretability`, `topic specificity`, etc 
2. dimension of `space` to appreciate spatial distribution of the concept / clustering etc
3. internal `matching` the distribution of topics over different corpora (e.g. project PDO vs. project sector tags)
3. external `matching` the distribution of topics over different corpora (e.g. operational vs. WDRs vs. president statements, etc)

## 1. What are the key aspects of the "...." concept?

# Topic modeling 

+ Topic modeling stands as currently the most widely used `text-mining technique`
+ Grounded in ML and NLP, topic modeling consists of `probabilistic word distributions` often resulting in a presentation of the most frequent terms associated with each identified topic  
+ The `output of topic modeling` comprises two elements: the proportions of words within topics and the proportions of topics within documents.

### 0. **Latent Semantic Indexing (LSI)** 
   - **Latent Semantic Indexing** employs singular value decomposition (SVD) on a large term–document matrix to identify a linear subspace that captures the connection between words and documents. 
   - LSI has limitations, as it does not assign probabilities to topics
   
### 1. **Latent Semantic Analysis (LSA)**
   - **Latent Semantic Analysis** is a method in NLP, specifically distributional semantics, that creates a collection of ideas associated with the documents and terms. 
   - helps automatically uncover topics from large text corpora 

### 2. **Latent Dirichlet Allocation (LDA)**
   - **Latent Dirichlet Allocation** is a generative probabilistic model that explains a set of observations through unseen groups, with each group explaining why certain portions of the data are similar. 
   + (the Bayesian version of LSA!!)

### 3. **BERTopic** (Bidirectional Encoder Representation Transformer)

[See @kousis_investigating_2023]
   - **BERTopic** is a `Python` library that applies topic modeling using BERT. Unlike conventional topic models that depend on the “Bag-of-Words” representations, which disregard the inherent semantic connections among words by employing a mere lexical bag format, `BERTopic`, functioning as a neural topic model, has the capability to depict words as multidimensional vectors. This allows it to capture contextual information, resulting in more precise and comprehensive attributes
   - It allows for the extraction of topics from large corpora of text data, making it suitable for analyzing trends and patterns in textual data.
   - it requires a large amount of computing power


#### Transformers
Once tokens have been converted into meaningful embeddings, the next step is to use an architecture that can effectively leverage these embeddings. **Transformers** are the dominant architecture powering modern NLP and large language models (LLMs), including GPT models. Unlike older architectures such as recurrent neural networks, transformers process text using a mechanism called self-attention (the "T" in GPT stands for "Transformer"), enabling them to:

+ Analyze all words simultaneously, dramatically speeding up training and inference
+ Capture complex relationships between words regardless of their distance in a sentence
+ Understand subtle context and nuance more effectively

### 4. **DistilBERT**
   -  **DistilBERT** is a smaller, faster, cheaper, and lighter version of BERT. It retains 97% of BERT's language understanding while being 60% faster and 40% smaller. DistilBERT is ideal for applications where computational resources are limited or when real-time processing is required.
   - see [@levy_natural_2025]

# Time-dependent modeling strategy 

> Which model could help me detect the change over time of a certain concept in a corpora of company documents
[see @guillen-pacho_dynamic_2023]

To detect the change over time of a certain concept in a corpus of company documents, you can use a combination of **Topic Modeling**, **Word Embeddings**, and **Temporal Analysis** techniques. Here are some models and methods that can help with this task:

### 1. **Dynamic Topic Models (DTM)**
   - **Dynamic Topic Models** extend traditional topic modeling approaches like **Latent Dirichlet Allocation (LDA)** to account for changes in topics over time.
   - DTM basically applies LDA to discrete time-slices  

```{r}
#| eval: false

# Analyze topics over different time periods
library(topicmodels)
library(tm)
library(ldatuning)

# Preprocess the corpus
corpus <- Corpus(VectorSource(documents))
dtm <- DocumentTermMatrix(corpus)

# Fit LDA model
lda_model <- LDA(dtm, k = num_topics)
```

### 2. **Temporal Word Embeddings**
   - **Temporal Word Embeddings** are extensions of static word embeddings like Word2Vec or GloVe that capture how the meaning of words changes over time.
   - By training word embeddings separately for different time periods (e.g., by splitting your corpus into time segments), you can analyze how the vector representation of a concept shifts over time.
   - Models like **Temporal Word2Vec** or **Dynamic Word Embeddings** can help identify changes in how a concept is discussed or understood over time.

```{r}
#| eval: false

library(wordVectors)

# Train word2vec model for each time period
model_t1 <- train_word2vec("text_time_period_1.txt", output_file = "vec_t1.bin")
model_t2 <- train_word2vec("text_time_period_2.txt", output_file = "vec_t2.bin")

# Compare word embeddings over time

```

### 3. **BERT with Temporal Fine-tuning**
   - **BERT (Bidirectional Encoder Representations from Transformers)** can be fine-tuned on your document corpus, with separate fine-tuning stages for different time periods.
   - By comparing the contextual embeddings of the concept across different time periods, you can analyze how the context and usage of the concept have changed over time.
   - Alternatively, you can use a time-aware variant like **BERTime**, which is designed to capture temporal dynamics in textual data.

> normally done with python 

```{r}
#| eval: false

library(bertR)

# Load pre-trained BERT model and fine-tune on time-specific data
model <- bert_load("bert-base-uncased")
fine_tuned_model <- bert_finetune(model, train_data)

# Generate embeddings and analyze changes over time
 

```

### 4. **Sentence Transformers with Time-Based Clustering**
   - Use **Sentence Transformers** (e.g., **SBERT**) to generate embeddings for sentences or paragraphs discussing the concept.
   - Apply clustering algorithms (with packages like `cluster` or `factoextra`) to these embeddings over different time periods to see how the clustering of topics around the concept changes.
   - This approach is effective for tracking nuanced changes in how the concept is discussed at different points in time.

```{r}
#| eval: false

library(text)
library(cluster)

# Generate sentence embeddings
embeddings <- textEmbed(texts = your_text_data)

# Perform clustering analysis
clustering <- kmeans(embeddings, centers = num_clusters)

```

### 5. **Sequential Neural Networks for Temporal Sequence Prediction**
   - Recurrent Neural Networks (RNNs) or **Long Short-Term Memory (LSTM)** networks can be used to model sequences of documents or sentences over time.
   - By training these models to predict future text sequences, you can analyze how the patterns associated with a concept evolve, which can help you detect changes in its importance or context.


```{r}
#| eval: false

library(keras)
library(tensorflow)

# Define and train LSTM model
model <- keras_model_sequential() %>%
  layer_lstm(units = 50, return_sequences = TRUE, input_shape = c(time_steps, features)) %>%
  layer_dense(units = 1)

model %>% compile(loss = 'mse', optimizer = 'adam')
history <- model %>% fit(x_train, y_train, epochs = 20, batch_size = 32)
 

```

### 6. **Change Point Detection Algorithms**
   - Integrating change point detection algorithms (e.g., Bayesian change point detection) with NLP techniques allows you to pinpoint when significant shifts in the usage or context of a concept occur.
   - This is particularly useful if you’re interested in identifying specific events or periods that caused a shift in how a concept is discussed.

```{r}
#| eval: false

library(changepoint)
library(bcp)

# Apply change point detection to time series of concept scores
cpt <- cpt.meanvar(your_time_series_data)
plot(cpt)

```

### Workflow Example:
1. **Data Collection**: The researchers collected a large corpus of CSR reports from Fortune 500 companies spanning multiple years (e.g., from the 1990s to the 2010s).

2. **Preprocessing**: The text from these reports was cleaned, tokenized, and prepared for analysis. Segment your document corpus by time (e.g., by year or quarter).

3. **Modeling**:
   - Apply **Dynamic Topic Models**, e.g.  *Latent Dirichlet Allocation (LDA)* to identify key topics discussed in the reports. By applying LDA to different time segments (e.g., reports from the 1990s, 2000s, 2010s), you could track the prominence of each topic over time.
   - or train Temporal Word Embeddings for each time segment.
   - Alternatively, fine-tune BERT or Sentence Transformers for each time segment.

> ES [@zhang_dynamic_2015] This study utilizes `Dynamic Topic Models (DTM)` to analyze and track changes in market competition by analyzing text data from financial news articles and company reports. The focus is on understanding how discussions about companies and markets evolve over time and how these changes correlate with stock market returns.

4. **Temporal Analysis**:
   - Track the changes in topic distributions or embedding vectors associated with your concept.
   - Use clustering, cosine similarity, or other distance metrics to quantify the change over time.
   - Apply change point detection to identify periods of significant shift.
   
5. **Word Embeddings**:
   - Word2Vec models were trained on CSR reports from different time periods to observe how the semantic meaning of key terms (e.g., "sustainability," "diversity") evolved.
   - By comparing the embeddings of these terms over time, they assessed shifts in how these concepts were discussed.

6. **Change Detection**:
  - The study utilized statistical methods to identify significant change points in the emphasis on particular CSR themes. This helped pinpoint specific events or external pressures (like new regulations) that may have driven changes in corporate communication.   
   


By using these models, you can systematically detect and analyze how the concept of interest changes over time in your corpus of company documents.

### --- Text processing in R

