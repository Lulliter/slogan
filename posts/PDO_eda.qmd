---
title: "Tracing policy signals in text"
subtitle: "Exploring development priorities in World Bank operational documents with R"
description: |
  The idea of analyzing language as data has always intrigued me. In this deep dive, I focus on ~4,000 World Bank Projects & Operations, zooming in on the short texts that describe the *Project Development Objectives (PDOs)*—an abstract of sorts for Bank's operations.<br> This explorative analysis revealed fascinating—and surprising—insights, uncovering patterns and correlations in text but also solutions to enhance the quality of projects' data themselves.  
  
  (This is an ongoing project, so comments, questions, and suggestions are welcome.).<br>
#  #NLP #TextAnalytics #ML #DigitalHumanities #WorldBank #Rstats
categories: [WB, NLP, TextAnalytics, ML, DigitalHumanities, Rstats]
date-modified: today
date: "2024-10-29"
lang: en
number-sections: false

draft: FALSE

author:
   - given: "Luisa M."
     family: "Mimmi"   
     affiliation: "Independent researcher"
     affiliation-url: "https://luisamimmi.org/"
citation:
   type: "report"
   #doi: "10.13140/RG.2.2.20095.48801"
   container-title: "NLP study of World Bank operational documents with R"
   collection-title: "Working paper"
   url: "https://policylexicon.com/posts/PDO_eda.html/"

editor: source
engine: knitr
## ------  general Output Options
execute:     
  eval: false    # actually run? 
  echo: true     #  include source code in output
  warning: false  #  include warning code in output
  error: false    #  include error code in output
  output: false   # include output code in output (CHG in BLOCKS)
  # include: false   # R still runs but code and results DON"T appear in output  
  cache: false # normalmnte false
toc: true
toc-location: "right"
toc_float: true
fig-cap-location: top
tbl-cap-location: top
format:
  html:
    #theme: callout.scss
    code-fold: false # redundant bc echo false 
    embed-resources: true # external dependencies embedded (Not in ..._files/)
    toc: true  # disables the TOC at the top of the main body
    sidebar:
      right: 
        nav: "Outline"  # enables TOC (outline) in the sidebar
  # pdf:
  #   toc: true
  #   toc-depth: 2
  #   # toc-title: Indice
  #   highlight-style: github
  #   #lang: it
  #   embed-resources: true # external dependencies embedded (Not in ..._files/)
format-links: false
prefer-html: true

bibliography: slogan_selected.bib
nocite: |
  @*
---


```{r}
#| label: load_pkgs
#| eval: true
#| echo: false


# Servono x i c***o di grafici con il theme_pretty
library(here)
library(dplyr)
library(showtext)
library(tibble)
library(kableExtra)
#library(flextable)

font_add_google("Roboto Condensed", "roboto_condensed")
showtext_auto()

#flextable::set_flextable_defaults(font.family = "Roboto Regular")

# render_flex_table <- function(tbl) {
#   if (knitr::is_html_output()) {
#     knitr::render_flex_table(tbl) %>%
#       kableExtra::kable_styling(full_width = FALSE)
#   } else {
#     knitr::kable(tbl, format = "latex", booktabs = TRUE) %>%
#       kableExtra::kable_styling(latex_options = "hold_position")
#   }
# }
```


# MOTIVATION

I have always been fascinated by the idea of analyzing language as data and I finally found some time to study *Natural Language Processing (NLP)* and *Text Analytics* techniques.

For this learning project, I explore a dataset of World Bank Projects & Operations, with a focus on the text data contained in the **Project Development Objective (PDO) section** of World Bank's projects (loans, grants, technical assistance). A **PDO** outlines, in synthetic form, the proposed objectives of operations, as defined in the early stages of the World Bank project cycle. <!-- This includes: 1) Identification, 2) Preparation, 3) Appraisal, 4) Negotiation/approval, 5) Implementation, and 6) Completion/validation and evaluation.  -->

Normally, a few objectives are listed in paragraphs that are a couple sentences long. @tbl-pdo_exes shows two examples. 

```{r}
#| label: tbl-pdo_exes
#| eval: true
#| echo: false
#| output: true
#| tbl-cap: Illustrative PDOs text in Projects' documents


tibble::tribble(
   ~Project_ID, ~Project_Name, ~Project_Development_Objective,
   "P127665", "Second Economic Recovery Development Policy Loan", "This development policy loan supports the Government of Croatia's reform efforts with the aim to: (i) enhance fiscal sustainability through expenditure-based consolidation; and (ii) strengthen investment climate.",
   
   "P179010", "Tunisia Emergency Food Security Response Project", "To (a) ensure, in the short-term, the supply of (i) agricultural inputs for farmers to secure the next cropping seasons and for continued dairy production, and (ii) wheat for uninterrupted access to bread and other grain products for poor and vulnerable households; and (b) strengthen Tunisia’s resilience to food crises by laying the ground for reforms of the grain value chain.") %>% 
   kable() %>%
   kable_styling() %>%
   column_spec(3, background = "#f1e7d3")
  # flextable() %>%
  # bg(j = "Project_Development_Objective", bg = "#f1e7d3") %>%
  # autofit()
```

The dataset also includes some relevant **metadata** about the projects, including: *country*, *fiscal year of approval*, *project status*, *main sector*, *main theme*, *environmental risk category*, or *lending instrument*.s

::: {.callout-note collapse="true" style="background-color: #eef2fb;"}

I retrieved the data on this page [WBG Projects](https://projects.worldbank.org/en/projects-operations/projects-list?str_fiscalyear=1979&end_fiscalyear=1979&os=0). Such data is classified by the World Bank as **"public"** and accessible under a [Creative Commons Attribution 4.0 International License](https://creativecommons.org/licenses/by/4.0/).
:::

# DATA
```{r}
#| eval: true
#| echo: false
#| output: false

# needed for dynamic variables 
## --- a) 
ndist_pdo <- readRDS(here("data" , "derived_data", "ndist_pdo.rds")) # duplicates
duplicates <- readRDS(here("data" , "derived_data", "duplicates.rds"))  
count_dp <- readRDS(here("data" , "derived_data", "count_dp.rds")) # duplicates
## --- b) needed for dinamic variables
tracking <- readRDS(here("data", "derived_data", "tracking.rds"))
projs_train2 <- readRDS(here("data", "derived_data", "projs_train2.rds")) # 

first_year_pdo <- readRDS(here("data" , "derived_data", "first_year_pdo.rds"))
pjs_before_pdo <- readRDS(here("data" , "derived_data", "pjs_before_pdo.rds"))
```

The original dataset, retrieved on *August 31, 2024*, included **`r tracking[1,2]` World Bank projects** approved **from Fiscal Year 1947 through 2025**. Approximately `r round((tracking[11,2]/tracking[1,2])*100, 1)`%—**`r tracking[11,2]` projects**—had a *viable* Project Development Objective (PDO) text (i.e., not blank or labeled as "*TBD*", etc.), all approved after the mid 1980s. From this group, some projects were excluded due to missing key variables.

This left **`r sum(tracking[14,2])` projects** as **usable observations** for analysis.

Interestingly, within this refined subset, **`r n_distinct(duplicates$id)` projects share only `r n_distinct(duplicates$pdo)` unique PDOs**: *recycled* PDOs often appear in follow-up projects or components of a larger parent project.

Finally, from these **`r sum(tracking[14,2])` projects**, a representative sample of **`r nrow(projs_train2)` projects with PDOs** was selected for further analysis.



::: {.callout-note collapse="true" style="background-color: #eef2fb;"}
### PDO text data quality
First, it is important to notice that **none of the `r pjs_before_pdo` projects approved before FY `r first_year_pdo`** had any PDO text available (evidently, it became a requirement only later on).

The exploratory analysis of the **`r sum(tracking[14,2])` projects with PDO text** revealed some interesting findings:

1.  **PDO text length**: The PDO text is quite short, with a median of 2 sentences and a maximum of 9 sentences. <!-- From 22,659 Proj --> <!-- all_proj_t %>% filter(pdo %in% c(".","-","NA", "N/A")) %>% nrow() -->
2.  **PDO text missingness**: besides **`r tracking[2,3]`** projects with missing (i.e. `NA`) PDOs, **`r sum(tracking[3:11,3])` projects** had some invalid PDO values, namely: <!-- + **11,216** have missing PDO -->
    + `r sum(tracking[3:6,3])` have PDO as one of: *".","-","NA", "N/A"*
    + `r sum(tracking[7:9,3])` have PDO as one of: *"No change", "No change to PDO following restructuring.","PDO remains the same."*
    + `r sum(tracking[10,3])` have PDO as one of: *"TBD", "TBD.", "Objective to be Determined."*
    + `r sum(tracking[11,3])` have PDO as one of: *"XXXXXX", "XXXXX", "XXXX", "XXX, "a"*

<!-- FROM 22,659 total - 11,306 MISSING PDOs = 11,353 WITH PDO - 31 = 11,322 with *VALID* PDO -->

Of the **remaining `r sum(tracking[11,3])` projects with a valid PDO**, some more projects were **excluded** from the analysis for incompleteness:

+ **`r sum(tracking[12,3])` projects** without *"project status"* 
+ **`r sum(tracking[13,3])` projects** without *"board approval FY"* 
+ **`r sum(tracking[14,3])` projects** approved in FY \>= FY2024  (for incomplete approval stage) 

Lastly (and this was quite surprising to me) the **remaining, viable `r tracking[14,2]` unique projects**, were matched by **only `r ndist_pdo` unique PDOs**! In fact, **`r sum(count_dp$dup_count)` projects share `r n_distinct(duplicates$pdo)` NON-UNIQUE PDO text** in the clean dataset. Why? Apparently, the same PDO is re-used for multiple projects (from 2 to as many as 9 times), likely in cases of follow-up phases of a *parent* project or components of the same lending program."

In sum, the cleaning process yielded a usable set of **`r tracking[14,2]` functional projects**, which was split into a *training subset* (`r nrow(projs_train2)`) to explore and test models and a *testing/validation subset* (`r tracking[14,2]-nrow(projs_train2)`), held out for post-prediction evaluation.
:::

## Preprocessing the PDO text data

Cleaning text data entails extra steps compared to numerical data. A key process is **tokenization**, which breaks text into smaller units like `words`, `bigrams`, `n-grams`, or `sentences`. After that, a common cleaning task is **normalization**, where text is standardized (e.g., converting to lowercase). Similarly, **data reduction techniques** like *stemming* and *lemmatization* simplify words to their root form (e.g., "running," "ran," and "runs" become "run"). This can help to reduce dimensionality, especially with very large datasets, when the word form is not relevant.

Upon tokenization, it is very common to **remove irrelevant elements** like punctuation or `stop words` (unimportant words like "the", "ii)", "at", or repeated ones in context like "PDO") which add noise to the data.

In contrast, **data enhancement techniques** like *part-of-speech (POS) tagging* add value by identifying grammatical components, allowing focus on meaningful elements like `nouns`, `verbs`, or `adjectives`.

<!-- ::: {.callout-warning icon="false" collapse="true" style="background-color: #fffcf9;"}
### {{< bi terminal-fill color=#9b6723 >}} Nerdy Note

In R, Part-of-Speech (POS) tagging can be done using the `cleanNLP` package, which provides a wrapper around the `CoreNLP` Java library. Executing these tasks is very computationally expensive. Based on random checks, the classification of POS tags in the PDO text data was not always accurate, but I considered it good enough for the purpose of this analysis.
::: -->

 
# TERM FREQUENCY PATTERNS

@fig-combo_freq shows the most recurrent tokens and stems in the PDO text data. 

## Words and stems 
Evidently, after stemming, more words (or `stems`) reach the threshold frequency count of 800 (as they have been combined by root). Despite the pre-processing of PDOs' text data, these aren't particularly informative words.

<!-- ![](output/figures/combo_freq.png){#fig-combo_freq} -->

```{r}
#| label: fig-combo_freq
#| eval: true
#| echo: false
#| output: true
#| fig.width: 10 # adjust based on text width in inches
#| fig.height: 8 #9.382  # adjust based on text width in inches
#| out.width: "100%" # ensures the figure spans the full text width in HTML
#| fig-align: "center"
 
combo_freq_p <- readRDS(file = here ("analysis", "output", "figures", "combo_freq_p.rds"))
combo_freq_p 
```

## Bigrams 
@fig-bigr shows the most frequent `bigrams` in the PDO text data. The top-ranking bigrams align with expectations, featuring phrases like  *"increase access"*, *"service delivery"*, *"institutional capacity"*, *"poverty reduction"* at the top.
Notably, while *"health"* appears in several bigrams (e.g., *"health services"*, *"public health"*, *"health care"*), *"education"* is absent from the top 25. Another noteworthy observation is the frequent mention (over 100 instances) of *"eligible crisis"*, which was somewhat unexpected. 

```{r}
#| label: fig-bigr
#| eval: true
#| echo: false
#| output: true
#| fig.width: 10 # adjust based on text width in inches
#| fig.height: 8 #9.382  # adjust based on text width in inches
#| out.width: "100%" # ensures the figure spans the full text width in HTML
#| fig-align: "center"
 
# ---- Prepare data for plotting

# Define the bigrams you want to highlight
bigrams_to_highlight <- c("public sector", "private sector", "eligible crisis",
                          "health care", "health services", "public health")   


pdo_bigr_freq <- readRDS(file = here ("analysis", "output", "figures", "pdo_bigr_freq.rds"))
pdo_bigr_freq  
```


## Trigrams 

@fig-trigr shows the most frequent `trigrams` in the PDO text data. Here, the recurrence of phrases involving *"health"* is reiterated, along with a few phrases revolving around *"environmental"* goals, as well as terms that inherently belong together: like *"water resource management"*, *"social safety net"*, etc..

```{r}
#| label: fig-trigr 
#| eval: true
#| echo: false
#| output: true
#| fig.width: 10 # adjust based on text width in inches
#| fig.height: 8 #9.382  # adjust based on text width in inches
#| out.width: "100%" # ensures the figure spans the full text width in HTML
#| fig-align: "center"
 
pdo_trigram_freq_plot <- readRDS(file = here ("analysis", "output", "figures", "pdo_trigram_freq_plot.rds"))
pdo_trigram_freq_plot 
```

## Sectors in the PDO text
To focus on a meaningful set of `tokens`, I examined the frequency of **sector-related terms** within the PDO text data. To capture the broader concept of "sector," I created a **comprehensive SECTOR variable** that encompasses all relevant words within an expanded definition.

::: {.callout-note collapse="true" style="background-color: #e7edfa;"}
### *Custom* Sector Definitions
The ***"sector"*** term discussed here is *not* the `sector` variable available in the data, but it is an artificial construct reflecting the occurrence of terms referred to the same sector semantic field. Besides conceptual association, these definitions are rooted in the World Bank's own classification of sector and sub-sector.

Below are the *"broad SECTOR"* definitions used in this analysis:

-   **WAT_SAN** = water\|wastewater\|sanitat\|sewer\|sewage\|irrigat\|drainag\|river basin\|groundwater
-   **TRANSPORT** = transport\|railway\|road\|airport\|waterway\|bus\|metropolitan\|inter-urban\|aviation\|highway\|transit\|bridge\|port
-   **URBAN** = urban\|housing\|inter-urban\|peri-urban\|waste manag\|slum\|city\|megacity\|intercity\|inter-city\|town
-   **ENERGY** = energ\|electri\|hydroele\|hydropow\|renewable\|transmis\|grid\|transmission\|electric power\|geothermal\|solar\|wind\|thermal\|nuclear power\|energy generation
-   **HEALTH** = health\|hospital\|medicine\|drugs\|epidem\|pandem\|covid-19\|vaccin\|immuniz\|diseas\|malaria\|hiv\|aids\|tb\|maternal\|clinic\|nutrition
-   **EDUCATION** = educat\|school\|vocat\|teach\|univers\|student\|literacy\|training\|curricul\|pedagog
-   **AGR_FOR_FISH** = agricultural\|agro\|fish\|forest\|crop\|livestock\|fishery\|land\|soil
-   **MINING_OIL_GAS** = minin\|oil\|gas\|mineral\|quarry\|extract\|coal\|natural gas\|mine\|petroleum\|hydrocarbon
-   **SOCIAL_PROT** = social protec\|social risk\|social assistance\|living standard\|informality\|insurance\|social cohesion\|gig economy\|human capital\|employment\|unemploy\|productivity\|wage lev\|intergeneration\|lifelong learn\|vulnerab\|empowerment\|sociobehav
-   **FINANCIAL** = bank\|finan\|investment\|credit\|microfinan\|loan\|financial stability\|banking\|financial intermed\|fintech
-   **ICT** = information\|communication\|ict\|internet\|telecom\|cyber\|data\|ai\|artificial intelligence\|blockchain\|e-learn\|e-commerce\|platform\|software\|hardware\|digital
-   **IND_TRADE_SERV** = industry\|trade\|service\|manufactur\|tourism\|trade and services\|market\|export\|import\|supply chain\|logistic\|distribut\|e-commerce\|retail\|wholesale\|trade facilitation\|trade policy\|trade agreement\|trade barrier\|trade finance\|trade promotion\|trade integration\|trade liberalization\|trade balance\|trade deficit\|trade surplus\|trade war\|trade dispute\|trade negotiation\|trade cooperation\|trade relation\|trade partner\|trade route\|trade corridor
-   **INSTIT_SUPP** = government\|public admin\|institution\|central agenc\|sub-national gov\|law\|justice\|governance\|policy\|regulation\|public expenditure\|public investment\|public procurement
-   **GENDER_EQUAL** = gender\|women\|girl\|woman\|femal\|gender equal\|gender-base\|gender inclus\|gender mainstream\|gender sensit\|gender respons\|gender gap\|gender-based\|gender-sensitive\|gender-responsive\|gender-transform\|gender-equit\|gender-balance
-   **CLIMATE** = climate chang\|environment\|sustain\|resilience\|adaptation\|mitigation\|green\|eco\|eco-\|carbon\|carbon cycle\|carbon dioxide\|climate change\|ecosystem\|emission\|energy effic\|greenhouse\|greenhouse gas\|temperature anomalies\|zero net\|green growth\|low carbon\|climate resilient\|climate smart\|climate tech\|climate variab

:::

The occurrence trends over time for key *sector terms* are shown in @fig-sector_freq.  

Interestingly, all the broadly defined *"sector term"* in the PDO present one or more peaks at some point in time. For the (broadly defined) *HEALTH* sector, it is likely that **Covid-19** triggered the peak in 2020. What about the other sectors? What could be the driving reason?

```{r}
#| label: fig-sector_freq
#| eval: true
#| echo: false
#| output: true
#| fig.width: 10 # adjust based on text width in inches
#| fig.height: 8 #9.382  # adjust based on text width in inches
#| out.width: "100%" # ensures the figure spans the full text width in HTML
#| fig-align: "center"
 
pdo_sect_broad_freq <- readRDS(file = here ("analysis", "output", "figures", "pdo_sect_broad_freq.rds"))
pdo_sect_broad_freq   
```

A possible explanation is that the PDOs may echo themes from the **World Development Reports (WDR)**, the World Bank’s flagship annual publication that analyzes a key development issue each year. Far from being speculative research, each WDR is grounded in the Bank’s field-based insights and, in turn, it informs the Bank's policy and operational priorities. This would suggest a likely alignment between WDR themes and project objectives in the PDOs.

To some extent, visual exploration (see examples below) seems to support this hypothesis: thematically relevant WDRs consistently *appear* in close proximity to peaks in sector-related term frequencies. However, further validation is necessary. Additionally, preparing each WDR typically takes 2-3 years, so a temporal alignment with project documents may include some lag.

<!-- Additionally, it’s important to note that: -->

<!-- + Sector terms are not always captured with the same *precision*, which affects the association. -->
<!-- + Preparing each WDR typically takes 2-3 years, so a temporal alignment with project documents may naturally include some lag. -->

### Examples of sectors-term trend

@fig-agr shows a "combined sector" that is quite broadly defined (**AGRICULTURE, FORESTRY, FISHING**) with the highest peak in 2010, two years after the publication of the WDR on **"Agriculture for Development"**. Perhaps the "alignment" hypothesis is not very meaningful with such a broadly defined sector. 

```{r}
#| label: fig-agr
#| eval: true
#| echo: false
#| output: true
#| fig.width: 10 # adjust based on text width in inches
#| fig.height: 8 #9.382  # adjust based on text width in inches
#| out.width: "100%" # ensures the figure spans the full text width in HTML
#| fig-align: "center"
 
pdo_agr_WDR_plot <- readRDS(file = here ("analysis", "output", "figures", "pdo_agr_WDR_plot.rds"))
pdo_agr_WDR_plot 
```

@fig-climate, tracking frequency of **CLIMATE-related** terms, shows how the highest peak coincided with the publication of the WDR on **"Development and Climate Change"** in 2010.

```{r}
#| label: fig-climate
#| eval: true
#| echo: false
#| output: true
#| fig.width: 10 # adjust based on text width in inches
#| fig.height: 8 #9.382  # adjust based on text width in inches
#| out.width: "100%" # ensures the figure spans the full text width in HTML
#| fig-align: "center"
 
pdo_clim_WDR_plot <- readRDS(file = here ("analysis", "output", "figures", "pdo_clim_WDR_plot.rds"))
pdo_clim_WDR_plot 
```

@fig-educ reports two WDR publications relevant to **EDUCATION**, which seemingly preceded two peaks in the sector-related terms in the PDOs:

+ in 2007, on **"Development and the Next Generation"**
+ in 2018, on **"Learning to Realize Education's Promise"**

```{r}
#| label: fig-educ
#| eval: true
#| echo: false
#| output: true
#| fig.width: 10 # adjust based on text width in inches
#| fig.height: 8 #9.382  # adjust based on text width in inches
#| out.width: "100%" # ensures the figure spans the full text width in HTML
#| fig-align: "center"
 
pdo_edu_WDR_plot <- readRDS(file = here ("analysis", "output", "figures", "pdo_edu_WDR_plot.rds"))
pdo_edu_WDR_plot 
```

@fig-gender shows that the highest frequency of terms related to **GENDER EQUALITY** was instead recorded a couple of years before the publication of the WDR on **"Gender Equality and Development"** in 2012.

```{r}
#| label: fig-gender
#| eval: true
#| echo: false
#| output: true
#| fig.width: 10 # adjust based on text width in inches
#| fig.height: 8 #9.382  # adjust based on text width in inches
#| out.width: "100%" # ensures the figure spans the full text width in HTML
#| fig-align: "center"
 
pdo_gen_WDR_plot <- readRDS(file = here ("analysis", "output", "figures", "pdo_gen_WDR_plot.rds"))
pdo_gen_WDR_plot 
```

## Comparing PDO text against *variable* `sector`

The available data includes not only text but also relevant metadata, such as the `sector1` variable, which captures the project’s primary sector. Do the terms in the PDO text align with this sector label? To examine this, I applied the **two-sample Kolmogorov-Smirnov test** to compare the distribution of `sector-related terms` in the PDO text with the distribution of `sector1`.

::: {.callout-note collapse="true" style="background-color: #eef2fb;"}
The **Kolmogorov-Smirnov test** is non-parametric and makes no assumptions about the underlying distributions, making it a versatile tool for comparing distributions. The `null hypothesis` is that the two samples are drawn from the same distribution. Hence, if the `p-value` is less than the significance level (0.05), the null hypothesis is rejected, suggesting the observed distributions are in fact different. The `test statistic` is the maximum difference between the *cumulative distribution functions (CDF)* of the two samples.

+ **KS statistic**: The vectors of observed distributions have been rescaled (bringing `n_pdo` and `n_tag` to a [0, 1] range before applying the Kolmogorov-Smirnov (KS) test). This is useful when distributions differ substantially in scale or units, as it makes them directly comparable in relative terms.
:::

As shown in @tbl-comp_sec, the results indicate similar distributions across most sectors. This is promising, as it suggests that in cases where metadata is lacking, sector assignments can be reasonably inferred from the PDO text.

```{r}
#| label: tbl-comp_sec
#| eval: true
#| echo: false
#| output: true
#| tbl-cap: Comparing the freqeuncy distributions of SECTOR in text and metadata


# save as object
ks_results_k <- readRDS(file =  here("analysis", "output", "tables" ,"ks_results_k.rds"))
# tbl data.frame

# Count how often the phrase appears in the vicinity of the target bigram
ks_results_k %>% 
   kable(format = "html",
      col.names = c("SECTORS", "KS statistic","KS p-value",  "Distributions"),
      # Round  to 4 digits)
      digits = c(0, 4, 4, 0)) %>%
   kable_styling(full_width = FALSE) %>%
   row_spec(which(ks_results_k$similarity == "Dissimilar"), background = "#e7d8da")
```


```{r}
#| echo: false
# mutate(across(c(2, 3), ~ round(.x, 4))  # round 2nd and 3rd columns to 4 digits
   # ) %>%
   # rename(
   #    `SECTORS` = 1,
   #    `KS statistic` = 2,
   #    `KS p-value` = 3,
   #    `Distributions` = 4
   # ) %>% 
   # flextable() %>%
   # bg(i = ~ Distributions == "Dissimilar", 
   #    bg = "#e7d8da"
   # ) %>%
   # autofit()
```

Below is a graphical representation of two illustrative sectors, showing the most similar and the most dissimilar distributions of the *sector* as deducted form text data, versus the proper metadata *sector* labeling. 

@fig-comp_transp shows the distributions of the **TRANSPORT** sector in the  PDOs' text and in the metadata. The two distributions are the most similar, as confirmed by the Kolmogorov-Smirnov test with a p-value of 0.641. 

```{r}
#| label: fig-comp_transp
#| eval: true
#| echo: false
#| output: true
#| fig.width: 10 # adjust based on text width in inches
#| fig.height: 8 #9.382  # adjust based on text width in inches
#| out.width: "100%" # ensures the figure spans the full text width in HTML
#| fig-align: "center"
 
comp_pdo_sec_TRANSP_plot <- readRDS(file = here ("analysis", "output", "figures", "comp_pdo_sec_TRANSP_plot.rds"))
comp_pdo_sec_TRANSP_plot 
```




@fig-comp_ene compares visually the distributions of the **ENERGY** sector in the  PDOs' text data and the metadata. The two distributions are the most dissimilar, as the Kolmogorov-Smirnov test confirms with a p-value of 0.0001. 
 
 
```{r}
#| label: fig-comp_ene
#| eval: true
#| echo: false
#| output: true
#| fig.width: 10 # adjust based on text width in inches
#| fig.height: 8 #9.382  # adjust based on text width in inches
#| out.width: "100%" # ensures the figure spans the full text width in HTML
#| fig-align: "center"
 
comp_pdo_sec_ENE_plot <- readRDS(file = here ("analysis", "output", "figures", "comp_pdo_sec_ENE_plot.rds"))
comp_pdo_sec_ENE_plot 
```


## Comparing PDO text against *variable* `amount committed`

A similar question is: do word trends observed in PDOs also reflect the allocation of funds by sector? I explored this question with the same approach as before, but this time I compared the distribution of sector-related terms in the PDOs' text against the distribution of the sum of the `amount committed` in corresponding projects (i.e. filtered by `sector1` category). Given the very different ranges, I compared rescaled values (using the *Kolmogorov-Smirnov* two-sample test) to evaluate the independence of these two distributions.

As shown in @tbl-comp_sec_commit, the results indicate less homogeneity of the distributions across key sectors, somthing that could be further investigated. 

```{r}
#| label: tbl-comp_sec_commit
#| eval: true
#| echo: false
#| output: true
#| tbl-cap: Comparing the distributions of SECTOR in text and in corresponding $$ committed


# save as object
ks_results2_k <- readRDS(file =  here("analysis", "output", "tables" ,"ks_results2_k.rds"))

# Count how often the phrase appears in the vicinity of the target bigram
ks_results2_k %>% 
   kable(format = "html",
      col.names = c("SECTORS", "KS statistic","KS p-value",  "Distributions"),
         # Round  to 4 digits)
         digits = c(0, 4, 4, 0)) %>%
   kable_styling(full_width = FALSE) %>%
   row_spec(which(ks_results2_k$similarity == "Dissimilar"), background = "#e7d8da")
```


```{r}
#| echo: false
# mutate(across(c(2, 3), ~ round(.x, 4))  # Round 2nd and 3rd columns
   # ) %>%
   # rename(
   #    `SECTORS` = 1,
   #    `KS statistic` = 2,
   #    `KS p-value` = 3,
   #    `Distributions` = 4
   # ) %>%
   # flextable() %>%
   # bg(i = ~ Distributions == "Dissimilar",
   #    bg = "#e7d8da"
   # ) %>%
   # autofit()

```

Let us pick a couple of examples of specific sectors to check visually. 

### WATER & SANITATION sector: words v. funding 
The distributions in the "WATER & SANITATION" sector are among the *most similar* pairs (K-S test p-value is = 0.4218). 
```{r}
#| label: fig-comp_commit_ws
#| eval: true
#| echo: false
#| output: true
#| fig.width: 10 # adjust based on text width in inches
#| fig.height: 8 #9.382  # adjust based on text width in inches
#| out.width: "100%" # ensures the figure spans the full text width in HTML
#| fig-align: "center"
 
comp_commit_ws <-  readRDS(file = here ("analysis", "output", "figures", "comp_pdo_comm_WAT_SAN_plot.rds"))
comp_commit_ws
```
 
### ICT sector: words v. funding 
The distributions in the ICT sector are among the *least similar* (K-S test p-value is = 0.0001). 
 
```{r}
#| label: fig-comp_commit_ict
#| eval: true
#| echo: false
#| output: true
#| fig.width: 10 # adjust based on text width in inches
#| fig.height: 8 #9.382  # adjust based on text width in inches
#| out.width: "100%" # ensures the figure spans the full text width in HTML
#| fig-align: "center"
 
comp_commit_ict <-  readRDS(file = here ("analysis", "output", "figures", "comp_pdo_comm_ICT_plot.rds"))  
comp_commit_ict
```


<!-- The `chi-square test`, in this case, serves to evaluate the similarity of the distributions of sector-related terms in the PDOs' text and the sum of the `amount committed` in corresponding projects. The results suggest that the distributions are similar across all sectors. However, the visual comparison of the ICT and MINING_OIL_GAS sectors shows that the distributions are not identical. -->


## Concordances: a.k.a. keywords in context

Another useful analysis that can be done exploring text data refers to ***concordance***, which enables a closer look at the context surrounding a word (or combination of words). This approach can help clarify the word’s specific meaning or reveal underlying patterns in the data.


### The bigram *"eligible crisis"* in the PDOs
For instance, among the most frequent `bigrams` (two-word combinations) in the PDO text (illustrated in @fig-bigr), the phrase *"eligible crisis"* stands out. Besides appearing in the PDOs of 112 projects, this phrase is often used in a similar context. Specifically, in 32% of these cases, it is paired with phrases like *"respond promptly and effectively"* or *"immediate and effective response"*. As shown in @tbl-elgcris, this suggests a sort of recurring standard phrasing.

```{r}
#| label: tbl-elgcris
#| eval: true
#| echo: false
#| output: true
#| tbl-cap: Context of the bigram "eligible crisis" in the PDOs


# Load the Kable table
elcr_k <- readRDS(file = here( "analysis" , "output", "tables", "elcr_k.rds"))
elcr_k
```

### The bigram "climate change" in the PDOs
Another frequently occurring `bigram` is *"climate change"*, found in 92 PDOs. @tbl-clim displays words that commonly appear near this bigram. Notably, the word *"mitigation"* (which I associate with a more aspirational, long-term response) appears more frequently than *"adaptation"* (which I view as a more *practical*, short-term response). However, the ratio would flip considering that *"resilience"* may convey a similar practical intent as *"adaptation"*. Another interesting insight worth exploring further in the future.  

```{r}
#| label: tbl-clim
#| eval: true
#| echo: false
#| output: true
#| tbl-cap: Frequent words near "climate change"

# READ as object
clch_close_k <- readRDS( file = here("analysis", "output", "tables" ,"clch_close_k.rds"))
clch_close_k 
```

@tbl-clim2 shows a few examples for each of the words most frequently found in the vicinity of the bigram *"climate change"*. 

```{r}
#| label: tbl-clim2
#| eval: true
#| echo: false
#| output: true
#| tbl-cap: Context of the bigram "climate change" in the PDOs


# READ as object
# OKKIO: this table has HTML tags for color 
sentences_with_climchang2_k <-  readRDS(file = here("analysis", "output", "tables" ,"sentences_with_climchang2_k.rds"))

# Define more contrasting colors for alternating groups
background_colors <- c("#D6EAF8", "#EBF5FB")  # Use distinct colors for easier differentiation
  
#paint(sentences_with_climchang2_k)

# Prepare the kable table with subheaders based on 'contains_what'
sentences_with_climchang2_k %>%
   dplyr::ungroup() %>%
   dplyr::arrange(contains_what) %>%
   dplyr::select(contains_what, proj_id, closest_text) %>%
   kable(format = "html",
         # kable(
         escape = FALSE,
         col.names = c("Near word (root)", "WB Project ID", "Closest Text")) %>%
   kable_styling(full_width = FALSE)   %>%
   group_rows(index = table(sentences_with_climchang2_k$contains_what), 
              # Adjust the color as needed
              label_row_css = "background-color: #D6EAF8;")
# Add subheaders based on the unique values in `contains_what`
#group_rows(index = table(sentences_with_climchang2$contains_what))

```


---

# DATA QUALITY ENHANCEMENT

This section shifts focus to a new area of exploration: the possibility to enhance the metadata quality by predicting missing features in the World Bank project documents. The idea is to use the Project Development Objective (PDO) words as input to predict the missing *categorical descriptors* (`sector`, `environmental risk category`, etc.) for some of the observations . @tbl-missing_ML shows some missing features in the source dataset.

```{r}
#| label: tbl-missing_ML
#| eval: true
#| echo: false
#| output: true
#| tbl-cap: Missing features in source dataset

 
# source function
source(here("R","f_recap_values.R")) 
projs_train2 <- readRDS(file = here( "data", "derived_data", "projs_train2.rds"))

# check candidate lables for classification 
f_recap_values(projs_train2, c("sector1", "theme1","env_cat","ESrisk" )) %>%
   kable(format = "html",
   # kable(
      col.names = c("Variable", "N obs.", "N Distinct", "N Missing", "N Percent")) %>% 
   kable_styling(full_width = FALSE) %>% 
   row_spec(3, background = "#d8e600") # Light yellow background kable() 
```

One candidate variable that could be predicted is `env_cat` (*"Environmental Assessment Category"*). As per the current **Environmental and Social Framework (ESF)**, this is a categorical variable with 4 possible levels (A, B, C, F), but, to simplify, I collapsed it into a binary outcome defined as *"High-Med-risk"* and *"Low-risk-Othr"* (as illustrated in @tbl-env_cat_f_f2). 

::: {.callout-note collapse="true" style="background-color: #eef2fb;"}

All the categories of Environmental Assessment shown below were part of the World Bank’s **Environmental and Social Safeguards framework**, which transitioned in 2018 to the **Environmental and Social Framework (ESF)**

+ **Category A**: A proposed project is classified as Category A if it is likely to have significant adverse environmental impacts that are sensitive, diverse, or unprecedented. 
+ **Category B**: A proposed project is classified as Category B if its potential adverse environmental impacts on human populations or environmentally important areas--including wetlands, forests, grasslands, and other natural habitats--are less adverse than those of Category A projects. 
+ **Category C**: A proposed project is classified as Category C if it is likely to have minimal or no adverse environmental impacts.
+ **Category FI**: A proposed project is classified as Category FI if it involves investment of Bank funds through a financial intermediary, in subprojects that may result in adverse environmental impact.

The following additional categories were found in the data and are likely mistakes or legacy codes: 
+ **Category H**: Mistake of legacy coding – not in current use in most projects.
+ **Category M**: Mistake of legacy coding – not in current use in most projects.
+ **Category U**: Unclassified – not in current use in most projects.
:::

```{r}
#| label: tbl-env_cat_f_f2
#| eval: true
#| echo: false
#| output: true
#| tbl-cap: Binary outcome obtained from the `env_cat` variable

# Load the Kable table
env_cat_f_f2_k <- readRDS(file = here( "analysis" , "output", "tables", "env_cat_f_f2_k.rds"))
env_cat_f_f2_k 

```


## Using ML models to predict a missing feature

The goal at hand has to do with ***text classification***, that is assigning categories to some observations. To predict a missing feature based on a mix of text data and other available predictors, several machine learning (ML) algorithms can be applied. I tested a few suitable algorithms.

The sample splitting (necessary in ML to save testing dataset for model evaluation) was done based on the availability of the `env_cat` variable. The sample was actually split into three groups:  

1) **Training set** (with `env_cat` available) 2,260 observations
2) **Testing set** (with `env_cat` available) 970 observations
3) **Validation set** (with `env_cat` missing) 1,195 observations

<!-- ![Sampling strategy](../images/Sample_EnvCat2.png){fig-align="center"} -->

## Choosing the ML algorithm

To predict the missing binary categorical outcome `env_cat_f2`, I tried several models, including: ***Lasso logistic regression*** (with different specifications including only text or a mix of text and other predictors) and ***Naive Bayes classification*** (Here I only report the results, but details can be found on this  [webpage](https://policylexicon.com/analysis/02a_WB_project_pdo_feat_class_envcat.html)). 
Since text data is sparse and high-dimensional, it is critical to perform some pre-treatment of the `features` (i.e. the *explanatory* variables) before modeling. 

::: {.callout-note collapse="true" style="background-color: #eef2fb;"}
#### Models syntethic description 

+ **LASSO models (for logistic regression)** is an approach that basically defines `how much of a penalty` to put on some features in order to select only the most useful out of all the original possible variables (tokens). It is a good choice when dealing with a high-dimensional dataset, like text data.

+ **Naïve Bayes classification** is a simple and efficient algorithm for text classification. It assumes feature independence, which may not always hold, but it's often a good baseline, particularly with short texts.

Other supervised ML algorithms could be used in this case, such as *Random Forest*, *Support-Vector Machines*, *K-Nearest Neighbors*, but they were not tested here.

#### The steps to predict the missing feature 

1.  **Outcome label engineering**: Define what to predict (outcome variable, $y$), and its functional form (binary or multiclass, log form or not if numeric)./
    <!-- - Deal with *missing* value in $y$ (understand if there are systematic reasons for missingness, and if so, how to address them) + Deal *with* extreme values of $y$ (conservatively is best) -->

2.  **Sample design**: Select the observations to use. In ML this is typically done by splitting the sample into training and testing sets.
    <!-- - For *high external validity* it will have to be as close as possible to the population of interest (patterns of variables' distribution etc.) -->
    
3. **Feature Engineering**: Define the input data (predictors, $X$) and their format. Here, text data was combined with other predictors (e.g. `sector`, `region`, `FY approved`, etc.) to create a feature matrix.  
   + **Text preprocessing**: The text data was preprocessed by `tokenization`, filtering of tokens by frequency, removal of `stopwords`, weighting via `TF-IDF` (Term Frequency-Inverse Document Frequency), to make it suitable for ML algorithms. 

    <!-- - Deal with *missing* values in $X$ (understand, variable by variable, the reasons for missingness, and decide what to do: keep, impute value if numeric, drop the predictor?)\ -->
    <!-- - Select the most *relevant* predictors (which $X$ to have and in which form). For text predictor data, there are specific NLP transformations that can be applied (e.g. tokenization, lemmatization, etc.) -->
    <!-- - In some cases *interaction* between predictor variables makes sense. -->
    <!-- - Alternative models can be build with less predictors in simpler form to compare with others with more predictors in more complex form... Here domain knowledge + EDA are key to decide what to include and what to exclude. -->

4. **Model selection and fitting**: The models were trained on the training set. 
   + Different algorithms will have different parameters that can be adjusted which can affect the performance of the model (**hyperparameters tuning**, typically done while training the model).

<!-- - `model selection` It's impossible to try all possible models (i.e. all possible choices of $X$ variables to include, their possible functional form, and their possible interactions give too many combinations). -->
  <!-- - `cross-validation` is similar to training-test method, which basically splits the data into training and test sets, but it does this multiple times (e.g. `k-fold cross-validation` means $k$ = 10 test sets) and it helps selecting the best model without *overfitting*. -->
  <!-- - Here we do all the work said above (model building and best model selection) in the *work set*. This will be further divided $k-times$ into $k$ train-test splits, then we use the *holdout set* to evaluate the prediction itself. -->
  <!-- - `last_fit` means that, once the best model(s) is/are selected, they are re-run on all of the work set (training data) to evaluate the performance to obtain the final model. -->

5. **Prediction**: The best model was used to predict the missing `env_cat_f2` and evaluate the model's performance on the *hold-out sample* (testing set).

6. **Evaluation**: The predictions were evaluated on the testing set based on performance metrics:
   + `accuracy`, which is the proportion of correct predictions, and 
   + `ROC-AUC` (Receiver Operating Characteristic - Area Under the Curve), which summarizes how well the model can distinguish between classes.
   
    <!-- - evaluate the fit of the prediction (using, MSE, RMSE, accuracy, ROC etc. to summarize *goodness of fit* -->
    <!-- - (for continuous $y$) we can visualize the prediction interval around the prediction, for discrete $y$ the confusion matrix. -->
    <!-- - we can zoom in on the kinds of observations we care about the most or look at the fit in *certain sub-samples* of the data (e.g. by sector, by year, etc.) -->
    <!-- - finally we should assess the *external validity* (hold out set helps but is not representative of all the "live data") -->

7. **Interpretation**: The model was interpreted to understand which features were most important in predicting the outcome.

> ML is an iterative process, so it is common to revise (some of) the above steps multiple times to refine the model. 
:::


## Models and Results

<!-- da aggiornare  con analysis/02a_WB_project_pdo_feat_class_envcat.qmd -->
<!-- usare     models_metrics_recap_k -->

@tbl-ML_results reports the specifications of the models and their performance. 
```{r}
#| label: tbl-ML_results
#| eval: true
#| echo: false
#| output: true
#| tbl-cap: Comparison of models and results for binary outcome

tibble::tribble(
  ~Algorithm,                                         ~Features    , ~Specification  , 
  ~Accuracy, ~ROC_auc, ~Sensitivity, ~Specificity,
  "LASSO logistic regression",                        "Text only" , "env_cat_f2 ~ pdo" , 
  0.759, 0.774, 0.905, 0.473,
  
  "LASSO logistic regression \n(more preprocessing)", "Text only" , "env_cat_f2 ~ pdo" , 
  0.755, 0.785, 0.907, 0.457,   # last fit 
  
  "LASSO logistic regression \n(more preprocessing)", "Text + other predictors" , "env_cat_f2 ~ pdo + sector_f + regionname + FYapprov", 
  0.787, 0.823, 0.903, 0.558,  
  
  "Naïve Bayes classification",                       "Text + other predictors", "env_cat_f2 ~ pdo + sector_f + regionname + FYapprov" , 
  0.671, 0.735, 0.988, 0.052
) %>%  
   kable(format = "html") |> # kable() |> 
   kable_styling(full_width = FALSE)  %>% 
   row_spec(3, background = "#d8e600")  


# 1) env_recipe   
# • Tokenization for: pdo
# • Text filtering for: pdo
# • Term frequency-inverse document frequency with: pdo

# 2) env_stop_recipe
# ── Operations 
# • Tokenization for: pdo
# • Stop word removal for: pdo   x 
# • Text filtering for: pdo
# • Term frequency-inverse document frequency with: pdo


# 3) env_FEAT_recipe
# ── Operations 
# • Tokenization for: pdo
# • Stop word removal for: pdo
# • Text filtering for: pdo
# • Term frequency-inverse document frequency with: pdo
# • Unknown factor level assignment for: sector_f             x 
# • Unknown factor level assignment for: regionname             x 
# • Unknown factor level assignment for: boardapprovalFY             x 
# • Dummy variables from: sector_f, regionname, boardapprovalFY
```

The best model performance was achieved by the LASSO logistic regression model that combined both PDOs' text and some available metadata information to predict the missing `env_cat_f2` in the testing set. The model performed best in most of the key metrics, whereas: 

+ `accuracy` is the proportion of correct predictions made by the model out of all predictions or, in other words, how often the model is correct overall.
+ `ROC-AUC` (*Receiver Operating Characteristic - Area Under the Curve*) goes further by evaluating the model's ability to distinguish between classes across various thresholds. It summarizes how well the model can separates the classes, especially useful when the class distribution is uneven.
+ `sensitivity` (or *true positive rate*) measures the proportion of actual positives (here "high or medium environmental risk class") that are correctly identified by the model.
+ `specificity` (or *true negative rate*) measures the proportion of actual negatives (here, "low or other environmental risk class") that are correctly identified by the model. 

## Performance of the preferred ML model

@fig-ML_final_fit_cm presents the `confusion matrix` for the preferred ML model used to predict the missing *environment risk category* assigned to World Bank projects. This matrix shows the distribution of true and predicted classifications. Ideally, a high-performing model would have most observations (or darker shading) along the diagonal, indicating correct classifications—specifically, `true positives` in the top-left quadrant and `true negatives` in the bottom-right quadrant.

In this case, the model performs well in predicting the `environment risk category` for the **High-Med** group but struggles with the **Low & Other** group. Many of these cases are *incorrectly* classified as **High-Med Risk** (`false positives`). This result is understandable, as the **Low & Other** category is more loosely defined and even includes **Missing** observations (which, in hindsight, could have been excluded from the prediction).

```{r}
#| label: fig-ML_final_fit_cm
#| eval: true
#| echo: false
#| output: true
#| fig.width: 10 # adjust based on text width in inches
#| fig.height: 8 #9.382  # adjust based on text width in inches
#| out.width: "100%" # ensures the figure spans the full text width in HTML
#| fig-align: "center"
#| 
ML_final_fit_cm_p <- readRDS(file = here ("analysis", "output", "figures", "ML_final_fit_cm_p.rds"))
ML_final_fit_cm_p 
```

## Most important features for prediction 
It’s also insightful to examine which coefficients are most influential in the model. This can be done visually through the `feature importance` plot (see @fig-ML_feature_importance).

The feature importance plot displays the top 50 predictors of the `environmental risk (binary) category`, ranked by their impact in a LASSO logistic regression model. For clarity, predictors are divided according to the risk level they predict. As expected, given the structure of the data, words from the PDO text (those variables starting with `pdo_*`) are among the most important predictors. However, other predictors also play a significant role, such as `sector_f_TRANSPORT` (left panel), `regionname`, and `sector_f_FINANCIAL` (right panel).


```{r}
#| label: fig-ML_feature_importance
#| eval: true
#| echo: false
#| output: true
#| fig.width: 10 # adjust based on text width in inches
#| fig.height: 8 #9.382  # adjust based on text width in inches
#| out.width: "100%" # ensures the figure spans the full text width in HTML
#| fig-align: "center"
#| fig-cap: "Top 50 most important features in the preferred ML model"

ML_feature_importance_p <- readRDS(file = here ("analysis", "output", "figures", "ML_feature_importance_p.rds"))
ML_feature_importance_p
```

## Prediction and Interpretation

While the model's prediction performance is not particularly remarkable, it is sufficient to illustrate the potential of this analysis to **enhance the quality of incomplete datasets**. With further improvements in preprocessing, feature engineering, algorithm selection, and hyperparameter tuning, there is significant potential to optimize a similar ML model.

Although not reported here, I also explored predicting a multiclass outcome (`sector`, grouped into 7 levels). However, the results were less favorable compared to the binary classification. This outcome is expected, as multiclass classification is inherently more challenging, particularly with imbalanced data or limited sample sizes.

# CONCLUSIONS

-   This project was primarily a proof-of-concept for learning purposes, so optimizing ML performance and conducting in-depth data analysis were not priorities. Nevertheless, it showcased the potential of applying NLP techniques to unstructured text data, uncovering insights such as:

    -   identifying trends in sector-specific language and topics over time,
    -   revealing unexpected patterns and relationships, like recurring phrases or topics,
    -   enhancing text classification and metadata tagging with ML models,
    -   sparking additional text-based questions that could guide further research.

-   Future steps could include exploring explanations for observed patterns by combining this NLP analysis with other data sources (e.g., World Bank official statements or project data) and experimenting with advanced NLP techniques for topic modeling.
<!-- -   exploring more advanced NLP techniques, such as *Named Entity Recognition (NER)*, *Structural Topic Modeling (STM)*, or *BERTopic*, to enhance the analysis and insights drawn from World Bank project documents. --> 
-   One pain point with this type of work is accessing document data efficiently. Even with the World Bank’s *"Access to Information"* policy, getting programmatic access to their text data is still tricky (no dedicated API, outdated pages, broken links). This could benefit from an approach similar to the accessible, well-maintained World Development Indicators (WDI) data.

-   With all the buzz around AI and Large Language Models (LLMs), this kind of analysis might seem like yesterday's news. But I think there’s still huge, untapped potential for using NLP in development studies, policy analysis, and beyond—especially when it’s backed by domain expertise.

# Acknowledgements

Below are some great resources---especially geared toward [{{< fa brands r-project >}}]{style="color: #3b7697;"} programmers---to learn and implement NLP techniques. 
 
 <!-- 
- [SLIDES intro to Tidymodels](https://workshops.tidymodels.org/)
- [Emil....text recipes](https://emilhvitfeldt.com/blog#category=textrecipes)

+ [NLP demystified](https://www.nlpdemystified.org/course/advanced-preprocessing)  
+ [An Introduction to Quantitative Text Analysis for Linguistics](https://qtalr.com/book/) 
+ [Supervised Machine Learning for Text Analysis in R](https://smltar.com/)  
- [Text Mining with R](https://www.tidytextmining.com/) 
- [Text Analysis with R](https://cengel.github.io/R-text-analysis/textprep.html#detecting-patterns) 
+ [Text Analysis with R](https://cengel.github.io/R-text-analysis/textprep.html#detecting-patterns)
+ [A PAGAMENTO Text Analysis with R for Students of Literature](https://www.matthewjockers.net/text-analysis-with-r-for-students-of-literature/)
+ [Guidef from Penn Libraries](https://guides.library.upenn.edu/penntdm/r)
-->


