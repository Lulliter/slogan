{
  "hash": "2520600291e338655828018c6776e4ba",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Process and merge data - WB operations\"\nauthor: \"Luisa M. Mimmi\"\ndate: \"Last run: 2024-05-10\"\nlang: en\neditor: source\nengine: knitr\n## ------  general Output Options\nexecute:     \n  eval: false    # actually run? \n  echo: true     #  include source code in output\n  warning: false  #  include warning code in output\n  error: false    #  include error code in output\n  output: false   # include output code in output (CHG in BLOCKS)\n  # include: false   # R still runs but code and results DON\"T appear in output  \n  cache: false # normalmnte false\ntoc: true\nfig-cap-location: top\ntbl-cap-location: top\nformat:\n  html:\n    # theme: flatly #spacelab\n    code-fold: false # redundant bc echo false \n    toc-depth: 3\n    toc_float: true\n    toc-location: left\n    toc-title: Outline\n    embed-resources: true # external dependencies embedded (Not in ..._files/)\n  # pdf:\n  #   toc-depth: 2\n  #   toc-title: Indice\n  #   highlight-style: github\n  #   #lang: it\n  #   embed-resources: true # external dependencies embedded (Not in ..._files/)\nformat-links: false\nbibliography: ../bib/slogan.bib\n---\n\n\n\n<i class=\"fa fa-refresh\" style=\"color: firebrick\"></i> Work in progress\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#knitr::opts_chunk$set(include = TRUE, warning = FALSE)\n# Pckgs -------------------------------------\n#if (!require (\"pacman\")) (install.packages(\"pacman\"))\n\n#p_install_gh(\"luisDVA/annotater\")\n#p_install_gh(\"HumanitiesDataAnalysis/hathidy\")\n# devtools::install_github(\"HumanitiesDataAnalysis/HumanitiesDataAnalysis\") \nlibrary(here)\nlibrary(fs)\nlibrary(paint) \nlibrary(tidyverse) \nlibrary(magrittr)\nlibrary(skimr)\nlibrary(scales) \nlibrary(colorspace)\nlibrary(scales)\nlibrary(httr)\nlibrary(DT) # an R interface to the JavaScript library DataTables\nlibrary(knitr)\nlibrary(kableExtra) \nlibrary(flextable) \nlibrary(splitstackshape)  #Stack and Reshape Datasets After Splitting Concatenated Values\nlibrary(tm) # Text Mining Package\nlibrary(tidytext) # Text Mining using 'dplyr', 'ggplot2', and Other Tidy Tools\n# this requires pre-requirsites to install : https://github.com/quanteda/quanteda\nlibrary(quanteda)\nlibrary(igraph)\nlibrary(sjmisc) # Data and Variable Transformation Functions\nlibrary(ggraph) # An Implementation of Grammar of Graphics for Graphs and Networks\nlibrary(widyr) # Widen, Process, then Re-Tidy Data\nlibrary(SnowballC) # Snowball Stemmers Based on the C 'libstemmer' UTF-8 Library\n# library(#HumanitiesDataAnalysis, # Data and Code for Teaching Humanities Data Analysis\nlibrary(sentencepiece) # Text Tokenization using Byte Pair Encoding and Unigram Modelling\n \n# extra steo needed to install github version \n#if (!require(\"devtools\")) install.packages(\"devtools\")\n#library(devtools)\n#install_github(\"husson/FactoMineR\")     FAILED !!!!!!\n# library(FactoMineR)\n#library(factoextra)\n\n# Plot Theme(s) -------------------------------------\n#source(here(\"R\", \"ggplot_themes.R\"))\nggplot2::theme_set(theme_minimal())\n# color paletts -----\nmycolors_gradient <- c(\"#ccf6fa\", \"#80e8f3\", \"#33d9eb\", \"#00d0e6\", \"#0092a1\")\nmycolors_contrast <- c(\"#E7B800\", \"#a19100\", \"#0084e6\",\"#005ca1\", \"#e60066\" )\n\n\n# Function(s) -------------------------------------\n\n# Data -------------------------------------\n\n# -------------------- {cut bc made too heavy} -------------------------------------\n# # Tables [AH knit setup when using kbl() ]------------------------------------\n# knit_print.data.frame <- function(x, ...) {\n#   res <- paste(c('', '', kable_styling(kable(x, booktabs = TRUE))), collapse = '\\n')\n#   asis_output(res)\n# }\n# \n# registerS3method(\"knit_print\", \"data.frame\", knit_print.data.frame)\n# registerS3method(\"knit_print\", \"grouped_df\", knit_print.data.frame)\n```\n:::\n\n\n# World Bank Projects & Operations:\n\n## Data sources:\n\n**World Bank Projects & Operations**: [Data Catalog](https://datacatalog.worldbank.org/search/dataset/0037800> <https://datacatalog.worldbank.org/search/dataset/0037800/World-Bank-Projects---Operations)\nhttps://projects.worldbank.org/en/projects-operations/project-search \n\n-   Accessibility Classification: **public** under [Creative Commons Attribution 4.0](https://datacatalog.worldbank.org/public-licenses?fragment=cc)\n\n## Raw data\nI retrieved manually ALL WB projects approved *between FY 1973 and 2023 (last FY incomplete)* on 09/22/2022 (WDRs go from 1978-2022)\nusing this [example url](https://projects.worldbank.org/en/projects-operations/projects-list?str_fiscalyear=1979&end_fiscalyear=1979&os=0) and saved individual `.xlsx` files in `data/raw_data/`\n\nIn file `_my_stuff/WBprojects_1973-2023_data-ingestion.Rmd` I did these operations:\n+ Load all `.xlsx` files separately \n+ Save objs in folder as `.Rds` files separately `data/raw_data/projects`\n\n\n#### --- Load all `.Rds` files previosuly saved  \n\n::: {.cell}\n\n```{.r .cell-code}\n# --- define directory path\nfile_path <- here(\"data\",\"raw_data\", \"projects/\")\n\n# --- get a character vector of the names of files\nRds_file_names <- file_path %>% list.files(. , \n                         pattern = \".Rds\",\n                         full.names = FALSE) \n\n# df_paths\ndatasets_l  <-  as.list(list.files(path = here(\"data\", \"raw_data\", \"projects\"),\n                                   pattern = \"\\\\.Rds$\"))\n \n\n# # add a complete path with purrr\n# datasets_path_l  <-  purrr::map(datasets_l,\n#                                 ~ paste0(here(\"data\", \"raw_data\", \"projects/\"), .x))\n\n\n# ---- Load everything into the Global Environment\npurrr::map(.x =  Rds_file_names,\n           .f =  function(datasets_l){ # iterate through each file name\n             # Assign a Value to a Name       \n             assign(x = str_remove(datasets_l,# new value in GlobEnv\n                                   \".Rds\"), # Remove file extension \n                    # inputvalue in my folder to be assigned to x\n                    value = readRDS(paste0(\"data/raw_data/projects/\", datasets_l)),\n                    # the environment to use\n                    envir = .GlobalEnv)\n           }\n           )\n\n# names  \ndf_name_list <- ls(pattern = \"00$\")\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# df_list <-  list(\n# FY_1973_WB_Projects_1_500 = '1973_WB_Projects_1_500',\n# FY_1974_WB_Projects_1_500 = '1974_WB_Projects_1_500',\n# FY_1975_WB_Projects_1_500 = '1975_WB_Projects_1_500',\n# FY_1976_WB_Projects_1_500 = '1976_WB_Projects_1_500',\n# FY_1977_WB_Projects_1_500 = '1977_WB_Projects_1_500',\n# FY_1978_WB_Projects_1_500 = '1978_WB_Projects_1_500',\n# FY_1979_WB_Projects_1_500 = '1979_WB_Projects_1_500',\n# FY_1980_WB_Projects_1_500 = '1980_WB_Projects_1_500',\n# FY_1981_WB_Projects_1_500 = '1981_WB_Projects_1_500',\n# FY_1982_WB_Projects_1_500 = '1982_WB_Projects_1_500',\n# FY_1983_WB_Projects_1_500 = '1983_WB_Projects_1_500',\n# FY_1984_WB_Projects_1_500 = '1984_WB_Projects_1_500',\n# FY_1985_WB_Projects_1_500 = '1985_WB_Projects_1_500',\n# FY_1986_WB_Projects_1_500 = '1986_WB_Projects_1_500',\n# FY_1987_WB_Projects_1_500 = '1987_WB_Projects_1_500',\n# FY_1988_WB_Projects_1_500 = '1988_WB_Projects_1_500',\n# FY_1989_WB_Projects_1_500 = '1989_WB_Projects_1_500',\n# FY_1990_WB_Projects_1_500 = '1990_WB_Projects_1_500',\n# FY_1991_WB_Projects_1_500 = '1991_WB_Projects_1_500',\n# FY_1992_WB_Projects_1_500 = '1992_WB_Projects_1_500',\n# FY_1993_WB_Projects_1_500 = '1993_WB_Projects_1_500',\n# FY_1994_WB_Projects_1_500 = '1994_WB_Projects_1_500',\n# FY_1995_WB_Projects_1_500 = '1995_WB_Projects_1_500',\n# FY_1996_WB_Projects_1_500 = '1996_WB_Projects_1_500',\n# FY_1997_WB_Projects_1_500 = '1997_WB_Projects_1_500',\n# FY_1998_WB_Projects_1_500 = '1998_WB_Projects_1_500',\n# FY_1999_WB_Projects_1_500 = '1999_WB_Projects_1_500',\n# FY_2000_WB_Projects_1_500 = '2000_WB_Projects_1_500',\n# FY_2001_WB_Projects_1_500 = '2001_WB_Projects_1_500',\n# FY_2002_WB_Projects_1_500 = '2002_WB_Projects_1_500',\n# FY_2003_WB_Projects_1_500 = '2003_WB_Projects_1_500',\n# FY_2004_WB_Projects_1_500 = '2004_WB_Projects_1_500',\n# FY_2005_WB_Projects_1_500 = '2005_WB_Projects_1_500',\n# FY_2006_WB_Projects_1_500 = '2006_WB_Projects_1_500',\n# FY_2007_WB_Projects_1_500 = '2007_WB_Projects_1_500',\n# FY_2007_WB_Projects_500_1000 = '2007_WB_Projects_500_1000',\n# FY_2008_WB_Projects_1_500   = '2008_WB_Projects_1_500',\n# FY_2008_WB_Projects_500_1000 = '2008_WB_Projects_500_1000',\n# FY_2009_WB_Projects_1_500   = '2009_WB_Projects_1_500',\n# FY_2009_WB_Projects_500_1000 = '2009_WB_Projects_500_1000',\n# FY_2010_WB_Projects_1_500   = '2010_WB_Projects_1_500',\n# FY_2010_WB_Projects_500_1000 = '2010_WB_Projects_500_1000',\n# FY_2011_WB_Projects_1_500   = '2011_WB_Projects_1_500',\n# FY_2011_WB_Projects_500_1000 = '2011_WB_Projects_500_1000',\n# FY_2012_WB_Projects_1_500   = '2012_WB_Projects_1_500',\n# FY_2012_WB_Projects_500_1000 = '2012_WB_Projects_500_1000',\n# FY_2013_WB_Projects_1_500   = '2013_WB_Projects_1_500',\n# FY_2014_WB_Projects_1_500   = '2014_WB_Projects_1_500',\n# FY_2014_WB_Projects_500_1000 = '2014_WB_Projects_500_1000',\n# FY_2015_WB_Projects_1_500   = '2015_WB_Projects_1_500',\n# FY_2015_WB_Projects_500_1000 = '2015_WB_Projects_500_1000',\n# FY_2016_WB_Projects_1_500   = '2016_WB_Projects_1_500',\n# FY_2017_WB_Projects_1_500   = '2017_WB_Projects_1_500',\n# FY_2017_WB_Projects_500_1000 = '2017_WB_Projects_500_1000',\n# FY_2018_WB_Projects_1_500   = '2018_WB_Projects_1_500',\n# FY_2019_WB_Projects_1_500   = '2019_WB_Projects_1_500',\n# FY_2019_WB_Projects_500_1000 = '2019_WB_Projects_500_1000',\n# FY_2020_WB_Projects_1_500   = '2020_WB_Projects_1_500',\n# FY_2020_WB_Projects_500_1000 = '2020_WB_Projects_500_1000',\n# FY_2021_WB_Projects_1_500   = '2021_WB_Projects_1_500',\n# FY_2021_WB_Projects_500_1000 = '2021_WB_Projects_500_1000',\n# FY_2022_WB_Projects_1_500   = '2022_WB_Projects_1_500',\n# FY_2022_WB_Projects_500_1000 = '2022_WB_Projects_500_1000',\n# FY_2023_WB_Projects_1_500   = '2023_WB_Projects_1_500',\n# FY_2023_WB_Projects_500_1000 = '2023_WB_Projects_500_1000'\n# )\n```\n:::\n\n\n\n## Transform raw data\n\n> Following [SO](https://stackoverflow.com/questions/42028710/add-new-variable-to-list-of-data-frames-with-purrr-and-mutate-from-dplyr)\n\n#### --- Add FY column to all taking it from file name\n\n::: {.cell}\n\n```{.r .cell-code}\n#### ----- create a list of the df of interest ----- \n# actual DF in a list \ndf_list <- Filter(is.data.frame, as.list(.GlobalEnv))\nstr(df_list[1])\nlength(df_list)\n\n#### ----- Add col to each DF in teh list ----- \n\n#### --- modo 1 [nope]\n# [function] write as .Rds \n# for (i in 1:n) {\n#   Rds_file_names[i]$FY <- NA\n# }\n# \n#  Rds_file_names[1]$FY <-  NA\n# `1978_WB_Projects_1_500`$FY <- NA\n\n#### --- modo 2.a [si] {map2}\ndf_list2 <- map2(# map over 2 arguments GIVES LIST OF DFs\n  df_list, names(df_list),\n  ~ mutate(.x, FY = .y)\n  ) \n\n# #### --- modo 2.b [si] {map2_df}\ndf_all <- map2_df(# map over 2 arguments GIVES 1 MEGA DF\n  df_list, names(df_list),\n  ~ mutate(.x, FY = .y)\n  )\npaint(`1978_WB_Projects_1_500`)\npaint(`2023_WB_Projects_500_1000`)\npaint(df_all)\n```\n:::\n\n \n \n#### --- [CLean up]  \n\n::: {.cell}\n\n```{.r .cell-code}\n#rm(list=setdiff(ls(), c(\"df_all\")))\n```\n:::\n\n \n#### --- Correct columns' type \n\n::: {.cell}\n\n```{.r .cell-code}\nproj <- df_all %>% \n  janitor::clean_names() %>% \n  relocate(fy, .after = project_id) %>% \n  relocate( board_approval_date , .after = fy)  %>% \n  relocate( project_closing_date , .after = board_approval_date) %>% \n  mutate(FY = as.integer(str_sub(fy, 1,4))) %>% \nrelocate(FY, .before = fy) %>% \n  select(-fy) %>% \n  # delete empty columns \n  janitor::remove_empty(., which = \"cols\") %>% \n  # convert dates \nmutate(date_approval = lubridate::ymd_hms(board_approval_date)) %>% \nrelocate(date_approval, .after = board_approval_date) %>% \n  mutate(date_close = lubridate::mdy_hms(project_closing_date))%>% \nrelocate(date_close, .after = project_closing_date) %>% \n  select(-board_approval_date, -project_closing_date) %>% \n  # numeric \nmutate_at(c('current_project_cost', 'total_ida_and_ibrd_commitment', 'grant_amount'), as.numeric) %>% \n# factors \nmutate_at(c(\"region\", \"country\",\"project_status\", \"consultant_services_required\", \"lending_instrument\" , \"environmental_assessment_category\", \"environmental_and_social_risk\"), as.factor)  %>% \n  relocate(project_url, .after = project_name) %>% \n  mutate (PDO_miss = if_else(is.na(project_development_objective), \"miss\", \"avail\"))\n \n# Factors <- c(\"region\", \"country\",\"project_status\", \"consultant_services_required\", \"lending_instrument\" , \"environmental_assessment_category\", \"environmental_and_social_risk\")\n# proj[ ,Factors] <-  lapply( proj[ ,Factors], as.factor)\n```\n:::\n\n\n\n#### --- Check missing stuff\n\n::: {.cell}\n\n```{.r .cell-code}\nskim(proj) %>%\ndplyr::select(skim_type, skim_variable, n_missing)\n\n# check missing approval dates \nnodate <- proj %>% \n  filter(is.na(date_approval)) # 2374 of which 1750 Dropped + 585 pipeline + 39 closed\n\n# check missing PDOs\ntable(proj$PDO_miss, proj$FY)\ntable( proj$PDO_miss, proj$project_status)\n\nproj_PDO <- proj %>% \n  select (FY, project_id,project_name, project_status, project_url, project_development_objective) %>% \n  filter(FY == '2001')\n```\n:::\n\n\n\n## USABLE INFO RECAP\n\n> + missing approval dates 2374 \n  + of which 1750 Dropped + 585 pipeline + 39 closed...could drop \n> + WAY TOO MANY MISSING PDO 8708!!! \n    + none before 2001\n    + go check website.... \n> + SEEMS AT LEAST SECTOR 1 IS THERE... \n> + unsure about $ amounts... \n\n#### --- Can I recover/validate PDOs? \n\n\n::: {.cell}\n\n:::\n\n\n\n#### --- Can I recover/validate themes? \n\n\n\n#### --------------  NEXT ------------------\n\n\n# Reference Tutorials\n\n@robinson_1_2022 [Benjamin Soltoff: Computing 4 Social Sciences - API](https://cfss.uchicago.edu/syllabus/getting-data-from-the-web-api-access/) [Benjamin Soltoff: Computing 4 Social Sciences - text analysis](https://cfss.uchicago.edu/syllabus/text-analysis-fundamentals-and-sentiment-analysis/)\n\n[Ben Schmidt Book Humanities Crurse](https://hdf.benschmidt.org/R/) [Ben Schmidt Book Humanities](http://benschmidt.org/HDA/texts-as-data.html)\n\n[tidyTuesday cast on tidytext](https://github.com/dgrtwo/data-screencasts/tree/master/screencast-annotations)\n\n  1. ✔️ [MEDIUM articles: common words, pairwise correlations - 2018-12-04](https://www.youtube.com/watch?v=C69QyycHsgE)\n  2. [TidyTuesday Tweets -  2019-01-07](https://www.youtube.com/watch?v=KE9ItC3doEU)\n  3. [Wine Ratings - 2019-05-31](https://www.youtube.com/watch?v=AQzZNIyjyWM) Lasso regression | sentiment lexicon,\n  4. [Simpsons Guest Stars \t2019-08-30](https://www.youtube.com/watch?v=EYuuAGDeGrQ) geom_histogram\n  5. [Horror Movies \t2019-10-22](https://www.youtube.com/watch?v=yFRSTlk3kRQ) explaining glmnet package | Lasso regression\n  6. [The Office \t2020-03-16](https://www.youtube.com/watch?v=_IvAubTDQME) geom_text_repel from ggrepel | glmnet package to run a cross-validated LASSO regression\n  7. [Animal Crossing \t2020-05-05](https://www.youtube.com/watch?v=Xt7ACiedRRI) Using geom_line and geom_point to graph ratings over time | geom_text to visualize what words are associated with positive/negative reviews |topic modelling\n\n# Connections\n\nFollowing the example of [David Robinson on HN titles](http://varianceexplained.org/r/hn-trends/)\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}