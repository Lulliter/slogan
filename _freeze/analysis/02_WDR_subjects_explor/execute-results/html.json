{
  "hash": "0ce951d1ca063ef7496d4a5d1103e219",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Process and merge data - WDR subjects\"\nauthor: \"Luisa M. Mimmi\"\ndate: \"Last run: 2024-05-10\"\nlang: en\neditor: source\nengine: knitr\n## ------  general Output Options\nexecute:     \n  eval: false    # actually run? \n  echo: true     #  include source code in output\n  warning: false  #  include warning code in output\n  error: false    #  include error code in output\n  output: false   # include output code in output (CHG in BLOCKS)\n  # include: false   # R still runs but code and results DON\"T appear in output  \n  cache: false # normalmnte false\ntoc: true\nfig-cap-location: top\ntbl-cap-location: top\nformat:\n  html:\n    # theme: flatly #spacelab\n    code-fold: false # redundant bc echo false \n    toc-depth: 3\n    toc_float: true\n    toc-location: left\n    toc-title: Outline\n    embed-resources: true # external dependencies embedded (Not in ..._files/)\n  # pdf:\n  #   toc-depth: 2\n  #   toc-title: Indice\n  #   highlight-style: github\n  #   #lang: it\n  #   embed-resources: true # external dependencies embedded (Not in ..._files/)\nformat-links: false\nbibliography: ../bib/slogan.bib\n---\n\n\n\n<i class=\"fa fa-refresh\" style=\"color: firebrick\"></i> Work in progress\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#knitr::opts_chunk$set(include = TRUE, warning = FALSE)\n# Pckgs -------------------------------------\n#if (!require (\"pacman\")) (install.packages(\"pacman\"))\n\n#p_install_gh(\"luisDVA/annotater\")\n#p_install_gh(\"HumanitiesDataAnalysis/hathidy\")\n# devtools::install_github(\"HumanitiesDataAnalysis/HumanitiesDataAnalysis\") \nlibrary(here)\nlibrary(fs)\nlibrary(paint) \nlibrary(tidyverse) \nlibrary(magrittr)\nlibrary(skimr)\nlibrary(scales) \nlibrary(colorspace)\nlibrary(httr)\nlibrary(DT) # an R interface to the JavaScript library DataTables\nlibrary(knitr)\nlibrary(kableExtra) \nlibrary(flextable) \nlibrary(splitstackshape)  #Stack and Reshape Datasets After Splitting Concatenated Values\nlibrary(tm) # Text Mining Package\nlibrary(tidytext) # Text Mining using 'dplyr', 'ggplot2', and Other Tidy Tools\n# this requires pre-requirsites to install : https://github.com/quanteda/quanteda\nlibrary(quanteda)\nlibrary(igraph)\nlibrary(sjmisc) # Data and Variable Transformation Functions\nlibrary(ggraph) # An Implementation of Grammar of Graphics for Graphs and Networks\nlibrary(widyr) # Widen, Process, then Re-Tidy Data\nlibrary(SnowballC) # Snowball Stemmers Based on the C 'libstemmer' UTF-8 Library\n# library(#HumanitiesDataAnalysis, # Data and Code for Teaching Humanities Data Analysis\nlibrary(sentencepiece) # Text Tokenization using Byte Pair Encoding and Unigram Modelling\nlibrary(sysfonts) \nlibrary(ggdendro)\nlibrary(network)\nlibrary(GGally)\n\nlibrary(topicmodels)                #  with dep   FAILED !!!!!!\n\n# extra steo needed to install github version \n#if (!require(\"devtools\")) install.packages(\"devtools\")\n#library(devtools)\n#install_github(\"husson/FactoMineR\")     FAILED !!!!!!\n# library(FactoMineR)\n#library(factoextra)\n\n# Plot Theme(s) -------------------------------------\n#source(here(\"R\", \"ggplot_themes.R\"))\nggplot2::theme_set(theme_minimal())\n# color paletts -----\nmycolors_gradient <- c(\"#ccf6fa\", \"#80e8f3\", \"#33d9eb\", \"#00d0e6\", \"#0092a1\")\nmycolors_contrast <- c(\"#E7B800\", \"#a19100\", \"#0084e6\",\"#005ca1\", \"#e60066\" )\n\n\n# Function(s) -------------------------------------\n\n# Data -------------------------------------\n\n# -------------------- {cut bc made too heavy} -------------------------------------\n# # Tables [AH knit setup when using kbl() ]------------------------------------\n# knit_print.data.frame <- function(x, ...) {\n#   res <- paste(c('', '', kable_styling(kable(x, booktabs = TRUE))), collapse = '\\n')\n#   asis_output(res)\n# }\n# \n# registerS3method(\"knit_print\", \"data.frame\", knit_print.data.frame)\n# registerS3method(\"knit_print\", \"grouped_df\", knit_print.data.frame)\n```\n:::\n\n\n# World Development Reports (WRDs)\n\n-   DATA <https://datacatalog.worldbank.org/search/dataset/0037800>\n-   INSTRUCTIONS <https://documents.worldbank.org/en/publication/documents-reports/api>\n-   Following: [@kaye_ella_2019; @robinson_words_2017; @robinson_1_2022]\n\n## I) Pre-processing \n\n\n### I.ii) -- Set stopwords [more...]\n\n::: {.cell}\n\n```{.r .cell-code}\n# --- alt stop words\n# mystopwords <- tibble(word = c(\"eq\", \"co\", \"rc\", \"ac\", \"ak\", \"bn\", \n#                                    \"fig\", \"file\", \"cg\", \"cb\", \"cm\",\n#                                \"ab\", \"_k\", \"_k_\", \"_x\"))\n\n# --- set up stop words\nstop_words <- as_tibble(stop_words) %>% # in the tidytext dataset \n  add_row(word = \"WDR\", lexicon = NA_character_) %>%\n  # add_row(word = \"world\", lexicon = NA_character_) %>%\n  add_row(word = \"report\", lexicon = NA_character_) %>%\n  # add_row(word = \"development\", lexicon = NA_character_) %>%\n  add_row(word = \"1978\", lexicon = NA_character_) %>%\n  add_row(word = \"1979\", lexicon = NA_character_) %>%\n  add_row(word = \"1980\", lexicon = NA_character_) %>%\n  add_row(word = \"1981\", lexicon = NA_character_) %>%\n  add_row(word = \"1982\", lexicon = NA_character_) %>%\n  add_row(word = \"1983\", lexicon = NA_character_) %>%\n  add_row(word = \"1984\", lexicon = NA_character_) %>%\n  add_row(word = \"1985\", lexicon = NA_character_) %>%\n  add_row(word = \"1986\", lexicon = NA_character_) %>%\n  add_row(word = \"1987\", lexicon = NA_character_) %>%\n  add_row(word = \"1988\", lexicon = NA_character_) %>%\n  add_row(word = \"1989\", lexicon = NA_character_) %>%\n  add_row(word = \"1990\", lexicon = NA_character_) %>%\n  add_row(word = \"1991\", lexicon = NA_character_) %>%\n  add_row(word = \"1992\", lexicon = NA_character_) %>%\n  add_row(word = \"1993\", lexicon = NA_character_) %>%\n  add_row(word = \"1994\", lexicon = NA_character_) %>%\n  add_row(word = \"1995\", lexicon = NA_character_) %>%\n  add_row(word = \"1996\", lexicon = NA_character_) %>%\n  add_row(word = \"1997\", lexicon = NA_character_) %>%\n  add_row(word = \"1998\", lexicon = NA_character_) %>%\n  add_row(word = \"1999\", lexicon = NA_character_) %>%\n  add_row(word = \"2000\", lexicon = NA_character_) %>%\n  add_row(word = \"2001\", lexicon = NA_character_) %>%\n  add_row(word = \"2002\", lexicon = NA_character_) %>%\n  add_row(word = \"2003\", lexicon = NA_character_) %>%\n  add_row(word = \"2004\", lexicon = NA_character_) %>%\n  add_row(word = \"2005\", lexicon = NA_character_) %>%\n  add_row(word = \"2006\", lexicon = NA_character_) %>%\n  add_row(word = \"2007\", lexicon = NA_character_) %>%\n  add_row(word = \"2008\", lexicon = NA_character_) %>%\n  add_row(word = \"2009\", lexicon = NA_character_) %>%\n  add_row(word = \"2010\", lexicon = NA_character_) %>%\n  add_row(word = \"2011\", lexicon = NA_character_) %>%\n  add_row(word = \"2012\", lexicon = NA_character_) %>%\n  add_row(word = \"2013\", lexicon = NA_character_) %>%\n  add_row(word = \"2014\", lexicon = NA_character_) %>%\n  add_row(word = \"2015\", lexicon = NA_character_) %>%\n  add_row(word = \"2016\", lexicon = NA_character_) %>%\n  add_row(word = \"2017\", lexicon = NA_character_) %>%\n  add_row(word = \"2018\", lexicon = NA_character_) %>%\n  add_row(word = \"2019\", lexicon = NA_character_) %>%\n  add_row(word = \"2020\", lexicon = NA_character_) %>%\n  add_row(word = \"2021\", lexicon = NA_character_) %>%\n  add_row(word = \"2022\", lexicon = NA_character_) %>% \n  filter (word != \"changes\") %>% \n   filter (word != \"value\") %>% \n   filter (word != \"member\") %>% \n   filter (word != \"part\") %>% \n   filter (word != \"possible\") %>% \n   filter (word != \"point\") %>% \n   filter (word != \"present\") %>% \n   filter (word != \"zero\") %>% \n     filter (word != \"young\") %>% \n     filter (word != \"old\") %>% \n     filter (word != \"trying\") \n\n# --- set up stop words stemmed\nstop_words_stem <- stop_words  %>% \nmutate (word = SnowballC::wordStem(word ))\n```\n:::\n\n\n## II) Data (ingestion), loading & cleaning\n\nIngestion of WDR **basic metadata** was done in `./_my_stuff/WDR-data-ingestion.Rmd` and the result saved as `WDR.rds` \\<-- (Being somewhat computational intensive, I only did it once.)\n\n> \n    + **WDR = tibble [45, 8]**\n    + **doc_mt_identifier_1** chr oai:openknowledge.worldbank.org:109~ \n    + **doc_mt_identifier_2** chr http://www-wds.worldbank.org/extern~ \n    + **doc_mt_title**        chr Development Economics through the  ~ \n    + **doc_mt_date   **      chr 2012-03-19T10:02:25Z 2012-03-19T19:~ \n    + **doc_mt_creator **     chr Yusuf, Shahid World Bank World Bank~ \n    + **doc_mt_subject**      chr ABSOLUTE POVERTY AGGLOMERATION BENE~ \n    + **doc_mt_description**  chr The World Development Report (WDR) ~ \n    + **doc_mt_set_spec**     chr oai:openknowledge.worldbank.org:109~ \n\nIngestion of WDR **lists of subjects** was available among metadata but presented issues (difficulty to extract, many records with repetition,apparently wrong) so I reconstructed them manually in `data/raw_data/WDR_subjects_corrected2010_2011.xlsx` taking them from site https://elibrary.worldbank.org/ which lists **keywords** correctly [Es 2022 WDR](https://elibrary.worldbank.org/doi/abs/10.1596/978-1-4648-1730-4)\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# # WRD metadata taken with API get (issues) \n# WDR <- readr::read_rds(here::here(\"data\", \"raw_data\", \"WDR.rds\" )) %>% \n#   # Extract only the portion of string AFTER the backslash {/}\n#   mutate(id = as.numeric(stringr::str_extract(doc_mt_identifier_1, \"[^/]+$\"))) %>% \n#   dplyr::relocate(id, .before = doc_mt_identifier_1) %>% \n#   mutate(url_keys = paste0(\"https://openknowledge.worldbank.org/handle/10986/\", id , \"?show=full\"))  %>% \n#  # eliminate NON WDR book\n#   dplyr::filter(id != \"2586\") \n# \n# # WRD subject/date_issued taken by manual review \n# WDR_subjects <- readxl::read_excel(here::here(\"data\", \"raw_data\", \n#                                               \"WDR_subjects_corrected2010_2011.xlsx\")) %>%\n#   drop_na(id) %>% \n#  # eliminate NON WDR book\n#   dplyr::filter(id != \"2586\") \n# \n# # delete empty cols \n# ColNums_NotAllMissing <- function(df){ # helper function\n#   as.vector(which(colSums(is.na(df)) != nrow(df)))\n# }\n# \n# WDR_subjects <- WDR_subjects  %>% \n#   select(ColNums_NotAllMissing(.))\n#  # # convert all columns that start with \"subj_\" to lowercase\n#  # WDR_subjects[3:218] <- sapply(WDR_subjects[3:218], function(x) tolower(x))\n# \n# # join\n# WDR_com <- left_join(WDR, WDR_subjects, by = \"id\") %>% \n#   dplyr::relocate(date_issued, .before = id ) %>% \n#   # drop useles clmns \n#   dplyr::select(#-doc_mt_identifier_1, \n#                 -doc_mt_identifier_2, -doc_mt_date, \n#                 -doc_mt_subject, -doc_mt_creator, -doc_mt_set_spec) %>% \n#   # dplyr::relocate(url_keys, .after = subj_216 ) %>% \n#   dplyr::rename(abstract = doc_mt_description) %>% \n#   # correct titles -> portion after {:}\n#   dplyr::mutate(., title = str_extract(doc_mt_title,\"[^:]+$\")) %>% \n#   dplyr::relocate(title, .after = id)  %>% \n#   dplyr::rename(title_miss = doc_mt_title) %>% \n#   dplyr::mutate(title_miss = case_when(\n#     str_starts(title, \"World Development Report\") ~ \"Y\",\n#     TRUE ~ NA_character_) \n#   ) %>% \n#   dplyr::mutate(subject_miss = if_else(is.na(subj_1), \n#                                        \"Y\", \n#                                        NA_character_)) %>% \n#   dplyr::relocate(subject_miss, .after = title_miss)    %>% \n#   dplyr::relocate(ISBN, .after = id)    \n#   \n# #paint(WDR_com)\n# \n# # convert all columns that start with \"subj_\" to lowercase (maybe redundant)\n# WDR_com[, grep(\"^subj_\", names(WDR_com))] <- sapply(WDR_com[, grep(\"^subj_\", names(WDR_com))], function(x) tolower(x))\n# \n# # combine all `subj_...` vars into a vector separated by comma\n# col_subj <- names(WDR_com[, grep(\"^subj_\", names(WDR_com))] )\n# \n# WDR_com <- WDR_com %>% tidyr::unite(\n#   col = \"all_subj\", \n#   subj_1:subj_46, \n#   sep = \",\",\n#   remove = FALSE,\n#   na.rm = TRUE) %>% \n#   arrange(date_issued)\n# \n# #paint(WDR_com)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nwdr <- readr::read_rds(here::here(\"data\", \"derived_data\", \"wdr.rds\" ))\npaint(wdr)\n```\n:::\n\n\n\n### I.iii) > > Part of Speech Tagging \nTagging segments of speech for part-of-speech (nouns, verbs, adjectives, etc.) or entity recognition (person, place, company, etc.)\nhttps://m-clark.github.io/text-analysis-with-R/part-of-speech-tagging.html \n\n\n#### -- tagging with `cleanNLP`\nAH: https://datavizs22.classes.andrewheiss.com/example/13-example/#sentiment-analysis\n\nHere’s the general process for tagging (or “annotating”) text with the cleanNLP package:\n\n   1. Make a dataset where one column is the id (line number, chapter number, book+chapter, etc.), and another column is the text itself.\n   2. Initialize the NLP tagger. You can use any of these:\n      + `cnlp_init_udpipe()`: Use an R-only tagger that should work without installing anything extra (a little slower than the others, but requires no extra steps!)\n      + `cnlp_init_spacy()`: Use spaCy (if you’ve installed it on your computer with Python)\n      + `cnlp_init_corenlp()`: Use Stanford’s NLP library (if you’ve installed it on your computer with Java)\n   3. Feed the data frame from step 1 into the cnlp_annotate() function and wait.\n   4. Save the tagged data on your computer so you don’t have to re-tag it every time.\n\n#### -------------- [TITLES ?]  ------------------\n\n## IV.i) Tokenization\nFollowing: http://varianceexplained.org/r/hn-trends/\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# unnest titles \ntitle_words <- wdr %>%                           # 44 obs X 5 var \n  mutate (year = date_issued) %>% \n  # isolate necessary \n  dplyr::select(id, year, decade, title, altmetric ) %>% # isolate titles\n  arrange(desc(year)) %>%\n  # (redundant) Select only unique/distinct rows from a data frame \n  # dplyr::distinct(title, .keep_all = TRUE) %>%\n  # ----- tidytext’s unnest_tokens function = {turn titles in individual words}\n  unnest_tokens(output = word, \n                input = title, \n                drop = FALSE, # Whether original input column should get dropped\n                to_lower = T, # (implicit) otherwise cannot match the stop_words\n                strip_punc = TRUE) %>%            # 196 obs \n   # ---- token processing\n  # [Optional] stems words\n  mutate(word = SnowballC::wordStem(word)) %>% # **\n  # [Optional] sometimes needed to graph \n  mutate(title = factor(title, ordered = TRUE))  %>%\n  mutate(year = factor(year, ordered = TRUE)) %>% # 196 obs X 5 var \n  # creates a data frame with one row per word per post!!!\n  # Select only unique/distinct rows from a data frame (if not unique keep first) | keep all vars\n  distinct(id, word, .keep_all = TRUE) %>% # (redundant, no repetition of words in title) \n  # delete stop words (also previously stemmed)\n  anti_join(stop_words_stem, by = \"word\") %>% # ** # 101 obs | 95 if stemmed \n  filter(str_detect(word, \"[^\\\\d]\")) %>%\n  # calculate totals by word across all titles (eg agricultur = in 3 WDR) \n  group_by(word) %>%\n  mutate(word_total = n()) %>%\n  ungroup()  \n```\n:::\n\n\n#### --- Plot/save most common words in *ALL* 44 TITLES \n\n::: {.cell}\n\n```{.r .cell-code}\n# this is the same as title_words$word_total, but just the totals NO REPETITION\nword_counts <- title_words %>%\n  # Count observations by group(ed words)\n  count(word, sort = TRUE)\n\n# plot \np_most_common_word_in_title <- word_counts %>%\n   head(30) %>%\n  # filter ( n >1) %>% \n  mutate(word = reorder(word, n)) %>%\n  ggplot(aes(word, n)) +\n  # geom_col() uses stat_identity(): it leaves the data as is.\n  geom_col(fill = \"lightblue\") +\n  scale_y_continuous(labels = comma_format()) +\n  coord_flip() +\n  labs(title = \"50 most common words in 44 World Development Reports' titles\",\n       subtitle = \"[stemmed & stop words removed]\" #, y = \"# of uses\"\n       )\n\np_most_common_word_in_title %T>% \n  print() %T>%\n  ggsave(., filename = here(\"analysis\", \"output\", \"figures\", \"most_common_word_in_title.pdf\"),\n         #width = 4, height = 2.25, units = \"in\",\n         device = cairo_pdf) %>% \n  ggsave(., filename = here(\"analysis\", \"output\", \"figures\", \"most_common_word_in_title.png\"),\n         #width = 4, height = 2.25, units = \"in\", \n         type = \"cairo\", dpi = 300)  \n```\n:::\n\n\nWhat are specific words that get a high altmetric ? \nhttps://youtu.be/C69QyycHsgE\n\n\n::: {.cell}\n\n```{.r .cell-code}\nalt_title_words <-  title_words %>%\n  # Count observations by group(ed words)\n  add_count(word ) %>% \n  group_by(word) %>% \n  summarise(median_alt = median (altmetric),\n            # compresses the scale and you go up by smaller increments \n            geometric_mean_alt = exp(mean(log(altmetric + 1))) -1,\n            occurrences = n()) %>% \n  arrange(desc(median_alt))  \n  \nalt_title_words\n```\n:::\n\n\n#### --- Plot/save most common words in ALL 44 TITLES - over time\nWhat words and topics have become more frequent, or less frequent, over time? These could give us a sense of the changing focus in dev econ, and let us predict what topics will continue to grow in relevance.\n\nTo achieve this, we’ll first count the occurrences of words in titles by decades\n\n::: {.cell}\n\n```{.r .cell-code}\nwdr_decade <- wdr %>% \n  mutate (year = date_issued) %>% \n  # isolate necessary \n  dplyr::select(id, year, decade, title ) %>%  \n  arrange(desc(year))  \n  \n  \n# 1) obtain \"decade_total\"\ntitle_per_decade <- wdr_decade   %>%\n  group_by(decade) %>%\n  summarize(decade_total = n()) %>% \n  ungroup()\n\n# 2) obtain count \"n\" <--  (group BY word*decade) \nword_decade_counts <- title_words %>%\n  # filter(word_total >= 1000) %>%\n  count(word, decade) %>%\n  complete(word, decade, fill = list(n = 0)) %>% \n  # join with 1)  \n  inner_join(title_per_decade, by = \"decade\") %>%\n  mutate(percent = n / decade_total) %>% \n  # weird step to re-attach year \n  inner_join(title_words, by = c(\"word\", \"decade\")) %>% \n  select (-id, -title,  word,  word_total, decade, n, decade_total, percent, year) %>% \n  mutate (year =  as.character(year)) %>% \n  mutate (year =  as.numeric(year))  \n  \n  paint(word_decade_counts)\n```\n:::\n\n\n#### -------------- {START: troppo difficile} ------------------\n\n::: {.cell}\n\n```{.r .cell-code}\n# library(broom)\n# \n# mod <- ~ glm(cbind(n, decade_total - n) ~ decade, ., family = \"binomial\")\n# \n# slopes <- word_decade_counts %>%\n#   nest(-word) %>%\n#   mutate(model = map(data, mod)) %>%\n#   unnest(map(model, tidy)) %>%\n#   filter(term == \"year\") %>%\n#   arrange(desc(estimate))\n# \n# slopes\n```\n:::\n\n\n\ntibble [100, 7]\nword         chr 21st adjust ag agricultur agricultur agric~\nword_total   int 1 1 1 3 3 3                  = [how many times word appear in titles ]\ndecade       chr 1990s 1980s 2020s 1980s 1980s 2000s\nn            int 1 1 1 2 2 1                  = [how many times word appear in titles ]\ndecade_total int 10 10 3 10 10 9                = [how many doc per decade ]\npercent      dbl 0.1 0.1 0.333333 0.2 0.2 0.111111  = [% of doc per decade mentioning the word]\nyear         dbl 1999 1981 2020 1986 1982 2008\n\n\n\nsimple lineover time \n\n::: {.cell}\n\n```{.r .cell-code}\nword_decade_counts %>%\n filter(word_total > 2) %>% \n  ggplot(aes(year, percent, color = word)) +\n  geom_point() +\n  scale_y_continuous(labels = percent_format()) +\n  labs(x = \"year\",\n       y = \"% of word in title per decade\",\n       color = \"\")\n```\n:::\n\n#### -------------- {END: troppo difficile} ------------------\n\n## IV.ii) > >  Word and document frequency: TF-IDF\n\n## IV.iii) Relationships b/w words: Word clusters\n\nI want to consider clusters, but I don't want to guess them I want to draw them from the data\n\n::: {.cell}\n\n```{.r .cell-code}\n# hereI want to unstemm the title words \n# unnest titles \ntitle_words2 <- wdr %>%                           # 44 obs X 5 var \n  mutate (year = date_issued) %>% \n  # isolate necessary \n  dplyr::select(id, year, decade, title, altmetric ) %>% # isolate titles\n  arrange(desc(year)) %>%\n  # (redundant) Select only unique/distinct rows from a data frame \n  # dplyr::distinct(title, .keep_all = TRUE) %>%\n  # ----- tidytext’s unnest_tokens function = {turn titles in individual words}\n  unnest_tokens(output = word, \n                input = title, \n                drop = FALSE, # Whether original input column should get dropped\n                to_lower = T, # (implicit) otherwise cannot match the stop_words\n                strip_punc = TRUE) %>%            # 196 obs \n   # ---- token processing\n  # [Optional] stems words\n  # mutate(word = SnowballC::wordStem(word)) %>% # **\n  # [Optional] sometimes needed to graph \n  mutate(title = factor(title, ordered = TRUE))  %>%\n  mutate(year = factor(year, ordered = TRUE)) %>% # 196 obs X 5 var \n  # creates a data frame with one row per word per post!!!\n  # Select only unique/distinct rows from a data frame (if not unique keep first) | keep all vars\n  distinct(id, word, .keep_all = TRUE) %>% # (redundant, no repetition of words in title) \n  # delete stop words (also previously stemmed)\n  anti_join(stop_words_stem, by = \"word\") %>% # ** # 101 obs | 95 if stemmed \n  filter(str_detect(word, \"[^\\\\d]\")) %>%\n  # calculate totals by word across all titles (eg agricultur = in 3 WDR) \n  group_by(word) %>%\n  mutate(word_total = n()) %>%\n  ungroup()  \n\n# I will also make the alt alt_title_words2\n\nalt_title_words2 <-  title_words2 %>%\n  # Count observations by group(ed words)\n  add_count(word ) %>% \n  group_by(word) %>% \n  summarise(median_alt = median (altmetric),\n            # compresses the scale and you go up by smaller increments \n            geometric_mean_alt = exp(mean(log(altmetric + 1))) -1,\n            occurrences = n()) %>% \n  arrange(desc(median_alt))\n```\n:::\n\n\n### corr GRAPHS\n\n::: {.cell}\n\n```{.r .cell-code}\n# get pairwise correlation with {widyr}\ntop_corr <- title_words2 %>% \n  select (id, word) %>% \n  widyr::pairwise_cor(word, id, sort = TRUE) %>% \n  head(150)\n \n#str(top_corr)\n\nset.seed(2022)\n# graph them \ntop_corr %>% \n  graph_from_data_frame() %>% \n  ggraph() +\n  geom_edge_link() +\n  geom_node_point() +\n  geom_node_text(aes(label = name), repel = TRUE) + theme_void()\n```\n:::\n\n\nNow I want to add some metrics to the graph, so I take `alt_title_words` which had some calculated things in it \n\n\n::: {.cell}\n\n```{.r .cell-code}\nvertices <- alt_title_words2 %>%\n  # filter words that have correlation\n filter(word %in% top_corr$item1 | \n         word %in% top_corr$item2)\n\nset.seed(2022)\n# graph them \n# here I add what clusters get more altmetric than others\ntop_corr %>% \n  graph_from_data_frame(vertices = vertices) %>%  # df !\n  ggraph() +\n  geom_edge_link() +\n  geom_node_point(aes(size = occurrences,\n                      color = geometric_mean_alt)) + # aes !\n  geom_node_text(aes(label = name), repel = TRUE) + \n  scale_color_gradient2(low = \"blue\",\n                        high = \"red\",\n                        midpoint = 1000) +\n  theme_void() + \n  labs(title = \"what's hot in WDR titles?\",\n       subtitle = \"Color shows the geom mean of altmetric score on WDR titles containing this word\",\n       size = \"# of occurrences\",\n       color = \"Altmetric (mean)\")   \n```\n:::\n\n\n### ~~Predicting altmetric based on title + topic~~ \n\n\n::: {.cell}\n\n```{.r .cell-code}\n# some reshaping \ntitle_word_matrix <- title_words2 %>% \n  distinct(id, word, altmetric) %>% \n  # turn into a sparse matrix \n  cast_sparse(id, word)\n\ndim(title_word_matrix)\n\n# ... \n```\n:::\n\n\n## IV.iv) > >  Relationships b/w words: n-grams and correlations Word clusters\n\n## IV.v) > >  Topic modeling\n\n\n\n#### -------------- [SUBJECTS & TOPICS  !!!]  ------------------\n \nmust spread all_subj so that I have colum = \"agric\" row equal 0,1 thenn  \n\n::: {.cell}\n\n```{.r .cell-code}\n#noquote(names(wdr))\nwdr_subj <- wdr %>% \n  # delete subj_\n  select (date_issued, decade, title, abstract,\n          altmetric, all_topic, all_subj) \n \n# rownames_to_column(wdr_subj, 'all_subj') %>%\n#         separate_rows(col) %>% \n#         filter(col !=\"\")  %>% \n#         count( all_subj, col) %>%\n#         spread(col, n, fill = 0) %>%\n#         ungroup() %>% \n#         select(-all_subj)\n\n# # base \n# x   <- strsplit(as.character(wdr_subj$all_subj), \",\\\\s?\") # split the strings\n# lvl <- unique(unlist(x))                         # get unique elements\n# x   <- lapply(x, factor, levels = lvl)           # convert to factor\n# subj_df <- as_tibble(t(sapply(x, table)) )      # count elements and transpose \n\n\n# # data.table\n# library(data.table)\n# wdr_subj2 <- setDT(wdr_subj)[, tstrsplit(all_subj, \", |,\")]\n# dcast(melt(wdr_subj2, measure = names(wdr_subj2)), rowid(variable) ~ value, length)\n\nlibrary(splitstackshape)\nwdr_subj2 <- splitstackshape::cSplit_e(wdr_subj, \"all_subj\", \",\", mode = \"binary\", type = \"character\", fill = 0)\n\nwdr_subj3 <- splitstackshape::cSplit_e(wdr_subj, \"all_topic\", \",\", mode = \"binary\", type = \"character\", fill = 0)\n```\n:::\n\n\n### ---  which SUBJ are the most common?\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwdr_subj2 %>%\n  # summarise whole bunch of columns with sum\n  summarise_at(vars(starts_with(\"all_subj_\")), sum)\n\n# most popular AFTER RESHAPING \nwdr_subj_gathered <-  wdr_subj2 %>%\n  # summarise whole bunch of columns with sum\n  gather(subj, value,(starts_with(\"all_subj_\"))) %>% \n  mutate(subj = str_remove(subj, \"all_subj_\")) %>% \n  filter (value ==1) \n\nwdr_subj_gathered %>% \n  count(subj, sort = TRUE)\n\nwdr_subj_gathered %>% \n  group_by(decade) %>% \n  count(subj, sort = TRUE) %>% \n  arrange (desc(n) )-> subj_bydecade\n\nsubj_bydecade %>% \n  ggplot(aes(n)) + \n  geom_histogram()   #  scale_x_log10() # when data is very skewed\n```\n:::\n\n\n### ---  which TOPICS are the most common?\n\n::: {.cell}\n\n```{.r .cell-code}\nwdr_subj3 %>%\n  # summarise whole bunch of columns with sum\n  summarise_at(vars(starts_with(\"all_topic_\")), sum)\n\n\nwdr_subj3 %>% \n  ggplot(aes(altmetric)) +\n  geom_histogram() +\n  scale_x_log10(labels =scales::comma_format())\n  \n \nwdr_subj3 %>%   ggplot( aes(x=altmetric, fill=decade)) +\n    geom_histogram( color=\"#e9ecef\", alpha=0.6, position = 'identity') +\n    #scale_fill_manual(values=c(\"#69b3a2\", \"#404080\")) +\n    #theme_ipsum() +\n    labs(fill=\"\") +\n    facet_wrap(~decade)\n# not super meaningful but is says that over the decades the altmetric have been moving to the right (i.e. getting higher)\n\n# most popular AFTER RESHAPING \nwdr_top_gathered <-  wdr_subj3 %>%\n  # summarise whole bunch of columns with sum\n  gather(top, value,(starts_with(\"all_topic_\"))) %>% \n  mutate(top = str_remove(top, \"all_topic_\")) %>% \n  filter (value ==1) \n\nwdr_top_gathered %>% \n  count(top, sort = TRUE)\n\nwdr_top_gathered %>% \n  group_by(decade) %>% \n  count(top, sort = TRUE) %>% \n  arrange (desc(n) ) -> topic_bydecade\n\n topic_bydecade %>% \n   ggplot(aes(n))\n```\n:::\n\n\n### ---  plot most common TOPICS by decades\n\n::: {.cell}\n\n```{.r .cell-code}\n# skimr::n_unique(topic_bydecade$top) # 26 \n# skimr::skim(topic_bydecade$n) # 26 \n\n# geom_col \np_topic_over_decades <-  topic_bydecade  %>%\n  # filter ( n >1) %>% \n  # mutate(top = reorder(top, n)) %>%\n  # need reorder here or it won't stay \n  ggplot(aes(x= reorder(top, n), y = n), fill = decade) +\n  # geom_col() uses stat_identity(): it leaves the data as is.\n  geom_col(fill = \"lightblue\") +\n    scale_y_continuous( breaks = seq(1,9,1),\n                        labels = c(seq(1,8,1), \"9+\" )\n                        ) +\n  # more readable\n  coord_flip() +\n  labs(title = \"Most common topics in 44 WDRs over decades\",\n       subtitle = \"[High level topics covered = 26]\",\n       y = \"# of WDRs on topic per decade\", x = \"\"\n       )  +  facet_wrap(~decade)\n\np_topic_over_decades\n\np_topic_over_decades %T>% \n  print() %T>%\n  ggsave(., filename = here(\"analysis\", \"output\", \"figures\", \"p_topic_over_decades.pdf\"),\n         #width = 4, height = 2.25, units = \"in\",\n         device = cairo_pdf) %>% \n  ggsave(., filename = here(\"analysis\", \"output\", \"figures\", \"p_topic_over_decades.png\"),\n         #width = 4, height = 2.25, units = \"in\", \n         type = \"cairo\", dpi = 300)  \n```\n:::\n\n\n\nneed to create groups \"umbrella subjects\"\n\n::: {.cell}\n\n```{.r .cell-code}\n# ggplot(subj_bydecade, aes(n, fill = decade)) +\n#   geom_histogram(binwidth = 1,\n#                  color = \"white\") +\n#   scale_y_continuous(breaks= pretty_breaks()) +\n#   xlim(0, 20) +\n#   labs(#title = ~date_issued, \n#     x = \"frequency\",\n#     y = \"N of words @ that frequency\") + \n#   facet_wrap( ~decade ) #+ # , ncol = 2, scales = \"free_y\")\n#   #guides( fill = \"none\") # way to turn legend off\n# \n\n# geom_col \np_most_common_word_in_subj <- subj_bydecade %>%\n  head(50) %>%\n  # filter ( n >1) %>% \n  mutate(subj = reorder(subj, n)) %>%\n  ggplot(aes(subj, n), fill = decade) +\n  # geom_col() uses stat_identity(): it leaves the data as is.\n  geom_col(fill = \"lightblue\") +\n  scale_y_continuous(labels = comma_format()) +\n  coord_flip() +\n  labs(title = \"50 most common subjects in 44 World Development Reports' titles\",\n       subtitle = \"[ ]\" #, y = \"# of uses\"\n       ) +\n  facet_wrap(~decade)\n\np_most_common_word_in_subj\n\np_most_common_word_in_subj %T>% \n  print() %T>%\n  ggsave(., filename = here(\"analysis\", \"output\", \"figures\", \"most_common_word_in_subj.pdf\"),\n         #width = 4, height = 2.25, units = \"in\",\n         device = cairo_pdf) %>% \n  ggsave(., filename = here(\"analysis\", \"output\", \"figures\", \"most_common_word_in_subj.png\"),\n         #width = 4, height = 2.25, units = \"in\", \n         type = \"cairo\", dpi = 300)  \n```\n:::\n\n\n### ---  [word clouds by decade ???]\n\n### ---  [CORRELATION GRAPHS ???]\n\n### ---  PREDICTION OF DOWNLOADS ???\n\n## A4. Tokenization by n-gram - ITERATIVELY]\n\n### -- using abstract?\n\n### -- using subjects?\n\n# ---\n\n\n# -------------- TO do \\| To Rethink ------------------\n\n-   DAG graph of my research question\n-   metadata $downloads? on WDRs ? (I am using \"altmetrics\" but I don't know how important it is)\n-   create my own stop_words list (which excludes also \"date_issued\", \"report\", etc)\n-   leggere [@yusuf_development_2008]\n-   need to create groups \"umbrella subjects\"\n- Which of this bigram might be a SLOGAN?\n\n# Reference Tutorials\n\n@robinson_1_2022 \n\n[Benjamin Soltoff: Computing 4 Social Sciences - API](https://cfss.uchicago.edu/syllabus/getting-data-from-the-web-api-access/)\\ \n\n[Benjamin Soltoff: Computing 4 Social Sciences - text analysis](https://cfss.uchicago.edu/syllabus/text-analysis-fundamentals-and-sentiment-analysis/)\\\n\n[Ben Schmidt Book Humanities Crurse](https://hdf.benschmidt.org/R/) [Ben Schmidt Book Humanities](http://benschmidt.org/HDA/texts-as-data.html)\\\n\n[TidyTuesday casts on tidytext](https://github.com/dgrtwo/data-screencasts/tree/master/screencast-annotations)\\\n\n  1. ✔️ [MEDIUM articles: common words, pairwise correlations - 2018-12-04](https://www.youtube.com/watch?v=C69QyycHsgE)\n  2. ✔️ [TidyTuesday Tweets -  2019-01-07](https://www.youtube.com/watch?v=KE9ItC3doEU)\n  3. [Wine Ratings - 2019-05-31](https://www.youtube.com/watch?v=AQzZNIyjyWM) Lasso regression | sentiment lexicon,\n  4. [Simpsons Guest Stars \t2019-08-30](https://www.youtube.com/watch?v=EYuuAGDeGrQ) geom_histogram\n  5. [Horror Movies \t2019-10-22](https://www.youtube.com/watch?v=yFRSTlk3kRQ) explaining glmnet package | Lasso regression\n  6. [The Office \t2020-03-16](https://www.youtube.com/watch?v=_IvAubTDQME) geom_text_repel from ggrepel | glmnet package to run a cross-validated LASSO regression\n  7. [Animal Crossing \t2020-05-05](https://www.youtube.com/watch?v=Xt7ACiedRRI) Using geom_line and geom_point to graph ratings over time | geom_text to visualize what words are associated with positive/negative reviews |topic modelling\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}