{
  "hash": "30f21c98f5c49374d2335496372b607c",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Process and merge data - WDR abstracts\"\nauthor: \"Luisa M. Mimmi\"\ndate: \"Last run: 2024-05-10\"\nlang: en\neditor: source\nengine: knitr\n## ------  general Output Options\nexecute:     \n  eval: false    # actually run? \n  echo: true     #  include source code in output\n  warning: false  #  include warning code in output\n  error: false    #  include error code in output\n  output: false   # include output code in output (CHG in BLOCKS)\n  # include: false   # R still runs but code and results DON\"T appear in output  \n  cache: false # normalmnte false\ntoc: true\nfig-cap-location: top\ntbl-cap-location: top\nformat:\n  html:\n    # theme: flatly #spacelab\n    code-fold: false # redundant bc echo false \n    toc-depth: 3\n    toc_float: true\n    toc-location: left\n    toc-title: Outline\n    embed-resources: true # external dependencies embedded (Not in ..._files/)\n  # pdf:\n  #   toc-depth: 2\n  #   toc-title: Indice\n  #   highlight-style: github\n  #   #lang: it\n  #   embed-resources: true # external dependencies embedded (Not in ..._files/)\nformat-links: false\nbibliography: ../bib/slogan.bib\n---\n\n\n<i class=\"fa fa-refresh\" style=\"color: firebrick\"></i> Work in progress\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#knitr::opts_chunk$set(include = TRUE, warning = FALSE)\n# Pckgs -------------------------------------\n#if (!require (\"pacman\")) (install.packages(\"pacman\"))\n\n#p_install_gh(\"luisDVA/annotater\")\n#p_install_gh(\"HumanitiesDataAnalysis/hathidy\")\n# devtools::install_github(\"HumanitiesDataAnalysis/HumanitiesDataAnalysis\") \nlibrary(here)\nlibrary(fs)\nlibrary(paint) \nlibrary(tidyverse) \nlibrary(magrittr)\nlibrary(skimr)\nlibrary(scales) \nlibrary(colorspace)\nlibrary(httr)\nlibrary(DT) # an R interface to the JavaScript library DataTables\nlibrary(knitr)\nlibrary(kableExtra) \nlibrary(flextable) \nlibrary(splitstackshape)  #Stack and Reshape Datasets After Splitting Concatenated Values\nlibrary(tm) # Text Mining Package\nlibrary(tidytext) # Text Mining using 'dplyr', 'ggplot2', and Other Tidy Tools\n# this requires pre-requirsites to install : https://github.com/quanteda/quanteda\nlibrary(quanteda)\nlibrary(igraph)\nlibrary(sjmisc) # Data and Variable Transformation Functions\nlibrary(ggraph) # An Implementation of Grammar of Graphics for Graphs and Networks\nlibrary(widyr) # Widen, Process, then Re-Tidy Data\nlibrary(SnowballC) # Snowball Stemmers Based on the C 'libstemmer' UTF-8 Library\n# library(#HumanitiesDataAnalysis, # Data and Code for Teaching Humanities Data Analysis\nlibrary(sentencepiece) # Text Tokenization using Byte Pair Encoding and Unigram Modelling\nlibrary(sysfonts) \nlibrary(ggdendro)\nlibrary(network)\nlibrary(GGally)\n\nlibrary(topicmodels)                #  with dep   FAILED !!!!!!\n\n# extra steo needed to install github version \n#if (!require(\"devtools\")) install.packages(\"devtools\")\n#library(devtools)\n#install_github(\"husson/FactoMineR\")     FAILED !!!!!!\n# library(FactoMineR)\n#library(factoextra)\n\n# Plot Theme(s) -------------------------------------\n#source(here(\"R\", \"ggplot_themes.R\"))\nggplot2::theme_set(theme_minimal())\n# color paletts -----\nmycolors_gradient <- c(\"#ccf6fa\", \"#80e8f3\", \"#33d9eb\", \"#00d0e6\", \"#0092a1\")\nmycolors_contrast <- c(\"#E7B800\", \"#a19100\", \"#0084e6\",\"#005ca1\", \"#e60066\" )\n\n\n# Function(s) -------------------------------------\n\n# Data -------------------------------------\n\n# -------------------- {cut bc made too heavy} -------------------------------------\n# # Tables [AH knit setup when using kbl() ]------------------------------------\nknit_print.data.frame <- function(x, ...) {\n  res <- paste(c('', '', kable_styling(kable(x, booktabs = TRUE))), collapse = '\\n')\n  asis_output(res)\n}\n\nregisterS3method(\"knit_print\", \"data.frame\", knit_print.data.frame)\nregisterS3method(\"knit_print\", \"grouped_df\", knit_print.data.frame)\n```\n:::\n\n\n\n\n# World Development Reports (WRDs)\n\n-   DATA <https://datacatalog.worldbank.org/search/dataset/0037800>\n-   INSTRUCTIONS <https://documents.worldbank.org/en/publication/documents-reports/api>\n-   Following: [@kaye_ella_2019; @robinson_words_2017; @robinson_1_2022]\n\n## I) Pre-processing \n\n\n### I.ii) -- Set stopwords [more...]\n\n::: {.cell}\n\n```{.r .cell-code}\n# --- alt stop words\n# mystopwords <- tibble(word = c(\"eq\", \"co\", \"rc\", \"ac\", \"ak\", \"bn\", \n#                                    \"fig\", \"file\", \"cg\", \"cb\", \"cm\",\n#                                \"ab\", \"_k\", \"_k_\", \"_x\"))\n\n# --- set up stop words\nstop_words <- as_tibble(stop_words) %>% # in the tidytext dataset \n  add_row(word = \"WDR\", lexicon = NA_character_) %>%\n  # add_row(word = \"world\", lexicon = NA_character_) %>%\n  add_row(word = \"report\", lexicon = NA_character_) %>%\n  # add_row(word = \"development\", lexicon = NA_character_) %>%\n  add_row(word = \"1978\", lexicon = NA_character_) %>%\n  add_row(word = \"1979\", lexicon = NA_character_) %>%\n  add_row(word = \"1980\", lexicon = NA_character_) %>%\n  add_row(word = \"1981\", lexicon = NA_character_) %>%\n  add_row(word = \"1982\", lexicon = NA_character_) %>%\n  add_row(word = \"1983\", lexicon = NA_character_) %>%\n  add_row(word = \"1984\", lexicon = NA_character_) %>%\n  add_row(word = \"1985\", lexicon = NA_character_) %>%\n  add_row(word = \"1986\", lexicon = NA_character_) %>%\n  add_row(word = \"1987\", lexicon = NA_character_) %>%\n  add_row(word = \"1988\", lexicon = NA_character_) %>%\n  add_row(word = \"1989\", lexicon = NA_character_) %>%\n  add_row(word = \"1990\", lexicon = NA_character_) %>%\n  add_row(word = \"1991\", lexicon = NA_character_) %>%\n  add_row(word = \"1992\", lexicon = NA_character_) %>%\n  add_row(word = \"1993\", lexicon = NA_character_) %>%\n  add_row(word = \"1994\", lexicon = NA_character_) %>%\n  add_row(word = \"1995\", lexicon = NA_character_) %>%\n  add_row(word = \"1996\", lexicon = NA_character_) %>%\n  add_row(word = \"1997\", lexicon = NA_character_) %>%\n  add_row(word = \"1998\", lexicon = NA_character_) %>%\n  add_row(word = \"1999\", lexicon = NA_character_) %>%\n  add_row(word = \"2000\", lexicon = NA_character_) %>%\n  add_row(word = \"2001\", lexicon = NA_character_) %>%\n  add_row(word = \"2002\", lexicon = NA_character_) %>%\n  add_row(word = \"2003\", lexicon = NA_character_) %>%\n  add_row(word = \"2004\", lexicon = NA_character_) %>%\n  add_row(word = \"2005\", lexicon = NA_character_) %>%\n  add_row(word = \"2006\", lexicon = NA_character_) %>%\n  add_row(word = \"2007\", lexicon = NA_character_) %>%\n  add_row(word = \"2008\", lexicon = NA_character_) %>%\n  add_row(word = \"2009\", lexicon = NA_character_) %>%\n  add_row(word = \"2010\", lexicon = NA_character_) %>%\n  add_row(word = \"2011\", lexicon = NA_character_) %>%\n  add_row(word = \"2012\", lexicon = NA_character_) %>%\n  add_row(word = \"2013\", lexicon = NA_character_) %>%\n  add_row(word = \"2014\", lexicon = NA_character_) %>%\n  add_row(word = \"2015\", lexicon = NA_character_) %>%\n  add_row(word = \"2016\", lexicon = NA_character_) %>%\n  add_row(word = \"2017\", lexicon = NA_character_) %>%\n  add_row(word = \"2018\", lexicon = NA_character_) %>%\n  add_row(word = \"2019\", lexicon = NA_character_) %>%\n  add_row(word = \"2020\", lexicon = NA_character_) %>%\n  add_row(word = \"2021\", lexicon = NA_character_) %>%\n  add_row(word = \"2022\", lexicon = NA_character_) %>% \n  # filter (word != \"changes\") %>% \n   # filter (word != \"value\") %>% \n   filter (word != \"member\") %>% \n   filter (word != \"part\") %>% \n   filter (word != \"possible\") %>% \n   filter (word != \"point\") %>% \n   filter (word != \"present\") %>% \n   # filter (word != \"zero\") %>% \n     filter (word != \"young\") %>% \n     filter (word != \"old\") %>% \n     filter (word != \"trying\") \n\n\n# --- set up stop words stemmed\nstop_words_stem <- stop_words  %>% \nmutate (word = SnowballC::wordStem(word ))\n```\n:::\n\n\n## II) Data (ingestion), loading & cleaning\n\nIngestion of WDR **basic metadata** was done in `./_my_stuff/WDR-data-ingestion.Rmd` and the result saved as `./data/raw_data/WDR.rds` \\<-- (Being somewhat computational intensive, I only did it once.)\n\n> \n    + **WDR = tibble [45, 8]**\n    + **doc_mt_identifier_1** chr oai:openknowledge.worldbank.org:109~ \n    + **doc_mt_identifier_2** chr http://www-wds.worldbank.org/extern~ \n    + **doc_mt_title**        chr Development Economics through the  ~ \n    + **doc_mt_date   **      chr 2012-03-19T10:02:25Z 2012-03-19T19:~ \n    + **doc_mt_creator **     chr Yusuf, Shahid World Bank World Bank~ \n    + **doc_mt_subject**      chr ABSOLUTE POVERTY AGGLOMERATION BENE~ \n    + **doc_mt_description**  chr The World Development Report (WDR) ~ \n    + **doc_mt_set_spec**     chr oai:openknowledge.worldbank.org:109~ \n\nIngestion of WDR **lists of subjects** was available among metadata but presented issues (difficulty to extract, many records with repetition,apparently wrong) so I reconstructed them manually in `data/raw_data/WDR_subjects_corrected2010_2011.xlsx` taking them from site https://elibrary.worldbank.org/ which lists **keywords** correctly [e.g. see 2022 WDR](https://elibrary.worldbank.org/doi/abs/10.1596/978-1-4648-1730-4)\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# WRD metadata taken with API get (issues) \nWDR <- readr::read_rds(here::here(\"data\", \"raw_data\", \"WDR.rds\" )) %>% \n  # Extract only the portion of string AFTER the backslash {/}\n  mutate(id = as.numeric(stringr::str_extract(doc_mt_identifier_1, \"[^/]+$\"))) %>% \n  dplyr::relocate(id, .before = doc_mt_identifier_1) %>% \n  mutate(url_keys = paste0(\"https://openknowledge.worldbank.org/handle/10986/\", id , \"?show=full\"))  %>% \n # eliminate NON WDR book\n  dplyr::filter(id != \"2586\") \n\n# WRD subject/date_issued taken by manual review \nWDR_subjects <- readxl::read_excel(here::here(\"data\", \"raw_data\", \n                                              \"WDR_subjects_corrected2010_2011.xlsx\")) %>%\n  drop_na(id) %>% \n # eliminate NON WDR book\n  dplyr::filter(id != \"2586\") \n\n# delete empty cols \nColNums_NotAllMissing <- function(df){ # helper function\n  as.vector(which(colSums(is.na(df)) != nrow(df)))\n}\n\nWDR_subjects <- WDR_subjects  %>% \n  select(ColNums_NotAllMissing(.))\n # # convert all columns that start with \"subj_\" to lowercase\n # WDR_subjects[3:218] <- sapply(WDR_subjects[3:218], function(x) tolower(x))\n\n# join\nWDR_com <- left_join(WDR, WDR_subjects, by = \"id\") %>% \n  dplyr::relocate(date_issued, .before = id ) %>% \n  # drop useles clmns \n  dplyr::select(#-doc_mt_identifier_1, \n                -doc_mt_identifier_2, -doc_mt_date, \n                -doc_mt_subject, -doc_mt_creator, -doc_mt_set_spec) %>% \n  # dplyr::relocate(url_keys, .after = subj_216 ) %>% \n  dplyr::rename(abstract = doc_mt_description) %>% \n  # correct titles -> portion after {:}\n  dplyr::mutate(., title = str_extract(doc_mt_title,\"[^:]+$\")) %>% \n  dplyr::relocate(title, .after = id)  %>% \n  dplyr::rename(title_miss = doc_mt_title) %>% \n  dplyr::mutate(title_miss = case_when(\n    str_starts(title, \"World Development Report\") ~ \"Y\",\n    TRUE ~ NA_character_) \n  ) %>% \n  dplyr::mutate(subject_miss = if_else(is.na(subj_1), \n                                       \"Y\", \n                                       NA_character_)) %>% \n  dplyr::relocate(subject_miss, .after = title_miss)    %>% \n  dplyr::relocate(ISBN, .after = id)    \n  \n#paint(WDR_com)\n\n# convert all columns that start with \"subj_\" to lowercase (maybe redundant)\nWDR_com[, grep(\"^subj_\", names(WDR_com))] <- sapply(WDR_com[, grep(\"^subj_\", names(WDR_com))], function(x) tolower(x))\n\n# combine all `subj_...` vars into a vector separated by comma\ncol_subj <- names(WDR_com[, grep(\"^subj_\", names(WDR_com))] )\n\nWDR_com <- WDR_com %>% tidyr::unite(\n  col = \"all_subj\", \n  subj_1:subj_46, \n  sep = \",\",\n  remove = FALSE,\n  na.rm = TRUE) %>% \n  arrange(date_issued)\n\n#paint(WDR_com)\n```\n:::\n\n\n#### -- Some manual correction of wrong metadata\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# adding actual titles \n#WDR_com[WDR_com$date_issued == \"1978\", c(\"date_issued\", \"id\", \"title\")]\nWDR_com$title[WDR_com$id == \"5961\"] <- \"Prospects for Growth and Alleviation of Poverty\"\n\n#WDR_com[WDR_com$date_issued == \"1979\", c(\"date_issued\", \"id\", \"title\")]\nWDR_com$title[WDR_com$id == \"5962\"] <- \"Structural Change and Development Policy\"\n\n#WDR_com[WDR_com$date_issued == \"1980\", c(\"date_issued\", \"id\", \"title\")]\nWDR_com$title[WDR_com$id == \"5963\"] <- \"Poverty and Human Development\"\n\n#WDR_com[WDR_com$date_issued == \"1981\", c(\"date_issued\", \"id\", \"title\")]\nWDR_com$title[WDR_com$id == \"5964\"] <- \"National and International Adjustment\"\n\n#WDR_com[WDR_com$date_issued == \"1982\", c(\"date_issued\", \"id\", \"title\")]\nWDR_com$title[WDR_com$id == \"5965\"] <- \"Agriculture and Economic Development\"\n\n#WDR_com[WDR_com$date_issued == \"1983\", c(\"date_issued\", \"id\", \"title\")]\nWDR_com$title[WDR_com$id == \"5966\"] <- \"Management in Development\"\n\n#WDR_com[WDR_com$date_issued == \"1984\", c(\"date_issued\", \"id\", \"title\")]\nWDR_com$title[WDR_com$id == \"5967\"] <- \"Population Change and Development\"\n\n#WDR_com[WDR_com$date_issued == \"1985\", c(\"date_issued\", \"id\", \"title\")]\nWDR_com$title[WDR_com$id == \"5968\"] <- \"International Capital and Economic Development\"\n\n#WDR_com[WDR_com$date_issued == \"1986\", c(\"date_issued\", \"id\", \"title\")]\nWDR_com$title[WDR_com$id == \"5969\"] <- \"Trade and Pricing Policies in World Agriculture\"\n\n#WDR_com[WDR_com$date_issued == \"1987\", c(\"date_issued\", \"id\", \"title\")]\nWDR_com$title[WDR_com$id == \"5970\"] <- \"Industrialization and Foreign Trade\"\n\n#WDR_com[WDR_com$date_issued == \"1988\", c(\"date_issued\", \"id\", \"title\")]\nWDR_com$title[WDR_com$id == \"5971\"] <- \"Public Finance in Development\"\n\n# wrong year \n#WDR_com[WDR_com$date_issued %in% c( \"2011\",\"2012\",\"2013\", \"2014\", \"2015\"), c(\"date_issued\", \"id\", \"title\")]\nWDR_com$date_issued[WDR_com$id == \"11843\"] <- \"2013\"\nWDR_com$date_issued[WDR_com$id == \"16092\"] <- \"2014\"\n```\n:::\n\n\n### II.i) Troubleshooting some documents\n\n> PROBLEM: some of the subjects collections are evidently wrong (either they are the same of another WDR or the list is impossibly long)\n\n> MY SOLUTION #1: I took them manually from the website \"elibrary\" <https://elibrary.worldbank.org/action/showPublications?SeriesKey=b02>\n\nBut, there still is WDR 2011 (\"Conflict, Security, and Development\") which misses keywords\n\n> MY SOLUTION #2: I take the abstract and I create my own \"plausible list\" of subjects\n\n#### --- Extrapolate subjects from abstracts - for record with missing subjects/keywords\n\n(\\*) There will remain a problem: this corrected records have tokens and not bi\\|n-grams (which make more sense)!\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# --- identify wrong subject obs\nWDR_wr <- WDR_com %>% \n  filter( subject_miss == \"Y\") \n# names(WDR_com)\n```\n:::\n\n\n#### -- WDR caseid = 4389\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# --- tokenize abstract \nWDR_4389 <- WDR_wr %>% \n  filter(id ==\"4389\") %>% \n  select (abstract) %>% as_tibble() %>% \n  tidytext::unnest_tokens(word, abstract) # -> 251 words \n\n# --- remove stop words\n# --- isolate meaningful tokens\nWDR_4389 <- WDR_4389 %>% \n  anti_join( stop_words , by = \"word\") %>%   # -> 131  words \n  # Count observations by group\n  count(word, sort = T)  # -> 101  words \n\n# rename column in result corrected   \nWDR_4389_w <- t(WDR_4389) %>% as_tibble()  \nnames(WDR_4389_w) <- gsub(x = names(WDR_4389_w), pattern = \"\\\\V\", replacement = \"subj_\")\n # --- graph words\n  # p <- WDR_4389 %>% count(word, sort = TRUE) %>%\n  #   filter(n > 600) %>%\n  #   mutate(word = reorder(word, n)) %>%\n  #   ggplot(aes(n, word)) +\n  #   geom_col() +\n  #   labs(y = NULL)\n  # p  \n  #   \n\n# --- replace as subjects \nWDR_4389_w  <- WDR_4389_w %>%  \n  filter(row_number() == 1 ) %>% \n  mutate(id = 4389) %>% \n  relocate(id, .before = subj_1)\n\n# names(WDR_4389_w)\nWDR_4389_w <- WDR_4389_w %>% \n  tidyr::unite(\n    col = \"all_subj\", \n    subj_1:subj_100, \n    sep = \",\",\n    remove = FALSE) \n\n\n#names(WDR_com)\nWDR_com_2 <- WDR_com %>% \n  relocate(subj_1 ,.after = all_subj)\n\n# # create a vector of column names to add to reach the n length(WDR_com)\n# col_names <-  as.vector(paste0('subj_', length(WDR_4389_w):105))\n# \n# # make a df with those cols and 1 row made of Nas values \n# to_add <- bind_rows(setNames(rep(\"\", length(col_names)), col_names))[NA_character_, ]\n# \n# # --- pad until subj_216 with nas.........  \n# WDR_4389_w_pad <- bind_cols(WDR_4389_w, to_add)\n\n#### -- NOW: replace corrected single WDR case into master df \n  # # initial check ---\n  # WDR_com$subj_1[WDR_com$id == 4389] \n  # WDR_com$subj_3[WDR_com$id == 4389] \n  # #--- \n\n#id <-  4389 \ncol_subj_names <- names(WDR_4389_w)[-(1)] # without \"id\"  \"all_subj\"\n\n# pick the id \ni <- 4389\n\n# # --- function --- NO JOY!\n# for (j in 1:length(col_subj_names)) {\n#   col <-  col_subj_names[j]\n#   # print(col) # nolint\n#   WDR_com %>%\n#     dplyr::filter (id == i) %>% \n#     dplyr::mutate (., col = WDR_4389_w_pad$col) \n#   WDR_com_2 <- WDR_com\n# }\n\n# ---------# Solution from SO guy \n# r is vectorized so \nWDR_com_2[WDR_com_2$id %in% WDR_4389_w$id, 9:55] <- WDR_4389_w[, 2:48] \n```\n:::\n\n\n\\* I cut some ...so this record WDR_4389 will be incomplete\n\n<!-- #### -- WDR caseid = 2586 -->\n\n### II.ii)  -- SAVE wdr and cleanenv\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwdr <- WDR_com_2 %>% \n  select(-title_miss) %>% \n  mutate(decade = case_when(\n    str_detect (string = date_issued, pattern = \"^197\") ~ \"1970s\", \n    str_detect (string = date_issued, pattern = \"^198\") ~ \"1980s\", \n    str_detect (string = date_issued, pattern = \"^199\") ~ \"1990s\", \n    str_detect (string = date_issued, pattern = \"^200\") ~ \"2000s\", \n    str_detect (string = date_issued, pattern = \"^201\") ~ \"2010s\", \n    str_detect (string = date_issued, pattern = \"^202\") ~ \"2020s\"\n  )) %>% \n  relocate(decade, .after = date_issued) %>% \n# correct some datatype\nmutate_at(vars(date_issued, altmetric), as.numeric)  \n\ndataDir <- fs::path_abs(here::here(\"data\",\"derived_data\"))\nfileName <- \"/wdr.rds\"\nDir_File <- paste0(dataDir, fileName)\nwrite_rds(x = wdr, file = Dir_File)\n\n# # ls objects\n# list_old_WDR <-  ls(# pattern = \"^WDR\", \n#                     all.names = TRUE)\n# list_old_WDR\n# rm(list = setdiff(list_old_WDR, c(\"stop_words\", \"stop_words_stem\")))\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nwdr <- readr::read_rds(here::here(\"data\", \"derived_data\", \"wdr.rds\" ))\n```\n:::\n\n\n\n### I.iii) > > Part of Speech Tagging \nTagging segments of speech for part-of-speech (nouns, verbs, adjectives, etc.) or entity recognition (person, place, company, etc.)\nhttps://m-clark.github.io/text-analysis-with-R/part-of-speech-tagging.html \n\n\n#### -- tagging with `cleanNLP`\nAH: https://datavizs22.classes.andrewheiss.com/example/13-example/#sentiment-analysis\n\nHere’s the general process for tagging (or “annotating”) text with the cleanNLP package:\n\n   1. Make a dataset where one column is the id (line number, chapter number, book+chapter, etc.), and another column is the text itself.\n   2. Initialize the NLP tagger. You can use any of these:\n      + `cnlp_init_udpipe()`: Use an R-only tagger that should work without installing anything extra (a little slower than the others, but requires no extra steps!)\n      + `cnlp_init_spacy()`: Use spaCy (if you’ve installed it on your computer with Python)\n      + `cnlp_init_corenlp()`: Use Stanford’s NLP library (if you’ve installed it on your computer with Java)\n   3. Feed the data frame from step 1 into the cnlp_annotate() function and wait.\n   4. Save the tagged data on your computer so you don’t have to re-tag it every time.\n\n\n\n\n# III) ABSTRACTS\n\nAll the following tasks were performend on the ***abstracts*** of WDRs. Why? \n\n    + because I needed to learn\n    + because abstracts tend to include the ***keywords***\n    \n## III.i) Tokenization\nWhere a word is more abstract, a **“type”** is a concrete term used in actual language, and a **“token”** is the particular instance we’re interested in (e.g. abstract things (‘wizards’) and individual instances of the thing (‘Harry Potter.’).\nBreaking a piece of text into words is thus called **“tokenization”**, and it can be done in many ways. \n\n### --- The choices of tokenization\n\n  1. Should words be lowercased? x \n  2. Should punctuation be removed? x \n  3. Should numbers be replaced by some placeholder?\n  4. Should words be stemmed (also called lemmatization). x \n  5. Should bigrams/multi-word phrase be used instead of single word phrases?\n  6. Should stopwords (the most common words) be removed? x \n  7. Should rare words be removed? \n\n### --- Tokenization using `regular expression` syntax\n\nThe R function `strsplit` lets us do just this: split a string into pieces.\n*Note, for example, that this makes the word “Don’t” into two words.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntok_simple <- as_tibble(wdr$abstract[1] ) %>%\n  str_split(\"[^A-Za-z]\") # “split on anything that isn’t a letter between A and Z.”\n\nstr(tok_simple) # list of characters \n\n#tok_simple[[1]]\n```\n:::\n\n\n### --- Tokenization using `tidytext`\nThe simplest way is to remove anything that isn’t a letter. The workhorse function in `tidytext` is `unnest_tokens`. It creates a new columns (here called ‘words’) from each of the individual ones in text.\n \n\n::: {.cell}\n\n```{.r .cell-code}\nabs_1 <- as_tibble(wdr$abstract[1] )\n\n# LIST OF features I can add to `unnest_tokens`\ntok_feat_l <- list(\n  # 1) all 2 lowercase \n  abs_1 %>% unnest_tokens(word, value) %>% select(lowercase = word),\n  # 4) `SnowballC::wordStem` extracts stems of each given words in the vector.\n  abs_1 %>% unnest_tokens(word, value) %>% rowwise() %>% \n    mutate(word = SnowballC::wordStem(word)) %>% select(stemmed = word),\n  # 1.b) keep uppercase if there are \n  abs_1 %>% unnest_tokens(word, value, to_lower = F) %>% \n    select(uppercase = word),\n  # 2) keep punctuation {default is rid} \n  abs_1 %>% unnest_tokens(word, value, to_lower = F, strip_punc = FALSE) %>% \n    select(punctuations = word),\n  # 5) bigram\n  abs_1 %>% \n    unnest_tokens(word, value, token = \"ngrams\", n = 2, to_lower = F) %>%\n    select(bigrams = word)\n)\n\n# Return a data frame created by column-binding.\ntok_feat_df <- map_dfc(tok_feat_l  , ~ .x %>% head(10))\ntok_feat_df\n\n# # my choice \n# abs_1_t_mod <- abs_1 %>% \n#   # no punctuation, yes capitalized\n#   unnest_tokens(word, value, to_lower = F, strip_punc = TRUE) %>% # 249 obs\n#   # exclude stopwords \n#   anti_join(stop_words) # 109 obs\n# \n# head(abs_1_t_mod, 15)\n```\n:::\n\n\n### --- Tokenizing ALL abstracts\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# isolate only abstracts \nabs_all <- wdr %>% \n  dplyr::select(id, date_issued, title, abstract)\n\nabs_all_token <- abs_all %>% \n  unnest_tokens(output =  word,\n                input = abstract ,\n                to_lower = T, # otherwise cannot match the stop_words  \n                strip_punc = TRUE\n  ) %>% #10018\n  anti_join(stop_words) # 4613\n\n# Count words\nwordcounts <- abs_all_token %>%\n  group_by(word) %>%\n  summarize(n = n()) %>%\n  #arrange(-n) %>%\n  # head(5) %>% \n  mutate(rank = rank(-n)) %>%\n  filter(n > 2, word != \"\")\n```\n:::\n\n\n\n## III.ii) Word and document frequency: TF-IDF\n\nThe goal is to quantify what a document is about. What is the document about?\n\n-   **term frequency (tf)** = how frequently a word occurs in a document... but there are words that occur many time and are not important\n-   term's **inverse document frequency (idf)** = decreases the weight for commonly used words and increases the weight for words that are not used very much in a collection of documents.\n-   **statistic tf-idf (= tf*-*idf)** = an alternative to using stopwords is *the frequency of a term adjusted for how rarely it is used*. [It measures how important a word is to a document in a collection (or corpus) of documents, but it is still a rule-of-thumb or heuristic quantity]\n\n> The tf-idf is the product of the term frequency and the inverse document frequency::\n\n$$\n\\begin{aligned}\ntf(\\text{term}) &= \\frac{n_{\\text{term}}}{n_{\\text{terms in document}}} \\\\\nidf(\\text{term}) &= \\ln{\\left(\\frac{n_{\\text{documents}}}{n_{\\text{documents containing term}}}\\right)} \\\\\ntf\\text{-}idf(\\text{term}) &= tf(\\text{term}) \\times idf(\\text{term})\n\\end{aligned}\n$$\n\n### --- Pre-process 4 TF-IDF\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# PREP X TF-IDF\n#paint(abs_all)\nskimr::n_unique(abs_all$title)\nskimr::n_unique(abs_all$date_issued)\n\n# Count words (with stopwords)\ntemp <-  abs_all %>% \n  unnest_tokens(output =  word,\n                input = abstract,\n                to_lower = T, # otherwise cannot match the stop_words  \n                strip_punc = TRUE) %>%  #9769 = tot words\n  # mutate(word = SnowballC::wordStem(word)) %>% \n  ## other important pre-process step\n  # mutate(title = factor(title, ordered = TRUE))  %>% \n  # mutate(date_issued = factor(date_issued, ordered = TRUE)) %>% \n  ## implicit group by ...\n  group_by(date_issued) %>%\n  # Count observations by group\n  count(word, sort = TRUE) %>% # 5860 # unique words\n  ungroup() \n\n\nabs_words <- left_join(temp, abs_all, by = \"date_issued\")\npaint(abs_words)\n```\n:::\n\n\n### ~~modo I) {Kumaran Ponnambalam} Create a Word Frequency Table~~\n\n[tm way](https://www.linkedin.com/learning/processing-text-with-r-essential-training/improving-term-frequency-matrix?autoSkip=true&autoplay=true&contextUrn=urn%3Ali%3AlearningCollection%3A6932688789001674752&resume=false)\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# abs_words_freq <-  abs_words %>%\n#   anti_join(stop_words, by= \"word\" )  %>%\n#   tidyr::pivot_wider(names_from = word, values_from = n, values_fill = 0)\n# \n# # skim(abs_words_freq)\n# \n# #Generate the Document Term matrix\n# abs_words_freq_matrix <- as.matrix(abs_words_freq)\n# \n# abs_words_freq_matrix[ , 'gender']\n# \n# # .... uses {tm} \n\n#str(abs_words_freq_matrix)\n```\n:::\n\n\n### modo II) {Julia Silge and David Robinson} tidytext::bind_tf_idf\n\n[tidytext way](https://www.tidytextmining.com/tfidf.html)\n\nThe idea of `tf-idf` is to find the important words for the content of each document by *decreasing the weight for commonly used words and increasing the weight for words that are not used very much in a collection or corpus of documents*. Calculating tf-idf attempts to find the words that are important (i.e., common) in a text, but not too common. Let's do that now.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n### ---  abstracts with totals \n\n# Calculate the total appearances of each words per doc\nabs_total_words <- abs_words %>%\n  dplyr::group_by(title) %>%\n  dplyr::summarise(total = sum(n))  \n\n# Join the total appearances of each words per doc\nabs_words_T <- left_join(abs_words, abs_total_words) %>% \n  select(id, date_issued, title, abstract, word, n, total )\n\n# The usual suspects are here, “the”, “and”, “to”, and so forth. \n# ggplot(abs_words_T, aes(n/total, fill = title)) +\n#   geom_histogram(show.legend = FALSE) +\n#    xlim(NA, 0.01) +\n#   facet_wrap(~title, ncol = 2, scales = \"free_y\")\n```\n:::\n\n\n> `tidytext::bind_tf_idf`: Calculate and bind the term frequency and inverse document frequency of a tidy text dataset, along with the product, tf-idf, to the dataset. Each of these values are added as columns. This function supports non-standard evaluation through the tidyeval framework.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nabs_words_tf_idf <- abs_words_T %>%\n  #bind_tf_idf(tbl, term, document, n)\n  bind_tf_idf(      word, title, n) %>% # 5860\n  # get rid of stopwords anyway &...\n  anti_join(stop_words, by = \"word\") %>% # 3703\n  # fileter most importantly weighted \n  # filter(tf_idf > 0.01 ) %>% # 2278\n  arrange(date_issued, desc(tf_idf))\n\nabs_words_tf_idf\n```\n:::\n\n\nNotice that `idf` and thus `tf-idf` are zero for these extremely common words. These are all words that appear in all docs, so the `idf` term (which will then be the natural log of 1) is zero. \\\n\n> The ***inverse document frequency*** (and thus `tf-idf`) is very low (near zero) for words that occur in many of the documents in a collection; this is how this approach decreases the weight for common words.\n***IDF*** will be a higher number for words that occur in fewer of the documents in the collection.\n\nLet's look at recurring terms terms with high tf-idf in WDRs.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# wdr[ wdr$id %in% c(\"4391\" ) , c(\"date_issued\", \"title\" )]\n\n# let's look specifically at \"Gender Equality and Development\" \ntf_idf_2012 <- abs_words_tf_idf %>%\n  filter(date_issued == \"2012\") %>%\n  select(-total) %>%\n  arrange(desc(tf_idf))\n\n# These words are, as measured by tf-idf, the most important to \"Gender Equality and Development\"  and most readers would likely agree.\ntf_idf_2012[tf_idf_2012$word %in% c(\"gender\", \"equality\", \"development\") ,]\n\n# # A tibble: 3 × 7\n#   date_issued title                              word            n     tf   idf tf_idf\n#   <chr>       <chr>                              <chr>       <int>  <dbl> <dbl>  <dbl>\n# 1 2012        \" Gender Equality and Development\" gender         13 0.0588  3.09  0.182\n# 2 2012        \" Gender Equality and Development\" equality        6 0.0271  3.78  0.103\n# 3 2012        \" Gender Equality and Development\" development    10 0.0452  0     0 \n```\n:::\n\n\n### --- TF-IDF tables/ viz for selected WDRs\n\nInterestingly, some themes are recurrent in cycles (as per [@yusuf_development_2008]). So I wanted to check TF_IDF in these \"subsets\" of WDRs\n\n#### Poverty\n\n::: {.cell}\n\n```{.r .cell-code}\n # wdr[ wdr$id %in% c(\"5961\", \"5963\", \"5973\", \"11856\" ) , c(\"date_issued\", \"title\" )]\n\n# SIMPLE TABLE WITH FILTER \ntf_idf_poverty  <-  abs_words_tf_idf %>% \n  dplyr::filter(date_issued %in%  c(\"1978\", \"1980\", \"1990\", \"2001\")) %>% \n    dplyr::filter(n > 1) %>% \n  select(-id, -abstract) %>%\n  dplyr::arrange(date_issued, desc(tf_idf)) \n```\n:::\n\n::: {.cell}\n\n:::\n\n\n> What this TF-IDF measure shows is the specific words that distinguish each WDR in this subset themed on poverty: i.e. the point of tf-idf is to identify words that are important to one document within a collection of documents.\n\n#### ...viz\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(forcats) # Tools for Working with Categorical Variables (Factors)\n\ngg_pov_tfidf <-  tf_idf_poverty %>%\n  mutate (title2 = paste( \"WDR of \", date_issued )) %>% \n  group_by(title2) %>%\n  slice_max(tf_idf, n = 50) %>%\n  ungroup() %>% \n  ggplot(aes(tf_idf, fct_reorder(word, tf_idf), fill = title2)) +\n  geom_col(show.legend = FALSE) + \n  #my_theme() +\n  scale_fill_manual(values = mycolors_contrast) + # my palette\n  # labs(x = \"tf-idf for \", y = NULL) +\n  labs(title= bquote(\"TF-IDF ranking in 4 WDRs focused on \"~bold(\"poverty topic\")),  \n     subtitle=\"(overall 50 tokens with highest TF-IDF)\",\n     #caption=\"Source: ????\",\n     x=\"TF-IDF values\",\n     y=\"\"\n     ) + \n    facet_wrap(~title2, ncol = 2, scales = \"free\")  \n\ngg_pov_tfidf \n\ngg_pov_tfidf %T>% \n  ggsave(., filename = here(\"analysis\", \"output\", \"figures\", \"gg_pov_tfidf.pdf\"),\n        # width = 2.75, height = 1.5, units = \"in\", device = cairo_pdf\n        ) %>%\n  ggsave(., filename = here(\"analysis\", \"output\", \"figures\", \"gg_pov_tfidf.png\"),\n         #width = 2.75, height = 1.5, units = \"in\", type = \"cairo\", dpi = 300\n         )\n```\n:::\n\n\n\n\n#### Environment/Climate \n\n::: {.cell}\n\n```{.r .cell-code}\n#wdr[ wdr$id %in% c(\"5975\", \"4387\" ) , c(\"date_issued\", \"title\" )]\n\n# SIMPLE TABLE WITH FILTER \ntf_idf_env  <-  abs_words_tf_idf %>% \n  dplyr::filter(date_issued %in%  c(\"1992\", \"2010\")) %>% \n    dplyr::filter(n > 1) %>% \n  select(-id, -abstract) %>%\n  dplyr::arrange(date_issued, desc(tf_idf)) \n```\n:::\n\n::: {.cell}\n\n:::\n\n\n#### ...viz\n\nEvident how in the 2010 WDR, words like \"warming\" and \"temperatures\" appear, while they were unimportant in the 1992 flagship report.\n\n::: {.cell}\n\n```{.r .cell-code}\ngg_env_tfidf <- tf_idf_env %>%\n  mutate (title2 = paste( \"WDR of \", date_issued )) %>% \n  group_by(title2) %>%\n  slice_max(tf_idf, n = 50) %>%\n  ungroup()  %>% \n  ggplot(aes(tf_idf, fct_reorder(word, tf_idf), fill = title2)) +\n  geom_col(show.legend = FALSE) + \n  #my_theme() +\n  scale_fill_manual(values = mycolors_contrast) + # my palette\n  # labs(x = \"tf-idf for \", y = NULL) +\n  labs(title= bquote(\"TF-IDF ranking in 4 WDRs foucused on \"~bold(\"environment/climate change\")),      #title=\"TF-IDF ranking in 4 WDRs dedicated to environment/climate change topic\", \n    subtitle=\"(overall 50 tokens with highest TF-IDF)\",\n     #caption=\"Source: ????\",\n     x=\"TF-IDF values\",\n     y=\"\"\n     ) + \n    facet_wrap(~title2, ncol = 2, scales = \"free\")  \n\ngg_env_tfidf \n\ngg_env_tfidf %T>% \n  ggsave(., filename = here(\"analysis\", \"output\", \"figures\", \"gg_env_tfidf.pdf\"),\n        # width = 2.75, height = 1.5, units = \"in\", device = cairo_pdf\n        ) %>%\n  ggsave(., filename = here(\"analysis\", \"output\", \"figures\", \"gg_env_tfidf.png\"),\n         #width = 2.75, height = 1.5, units = \"in\", type = \"cairo\", dpi = 300\n         )\n```\n:::\n\n\n#### Knowledge/data\n\n::: {.cell}\n\n```{.r .cell-code}\n# skim(abs_words_tf_idf$tf_idf)\n# wdr[ wdr$id %in% c(\"5981\", \"35218\" ) , c(\"date_issued\", \"title\" )]\n\n# SIMPLE TABLE WITH FILTER \ntf_idf_knowl  <-  abs_words_tf_idf %>% \n  dplyr::filter(date_issued %in%  c(\"1998\", \"2021\")) %>% \n    dplyr::filter(n > 1) %>% \n  select(-id, -abstract) %>%\n  dplyr::arrange(date_issued, desc(tf_idf))\n```\n:::\n\n::: {.cell}\n\n:::\n\n\n#### ...viz\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngg_knowl_tfidf  <- tf_idf_knowl %>% \n  dplyr::filter(date_issued %in%  c(\"1998\", \"2021\")) %>%\n  mutate (title2 = paste( \"WDR of \", date_issued )) %>% \n  group_by(title2) %>%\n  slice_max(tf_idf, n = 50) %>%\n  ungroup()  %>% \n  ggplot(aes(tf_idf, fct_reorder(word, tf_idf), fill = title2)) +\n  geom_col(show.legend = FALSE) + \n  #my_theme() +\n  scale_fill_manual(values = mycolors_contrast) + # my palette\n  # labs(x = \"tf-idf for \", y = NULL) +\n  labs(title= bquote(\"TF-IDF ranking in 4 WDRs foucused on \"~bold(\"knowledge/data\")),\n       #title=\"TF-IDF ranking in 4 WDRs dedicated to knowledge/data change topic\", \n     subtitle=\"(overall 50 tokens with highest TF-IDF)\",\n     #caption=\"Source: ????\",\n     x=\"TF-IDF values\",\n     y=\"\"\n     ) + \n    facet_wrap(~title2, ncol = 2, scales = \"free\")  \n\ngg_knowl_tfidf \n\ngg_knowl_tfidf %T>% \n  ggsave(., filename = here(\"analysis\", \"output\", \"figures\", \"gg_knowl_tfidf.pdf\"),\n        # width = 2.75, height = 1.5, units = \"in\", device = cairo_pdf\n        ) %>%\n  ggsave(., filename = here(\"analysis\", \"output\", \"figures\", \"gg_knowl_tfidf.png\"),\n         #width = 2.75, height = 1.5, units = \"in\", type = \"cairo\", dpi = 300\n         )\n```\n:::\n\n\n\n## III.iii) Word frequency histogram *{meaningless}*\n\n### --- SCHMIDT's Plotting most frequent words (all abstractS) ----------------\n\nhttp://benschmidt.org/HDA/texts-as-data.html\n\n\n\n\nThe simple plot gives a very skewed curve: As always, you should experiment with multiple scales, and especially *think about logarithms*. Putting logarithmic scales on both axes reveals something interesting about the way that data is structured; this turns into a straight line. \n\n> “Zipf’s law:” the most common word is twice as common as the second most common word, three times as common as the third most common word, four times as common as the fourth most common word, and so forth.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Putting logarithmic scales on both axes  \nggplot(wordcounts) +\n  aes(x = rank, y = n, label = word) +\n  geom_point(alpha = .3, color = \"grey\") +\n  geom_text(check_overlap = TRUE) +\n  scale_x_continuous(trans = \"log\") +\n  scale_y_continuous(trans = \"log\") +\n  labs(title = \"Zipf's Law\",\n       subtitle=\"The log-frequency of a term is inversely correlated with the logarithm of its rank.\")\n# ...the logarithm of rank decreases linearly with the logarithm of count\n```\n:::\n\n[the logarithm of rank decreases linearily with the logarithm of count.] --> **common words are very common indeed, and logarithmic scales are more often appropriate for plotting than linear ones. **\n\n\n### --- SILGE's Plotting most frequent words (all abstractS) ----------------\n\n* OKKIO n instead of n/total \n\n\n::: {.cell}\n\n```{.r .cell-code}\n# abs_words_T --> 5860\n\n# let's eliminate stopwords \nabs_words2 <-  anti_join(x = abs_words_T, y = stop_words, by= \"word\" )  %>% \n  #filter(n > 1) %>% \n select(date_issued, title, word, n, total)\n\nabs_words2 #--> 3699\n# paint(abs_words2)\n# here there is one row for each word-WDR(abs) combination \n# `n` is the number of times that word is used in that book and \n# `total` is the total words in that abstract\n```\n:::\n\n\n***frequency*** = let's look at the distribution of `n/total` for each doc, the number of times a word appears in a doc divided by the total number of terms (words) in that doc\n\n> I actually use `n` instead because I have only small numbers having used the abstracts alone\n\n\n::: {.cell}\n\n```{.r .cell-code}\none <- abs_words2 %>% \n  filter ( date_issued == \"2021\") %>% \n  mutate (freq = n/total)\n \nggplot(data = one, \n       mapping = aes(x = n, fill = title)) + # y axis not needed ... R will count\n  geom_histogram(binwidth = 1,\n                 color = \"white\") +\n  scale_y_continuous(breaks= pretty_breaks()) +\n  xlim(0,10) +\n  labs(# title =  title, \n    x = \"frequency\",\n    y = \"N of words @ that frequency\") + \n  guides( fill = \"none\")\n\n# #  skim(one$freq)\n# ggplot(data = one, \n#        mapping = aes(x = freq, fill = title)) + # y axis not needed ... R will count\n#   geom_histogram(binwidth = 1,\n#                  color = \"white\") +\n#   scale_y_continuous(breaks= pretty_breaks()) +\n#    xlim(0, 0.1) +\n#   labs(# title =  title, \n#     x = \"frequency\",\n#     y = \"N of words @ that frequency\") + \n#   guides( fill = \"none\")\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# overlayed mess!\nggplot(abs_words2, aes(n, fill = title)) +\n  geom_histogram(binwidth = 1,\n                 color = \"white\") +\n  scale_y_continuous(breaks= pretty_breaks()) +\n  xlim(0, 20) +\n  labs(#title = ~date_issued, \n    x = \"frequency\",\n    y = \"N of words @ that frequency\") + \n  guides( fill = \"none\")\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(abs_words2, aes(n, fill = title)) +\n  geom_histogram(binwidth = 1,\n                 color = \"white\") +\n  scale_y_continuous(breaks= pretty_breaks()) +\n  xlim(0, 10) +\n  labs(#title = ~date_issued, \n    x = \"frequency\",\n    y = \"N of words @ that frequency\") + \n  facet_wrap( ~date_issued ) + # , ncol = 2, scales = \"free_y\")\n  guides( fill = \"none\") # way to turn legend off\n```\n:::\n\n\n#### --- Multiple plots of Word Freq with [purrr]\n\n- [Jennifer Thompson, An Intro to the Magic of purrr](https://htmlpreview.github.io/?https://github.com/jenniferthompson/RLadiesIntroToPurrr/blob/master/purrr_intro.nb.html)\n\n-  [automate ggplots while using variable labels as title and axis titles](https://stackoverflow.com/questions/59095237/automate-ggplots-while-using-variable-labels-as-title-and-axis-titles)\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# ---- NON Capisco \n# # Preferred approach\n# histos <- abs_words2 %>%\n#   group_by(title) %>%\n#   nest() %>%\n#   mutate(plot = map2(data, title, \n#                      ~ggplot(data = .x , aes(n/total, fill = title)) + \n#                       geom_histogram( show.legend = FALSE) + \n#                        xlim(NA, 0.05) +\n#                        # ggtitle(.y) +\n#                        ylab(\"Words Frequency\") +\n#                        xlab(\"Distribution per WDR title\")))\n# \n#  histos$plot[[1]]\n\n\n# ---- Capisco \n# https://stackoverflow.com/questions/60671725/ggplot-add-title-based-on-the-variable-in-the-dataframe-used-for-plotting\nlist_plot <- abs_words2 %>%\n  dplyr::group_split(date_issued) %>% # Split data frame by groups\n  map(~ggplot(., \n              mapping = aes(x = n, fill = title)) +\n        geom_histogram(binwidth = 1,\n                       color = \"white\") +\n        scale_y_continuous(breaks= pretty_breaks()) + # integer ticks {scales}\n        xlim(0, 18) + # max of freq is 17\n        labs(title = .$date_issued, \n             x = \"frequency\",\n             y = \"N of words @ that frequency\")\n  )\n\nlist_plot[[1]]\nlist_plot[[44]]\nlist_plot[[40]]\n#grid.arrange(grobs = list_plot, ncol = 1)\n```\n:::\n\n\n#### --- Zipf’s law for WDR’s abstracts\nExamine Zipf’s law for WDR’s abstracts with just a few lines of dplyr functions.\n> The rank column here tells us the rank of each word within the frequency table; the table was already ordered by n so we could use row_number() to find the rank\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfreq_by_rank <- abs_words2 %>% \n  arrange(desc(n)) %>% # order by n... \n  group_by(title) %>% \n  mutate(rank = row_number(), # so this can act as the rank\n         `term frequency` = n/total) %>% # term frequency\n  ungroup()\nfreq_by_rank\n```\n:::\n\n\nZipf’s law is often visualized by plotting rank on the x-axis and term frequency on the y-axis, on logarithmic scales. Plotting this way, an inversely proportional relationship will have a constant, negative slope.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfreq_by_rank %>% \n  filter (date_issued == \"2021\" | date_issued == \"1998\") %>% \n  ggplot(aes(rank, `term frequency`, color = title)) + \n  geom_line(size = 1.1, alpha = 0.8, show.legend = FALSE) + \n  scale_x_log10() +\n  scale_y_log10() + \n  labs(title = \"Zipf’s law seen for  knowledge (1998) & data (2021) WDRs\",\n       subtitle = \"(1998) = blue | (2021) = red\",\n       x = \"rank (log)\",\n       y = \"term frequency (log)\",\n       color = \"Legend\")   \n```\n:::\n\n\nhttps://www.tidytextmining.com/tfidf.html#zipfs-law\n\nperhaps we could view this as a broken power law with, say, three sections. Let’s see what the exponent of the power law is for the middle section of the rank range.\n\n::: {.cell}\n\n```{.r .cell-code}\nrank_subset <- freq_by_rank %>% \n  filter(rank < 500,\n         rank > 10)\n\nlm(log10(`term frequency`) ~ log10(rank), data = rank_subset)\n```\n:::\n\n\nLet’s plot this fitted power law with the obtaied data  to see how it looks\n\n::: {.cell}\n\n```{.r .cell-code}\nfreq_by_rank %>% \n  filter (date_issued == \"2021\" | date_issued == \"1998\") %>% \n  ggplot(aes(rank, `term frequency`, color = title)) + \n  geom_abline(intercept = -1.80, slope = -0.33, \n              color = \"gray50\", linetype = 2) +\n  geom_line(size = 1.1, alpha = 0.8, show.legend = FALSE) + \n  scale_x_log10() +\n  scale_y_log10()\n```\n:::\n\n\n> The deviations we see here at high rank are not uncommon for many kinds of language; **a corpus of language often contains fewer rare words than predicted by a single power law**.\n\n\n## III.iv) Relationships between words: n-grams and correlations\n\nhttps://www.tidytextmining.com/ngrams.html\nhttps://bookdown.org/Maxine/tidy-text-mining/tokenizing-by-n-gram.html\n\n>The one-token-per-row framework can be extended from single words to n-grams and other meaningful units of text(e.g. to see which words tend to follow others immediately, or that tend to co-occur within the same documents.)\n\n  + `tidytext::token = \"ngrams\" argument` is a method tidytext offers for calculating and visualizing relationships between words in your text dataset. It tokenizes by pairs of adjacent words rather than by individual ones. \n  + `ggraph` extends `ggplot2` to construct network plots, \n  + `widyr` calculates pairwise correlations and distances within a tidy data frame.  \n\n#### --- Tokenizing by n-gram\nThe `unnest_tokens` function can also be used to tokenize into consecutive sequences of words, called _n-grams_. By seeing how often word X is followed by word Y, we can then build a model of the relationships between them.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# break the text [e.g. 1 abstract] into bi-gram \nabs_2022_bigrams <- wdr %>% \n  dplyr::filter(id == 36883) %>% \n  select(abstract) %>% \n  as_tibble() %>% \n  unnest_tokens(., output = bigram, input = abstract, token = \"ngrams\", n=2 )  \n# notice how these bigrams overlap\nhead(abs_2022_bigrams)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# # my choice \n# abs_1_t_mod <- abs_1 %>% \n#   # no punctuation, yes capitalized\n#   unnest_tokens(word, value, to_lower = F, strip_punc = TRUE) %>% # 249 obs\n#   # exclude stopwords \n#   anti_join(stop_words) # 109 obs\n\nabs_all_bigram <- abs_all %>% \n  unnest_tokens(., output = bigram, input = abstract, token = \"ngrams\", n=2 )  \n\nhead(abs_all_bigram[c(\"date_issued\",\"bigram\")], 10)\n```\n:::\n\n\n> This data structure is still a variation of the tidy text format. It is structured as one-token-per-row but each token now represents a bigram.\n\n#### --- Operations on n-grams: counting and filtering \n\n\n::: {.cell}\n\n```{.r .cell-code}\nbigrams_counts <- abs_all_bigram %>% \n   count(bigram, sort = TRUE)\n\nhead(bigrams_counts)\n```\n:::\n\n> Not surprisingly, a lot are pairs of stopwords  \n\nHere, I can use `tidyr::separate()`, which splits a column into multiple based on a delimiter. This separate it into two columns, “word1” and “word2”, to _then remove cases where either is a stop-word_.\n\nIn other analyses, we may want to work with the recombined words. `tidyr’s unite()` function is the inverse of `separate()`, and lets us recombine the columns into one. \n\n\n::: {.cell}\n\n```{.r .cell-code}\n# separate words \nbigrams_separated <- abs_all_bigram %>% \n  tidyr::separate(bigram, c(\"word1\", \"word2\"), sep = \" \")\n\n# since many are bigram with a stopword\nbigrams_filtered <- bigrams_separated %>% \n  # remove cases where either is a stop-word\n  filter(!word1 %in% stop_words$word) %>%\n  filter(!word2 %in% stop_words$word)\n\n# OPPOSITE: reunite words \nbigrams_united <- bigrams_filtered  %>% \n  unite(bigram, word1, word2, sep = \" \")\n```\n:::\n\n\n\n#### --- (Trigrams)\n\nIn other analyses you may be interested in the most common trigrams, which are consecutive sequences of 3 words. We can find this by setting n = 3:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntrigram <- abs_all %>%\n  unnest_tokens(trigram, abstract, token = \"ngrams\", n = 3) %>%\n  separate(trigram, c(\"word1\", \"word2\", \"word3\"), sep = \" \") %>%\n  filter(!word1 %in% stop_words$word,\n         !word2 %in% stop_words$word,\n         !word3 %in% stop_words$word) %>%\n  count(word1, word2, word3, sort = TRUE)\n```\n:::\n\n\n\n#### --- Bigram ~ potential meaningful SLOGANs? \n\n<i class=\"fa fa-question-circle\" style=\"color: firebrick\"></i> Which of this bigram might be a SLOGAN candidate? \n\n\n::: {.cell}\n\n```{.r .cell-code}\n# new bigram counts:\nbigrams_counts_clean <- bigrams_filtered %>% \n  # Count observations by group\n  count(word1, word2, sort = TRUE)\n\nhead(bigrams_counts_clean, 20) # cleaned up stopwords\n```\n:::\n\n\n<i class=\"fa fa-exclamation-circle\" style=\"color: blue\"></i> ...Maybe some of these bigram with high tf-idf\n\n+ external finance\n+ gender equality\n+ development impact\n+ digital revolution\n+ investment climate\n+ accelerating growth\n+ alleviating poverty\n+ ...\n \n#### --- Analyzing bigrams\n\nThis one-bigram-per-row format is helpful for exploratory analyses of the text. Let's see what comes _before_ \"poverty\", \"change\", \"knowledge\"...\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbigrams_filtered %>%\n  filter(word2 == \"poverty\") %>%\n  count(date_issued, word1, sort = TRUE)\n\nbigrams_filtered %>%\n  filter(word2 == \"change\") %>%\n  count(date_issued, word1, sort = TRUE)\n\nbigrams_filtered %>%\n  filter(word2 == \"knowledge\") %>%\n  count(date_issued, word1, sort = TRUE)\n```\n:::\n\n\n...or _after_ \"human\", \"finance\", \"bottom\": \n\n::: {.cell}\n\n```{.r .cell-code}\n# after  \nbigrams_filtered %>%\n  filter(word1 == \"human\") %>%\n  count(date_issued, word2, sort = TRUE)\n\nbigrams_filtered %>%\n  filter(word1 == \"finance\") %>%\n  count(date_issued, word2, sort = TRUE)\n\nbigrams_filtered %>%\n  filter(word1 == \"bottom\") %>%\n  count(date_issued, word2, sort = TRUE)\n```\n:::\n\n\n#### --- Analyzing bigrams: tf-idf\n> There are advantages and disadvantages to examining the tf-idf of bigrams rather than individual words. Pairs of consecutive words might capture structure that isn’t present when one is just counting single words, and _may provide context that makes tokens more understandable_. However, ***the per-bigram counts are also sparser*** (a typical two-word pair is rarer than either of its component words). \n\n\n::: {.cell}\n\n```{.r .cell-code}\nbigram_tf_idf <- abs_all_bigram %>% \n  # reconstruct the  separated + filtered + united\n  separate(bigram, c(\"word1\", \"word2\"), sep = \" \") %>% \n   filter(!word1 %in% stop_words$word) %>%\n   filter(!word2 %in% stop_words$word) %>% \n  # Put the two word columns back together\n  unite(bigram, word1, word2, sep = \" \") %>% \n\n  # then on that calculate tf-idf\n count(date_issued, bigram) %>%\n  bind_tf_idf(bigram, date_issued, n) %>%\n  arrange(desc(tf_idf))\n\nhead(bigram_tf_idf)\n```\n:::\n\n\n\n#### --- (Using bigrams to provide context in sentiment analysis)\n...\n\n#### --- Visualizing a network of bigrams with `ggraph`\n\nThe `igraph` package has many powerful functions for manipulating and analyzing networks. \n\nOne way to create an igraph object from tidy data is the `igraph::graph_from_data_frame()` function, which takes a data frame of edges with columns for “from”, “to”, and edge attributes (in this case \"n\"):\n\n> If vertices is NULL, then the first two columns of df (e.g. word1 = FROM & word2 = TO) are used as a symbolic edge list and additional columns (e.g. n) as edge attributes/weight. The names of the attributes are taken from the names of the columns. \n\nHere, a graph can be constructed from the tidy object `bigrams_counts_clean` since it has three variables.  \n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(igraph) # Network Analysis and Visualization\nbigrams_counts_clean\n\n# filter for only relatively common combinations\nbigram_graph <- bigrams_counts_clean %>%\n  filter(n > 2) %>%\n  # create an igraph graph from data frames containing the (symbolic) edge list and edge/vertex attributes. \n  igraph::graph_from_data_frame()\n```\n:::\n\n\nThen we can convert an igraph object into a `ggraph` with the `ggraph` function (extension of `ggplot2`), after which we can add layers to it, much like layers are added in `ggplot2.` For example, for a basic graph we need to add three layers: \"nodes\", \"edges\", and \"text\".\n\n::: {.cell}\n\n```{.r .cell-code}\n#convert an igraph object into a ggraph with the ggraph function\nlibrary(ggraph) # An Implementation of Grammar of Graphics for Graphs and Networks\nset.seed(2022)\n\nggraph(bigram_graph, layout = \"fr\") +\n  # needed basic arguments passed\n  geom_edge_link() +\n  geom_node_point() +\n  geom_node_text(aes(label = name), vjust = 1, hjust = 1)\n```\n:::\n\n\n> I can already see some common center nodes \n\nWe conclude with a few polishing operations to make a better looking graph (Figure 4.5):\n\n  + We add the `edge_alpha` aesthetic to the link  layer to make links transparent based on how common or rare the bigram is (= n)\n  + We add directionality with an arrow, constructed using `grid::arrow()`, including an `end_cap` option that tells the arrow to end before touching the node\n  + We tinker with the options to the node layer to make the nodes more attractive (larger, blue points)\n  + We add a theme that’s useful for plotting networks, `theme_void()`\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(2022)\n\na <- grid::arrow(type = \"closed\", length = unit(.08, \"inches\"))\n\nabs_bigram_graph <- ggraph(bigram_graph, layout = \"fr\") +\n  # LINK layer\n  geom_edge_link(aes(edge_alpha = n), # transparency of link based on n\n                 show.legend = FALSE,\n                 # direction\n                 arrow = a,\n                 # arrow to end before touch node\n                 end_cap = circle(.08, 'inches')) +\n  # NODE layer\n  geom_node_point(color = \"lightblue\", size = 3) +\n  geom_node_text(aes(label = name), \n                 vjust = 1, hjust = 1,\n                 check_overlap = TRUE, \n                 repel = FALSE  # adds more lines\n                 \n  ) +\n  # THEME\n  theme_void() +\n  ggtitle(\"Word Network in WDR's abstracts\")  \n\nabs_bigram_graph\n\nabs_bigram_graph %T>% \n  print() %T>%\n  ggsave(., filename = here(\"analysis\", \"output\", \"figures\", \"abs_bigram_graph.pdf\"),\n         #width = 4, height = 2.25, units = \"in\",\n         device = cairo_pdf) %>% \n  ggsave(., filename = here(\"analysis\", \"output\", \"figures\", \"abs_bigram_graph.png\"),\n         #width = 4, height = 2.25, units = \"in\", \n         type = \"cairo\", dpi = 300)  \n```\n:::\n\n\n\n#### --- Counting and correlating among sections\n[Notes for “Text Mining with R: A Tidy Approach”](https://bookdown.org/Maxine/tidy-text-mining/counting-and-correlating-pairs-of-words-with-widyr.html)\n\nThe `widyr` package makes operations such as computing counts and correlations easy, by simplifying the pattern of _“widen data -> perform an operation -> then re-tidy data”_. We’ll focus on a set of functions that make pairwise comparisons between groups of observations (for example, between documents, or sections of text).\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# divide abstracts into 5-line sections \nabs_section_words <- abs_all %>%\n  mutate(text = stringi::stri_split_lines(abstract, omit_empty = FALSE)\n) %>% \n  #filter(date_issued == \"1978\") %>%\n  mutate(section = row_number(.$abstract) %/% 5) %>%\n  filter(section > 0) %>%\n  unnest_tokens(output =  word,\n                input = abstract) %>%\n  filter(!word %in% stop_words$word)\n```\n:::\n\n\n`widyr::pairwise_counts()` counts the number of times each pair of _items_ (words) appear together within a group defined by _“feature”_ (section). \n> note it still returns a tidy data frame, although the underlying computation took place in a matrix form :\n\n\n::: {.cell}\n\n```{.r .cell-code}\nabs_section_words %>% \n  widyr::pairwise_count(item = word, feature = section, sort = TRUE) %>% \n# Since pairwise_count records both the counts of (word_A, word_B) and \n#(word_B, word_B), it does not matter we filter at item1 or item2\n  filter(item1 == \"developing\")\n```\n:::\n\n\n#### --- Pairwise correlation\nWe may want to examine correlation among words, which indicates how often they appear together relative to how often they appear separately.\n\nwe compute the $\\phi$ coefficient. Introduced by Karl Pearson, this measure is similar to the Pearson correlation coefficient in its interpretation. In fact, a Pearson correlation coefficient estimated for two binary variables will return the $\\phi$ coefficient. The phi coefficient is related to the chi-squared statistic for a 2 × 2 contingency table \n\n$$\n\\phi = \\sqrt{\\frac{\\chi^2}{n}}\n$$\n\nwhere $n$ denotes sample size. In the case of pairwise counts, $\\phi$ is calculated by \n\n\n::: {.cell}\n\n:::\n\n\n<!-- ![](../images/phi.png) -->\n\n$$\n\\phi = \\frac{n_{11}n_{00} - n_{10}n_{01}}{\\sqrt{n_{1·}n_{0·}n_{·1}n_{·0}}}\n$$\n\nWe see, from the above equation, that $\\phi$ is \"standardized\" by individual counts, so various word pair with different individual frequency can be compared to each other:  \n\nThe computation of $\\phi$ can be simply done by `pairwise_cor` (other choice of correlation coefficients specified by `method`). The procedure can be somewhat computationally expensive, so we filter out uncommon words\n\n\n::: {.cell}\n\n```{.r .cell-code}\nword_cors <- abs_section_words %>% \n  add_count(word) %>% \n  filter(n >= 20) %>% \n  select(-n) %>%\n  pairwise_cor(word, section, sort = TRUE)\n```\n:::\n\n\nWhich word is most correlated with \"poor\"? [health,people,governments, data ]\n\n\n::: {.cell}\n\n```{.r .cell-code}\nword_cors %>% \n  filter(item1 == \"poor\")\n```\n:::\n\n\nThis lets us pick particular interesting words and find the other words most associated with them  \n\n\n::: {.cell}\n\n```{.r .cell-code}\nsource(\"R/f_facetted_bar_plot.R\")\n\np_ass_words <- word_cors %>%\n  filter(item1 %in% c( \"people\", \"governments\", \"markets\", \"institutions\")) %>%\n  group_by(item1) %>%\n  top_n(6) %>%\n  ungroup() %>%\n  facet_bar(y = item2, x = correlation, by = item1) +\n  labs(title=\"Words most correlated to selected words of interest\", \n    subtitle=\"(Taken from WDRs' abstracts)\",\n     #caption=\"Source: ????\",\n     )   \n\np_ass_words %T>% \n  print() %T>%\n  ggsave(., filename = here(\"analysis\", \"output\", \"figures\", \"p_ass_words.pdf\"),\n         #width = 4, height = 2.25, units = \"in\",\n         device = cairo_pdf) %>% \n  ggsave(., filename = here(\"analysis\", \"output\", \"figures\", \"p_ass_words.png\"),\n         #width = 4, height = 2.25, units = \"in\", \n         type = \"cairo\", dpi = 300)  \n```\n:::\n\n\nHow about a network visualization to see the overall correlation pattern? \n\n\n::: {.cell}\n\n```{.r .cell-code}\nword_cors %>%\n  filter(correlation > .15) %>%\n  tidygraph::as_tbl_graph() %>%\n  ggraph(layout = \"fr\") +\n  geom_edge_link(aes(edge_alpha = correlation), show.legend = FALSE) +\n  geom_node_point(color = \"lightblue\", size = 5) +\n  geom_node_text(aes(label = name), repel = TRUE)\n```\n:::\n\n\n\nNote that unlike the bigram analysis, the relationships here are **symmetrical, rather than directional** (there are no arrows). \n\n##  III.iv) Concordances -> KWIC -> Collocation   \n\n+ {Following [LADAL Tutorial](https://ladal.edu.au/kwics.html)}\n+ {Following [Ben Schmidt, Chp 8.2.3](http://benschmidt.org/HDA/texts-as-data.html#concordances)}\n+ https://alvinntnu.github.io/NTNU_ENC2036_LECTURES/corpus-analysis-a-start.html\n\nIn the language sciences, ***concordancing*** refers to the extraction of words from a given text or texts. Concordances are commonly displayed in the form of ***keyword-in-context displays (KWICs)*** where the search term is shown in context, i.e. with preceding and following words. \n\nConcordancing is central to analyses of text and they often represents the first step in more sophisticated analyses of language data, because _concordances are extremely valuable for understanding how a word or phrase is used, how often it is used, and in which contexts is used_. As concordances allow us to analyze the context in which a word or phrase occurs and provide frequency information about word use, they also enable us to analyze collocations or the collocational profiles of words and phrases (Stefanowitsch 2020, 50–51). Finally, concordances can also be used to extract examples and it is a very common procedure.\n\n#### --- Concordances\n<!-- + Concordances: [Libro Agnese - ITA](https://www.agnesevardanega.eu/books/analisi-testuale-2021/05-esplorazione.html#espl-quanteda) -->\n+ https://ladal.edu.au/textanalysis.html#Concordancing\n+ https://www.quantumjitter.com/project/deal/\n\n> Using `quanteda` \n\n#### --- create kwic with individual keyword | purrr + print + save png\n\n::: {.cell}\n\n```{.r .cell-code}\n# I use again data = abs_words\nabs_q_corpus <- quanteda::corpus(as.data.frame(abs_all), \n                               docid_field = \"title\",\n                               text_field = \"abstract\",\n                               meta = list(\"id\", \"date_issued\")\n)\n \n# --- example with individual keyword \n# Step 1) tokens\nabs_q_tokens <- tokens(x = abs_q_corpus,\n                       remove_punct = TRUE,\n                       remove_symbols = TRUE#,remove_numbers = TRUE\n )\n# Step 2) kwic (individual exe )\n# kwic_abs_data <- quanteda::kwic(x = abs_q_tokens, # define text(s) \n#                                  # define pattern\n#                                  pattern = phrase(c(\"data\", \"knowledge\")),\n#                                  # define window size\n#                                  window = 5) %>%\n#     # convert into a data frame\n#     as_tibble() %>%\n#     left_join(abs_all, by = c(\"docname\" =  \"title\")) %>%  \n#     # remove superfluous columns\n#      dplyr::select( 'Year' = date_issued, 'WDR title' = docname, pre, keyword, post) %>%\n#   #  slice_sample( n = 50) %>% \n#    kbl(align = \"c\") # %>% kable_styling()\n \n# Step 2) kwic (on vector)\n# Iterate `quanteda::kwic` over a vector of tokens | regex-modified-keywords\nkeywords <- c(\"data\", \"globalization\", \"sustainab*\", \"conditionalit*\", \"regulat*\", \"ODA\"  )\n\n# apply iteratively kwic over a vector of keywords\noutputs_key <-  map(keywords, \n      ~quanteda::kwic(abs_q_tokens,\n                      pattern =  .x,\n                      window = 5) %>% \n        as_tibble() %>%\n        left_join(abs_all, by = c(\"docname\" =  \"title\")) %>%  \n        # remove superfluous columns\n        dplyr::select( 'Year' = date_issued, 'WDR title' = docname, pre, keyword, post) \n  )\n\n# # all togetha 3\nn = length(keywords)\n  \n# outputs_key[[1]] %>% \n#    kbl(align = \"c\") \n\n# this list  has no element names \nnames(outputs_key)\nn = length(keywords)\n# set names for elements \noutputs_key <- outputs_key %>% \n  set_names(paste0(\"kwic_\", keywords))\n\n# get rid of empty output dfs in list  \noutputs_key <- outputs_key[sapply(\n  outputs_key, function(x) dim(x)[1]) > 0] # 4 left!\n \n# -------------- print all \n# Modo 1 - walk + print -\nwalk(.x = outputs_key, .f = print)  \n\n# Modo 2 - walk + kbl -\n#walk(.x = outputs_key, .f = kbl)\n\n# # Modo 3 - imap??? + kbl -\n# purrr::imap(.x = outputs_key,\n#             .f = ~ {\n#               kbl(x = .x,\n#                   align = \"c\",\n#                   #format  = \"html\",\n#                   caption =.y\n#               ) # %>% kable_styling()\n#             }\n# )\n\n# MODO 4 -> create multiple tables from a single dataframe and save them as images\n# https://stackoverflow.com/questions/69323569/how-to-save-multiple-tables-as-images-using-kable-and-map/69323893#69323893\noutputs_key  %>%\n  imap(~save_kable(file = paste0('analysis/output/tables/', .y, '_.png'),\n                 # bs_theme = 'journal', \n                  self_contained = T, \n                  x = kbl(.x, booktabs = T, align = c('l','l', 'c')) %>%\n                    kable_styling() \n                   )\n    )\n```\n:::\n\n\n#### --- create kwic with phrases | purrr + print + save png\n\n::: {.cell}\n\n```{.r .cell-code}\n# Iterate `quanteda::kwic` over a vector of phrases/bigrams \nkeywords_phrase <- c(\"climate change\", \"investment climate\", \"pro-poor\", \n                     \"gender equality\", \"maximizing finance\", \"digital revolution\",\n                     \"private finance\")\n\n# Step 1) tokens\n# (done above) -> abs_q_tokens\n\n# Step 2) kwic \n# apply iteratively kwic over a vector of bigrams\noutputs_bigrams <- map(keywords_phrase,\n                       ~quanteda::kwic(x = abs_q_tokens, # define text(s) \n                                       # define pattern\n                                       pattern = quanteda::phrase(.x),\n                                       # define window size\n                                       window = 5) %>%\n                         # convert into a data frame\n                         as_tibble() %>%\n                         left_join(abs_all, by = c(\"docname\" =  \"title\")) %>%  \n                         # remove superfluous columns\n                         dplyr::select( 'Year' = date_issued,\n                                        'WDR title' = docname, pre, keyword, post)\n                       )  \n\n#  number ofo cbigrams \nn_bi = length(keywords_phrase)\nn_bi # 7\n# name this list's elements \noutputs_bigrams <- outputs_bigrams %>% \n  set_names(paste0(\"kwic_\", keywords_phrase))  \n\n# get rid of empty output dfs in list  \noutputs_bigrams2 <- outputs_bigrams[sapply(\n  outputs_bigrams, function(x) dim(x)[1]) > 0] # 4 left!\n \n#or \noutputs_bigrams3 <- purrr::keep(outputs_bigrams, ~nrow(.) > 0)  # 4 left!\n\n# -------------- print all \n#  walk + print -\nwalk(.x = outputs_bigrams2, .f = print)  \n\n\n# -------------- save  all -> create multiple tables from a single dataframe and save them as images\n# https://stackoverflow.com/questions/69323569/how-to-save-multiple-tables-as-images-using-kable-and-map/69323893#69323893\noutputs_bigrams2  %>%\n  imap(~save_kable(file = paste0('analysis/output/tables/', .y, '_.png'),\n                   # bs_theme = 'journal', \n                   self_contained = T, \n                   x = kbl(.x, booktabs = T, align = c('l','l', 'c')) %>%\n                     kable_styling() \n  )\n  )\n```\n:::\n\n\n#### --- Collocation\n<!-- + Collocation - words commonly appearing near each other As in [Computing for the Social Sciences](https://cfss.uchicago.edu/notes/text-analysis-workflow/)\\ -->\n\n+ https://ladal.edu.au/coll.html#2_Finding_Collocations\n\n**Collocations** are words that are attracted to each other (and that co-occur or co-locate together), e.g., Merry Christmas, Good Morning, No worries. Any word in any given language has collocations, i.e., others words that are attracted/attractive to that word. This allows us to anticipate what word comes next and collocations are context/text type specific. There are various different statistical measures are used to define the strength of the collocations, like the Mutual Information (MI) score and log-likelihood (see here for an over view of different association strengths measures).\n\n##### --> EXE: Collocation for subset on poverty WDR\n1. In a first step, we will split the Abstract into individual sentences. \n\n::: {.cell}\n\n```{.r .cell-code}\n# reduce to just one long concatenated string \nabs_pov <- abs_all %>% \n  dplyr::filter(date_issued %in%  c(\"1978\", \"1980\", \"1990\", \"2001\")) %>%  \n  select( abstract) %>%\n summarize(text = str_c(abstract, collapse = \". \")) %>% \n  as.character()\n  \n# read in and process text\nabs_pov_sentences <-  abs_pov %>%\n  stringr::str_squish() %>%\n  # divide into sentences\n  tokenizers::tokenize_sentences(.) %>%\n  unlist() %>%\n  stringr::str_remove_all(\"- \") %>%\n  stringr::str_replace_all(\"\\\\W\", \" \") %>%\n  stringr::str_squish()\n\n# inspect data\nhead(abs_pov_sentences)\n```\n:::\n\n\nIn a next step, we will create a matrix that shows how often each word co-occurred with each other word in the data.\n\n::: {.cell}\n\n```{.r .cell-code}\n# convert into corpus\nabs_pov_corpus <- Corpus(VectorSource(abs_pov_sentences))\n\n# create vector with words to remove\nextrawords <- c(\"the\", \"can\", \"get\", \"got\", \"can\", \"one\", \n                \"dont\", \"even\", \"may\", \"but\", \"will\", \n                \"much\", \"first\", \"but\", \"see\", \"new\", \n                \"many\", \"less\", \"now\", \"well\", \"like\", \n                \"often\", \"every\", \"said\", \"two\")\n\n# clean corpus\nabs_pov_corpus_clean <- abs_pov_corpus %>%\n  tm::tm_map(removePunctuation) %>%\n  tm::tm_map(removeNumbers) %>%\n  tm::tm_map(tolower) %>%\n  tm::tm_map(removeWords, stopwords()) %>%\n  tm::tm_map(removeWords, extrawords)\n\n# create document term matrix\nabs_pov_dtm <- DocumentTermMatrix(\n  abs_pov_corpus_clean, \n  control=list(bounds = list(global=c(1, Inf)),\n               weighting = weightBin))\n\n# convert dtm into sparse matrix\nabs_pov_sdtm <- Matrix::sparseMatrix(i = abs_pov_dtm$i, j = abs_pov_dtm$j, \n                           x = abs_pov_dtm$v, \n                           dims = c(abs_pov_dtm$nrow, abs_pov_dtm$ncol),\n                           dimnames = dimnames(abs_pov_dtm))\n# calculate co-occurrence counts\ncoocurrences <- t(abs_pov_sdtm) %*% abs_pov_sdtm\n# convert into matrix\ncollocates <- as.matrix(coocurrences)\n```\n:::\n\n::: {.cell tbl-cap='boh'}\n\n:::\n\n \nWe can inspect this **co-occurrence matrix** and check how many terms (words or elements) it represents using the `ncol` function from base R. \nWe can also check how often terms occur in the data using the `summary` function from base R. \n\n\n::: {.cell}\n\n```{.r .cell-code}\n# inspect size of matrix\nncol(collocates) # 239\nsummary(rowSums(collocates))\n```\n:::\n\nThe ncol function reports that the data represents 239 words and that the most frequent word occurs 163 times in the text.\n\nThe output of the summary function tells us that the minimum frequency of a word in the data is 5 with a maximum of 163. The difference between the median (18) and the mean (22) indicates that the frequencies are distributed non-normally - which is common for language data.\n\n##### --> (EXE) Visualizing Collocations EXE \"poverty\"\nWe will now use an example of one individual word ( _poverty_ ) to show, how collocation strength for individual terms is calculated and how it can be visualized. \n\nThe function `calculateCoocStatistics` is taken from @wiedemann_hands-_nodate and applied to the `abs_pov_sdtm` **SPARSE DOCUMENT TEXT MATRIX**\n\nVisualizing Collocations\n\n::: {.cell}\n\n```{.r .cell-code}\n# load function for co-occurrence calculation\nsource(\"https://slcladal.github.io/rscripts/calculateCoocStatistics.R\")\n# define term\ncoocTerm <- \"development\"\n# calculate co-occurrence statistics\ncoocs <- calculateCoocStatistics(coocTerm, abs_pov_sdtm, measure=\"LOGLIK\")\n# inspect results\ncoocs[1:20]\n\n# define term # 2 \ncoocTerm2 <- \"poverty\"\n# calculate co-occurrence statistics\ncoocs2 <- calculateCoocStatistics(coocTerm2, abs_pov_sdtm, measure=\"LOGLIK\")\n# inspect results\ncoocs2[1:20]\n```\n:::\n\n\nThe output shows that the word most strongly associated with _development_ in the poverty WDR subset is _issues_ - here there is no substantive strength (a substantive strength of the association would indicate these term are definitely collocates and almost - if not already - a lexicalized construction)\n\n##### --> (EXE) Association Strength\nThere are various _visualizations options for collocations_. Which visualization method is appropriate depends on what the visualizations should display.\n\nWe start with the most basic and visualize the collocation strength using a **simple dot chart**. We use the vector of association strengths generated above and transform it into a table. Also, we exclude elements with an association strength lower than 30.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncoocdf <- coocs2 %>%\n  as.data.frame() %>%\n  dplyr::mutate(CollStrength = coocs2,\n                Term = names(coocs2)) %>%\n  dplyr::filter(CollStrength > 0.1) # this is kind of weak but for exe's sake\n```\n:::\n\n::: {.cell tbl-cap='boh 2'}\n\n:::\n\n\n#### ...[viz] association strengths\nWe can now visualize the association strengths as shown in the code chunk below.\n\n\n::: {.cell}\n\n```{.r .cell-code}\np_ass_words_poverty <- ggplot(coocdf, aes(x = reorder(Term, CollStrength, mean), y = CollStrength)) +\n  geom_point() +\n  coord_flip() +\n  #theme_void() +\n  labs(title = \"Association to word \\\"poverty\\\"\",\n       subtitle = \"Collocation strenght measured by log-likelihood\",\n       caption = \"Source: https://ladal.edu.au/coll.html#Association_Strength\",\n       y = \"\", \n       x = \"\"\n       )\n\np_ass_words_poverty %T>% \n  print() %T>%\n  ggsave(., filename = here(\"analysis\", \"output\", \"figures\", \"p_ass_words_poverty.pdf\"),\n         #width = 4, height = 2.25, units = \"in\",\n         device = cairo_pdf) %>% \n  ggsave(., filename = here(\"analysis\", \"output\", \"figures\", \"p_ass_words_poverty.png\"),\n         #width = 4, height = 2.25, units = \"in\", \n         type = \"cairo\", dpi = 300)  \n```\n:::\n\n\nThe dot chart shows that *poverty* is collocating more strongly with *economic* compared to any other term.  \n\n##### --> (EXE) Dendrograms{-}\n\nAnother method for visualizing collocations are `dendrograms.` Dendrograms (also called tree-diagrams) show how similar elements are based on one or many features. As such, dendrograms are used to indicate groupings as they show elements (words) that are notably similar or different with respect to their association strength. To use this method, we first need to generate a distance matrix from our co-occurrence matrix.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncoolocs <- c(coocdf$Term, \"poverty\")\n# remove non-collocating terms\ncollocates_redux <- collocates[rownames(collocates) %in% coolocs, ]\ncollocates_redux <- collocates_redux[, colnames(collocates_redux) %in% coolocs]\n# create distance matrix\ndistmtx <- dist(collocates_redux)\n\nclustertexts <- hclust(    # hierarchical cluster object\n  distmtx,                 # use distance matrix as data\n  method=\"ward.D2\")        # ward.D as linkage method\n\nggdendrogram(clustertexts) +\n  ggtitle(\"Terms strongly collocating with *poverty*\")\n```\n:::\n\n\n##### --> (EXE) Network Graphs{-}\n\nNetwork graphs are a very useful tool to show relationships (or the absence of relationships) between elements. Network graphs are highly useful when it comes to displaying the relationships that words have among each other and which properties these networks of words have.\n\n##### --> (EXE) Basic Network Graphs{-}\n\nIn order to display a network, we need to create a network graph by using the `network` function from the `network` package. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nnet = network::network(collocates_redux, \n                       directed = FALSE,\n                       ignore.eval = FALSE,\n                       names.eval = \"weights\")\n# vertex names\nnetwork.vertex.names(net) = rownames(collocates_redux)\n# inspect object\nnet\n```\n:::\n\n\nNow that we have generated a network object, we visualize the network with `GGally::ggnet2`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nGGally::ggnet2(net, \n       label = TRUE, \n       label.size = 4,\n       alpha = 0.2,\n       size.cut = 3,\n       edge.alpha = 0.3) +\n  guides(color = FALSE, size = FALSE)\n```\n:::\n\n\nWe can customize the network object so that the visualization becomes more appealing and informative. To add information, we create vector of words that contain different groups, e.g. terms that rarely, sometimes, and frequently collocate with *poverty* (I used the dendrogram which displayed the cluster analysis as the basis for  the categorization). \n\nBased on these vectors, we can then change or adapt the default values of certain attributes or parameters of the network object (e.g. weights. linetypes, and colors).\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# create vectors with collocation occurrences as categories\nmid <- c(\"dimensions\", \"major\", \"developing\", \"social\", \"health\")\nhigh <- c(\"economic\", \"countries\")\ninfreq <- colnames(collocates_redux)[!colnames(collocates_redux) %in% mid & !colnames(collocates_redux) %in% high]\n# add color by group\nnet %v% \"Collocation\" = ifelse(network.vertex.names(net) %in% infreq, \"weak\", \n                   ifelse(network.vertex.names(net) %in% mid, \"medium\", \n                   ifelse(network.vertex.names(net) %in% high, \"strong\", \"other\")))\n# modify color\nnet %v% \"color\" = ifelse(net %v% \"Collocation\" == \"weak\", \"gray60\", \n                  ifelse(net %v% \"Collocation\" == \"medium\", \"orange\", \n                  ifelse(net %v% \"Collocation\" == \"strong\", \"indianred4\", \"gray60\")))\n# rescale edge size\nnetwork::set.edge.attribute(net, \"weights\", ifelse(net %e% \"weights\" < 1, 0.1, \n                                   ifelse(net %e% \"weights\" <= 2, .5, 1)))\n# define line type\nnetwork::set.edge.attribute(net, \"lty\", ifelse(net %e% \"weights\" <=.1, 3, \n                               ifelse(net %e% \"weights\" <= .5, 2, 1)))\n```\n:::\n\n\nWe can now display the network object and make use of the added information.\n\n\n::: {.cell}\n\n```{.r .cell-code}\np_ggnet_poverty <- GGally::ggnet2(net, \n                                color = \"color\", \n                                label = TRUE, \n                                label.size = 4,\n                                alpha = 0.2,\n                                size = \"degree\",\n                                edge.size = \"weights\",\n                                edge.lty = \"lty\",\n                                edge.alpha = 0.2) +\n  guides(color = FALSE, size = FALSE) +\n  #theme_void() +\n  labs(title = \"Degrees of association to word \\\"poverty\\\"\",\n       subtitle = \"Weak (grey), medium (orange), strong (red)\"#,\n       # caption = \"Source: https://ladal.edu.au/coll.html#Association_Strength\",\n       # y = \"\", \n       # x = \"\"\n       )\n\np_ggnet_poverty %T>% \n  print() %T>%\n  ggsave(., filename = here(\"analysis\", \"output\", \"figures\", \"p_ggnet_poverty.pdf\"),\n         #width = 4, height = 2.25, units = \"in\",\n         device = cairo_pdf) %>% \n  ggsave(., filename = here(\"analysis\", \"output\", \"figures\", \"p_ggnet_poverty.png\"),\n         #width = 4, height = 2.25, units = \"in\", \n         type = \"cairo\", dpi = 300)  \n```\n:::\n\n\n##### --> (EXE) Biplots{-}\n  \nAn alternative way to display co-occurrence patterns are bi-plots which are used to display the results of Correspondence Analyses. They are useful, in particular, when one is not interested in one particular key term and its collocations but in the overall similarity of many terms. ***Semantic similarity*** in this case refers to a shared semantic and this distributional profile. As such, words can be deemed semantically similar if they have a similar co-occurrence profile - i.e. they co-occur with the same elements. Biplots can be used to visualize collocations because collocates co-occur and thus share semantic properties which renders then more similar to each other compared with other terms. \n\n\n::: {.cell}\n\n```{.r .cell-code}\n# perform correspondence analysis\nres.ca <- FactoMineR::CA(collocates_redux, graph = FALSE)\n# plot results\nfactoextra::fviz_ca_row(res.ca, repel = TRUE, col.row = \"gray20\")\n```\n:::\n\n\n\nThe bi-plot shows that *poverty* and *development* collocate as they are plotted in close proximity. The advantage of the biplot becomes apparent when we focus on other terms because the biplot also shows other collocates such as *issues* and *growth* \n\n##### --> (EXE) Determining Significance\nIn order to identify which words occur together significantly more frequently than would be expected by chance, we have to determine if their co-occurrence frequency is statistical significant. This can be done wither for specific key terms or it can be done for the entire data. In this example, we will continue to focus on the key word *selection*.\n\nTo determine which terms collocate significantly with the key term (*selection*), we use multiple (or repeated) Fisher's Exact tests which require the following information:\n\n* a = Number of times `coocTerm` occurs with term j\n\n* b = Number of times `coocTerm` occurs without  term j\n\n* c = Number of times other terms occur with term j\n\n* d = Number of terms that are not `coocTerm` or term j\n\nIn a first step, we create a table which holds these quantities.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# convert to data frame\ncoocdf <- as.data.frame(as.matrix(collocates))\n# reduce data\ndiag(coocdf) <- 0\ncoocdf <- coocdf[which(rowSums(coocdf) > 10),]\ncoocdf <- coocdf[, which(colSums(coocdf) > 10)]\n# extract stats\ncooctb <- coocdf %>%\n  dplyr::mutate(Term = rownames(coocdf)) %>%\n  tidyr::gather(CoocTerm, TermCoocFreq,\n                colnames(coocdf)[1]:colnames(coocdf)[ncol(coocdf)]) %>%\n  dplyr::mutate(Term = factor(Term),\n                CoocTerm = factor(CoocTerm)) %>%\n  dplyr::mutate(AllFreq = sum(TermCoocFreq)) %>%\n  dplyr::group_by(Term) %>%\n  dplyr::mutate(TermFreq = sum(TermCoocFreq)) %>%\n  dplyr::ungroup(Term) %>%\n  dplyr::group_by(CoocTerm) %>%\n  dplyr::mutate(CoocFreq = sum(TermCoocFreq)) %>%\n  dplyr::arrange(Term) %>%\n  dplyr::mutate(a = TermCoocFreq,\n                b = TermFreq - a,\n                c = CoocFreq - a, \n                d = AllFreq - (a + b + c)) %>%\n  dplyr::mutate(NRows = nrow(coocdf))\n```\n:::\n\n::: {.cell tbl-cap='boh3'}\n\n:::\n\n\nWe now select the key term (*poverty*). If we wanted to find all collocations that are present in the data, we would use the entire data rather than only the subset that contains  *poverty*. \n\n\n::: {.cell}\n\n```{.r .cell-code}\ncooctb_redux <- cooctb %>%\n  dplyr::filter(Term == coocTerm2)\n```\n:::\n\n\nNext, we calculate which terms are (significantly) over- and under-proportionately used with *poverty*. It is important to note that this procedure informs about both: over- and under-use! This is especially crucial when analyzing if specific words are attracted o repelled by certain constructions. Of course, this approach is not restricted to analyses of constructions and it can easily be generalized across domains and has also been used in machine learning applications.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncoocStatz <- cooctb_redux %>%\n  dplyr::rowwise() %>%\n  dplyr::mutate(p = as.vector(unlist(fisher.test(matrix(c(a, b, c, d), \n                                                        ncol = 2, byrow = T))[1]))) %>%\n    dplyr::mutate(x2 = as.vector(unlist(chisq.test(matrix(c(a, b, c, d),                                                           ncol = 2, byrow = T))[1]))) %>%\n  dplyr::mutate(phi = sqrt((x2/(a + b + c + d)))) %>%\n      dplyr::mutate(expected = as.vector(unlist(chisq.test(matrix(c(a, b, c, d), ncol = 2, byrow = T))$expected[1]))) %>%\n  dplyr::mutate(Significance = dplyr::case_when(p <= .001 ~ \"p<.001\",\n                                                p <= .01 ~ \"p<.01\",\n                                                p <= .05 ~ \"p<.05\", \n                                                FALSE ~ \"n.s.\"))\n```\n:::\n\n::: {.cell tbl-cap='boh4'}\n\n:::\n\n\nWe now add information to the table and remove superfluous columns s that the table can be more easily parsed. \n\n\n::: {.cell}\n\n```{.r .cell-code}\ncoocStatz <- coocStatz %>%\n  dplyr::ungroup() %>%\n  dplyr::arrange(p) %>%\n  dplyr::mutate(j = 1:n()) %>%\n  # perform benjamini-hochberg correction\n  dplyr::mutate(corr05 = ((j/NRows)*0.05)) %>%\n  dplyr::mutate(corr01 = ((j/NRows)*0.01)) %>%\n  dplyr::mutate(corr001 = ((j/NRows)*0.001)) %>%\n  # calculate corrected significance status\n  dplyr::mutate(CorrSignificance = dplyr::case_when(p <= corr001 ~ \"p<.001\",\n                                                    p <= corr01 ~ \"p<.01\",\n                                                    p <= corr05 ~ \"p<.05\", \n                                                    FALSE ~ \"n.s.\")) %>%\n  dplyr::mutate(p = round(p, 6)) %>%\n  dplyr::mutate(x2 = round(x2, 1)) %>%\n  dplyr::mutate(phi = round(phi, 2)) %>%\n  dplyr::arrange(p) %>%\n  dplyr::select(-a, -b, -c, -d, -j, -NRows, -corr05, -corr01, -corr001) %>%\n  dplyr::mutate(Type = ifelse(expected > TermCoocFreq, \"Antitype\", \"Type\"))\n```\n:::\n\n::: {.cell tbl-cap='boh5'}\n\n:::\n\nThe results show that *poverty* DOES NOT collocates significantly with anywords. \n\n<!-- The results show that *poverty* collocates significantly with *poverty* (of course) but also, as expected, with *progress*.  -->\n<!-- The corrected p-values shows that after Benjamini-Hochberg correction for multiple/repeated testing these are the only significant collocates of *selection*. Corrections are necessary when performing multiple tests because otherwise, the reliability of the test result would be strongly impaired as repeated testing causes substantive $\\alpha$-error inflation. The Benjamini-Hochberg correction that has been used here is preferable over the more popular Bonferroni correction because it is less conservative and therefore less likely to result in $\\beta$-errors [see again @field2012discovering]. -->\n\n##### ~~--> (EXE) Changes in Collocation Strength~~\n##### ~~--> (EXE) Collostructional Analysis~~\n\n## III.vi) > > Sentiment Analysis \nhttps://cfss.uchicago.edu/notes/harry-potter-exercise/\n\n## III.v) Topic modeling\n+ [Robinson, Silge](https://www.tidytextmining.com/topicmodeling.html) \n+ https://cfss.uchicago.edu/notes/topic-modeling/\n+ https://m-clark.github.io/text-analysis-with-R/topic-modeling.html\n+ AH: https://datavizf18.classes.andrewheiss.com/class/11-class/#topic-modeling\n\n> [not sure applicable, they are all same topic here!]\n\nTopic modeling is a method for unsupervised classification of documents (blog post, news articles), similar to clustering on numeric data, which **finds natural groups of items** even when we’re not sure what we’re looking for.\n\nMethods:\n\n+ **Latent Dirichlet allocation (LDA)** is a particularly popular method for fitting a topic model -> It treats each document as a mixture of topics, and each topic as a mixture of words. _The basic idea is that we’ll take a whole lot of features and boil them down to a few ‘topics’_. In this sense LDA is akin to discrete PCA.\n\n\n#### --- LDA (Latent Dirichlet allocation) with `topicmodels` package \n\n> NOTE: The `topicmodels` package takes a Document-Term Matrix as input and produces a model that can be tided by tidytext, such that it can be manipulated and visualized with dplyr and ggplot2. \n\nPrinciples:\n\n1. imagine that each document may contain words from several topics in particular proportions\n2. Every topic is a mixture of words\n\n***LDA is a mathematical method for estimating both of these at the same time: finding the mixture of words that is associated with each topic, while also determining the mixture of topics that describes each document.*** \n\n#### --- From abstracts tiditext 2 dtm with `tidytext` \n\n::: {.cell}\n\n```{.r .cell-code}\n# starting from this \npaint(abs_words2)\n\n# cast into a Document-Term Matrix (*)\nabs_words_dtm <- abs_words2 %>%\n  tidytext::cast_dtm(date_issued, word, n)\nabs_words_dtm\n\n# cast into a Term-Document Matrix\nabs_words_tdm <- abs_words2 %>%\n  tidytext::cast_tdm(date_issued, word, n)\nabs_words_tdm\n\n# cast into quanteda's dfm Document-feature matrix\nabs_words_dfm <- abs_words2 %>%\n    cast_dfm(date_issued, word, n)\n\n# cast into a Matrix object\nabs_words_m <-  abs_words2 %>%\n  cast_sparse(date_issued, word, n)\nclass(abs_words_m)\n```\n:::\n\n \n#### --- ...from dtm 2 LDA document structure\nhttps://cfss.uchicago.edu/notes/topic-modeling/\n\n::: {.cell}\n\n```{.r .cell-code}\n# from tidytext format (one-row-per-token)\n# ---- 1/2 cast into a Document-Term Matrix (*)\nabs_words_dtm <- abs_words2 %>%\n  tidytext::cast_dtm(date_issued, word, n)\nabs_words_dtm\n\n\n# # ---- 2/2 using Document-Term Matrix (*)\n# # set a seed so that the output of the model is predictable\n# # k is the number of topic\nabs_lda <- topicmodels::LDA(abs_words_dtm, k = 2, control = list(seed = 1234))\nabs_lda\n```\n:::\n\n\nFitting the model was the “easy part”: the rest of the analysis will involve exploring and interpreting the model using tidying functions from the tidytext package.\n\n> NOTE: What if k change? Several different values for may be plausible, but by increasing we sacrifice clarity. \n\n#### ---  Word-topic probabilities\nThe tidytext package uses `broom::tidy` for extracting the per-topic-per-word  probabilities, called β (“beta”), from the model.\n\n> NOTE: For each combination, the model computes the probability of that term being generated from that topic.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# extract per-topic-per-word beta\nabs_topics <- tidytext::tidy(abs_lda, matrix = \"beta\")\nabs_topics # one-topic-per-term-per-row format\n\n#> For example, the term “data” has a 8.33×10−12 probability of being generated from topic 1, but a 1.1×10−3 probability of being generated from topic 2.\n```\n:::\n\n \nWe could use dplyr’s slice_max() to find the 10 terms that are most common within each topic. As a tidy data frame, this lends itself well to a ggplot2 visualization \n\n::: {.cell}\n\n```{.r .cell-code}\nabs_top_terms <- abs_topics %>%\n  group_by(topic) %>%\n  slice_max(beta, n = 10) %>% \n  ungroup() %>%\n  arrange(topic, -beta)\n\nabs_top_terms %>%\n  mutate(term = reorder_within(term, beta, topic)) %>%\n  ggplot(aes(beta, term, fill = factor(topic))) +\n  geom_col(show.legend = FALSE) +\n  facet_wrap(~ topic, scales = \"free\") +\n  scale_y_reordered()\n```\n:::\n\n\nIn alternative, we could consider the terms that had the *greatest difference* in $\\beta$ between topic 1 and topic 2. This can be estimated based on the log ratio of the two: $\\log_2(\\frac{\\beta_2}{\\beta_1})$ (a log ratio is useful because it makes the difference symmetrical: $\\beta_2$ being twice as large leads to a log ratio of 1, while $\\beta_1$ being twice as large results in -1). To constrain it to a set of especially relevant words, we can filter for relatively common words, such as those that have a $\\beta$ greater than 1/1000 in at least one topic.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbeta_wide <- abs_topics %>%\n  mutate(topic = paste0(\"topic\", topic)) %>%\n  pivot_wider(names_from = topic, values_from = beta) %>% \n  filter(topic1 > .001 | topic2 > .001) %>%\n  mutate(log_ratio = log2(topic2 / topic1))\nbeta_wide\n```\n:::\n\n\nThe words with the greatest differences between the two topics are visualized in Figure \\@ref(fig:topiccompare).\n\n(ref:topiccap) Words with the greatest difference in $\\beta$ between topic 2 and topic 1\n\n\n::: {.cell}\n\n:::\n\n \n#### ---  Document-topic probabilities\nBesides estimating each topic as a mixture of words, LDA also models each document as a mixture of topics. We can examine the per-document-per-topic probabilities, called $\\gamma$ (\"gamma\"), with the `matrix = \"gamma\"` argument to `tidy()`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nabs_documents <- tidy(abs_lda, matrix = \"gamma\")\nabs_documents # 44 WDR x 2 topics = 88!\n```\n:::\n\n\nEach of these values is an estimated proportion of words from that document that are generated from that topic. For example, the model estimates that only about `percent(abs_documents$gamma[1])` of the words in document 1 were generated from topic 1.\n\nWe can see that many of these documents were drawn from a mix of the two topics, but that `document` *2014* was drawn almost entirely from topic 1, having a $\\gamma$ from topic 2 close to zero. To check this answer, we could `tidy()` the document-term matrix (see Chapter \\@ref(tidy-dtm)) and check what the most common words in that document were.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntidy(abs_words_dtm) %>%\n  filter(document == 2014) %>%\n  arrange(desc(count))\n```\n:::\n\n\nBased on the most common words, this appears to be an article about the relationship between the American government and Panamanian dictator Manuel Noriega, which means the algorithm was right to place it in topic 2 (as political/national news).\n\n \n\n#### --------------  STOP ------------------\n\n#### ---  Alternative LDA implementations\n\n## III.vii) > > Supervised classification with text data\nhttps://cfss.uchicago.edu/notes/supervised-text-classification/\nwe can now use machine learning models to classify text into specific sets of categories. This is known as supervised learning.\n\n\n# Reference Tutorials\n\n@robinson_1_2022 \n\n[Benjamin Soltoff: Computing 4 Social Sciences - API](https://cfss.uchicago.edu/syllabus/getting-data-from-the-web-api-access/)\\ \n\n[Benjamin Soltoff: Computing 4 Social Sciences - text analysis](https://cfss.uchicago.edu/syllabus/text-analysis-fundamentals-and-sentiment-analysis/)\\\n\n[Ben Schmidt Book Humanities Crurse](https://hdf.benschmidt.org/R/) [Ben Schmidt Book Humanities](http://benschmidt.org/HDA/texts-as-data.html)\\\n\n[TidyTuesday casts on tidytext](https://github.com/dgrtwo/data-screencasts/tree/master/screencast-annotations)\\\n\n  1. ✔️ [MEDIUM articles: common words, pairwise correlations - 2018-12-04](https://www.youtube.com/watch?v=C69QyycHsgE)\n  2. ✔️ [TidyTuesday Tweets -  2019-01-07](https://www.youtube.com/watch?v=KE9ItC3doEU)\n  3. [Wine Ratings - 2019-05-31](https://www.youtube.com/watch?v=AQzZNIyjyWM) Lasso regression | sentiment lexicon,\n  4. [Simpsons Guest Stars \t2019-08-30](https://www.youtube.com/watch?v=EYuuAGDeGrQ) geom_histogram\n  5. [Horror Movies \t2019-10-22](https://www.youtube.com/watch?v=yFRSTlk3kRQ) explaining glmnet package | Lasso regression\n  6. [The Office \t2020-03-16](https://www.youtube.com/watch?v=_IvAubTDQME) geom_text_repel from ggrepel | glmnet package to run a cross-validated LASSO regression\n  7. [Animal Crossing \t2020-05-05](https://www.youtube.com/watch?v=Xt7ACiedRRI) Using geom_line and geom_point to graph ratings over time | geom_text to visualize what words are associated with positive/negative reviews |topic modelling\n\n\n",
    "supporting": [
      "01_WDR_abstracs_explor_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}