---
title: "WB Project PDO text analysis"
author: "Luisa M. Mimmi"
date: "Last run: `r format(Sys.time(), '%F')`"
lang: en
editor: source
engine: knitr
## ------  general Output Options
execute:     
  eval: true    # actually run? 
  echo: true     #  include source code in output
  warning: false  #  include warning code in output
  error: false    #  include error code in output
  output: false   # include output code in output (CHG in BLOCKS)
  # include: false   # R still runs but code and results DON"T appear in output  
  cache: false # normalmnte false
toc: true
fig-cap-location: top
tbl-cap-location: top
format:
  html:
    # theme: flatly #spacelab
    code-fold: false # redundant bc echo false 
    toc-depth: 2
    toc_float: true
    toc-location: left
    toc-title: Outline
    embed-resources: true # external dependencies embedded (Not in ..._files/)
  # pdf:
  #   toc-depth: 2
  #   toc-title: Indice
  #   highlight-style: github
  #   #lang: it
  #   embed-resources: true # external dependencies embedded (Not in ..._files/)
format-links: false
bibliography: ../bib/slogan.bib
---

<i class="fa fa-refresh" style="color: firebrick"></i> Work in progress

<!-- In this file I address the **research question 1.1.**, which is exploratory in nature. The hypothesis being tested here is that the WBG project document corpus shows a non-random variation in the incidence of certain policy concepts over time.  -->

<!-- + This question will be handled in a data-driven way, i.e. starting from the data and not from preconceived ideas...  -->
<!--    + (i.e. I see that after 2020, the word "pandemic" and "vaccine" peaks within PDOs' texts, so I will look for a correlation with the COVID-19 pandemic shock, instead of the other way around). -->

# Set up
```{r}
# Pckgs -------------------------------------
library(fs) # Cross-Platform File System Operations Based on 'libuv'
library(tidyverse) # Easily Install and Load the 'Tidyverse'
library(janitor) # Simple Tools for Examining and Cleaning Dirty Data
library(skimr) # Compact and Flexible Summaries of Data
library(here) # A Simpler Way to Find Your Files
library(paint) # paint data.frames summaries in colour
library(readxl) # Read Excel Files
library(tidytext) # Text Mining using 'dplyr', 'ggplot2', and Other Tidy Tools
library(SnowballC) # Snowball Stemmers Based on the C 'libstemmer' UTF-8 Library
library(rsample) # General Resampling Infrastructure
library(rvest) # Easily Harvest (Scrape) Web Pages
library(cleanNLP) # A Tidy Data Model for Natural Language Processing
library(kableExtra) # Construct Complex Table with 'kable' and Pipe Syntax)
```

::: {.callout-note collapse='true'}
#### --- Note on `cleanNLP` package
`cleanNLP` supports multiple *backends for processing text*, such as `CoreNLP`, `spaCy`, `udpipe`, and `stanza.` Each of these backends has different capabilities and might require different initialization procedures.

+ `CoreNLP` ~ powerful Java-based NLP toolkit developed by Stanford, which includes many linguistic tools like tokenization, part-of-speech tagging, and named entity recognition. 
   + ❕❗️ NEEDS EXTERNAL INSTALLATION (must be installed in Java with `cnlp_install_corenlp()` which installs the Java JAR files and models)
+ `spaCy` ~ fast and modern NLP library written in *Python.* It provides advanced features like dependency parsing, named entity recognition, and tokenization. 
   + + ❕❗️ NEEDS EXTERNAL INSTALLATION (fust be installed in Python (with `spacy_install()` which installs both `spaCy` and necessary Python dependencies) and the `spacyr` R package must be installed to interface with it.
+ `udpipe` ~ R package that provides bindings to the `UDPipe` NLP toolkit. Fast, lightweight and language-agnostic NLP library for tokenization, part-of-speech tagging, lemmatization, and dependency parsing.
+ `stanza`~ another modern NLP library from Stanford, similar to CoreNLP but built on PyTorch and supports over 66 languages...

when you initialize a backend (like CoreNLP) in cleanNLP, it stays active for the entire session unless you reinitialize or explicitly change it. 
```{r}
#| eval: false
#| echo: true

# ---- 1) Initialize the CoreNLP backend
library(cleanNLP)
cnlp_init_corenlp()
# If you want to specify a language or model path:
cnlp_init_corenlp(language = "en", 
                  # model_path = "/path/to/corenlp-models"
                  )

# ---- 2) Initialize the spaCy backend 
library(cleanNLP)
library(spacyr)
# Initialize spaCy in cleanNLP
cnlp_init_spacy()
# Optional: specify language model
cnlp_init_spacy(model_name = "en_core_web_sm")

# ---- 3) Initialize the udpipe backend
library(cleanNLP)
# Initialize udpipe backend
cnlp_init_udpipe(model_name = "english")

# ---- 4) Initialize the stanza backend

```


:::




### ----------------------------------------------------------------------------

# Data sources

# WB Projects & Operations [CHECK 🔴]
**World Bank Projects & Operations** can be explored at: 

1. [Data Catalog](https://datacatalog.worldbank.org/search/dataset/0037800> <https://datacatalog.worldbank.org/search/dataset/0037800/World-Bank-Projects---Operations). From which 
  + [API for projects](https://search.worldbank.org/api/v3/projects) 

2. [Advanced Search](https://projects.worldbank.org/en/projects-operations/project-search) 

-   Accessibility Classification: **public** under [Creative Commons Attribution 4.0](https://datacatalog.worldbank.org/public-licenses?fragment=cc)

- For example: <https://datacatalog.worldbank.org/search/dataset/0037800> <https://datacatalog.worldbank.org/search/dataset/0037800/World-Bank-Projects---Operations>

### ---------------------------------------------------------------------------
 
# Load pre-processed Projs' PDO dataset `pdo_train_t`
Syntactic annotation is a computationally expensive operation, so I don't want to repeat it every time I restart the session.  

## [Saved file `projs_train_t` ]

> Done in ** analysis/_01a_WB_project_pdo_prep.qmd "**

1. I retrieved manually ALL WB projects approved *between FY 1947 and 2026* as of *31/08/2024* using simply the `Excel button` on this page [WBG Projects](https://projects.worldbank.org/en/projects-operations/projects-list?str_fiscalyear=1979&end_fiscalyear=1979&os=0) 
   + By the way this is the link ["list-download-excel"](https://search.worldbank.org/api/projects/all.xls)
   + then saved HUUUGE `.xls` files in `data/raw_data/project2/all_projects_as_of29ago2024.xls`
     + (plus a `Rdata` copy of the original file )
2. Split the dataset and keep only  `projs_train` (50% of projects with PDO text, i.e. **4413 PDOs**)
3. Clean the dataset and save `projs_train_t` (cleaned train dataset)
4. Obtain **PoS tagging** + **tokenization** with `cleanNLP` package (functions  `cnlp_init_udpipe()` + `cnlp_annotate()`) and saved `projs_train_t` (cleaned train dataset).
 

```{r}
# Load clean Proj PDO train dataset `pdo_train_t`
pdo_train_t <- readRDS(here::here("data" , "derived_data", "pdo_train_t.rds"))
```

### Important mod
```{r}
# Ensure token_id is numeric
pdo_train_t <- pdo_train_t %>% 
  mutate(tid = as.numeric(tid))  # Convert token_id to numeric
```


## Explain Tokenization and PoS Tagging

### i) Tokenization
Breaking units of language into components relevant for the research question into components relevant for the research question is called **“tokenization”**. 
Components can be `words`, `ngrams`, `sentences`, etc. or combining smaller units into larger units.

+ Tokenization is a `row-wise` operation: it changes the number of rows in the dataset.

###  The choices of tokenization

  1. Should words be lower cased?  
  2. Should punctuation be removed?   
  3. Should numbers be replaced by some placeholder?
  4. Should words be stemmed (also called lemmatization). ☑️ 
  5. Should bigrams/multi-word phrase be used instead of single word phrases?
  6. Should stopwords (the most common words) be removed? ☑️ 
  7. Should rare words be removed?
  8. Should hyphenated words be split into two words? ❌
  
> for the moment I keep all as conservatively as possible 

### ii) Pos Tagging
Linguistic annotation is a common for of **enriching** text data, i.e. adding information about the text that is not directly present in the text itself.

Upon this, e.g. classifying noun, verb, adjective, etc., one can discover *intent* or *action* in a sentence, or scanning "verb-noun" *patterns.*

Here I have a training dataset file with:
```{r}
#| eval: true
#| output: true
#| echo: false

train_pdo_desc <- tribble(
  ~Variable, ~ Type,           ~Provenance,           ~Description,
"proj_id              ", "chr", "original PDO data" , "",
"pdo                  ", "chr", "original PDO data" , "",
"word_original        ", "chr", "original PDO data" , "",
"sid                  ", "int", "output cleanNLP"   , "sentence ID",
"tid                  ", "chr", "output cleanNLP"   , "token ID within sentence",
"token                ", "chr", "output cleanNLP"   , "Tokenized form of the token.",
"token_with_ws        ", "chr", "output cleanNLP"   , "Token with trailing whitespace",
"lemma                ", "chr", "output cleanNLP"   , "The base form of the token",
"upos                 ", "chr", "output cleanNLP"   , "Universal part-of-speech tag (e.g., NOUN, VERB, ADJ).",
"xpos                 ", "chr", "output cleanNLP"   , "Language-specific part-of-speech tags.",
"feats                ", "chr", "output cleanNLP"   , "Morphological features of the token",
"tid_source           ", "chr", "output cleanNLP"   , "Token ID in the source document",
"relation             ", "chr", "output cleanNLP"   , "Dependency relation between the token and its head token",
"pr_name              ", "chr", "output cleanNLP" , "Name of the parent token",
"FY_appr              ", "dbl", "original PDO data" , "",
"FY_clos              ", "dbl", "original PDO data" , "",
"status               ", "chr", "original PDO data" , "",
"regionname           ", "chr", "original PDO data" , "",
"countryname          ", "chr", "original PDO data" , "",
"sector1              ", "chr", "original PDO data" , "",
"theme1               ", "chr", "original PDO data" , "",
"lendinginstr         ", "chr", "original PDO data" , "",
"env_cat              ", "chr", "original PDO data" , "",
"ESrisk               ", "chr", "original PDO data" , "",
"curr_total_commitment", "dbl", "original PDO data" , ""
)

kableExtra::kable(train_pdo_desc)
```

### --- PoS Tagging: `upos` (Universal Part-of-Speech)
```{r}
#| eval: true
#| output: true
#| echo: false

upos_t <- tabyl(pdo_train_t, upos) %>% 
   add_column(explan = c(
      "Adjective", 
      "Adposition", 
      "Adverb", 
      "Auxiliary", 
      "Coordinating conjunction",
      "Determiner", 
      "Interjection",
      "Noun", 
      "Numeral", 
      "Particle", 
      "Pronoun", 
      "Proper noun",
      "Punctuation", 
      "Subordinating conjunction", 
      "Symbol", 
      "Verb",
      "Other" 
      )) 
kableExtra::kable(upos_t)
```

::: {.callout-warning collapse='true'}
On random visual check, these are not always correct, but they are a good starting point for now.
::: 

### iii) Make low case 
```{r}
pdo_train_t <- pdo_train_t %>% 
  mutate(token_l = tolower(token)) %>% 
   relocate(token_l, .after = token) %>% 
   select(-token_with_ws) %>%
  #Replace variations of "hyphenword" with "-"
  mutate(
    lemma = str_replace_all(lemma, regex("hyphenword|hyphenwor", 
                                         ignore_case = TRUE), "-")
  )
```



### iv) Stemming 
Using `SnowballC::wordStem` to stem the words. e.g.

```{r}
pdo_train_t <- pdo_train_t %>% 
   mutate(stem = SnowballC::wordStem(token_l)) %>%
   relocate(stem, .after = lemma)
```

***Why Stemming?***: For example, in topic modeling, *stemming* reduces noise by making it easier for the model to identify core topics without being distracted by grammatical variations. (*Lemmatization* is more computationally intensive as it requires linguistic context and dictionaries, making it slower, especially on large datasets)

| **Token**       | **Lemma**                              | **Stem**|
|-----------------|----------------------------------------|---------|
| development     | development                            | develop |
| quality         | quality                                | qualiti |
| high-quality    | high-quality                           | high-qual|
| include         | include                                | includ  |
| logistics       | logistic                               | logist  |
| government/governance | Governemnt/government/governance | govern  |

> NOTE: Among `word` / `stems` encountered in PDOs, there are a lot of acronyms which may refer to World Bank lingo, or local agencies, etc... Especially when looked at in low case form they don't make much sense... 


### v) Document-term matrix or TF-IDF

> The tf-idf is the product of the term frequency and the inverse document frequency::

$$
\begin{aligned}
tf(\text{term}) &= \frac{n_{\text{term}}}{n_{\text{terms in document}}} \\
idf(\text{term}) &= \ln{\left(\frac{n_{\text{documents}}}{n_{\text{documents containing term}}}\right)} \\
tf\text{-}idf(\text{term}) &= tf(\text{term}) \times idf(\text{term})
\end{aligned}
$$

### --- My own `custom_stop_words` |
Remove stop words, which are the most common words in a language. 

+ but I don't want to remove any *meaningful* word for now

```{r}
# Custom list of articles, prepositions, and pronouns
custom_stop_words <- c(
   # Articles
   "the", "a", "an",   
   "and", "but", "or", "yet", "so", "for", "nor", "as", "at", "by", "per",  
   # Prepositions
   "of", "in", "on", "at", "by", "with", "about", "against", "between", "into", "through", 
   "during", "before", "after", "above", "below", "to", "from", "up", "down", "under",
   "over", "again", "further", "then", "once",  
   # Pronouns
   "i", "me", "my", "myself", "we", "our", "ours", "ourselves", "you", "your",
   "yours", "yourself", "yourselves", "he", "him", "his", "himself", "she", "her", 
   "hers", "herself", "it", "its", "itself", "they", "them", "their", "theirs", "themselves" ,
   "this", "that", "these", "those", "which", "who", "whom", "whose", "what", "where",
   "when", "why", "how", "all", "any", "both", "each", "few", "more", "most", "other",
   # "some", "such", "no",  "not", 
   # "too", "very",   
   # verbs
   "is", "are", "would", "could", "will", "be"
)

# Convert to a data frame if needed for consistency with tidytext
custom_stop_words_df <- tibble(word = custom_stop_words)
```

### --- TF-IDF matrix on train pdo 
```{r}
# reduce size 

pdo_train_4_tf_idf <- pdo_train_t %>% # 255964
   # Keep only content words [very restrictive for now]
   # normally c("NOUN", "VERB", "ADJ", "ADV")
   filter(upos %in% c("NOUN")) %>% #    72,668 
   filter(!token_l %in% c("development", "objective", "project")) %>%   #  66,741
   # get rid of stop words (from default list)   
   filter(!token_l %in% custom_stop_words_df$word) %>%   #  66,704
   # Optional: Remove lemmas of length 1 or shorter
   filter(nchar(lemma) > 1)  #  66,350
```

Now, count the occurrences of each `lemma` for each document. (This is the term frequency or `tf`)

```{r}
# This is the term frequency or `tf`

# Count lemmas per document
lemma_counts <- pdo_train_4_tf_idf %>%
  count(proj_id, lemma, sort = TRUE)
# Preview the result
head(lemma_counts) 
```

With the lemma counts prepared, the `bind_tf_idf()` function from the tidytext package computes the TF-IDF scores.
```{r}
# Compute the TF-IDF scores
lemma_tf_idf <- lemma_counts %>%
  bind_tf_idf(lemma, proj_id, n) %>%
  arrange(desc(tf_idf))
```


::: {.callout-note collapse='true'}
What to use: token, lemma, or stem?

General Preference in Real-World NLP:

+ `Tokens` for analyses where word forms matter or for sentiment analysis.
+ `Lemmas` (*)  for most general-purpose NLP tasks where you want to reduce dimensionality while maintaining accuracy and clarity of meaning.
+ `Stems` for very large datasets, search engines, and applications where speed and simplicity are more important than linguistic precision.

(*) I use lemma, after "aggressively" reducing the number of words to consider, and removing stop words (at least for now). 
::: 

# _______ 
# TEXT ANALYSIS/SUMMARY
# _______ 
<!-- see https://cengel.github.io/R-text-analysis/textanalysis.html  -->

## Frequencies of documents/words/stems
```{r}
#| echo: false

# Count words
counts_pdo <- pdo_train_t %>%
     count(pdo, sort = TRUE)  # 4,071

counts_words <- pdo_train_t %>%
     count(word_original, sort = TRUE)  # 13,441

counts_token <- pdo_train_t %>%
  count(token, sort = TRUE)   # 13,420

counts_lemma <- pdo_train_t %>%
  count(lemma, sort = TRUE)   # 11,705

counts_stem <- pdo_train_t %>%
  count(stem, sort = TRUE)   # 8,812
```

We are looking at (**training data subset**) `pdo_train_t` which has `r nrow(pdo_train_t)` rows and `r ncol(pdo_train_t)` columns obtained from `r n_distinct(pdo_train_t$pdo)` PDOs of `r n_distinct(pdo_train_t$proj_id)` Wold Bank projects approved in Fiscal Years ranging from `r min(pdo_train_t$FY_appr)` to `r max(pdo_train_t$FY_appr)`. 
   
```{r}
#| eval: true
#| output: true
#| echo: false

# Create a tibble
train_recap <- tibble(
  entity = c("N proj", "N PDOs", "N words","N token", "N lemma", "N stem"),
  counts = c(n_distinct(pdo_train_t$proj_id),
              n_distinct(pdo_train_t$pdo),
              n_distinct(pdo_train_t$word_original),
              n_distinct(pdo_train_t$token_l),
              n_distinct(pdo_train_t$lemma),
              n_distinct(pdo_train_t$stem)
              ))

# Pipe the tibble into kable
train_recap %>%
  kableExtra::kable()
```

 
#### [FUNC] save plots 
```{r}
#| echo: false

f_save_plot <- function(plot_name, plot_object) {
   # Print the plot, save as PDF and PNG
   plot_object %T>%
      print() %T>%
      ggsave(., filename = here("analysis", "output", "figures", paste0(plot_name, ".pdf")),
             # width = 4, height = 2.25, units = "in",
             device = cairo_pdf) %>%
      ggsave(., filename = here("analysis", "output", "figures", paste0(plot_name, ".png")),
             # width = 4, height = 2.25, units = "in",
             type = "cairo", dpi = 300)
}

# Example of using the function
# f_save_plot("proj_wrd_freq", proj_wrd_freq)

```
 

## Term frequency

Note: normally, the most frequent words are *function words* (e.g. determiners, prepositions, pronouns, and auxiliary verbs), which are not very informative. Moreover, even *content words* (e.g. nouns, verbs, adjectives, and adverbs) can often be quite generic semantically speaking (e.g. "good" may be used for many different things). 
 
In this analysis, Ido not use the **STOPWORD** approach, but  use the **POS tags** to reduce our dataset to just the content words, that is nouns, verbs, adjectives, and adverbs

### [FIG] Overall `token` freq ggplot
+ Excluding "project" "develop","objective"  
+ Including only "content words" (NOUN, VERB, ADJ, ADV)

```{r}
#| output: TRUE

# Evaluate the title with glue first
title_text <- glue::glue("Most frequent token in {n_distinct(pdo_train_t$proj_id)} PDOs from projects approved between FY {min(pdo_train_t$FY_appr)}-{max(pdo_train_t$FY_appr)}") 

proj_wrd_freq <- pdo_train_t %>%   # 123,927
   # include only content words
   filter(upos %in% c("NOUN", "VERB", "ADJ", "ADV")) %>%
   #filter (!(upos %in% c("AUX","CCONJ", "INTJ", "DET", "PART","ADP", "SCONJ", "SYM", "PART", "PUNCT"))) %>%
   filter (!(relation %in% c("nummod" ))) %>% # 173,686 
 filter (!(token_l %in% c("pdo","project", "development", "objective","objectives", "i", "ii", "iii",
                          "is"))) %>% # whne it is VERB
   count(token_l) %>% 
   filter(n > 800) %>% 
   mutate(token_l = reorder(token_l, n)) %>%  # reorder values by frequency
   # plot 
   ggplot(aes(token_l, n)) +
   geom_col(fill = "gray") +
   coord_flip() + # flip x and y coordinates so we can read the words better
   labs(title = title_text,
        subtitle = "[token_l count > 800]", y = "", x = "")+
  theme(plot.title.position = "plot")

proj_wrd_freq
```

```{r}
#| echo: false
f_save_plot("proj_wrd_freq", proj_wrd_freq)
```


### [FIG] Overall `stem` freq ggplot
+ Without "project" "develop","objective"  
+ Including only "content words" (NOUN, VERB, ADJ, ADV)

```{r}
#| output: TRUE

# Evaluate the title with glue first
title_text <- glue::glue("Most frequent STEM in {n_distinct(pdo_train_t$proj_id)} PDOs from projects approved between FY {min(pdo_train_t$FY_appr)}-{max(pdo_train_t$FY_appr)}") 
# Plot
proj_stem_freq <- pdo_train_t %>%   # 256,632
   # include only content words
   filter(upos %in% c("NOUN", "VERB", "ADJ", "ADV")) %>%
   filter (!(relation %in% c("nummod" ))) %>% # 173,686 
 filter (!(stem %in% c("pdo","project", "develop", "object", "i", "ii", "iii"))) %>%
   count(stem) %>% 
   filter(n > 800) %>% 
   mutate(stem = reorder(stem, n)) %>%  # reorder values by frequency
   # plot 
   ggplot(aes(stem, n)) +
   geom_col(fill = "gray") +
   coord_flip() + # flip x and y coordinates so we can read the words better
   labs(title = title_text,
        subtitle = "[stem count > 800]", y = "", x = "") +
  theme(plot.title.position = "plot")

proj_stem_freq
```

```{r}
#| echo: false
f_save_plot("proj_stem_freq", proj_stem_freq)
```

Evidently, after stemming, more words (or stems) reach the threshold frequency count of 800.
# _______ 

## SECTOR-related term frequency

### Isolate SECTOR words and see frequency over years

To try and make it a bit more meaningful, let's focus on the frequency of the most common words related to SECTORS. 

From `token_l`, I created a *"broad SECTOR" variable* to group the sectors in broader definitions:

+ **WATER** = water, wastewater, sanitation
+ **TRANSPORT** = transport, railway, road, airport, port
+ **URBAN** = urban
+ **ENERGY** = energy, electricity, hydroelectric, hydropower, renewable, transmission
+ **HEALTH** = health, hospital, medicine, drugs, epidemic, pandemic, covid-19, vaccine
+ **EDUCATION** = education, school, vocational, teach, university, student, literacy, training, curricul 
 
```{r}
pdo_train_t <- pdo_train_t %>%
   # dealing with water/watershed/waterway
   mutate(tok_sector_broad = case_when(
      str_detect(token_l, "water|wastewater|sanitat") ~ "WATER",
      str_detect(token_l, "transport|railway|road|airport") ~ "TRANSPORT",
      token_l == "port" ~ "TRANSPORT",
      str_detect(token_l, "urban") ~ "URBAN",
      str_detect(token_l, "energ|electri|hydroele|hydropow|renewable|transmis") ~ "ENERGY",  # Matches either "energy" or "power"
      str_detect(token_l, "health|hospital|medicine|drugs|epidem|pandem|covid-19|vaccin") ~ "HEALTH",
      str_detect(token_l, "educat|school|vocat|teach|univers|student|literacy|training|curricul") ~ "EDUCATION",
      TRUE ~ NA_character_)) %>% 
   relocate(tok_sector_broad, .after = token_l) # move the new column to the right of token_l
```

#### [FIG] By sector (`tok_sector_broad`) freq ggplot
```{r}
#| output: TRUE

# prepare data for plotting (count)
sector_broad <- pdo_train_t %>% 
   filter(!is.na(tok_sector_broad)) %>% 
   count(FY_appr, tok_sector_broad) %>% 
   filter(n > 0) %>% 
   mutate(tok_sector_broad = factor(tok_sector_broad, levels = c("WATER", "TRANSPORT", "URBAN", "ENERGY", "HEALTH", "EDUCATION"))) # reorder values by frequency
#df$FY

# Evaluate the title with glue first
title_text <- glue::glue("Sector words frequency in PDO over FY {min(pdo_train_t$FY_appr)}-{max(pdo_train_t$FY_appr)}") 

# Plot
proj_sect_broad_fr <-  ggplot(data = sector_broad, 
                              aes(x = FY_appr, y = n, 
                                  group = tok_sector_broad, color = tok_sector_broad)) +
   geom_line() +
   geom_point() +
   scale_x_continuous(breaks =  seq(2001, 2023, by=  1)) +
   scale_color_viridis_d(option = "magma", end = 0.9) + 
   facet_wrap(~tok_sector_broad, ncol = 2, scales = "free")+ 
   guides(color = FALSE) +
   theme_bw()+
   theme(# Adjust angle and alignment of x labels
      axis.text.x = element_text(angle = 45, hjust = 1)) + 
   labs(title = "Sector words frequency in PDO over Fiscal Years",
        subtitle = "[Using \"custom\" broad sector definition]",
        x =   "Board approval FY", y = "Counts of 'sector' word (tok_sector_broad)") + 
   geom_vline(data = subset(sector_broad, tok_sector_broad == "HEALTH"), aes(xintercept = 2020), 
              linetype = "dashed", color = "#9b6723") +
   geom_text(data = subset(sector_broad, tok_sector_broad == "HEALTH"), 
             aes(x = 2020, y = max(sector_broad$n)*0.75, label = "Covid"), 
             angle = 90, vjust = -0.5, color = "#9b6723")


proj_sect_broad_fr
```

```{r}
#| echo: false
f_save_plot("proj_sect_broad_fr", proj_sect_broad_fr)
```

### Check (by sector) why peaks [CMPL 🟠]  

For the (broadly defined) health sector, it is quite clear that **Covid-19** is the main driver of the peak in 2020. What about the other sectors? (they all seem to have at least 1 evident peak)
  
### --- I could see if corresponds to any WDR publications

### Qualify: peak or trend (by sector) [CMPL 🟠]  

... capire come si stabilisce che c'e' un trend ... magari vedere google search trend... 


# _______ 
## Create bigrams 

> Here I use [`clnp_annotate()` output + ] `dplyr` to combine consecutive tokens into bigrams.

```{r}
# Create bigrams by pairing consecutive tokens by sentence ID and token IDs
bigrams <- pdo_train_t %>%
   # keeping FY with tokens
   group_by(FY_appr, proj_id, pdo, sid ) %>%
   arrange(tid) %>%
   # Using mutate() and lead(), we create bigrams from consecutive tokens 
   mutate(next_token = lead(token), 
          bigram = paste(token, next_token)) %>%
   # make bigram low case
   mutate(bigram = tolower(bigram)) %>%
   # only includes the rows where valid bigrams are formed
   filter(!is.na(next_token)) %>%
   ungroup() %>%
   arrange(FY_appr, proj_id, sid, tid) %>%
   select(FY_appr,proj_id, pdo,sid, tid, token, bigram) 
```

```{r}
# most frequent bigrams 
count_bigram <- bigrams %>% 
   count(bigram, sort = TRUE)  
```

## Clean bigrams
```{r}
# Separate the bigram column into two words
bigrams_cleaned <- bigrams %>%
  tidyr::separate(bigram, into = c("word1", "word2"), sep = " ")

# Remove stopwords and bigrams containing punctuation
bigrams_cleaned <- bigrams_cleaned %>%
   # custom stop words
   filter(!word1 %in% custom_stop_words_df$word, !word2 %in% custom_stop_words_df$word) %>% 
   # Remove punctuation   
   filter(!str_detect(word1, "[[:punct:]]"), !str_detect(word2, "[[:punct:]]"))  

# Reunite the cleaned words into the bigram column
bigrams_cleaned <- bigrams_cleaned %>%
   unite(bigram, word1, word2, sep = " ") %>% 
   # Remove too obvious bigrams 
   filter(!bigram %in% c("development objective", "development objectives", 
                         "proposed project", "project development"))

# View the cleaned dataframe
bigrams_cleaned

# Count the frequency of each bigram
bigram_freq <- bigrams_cleaned %>%
  count(bigram, sort = TRUE)


```

### [FIG] most frequent bigrams in PDOs
+ Excluding "development objective", "development objectives", "proposed project"
+ Excluding stopwords and bigrams containing punctuation
 
```{r}
#| output: true

# ---- Prepare data for plotting
# Evaluate the title with glue first
title_text <- glue::glue("Frequency of bigrams in PDOs over FY {min(pdo_train_t$FY_appr)}-{max(pdo_train_t$FY_appr)}") 
# Define the bigrams you want to highlight
bigrams_to_highlight <- c("public sector", "private sector")   

 
# Plot the most frequent bigrams
pdo_bigr_freq <- bigram_freq %>%
   slice_max(n, n = 25) %>%
   ggplot(aes(x = reorder(bigram, n), y = n,
              fill = ifelse(bigram %in% bigrams_to_highlight, bigram, "Other"))) +
   geom_col() +
  scale_fill_manual(values = c("public sector" = "#005ca1", "private sector" = "#e60066", "Other" = "grey")) +
   guides(fill = "none") +
   coord_flip() +
   labs(title = title_text,x = "Bigram", y = "Frequency", 
        subtitle = "ranking first 20" ) +
   theme(plot.title.position = "plot")  

pdo_bigr_freq
```

```{r}
#| echo: false
f_save_plot("pdo_bigr_freq", pdo_bigr_freq)
```
 
> I wasn't expecting "eligible crisis"?!
I was expecting education to appear!?!?

#### [FIG] Changes over time ? [CMPL 🟠] 
 

# _______ 
## Explore bigrams


## INSTITUTION-related words and see frequency over years 

A first aspect to explore could be whether WB project are more focused on public or private sectors. However, it is not easy to capture this information from the text.

### Isolate sector BIGRAM frequency

Things to keep in mind: 

+ "government" may be referred to the subject/counterpart of the project (e.g. "government of Mozambique")
+ "private" is not necessarily referred to the "private sector" (e.g. "private households")
+ "public" is not necessarily referred to the "public sector" (e.g. "public health")

So, I narrow down to consecutive *bigrams* **"public sector"** and **"private sector"** to get an indicative frequency of these terms. 


#### [FIG] Bigrams ("public sector", "private sector") freq plot
```{r}

# Filter for the specific bigrams "public sector" and "private sector"
bigrams_pub_priv_sec <- bigrams %>%
   filter(bigram %in% c("public sector", "private sector"))

# Display the result
#bigrams_pub_priv_sec

# prepare data for plotting (count)
sector_bigr_df <- bigrams_pub_priv_sec %>% 
   count(FY_appr, bigram) %>% 
   # reorder values by frequency
   mutate(bigram = factor(bigram, levels = c("public sector", "private sector")))
```


```{r}
#| output: true

# ---- Prepare data for plotting
# Evaluate the title with glue first
title_text <- glue::glue("Frequency of bigrams \"public sector\" and \"private sector\" in PDOs over FY {min(sector_bigr_df$FY_appr)}-{max(sector_bigr_df$FY_appr)}") 

two_col_contrast <- c( "#005ca1",  "#e60066" )

# Create a named vector for the legend labels with totals in a single pipeline
legend_labels <- sector_bigr_df %>%
   group_by(bigram) %>%
   # Calculate total counts for each bigram
   summarize(total_n = sum(n)) %>% 
   # Append totals to bigram names
   mutate(label = paste0(bigram, " (", total_n, ")")) %>%  
   # Create a named vector with bigram as names and labels as values
   {setNames(.$label, .$bigram)} # curly braces {} in a dplyr pipeline using . as ouptu from previous..

# ---- Plot
inst_bigr_fr <- ggplot(data = sector_bigr_df, aes(x = FY_appr, y = n, group = bigram, color = bigram)) +
   geom_line() +
   geom_point() +
   scale_x_continuous(breaks = seq(2001, 2023, by = 1)) +
   scale_color_manual(values = two_col_contrast, 
                      labels = legend_labels) +  # Use modified labels
   theme_bw() +
   theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
   labs(title = title_text, x = "Board approval FY", y = "Counts of bigrams", color = "bigram")+
  theme(plot.title.position = "plot")

inst_bigr_fr
```

```{r}
#| eval: false
#| output: false
#| echo: true

# Save the plot
f_save_plot("inst_bigr_fr", inst_bigr_fr)
```


> Note: these are much less common than the single words.


# _______ 
# >>>>>> QUI <<<<<<<<<<<<<<<<<< 
Main ref https://www.nlpdemystified.org/course/advanced-preprocessing 
rivedere cos'avevo fatto x pulire in `analysis//03_WDR_pdotracs_explor.qmd` 
https://cengel.github.io/R-text-analysis/textprep.html#detecting-patterns
https://guides.library.upenn.edu/penntdm/r
https://smltar.com/stemming#how-to-stem-text-in-r `BOOK STEMMING`
# _______ 

### Isolate other BIGRAM frequency...

#### [FIG] Most frequent bigrams 

```{r}
#| output: TRUE
# Evaluate the title with glue first
title_text <- glue::glue("Most frequent token in {n_distinct(pdo_train_t$proj_id)} PDOs from projects approved between FY {min(pdo_train_t$FY_appr)}-{max(pdo_train_t$FY_appr)}") 

pdo_bigr_freq <- pdo_train_t %>%   # 123,927
   # include only content words
   filter(upos %in% c("NOUN", "VERB", "ADJ", "ADV")) %>%
   #filter (!(upos %in% c("AUX","CCONJ", "INTJ", "DET", "PART","ADP", "SCONJ", "SYM", "PART", "PUNCT"))) %>%
   filter (!(relation %in% c("nummod" ))) %>% # 173,686 
 filter (!(token_l %in% c("pdo","project", "development", "objective","objectives", "i", "ii", "iii",
                          "is"))) %>% # whne it is VERB
   count(token_l) %>% 
   filter(n > 800) %>% 
   mutate(token_l = reorder(token_l, n)) %>%  # reorder values by frequency
   # plot 
   ggplot(aes(token_l, n)) +
   geom_col(fill = "gray") +
   coord_flip() + # flip x and y coordinates so we can read the words better
   labs(title = title_text,
        subtitle = "[token_l count > 800]", y = "", x = "")+
  theme(plot.title.position = "plot")

proj_wrd_freq
```

```{r}
#| echo: false
f_save_plot("proj_wrd_freq", proj_wrd_freq)
```

 
#### ... [FIG] Notable bigrams (climate change)!
 


 


## Word and document frequency: Tf-idf

The goal is to quantify what a document is about. What is the document about?

-   **term frequency (tf)** = how frequently a word occurs in a document... but there are words that occur many time and are not important
-   term's **inverse document frequency (idf)** = decreases the weight for commonly used words and increases the weight for words that are not used very much in a collection of documents.
-   **statistic tf-idf (= tf*-*idf)** = an alternative to using stopwords is *the frequency of a term adjusted for how rarely it is used*. [It measures how important a word is to a document in a collection (or corpus) of documents, but it is still a rule-of-thumb or heuristic quantity]


## N-Grams
...

## Co-occurrence
...


# _______ 
# TOPIC MODELING w ML
# _______ 
### Compare PDO text v. project METADATA  [CMPL 🟠]
Using NLP models trained on document metadata and structure can be combined with text analysis to improve classification accuracy.


STEPS 

1. Use document text (abstracts) as features to train a supervised machine learning model. The labeled data (documents with sector tags) will serve as training data, and the model can predict the missing sector tags for unlabeled documents.
2. TEXT preprocessing (e.g. tokenization, lemmatization, stopword removal, *TF-IDF*) 
   + Convert the processed text into a numerical format using Term Frequency-Inverse Document Frequency (TF-IDF), which gives more weight to terms that are unique to a document but less frequent across the entire corpus.
3. Define data features, e.g.
   + Document Length: Public sector documents might be longer, more formal.
   + Presence of Certain Keywords: Use specific keywords that correlate with either the public or private sector.
   + Sector Tags: In documents where the "sector tag" is present, you can use it as a feature for training.
4. Predicting Missing Sector Tags (Classification):
   + Use models like: Logistic Regression: For a binary classification (e.g., public vs. private). Random Forest or XGBoost: If you have a more complex tagging scheme (e.g., multiple sector categories).
   + Cross-validation: Ensure the model generalizes well by validating with the documents that already have the sector tag filled in.
   + Evaluate the model: Use metrics like accuracy, precision, recall, and F1 score to evaluate the model's performance.


```{r}
#| eval: FALSE
#| echo: FALSE
library(tidytext)
library(dplyr)
library(tidyr)
library(caret)

# ----- 1. Tokenization and stopwords removal using tidytext.
# Assuming df is your dataframe with "abstract" and "sector_tag"
# Tokenize the text and remove stopwords
tidy_abstracts <- df %>%
  unnest_tokens(word, abstract) %>%
  anti_join(stop_words)  # Remove stopwords

# Optional: Apply stemming (you can also use `SnowballC` if you prefer)
tidy_abstracts <- tidy_abstracts %>%
  mutate(word = SnowballC::wordStem(word))

# ----- 2. Document-term matrix or TF-IDF calculation using bind_tf_idf().
# Create a term-frequency matrix
abstract_dtm <- tidy_abstracts %>%
  count(document_id = row_number(), word) %>%  # Assuming each row is a document
  cast_dtm(document_id, word, n)

# Alternatively, use TF-IDF weighting
abstract_tfidf <- tidy_abstracts %>%
  count(document_id = row_number(), word) %>%
  bind_tf_idf(word, document_id, n)

# ----- 3. Model training using caret (Random Forest, Logistic Regression, etc.).
# Split data into training (with sector tags) and testing (missing sector tags)
train_data <- df[!is.na(df$sector_tag), ]
test_data  <- df[is.na(df$sector_tag), ]

# Combine the DFM or TF-IDF with the training dataset
train_tfidf <- abstract_tfidf %>%
  filter(document_id %in% train_data$row_number()) %>%
  spread(word, tf_idf, fill = 0)

# Merge with sector tags
train_tfidf <- left_join(train_tfidf, train_data, by = c("document_id" = "row_number"))

# Prepare for machine learning by ensuring you have sector tags in the final dataset

# ----- 4. Prediction of missing sector tags based on the trained model.
# Random Forest Model
model <- train(sector_tag ~ ., data = train_tfidf, method = "rf")

# Predict missing sector tags for the test data
test_tfidf <- abstract_tfidf %>%
  filter(document_id %in% test_data$row_number()) %>%
  spread(word, tf_idf, fill = 0)

# Predict sector tags for the missing observations
predicted_tags <- predict(model, newdata = test_tfidf)

# Add the predicted sector tags to the original dataset
df$sector_tag[is.na(df$sector_tag)] <- predicted_tags

# ----- 5. Evaluate and Refine the Model
confusionMatrix(predicted_tags, test_data$sector_tag)
```


### --- I could see if corresponds to sector flags in the project metadata
more missing but more objective!


# Topic modeling algorithms with Latent Dirichlet Allocation (LDA) 

Topic modeling algorithms like Latent Dirichlet Allocation (LDA) can be applied to automatically uncover underlying themes within a corpus. The detected topics may highlight key terms or subject areas that are strongly associated with either the public or private sector.
 
# Named Entity Recognition using CleanNLP and spaCy
NER is especially useful for analyzing unstructured text. 

NER can identify key entities (organizations, people, locations) mentioned in the text. By tracking which entities appear frequently (e.g., government agencies vs. corporations), it’s possible to categorize a document as more focused on the public or private sector.

### ---  Summarise the tokens by parts of speech
  
```{r}
#| eval: false
#| output: false
#| echo: true


# Initialize the spacy backend
cnlp_init_spacy() 
```













```{bash}
#| eval: false
quarto render analysis/01b_WB_project_pdo_anal.qmd --to html
open ./docs/analysis/01b_WB_project_pdo_anal.html
```

 
