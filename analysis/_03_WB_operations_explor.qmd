---
title: "Process and merge data - WB operations"
author: "Luisa M. Mimmi"
date: "Last run: `r format(Sys.time(), '%F')`"
lang: en
editor: source
engine: knitr
## ------  general Output Options
execute:     
  eval: false    # actually run? 
  echo: true     #  include source code in output
  warning: false  #  include warning code in output
  error: false    #  include error code in output
  output: false   # include output code in output (CHG in BLOCKS)
  # include: false   # R still runs but code and results DON"T appear in output  
  cache: false # normalmnte false
toc: true
fig-cap-location: top
tbl-cap-location: top
format:
  html:
    # theme: flatly #spacelab
    code-fold: false # redundant bc echo false 
    toc-depth: 3
    toc_float: true
    toc-location: left
    toc-title: Outline
    embed-resources: true # external dependencies embedded (Not in ..._files/)
  # pdf:
  #   toc-depth: 2
  #   toc-title: Indice
  #   highlight-style: github
  #   #lang: it
  #   embed-resources: true # external dependencies embedded (Not in ..._files/)
format-links: false
bibliography: ../bib/slogan.bib
---


<i class="fa fa-refresh" style="color: firebrick"></i> Work in progress


```{r setup-libraries-data }
#knitr::opts_chunk$set(include = TRUE, warning = FALSE)
# Pckgs -------------------------------------
#if (!require ("pacman")) (install.packages("pacman"))

#p_install_gh("luisDVA/annotater")
#p_install_gh("HumanitiesDataAnalysis/hathidy")
# devtools::install_github("HumanitiesDataAnalysis/HumanitiesDataAnalysis") 

library(here) # A Simpler Way to Find Your Files
library(fs) # Cross-Platform File System Operations Based on 'libuv'
library(paint) # paint data.frames summaries in colour 
library(tidyverse) # Easily Install and Load the 'Tidyverse' 
library(magrittr) # A Forward-Pipe Operator for R
library(skimr) # Compact and Flexible Summaries of Data
library(scales) # Scale Functions for Visualization # Scale Functions for Visualization 

library(colorspace) # A Toolbox for Manipulating and Assessing Colors and Palettes
library(scales) # Scale Functions for Visualization # Scale Functions for Visualization
library(httr) # Tools for Working with URLs and HTTP
library(DT) # an R interface to the JavaScript library DataTables
library(knitr) # A General-Purpose Package for Dynamic Report Generation in R
library(kableExtra) # Construct Complex Table with 'kable' and Pipe Syntax 
library(flextable) # Functions for Tabular Reporting 

library(splitstackshape)  #Stack and Reshape Datasets After Splitting Concatenated Values
library(tm) # Text Mining Package
library(tidytext) # Text Mining using 'dplyr', 'ggplot2', and Other Tidy Tools
# this requires pre-requirsites to install : https://github.com/quanteda/quanteda
library(quanteda) # Quantitative Analysis of Textual Data
library(igraph) # Network Analysis and Visualization
library(sjmisc) # Data and Variable Transformation Functions
library(ggraph) # An Implementation of Grammar of Graphics for Graphs and Networks
library(widyr) # Widen, Process, then Re-Tidy Data
library(SnowballC) # Snowball Stemmers Based on the C 'libstemmer' UTF-8 Library
# library(#HumanitiesDataAnalysis, # Data and Code for Teaching Humanities Data Analysis
library(sentencepiece) # Text Tokenization using Byte Pair Encoding and Unigram Modelling
 
# extra steo needed to install github version 
#if (!require("devtools")) install.packages("devtools")
#library(devtools)
#install_github("husson/FactoMineR")     FAILED !!!!!!
# library(FactoMineR)
#library(factoextra)

# Plot Theme(s) -------------------------------------
#source(here("R", "ggplot_themes.R"))
ggplot2::theme_set(theme_minimal())
# color paletts -----
mycolors_gradient <- c("#ccf6fa", "#80e8f3", "#33d9eb", "#00d0e6", "#0092a1")
mycolors_contrast <- c("#E7B800", "#a19100", "#0084e6","#005ca1", "#e60066" )


# Function(s) -------------------------------------

# Data -------------------------------------

# -------------------- {cut bc made too heavy} -------------------------------------
# # Tables [AH knit setup when using kbl() ]------------------------------------
# knit_print.data.frame <- function(x, ...) {
#   res <- paste(c('', '', kable_styling(kable(x, booktabs = TRUE))), collapse = '\n')
#   asis_output(res)
# }
# 
# registerS3method("knit_print", "data.frame", knit_print.data.frame)
# registerS3method("knit_print", "grouped_df", knit_print.data.frame)
```

# World Bank Projects & Operations:

## Data sources:

**World Bank Projects & Operations**: [Data Catalog](https://datacatalog.worldbank.org/search/dataset/0037800> <https://datacatalog.worldbank.org/search/dataset/0037800/World-Bank-Projects---Operations)
https://projects.worldbank.org/en/projects-operations/project-search 

-   Accessibility Classification: **public** under [Creative Commons Attribution 4.0](https://datacatalog.worldbank.org/public-licenses?fragment=cc)

## Raw data
I retrieved manually ALL WB projects approved *between FY 1973 and 2023 (last FY incomplete)* on 09/22/2022 (WDRs go from 1978-2022)
using this [example url](https://projects.worldbank.org/en/projects-operations/projects-list?str_fiscalyear=1979&end_fiscalyear=1979&os=0) and saved individual `.xlsx` files in `data/raw_data/`

In file `_my_stuff/WBprojects_1973-2023_data-ingestion.Rmd` I did these operations:
+ Load all `.xlsx` files separately 
+ Save objs in folder as `.Rds` files separately `data/raw_data/projects`


#### --- Load all `.Rds` files previosuly saved  
```{r }
# --- define directory path
file_path <- here("data","raw_data", "projects/")

# --- get a character vector of the names of files
Rds_file_names <- file_path %>% list.files(. , 
                         pattern = ".Rds",
                         full.names = FALSE) 

# df_paths
datasets_l  <-  as.list(list.files(path = here("data", "raw_data", "projects"),
                                   pattern = "\\.Rds$"))
 

# # add a complete path with purrr
# datasets_path_l  <-  purrr::map(datasets_l,
#                                 ~ paste0(here("data", "raw_data", "projects/"), .x))


# ---- Load everything into the Global Environment
purrr::map(.x =  Rds_file_names,
           .f =  function(datasets_l){ # iterate through each file name
             # Assign a Value to a Name       
             assign(x = str_remove(datasets_l,# new value in GlobEnv
                                   ".Rds"), # Remove file extension 
                    # inputvalue in my folder to be assigned to x
                    value = readRDS(paste0("data/raw_data/projects/", datasets_l)),
                    # the environment to use
                    envir = .GlobalEnv)
           }
           )

# names  
df_name_list <- ls(pattern = "00$")
```

```{r}
  
# df_list <-  list(
# FY_1973_WB_Projects_1_500 = '1973_WB_Projects_1_500',
# FY_1974_WB_Projects_1_500 = '1974_WB_Projects_1_500',
# FY_1975_WB_Projects_1_500 = '1975_WB_Projects_1_500',
# FY_1976_WB_Projects_1_500 = '1976_WB_Projects_1_500',
# FY_1977_WB_Projects_1_500 = '1977_WB_Projects_1_500',
# FY_1978_WB_Projects_1_500 = '1978_WB_Projects_1_500',
# FY_1979_WB_Projects_1_500 = '1979_WB_Projects_1_500',
# FY_1980_WB_Projects_1_500 = '1980_WB_Projects_1_500',
# FY_1981_WB_Projects_1_500 = '1981_WB_Projects_1_500',
# FY_1982_WB_Projects_1_500 = '1982_WB_Projects_1_500',
# FY_1983_WB_Projects_1_500 = '1983_WB_Projects_1_500',
# FY_1984_WB_Projects_1_500 = '1984_WB_Projects_1_500',
# FY_1985_WB_Projects_1_500 = '1985_WB_Projects_1_500',
# FY_1986_WB_Projects_1_500 = '1986_WB_Projects_1_500',
# FY_1987_WB_Projects_1_500 = '1987_WB_Projects_1_500',
# FY_1988_WB_Projects_1_500 = '1988_WB_Projects_1_500',
# FY_1989_WB_Projects_1_500 = '1989_WB_Projects_1_500',
# FY_1990_WB_Projects_1_500 = '1990_WB_Projects_1_500',
# FY_1991_WB_Projects_1_500 = '1991_WB_Projects_1_500',
# FY_1992_WB_Projects_1_500 = '1992_WB_Projects_1_500',
# FY_1993_WB_Projects_1_500 = '1993_WB_Projects_1_500',
# FY_1994_WB_Projects_1_500 = '1994_WB_Projects_1_500',
# FY_1995_WB_Projects_1_500 = '1995_WB_Projects_1_500',
# FY_1996_WB_Projects_1_500 = '1996_WB_Projects_1_500',
# FY_1997_WB_Projects_1_500 = '1997_WB_Projects_1_500',
# FY_1998_WB_Projects_1_500 = '1998_WB_Projects_1_500',
# FY_1999_WB_Projects_1_500 = '1999_WB_Projects_1_500',
# FY_2000_WB_Projects_1_500 = '2000_WB_Projects_1_500',
# FY_2001_WB_Projects_1_500 = '2001_WB_Projects_1_500',
# FY_2002_WB_Projects_1_500 = '2002_WB_Projects_1_500',
# FY_2003_WB_Projects_1_500 = '2003_WB_Projects_1_500',
# FY_2004_WB_Projects_1_500 = '2004_WB_Projects_1_500',
# FY_2005_WB_Projects_1_500 = '2005_WB_Projects_1_500',
# FY_2006_WB_Projects_1_500 = '2006_WB_Projects_1_500',
# FY_2007_WB_Projects_1_500 = '2007_WB_Projects_1_500',
# FY_2007_WB_Projects_500_1000 = '2007_WB_Projects_500_1000',
# FY_2008_WB_Projects_1_500   = '2008_WB_Projects_1_500',
# FY_2008_WB_Projects_500_1000 = '2008_WB_Projects_500_1000',
# FY_2009_WB_Projects_1_500   = '2009_WB_Projects_1_500',
# FY_2009_WB_Projects_500_1000 = '2009_WB_Projects_500_1000',
# FY_2010_WB_Projects_1_500   = '2010_WB_Projects_1_500',
# FY_2010_WB_Projects_500_1000 = '2010_WB_Projects_500_1000',
# FY_2011_WB_Projects_1_500   = '2011_WB_Projects_1_500',
# FY_2011_WB_Projects_500_1000 = '2011_WB_Projects_500_1000',
# FY_2012_WB_Projects_1_500   = '2012_WB_Projects_1_500',
# FY_2012_WB_Projects_500_1000 = '2012_WB_Projects_500_1000',
# FY_2013_WB_Projects_1_500   = '2013_WB_Projects_1_500',
# FY_2014_WB_Projects_1_500   = '2014_WB_Projects_1_500',
# FY_2014_WB_Projects_500_1000 = '2014_WB_Projects_500_1000',
# FY_2015_WB_Projects_1_500   = '2015_WB_Projects_1_500',
# FY_2015_WB_Projects_500_1000 = '2015_WB_Projects_500_1000',
# FY_2016_WB_Projects_1_500   = '2016_WB_Projects_1_500',
# FY_2017_WB_Projects_1_500   = '2017_WB_Projects_1_500',
# FY_2017_WB_Projects_500_1000 = '2017_WB_Projects_500_1000',
# FY_2018_WB_Projects_1_500   = '2018_WB_Projects_1_500',
# FY_2019_WB_Projects_1_500   = '2019_WB_Projects_1_500',
# FY_2019_WB_Projects_500_1000 = '2019_WB_Projects_500_1000',
# FY_2020_WB_Projects_1_500   = '2020_WB_Projects_1_500',
# FY_2020_WB_Projects_500_1000 = '2020_WB_Projects_500_1000',
# FY_2021_WB_Projects_1_500   = '2021_WB_Projects_1_500',
# FY_2021_WB_Projects_500_1000 = '2021_WB_Projects_500_1000',
# FY_2022_WB_Projects_1_500   = '2022_WB_Projects_1_500',
# FY_2022_WB_Projects_500_1000 = '2022_WB_Projects_500_1000',
# FY_2023_WB_Projects_1_500   = '2023_WB_Projects_1_500',
# FY_2023_WB_Projects_500_1000 = '2023_WB_Projects_500_1000'
# )
```


## Transform raw data

> Following [SO](https://stackoverflow.com/questions/42028710/add-new-variable-to-list-of-data-frames-with-purrr-and-mutate-from-dplyr)

#### --- Add FY column to all taking it from file name
```{r}
#### ----- create a list of the df of interest ----- 
# actual DF in a list 
df_list <- Filter(is.data.frame, as.list(.GlobalEnv))
str(df_list[1])
length(df_list)

#### ----- Add col to each DF in teh list ----- 

#### --- modo 1 [nope]
# [function] write as .Rds 
# for (i in 1:n) {
#   Rds_file_names[i]$FY <- NA
# }
# 
#  Rds_file_names[1]$FY <-  NA
# `1978_WB_Projects_1_500`$FY <- NA

#### --- modo 2.a [si] {map2}
df_list2 <- map2(# map over 2 arguments GIVES LIST OF DFs
  df_list, names(df_list),
  ~ mutate(.x, FY = .y)
  ) 

# #### --- modo 2.b [si] {map2_df}
df_all <- map2_df(# map over 2 arguments GIVES 1 MEGA DF
  df_list, names(df_list),
  ~ mutate(.x, FY = .y)
  )
paint(`1978_WB_Projects_1_500`)
paint(`2023_WB_Projects_500_1000`)
paint(df_all)

```
 
 
#### --- [CLean up]  
```{r}
#rm(list=setdiff(ls(), c("df_all")))
```
 
#### --- Correct columns' type 
```{r}
proj <- df_all %>% 
  janitor::clean_names() %>% 
  relocate(fy, .after = project_id) %>% 
  relocate( board_approval_date , .after = fy)  %>% 
  relocate( project_closing_date , .after = board_approval_date) %>% 
  mutate(FY = as.integer(str_sub(fy, 1,4))) %>% 
relocate(FY, .before = fy) %>% 
  select(-fy) %>% 
  # delete empty columns 
  janitor::remove_empty(., which = "cols") %>% 
  # convert dates 
mutate(date_approval = lubridate::ymd_hms(board_approval_date)) %>% 
relocate(date_approval, .after = board_approval_date) %>% 
  mutate(date_close = lubridate::mdy_hms(project_closing_date))%>% 
relocate(date_close, .after = project_closing_date) %>% 
  select(-board_approval_date, -project_closing_date) %>% 
  # numeric 
mutate_at(c('current_project_cost', 'total_ida_and_ibrd_commitment', 'grant_amount'), as.numeric) %>% 
# factors 
mutate_at(c("region", "country","project_status", "consultant_services_required", "lending_instrument" , "environmental_assessment_category", "environmental_and_social_risk"), as.factor)  %>% 
  relocate(project_url, .after = project_name) %>% 
  mutate (PDO_miss = if_else(is.na(project_development_objective), "miss", "avail"))
 
# Factors <- c("region", "country","project_status", "consultant_services_required", "lending_instrument" , "environmental_assessment_category", "environmental_and_social_risk")
# proj[ ,Factors] <-  lapply( proj[ ,Factors], as.factor)
```


#### --- Check missing stuff
```{r}
skim(proj) %>%
dplyr::select(skim_type, skim_variable, n_missing)

# check missing approval dates 
nodate <- proj %>% 
  filter(is.na(date_approval)) # 2374 of which 1750 Dropped + 585 pipeline + 39 closed

# check missing PDOs
table(proj$PDO_miss, proj$FY)
table( proj$PDO_miss, proj$project_status)

proj_PDO <- proj %>% 
  select (FY, project_id,project_name, project_status, project_url, project_development_objective) %>% 
  filter(FY == '2001')

```


## USABLE INFO RECAP

> + missing approval dates 2374 
  + of which 1750 Dropped + 585 pipeline + 39 closed...could drop 
> + WAY TOO MANY MISSING PDO 8708!!! 
    + none before 2001
    + go check website.... 
> + SEEMS AT LEAST SECTOR 1 IS THERE... 
> + unsure about $ amounts... 

#### --- Can I recover/validate PDOs? 

```{r}

```


#### --- Can I recover/validate themes? 



#### --------------  NEXT ------------------


# Reference Tutorials

@robinson_1_2022 [Benjamin Soltoff: Computing 4 Social Sciences - API](https://cfss.uchicago.edu/syllabus/getting-data-from-the-web-api-access/) [Benjamin Soltoff: Computing 4 Social Sciences - text analysis](https://cfss.uchicago.edu/syllabus/text-analysis-fundamentals-and-sentiment-analysis/)

[Ben Schmidt Book Humanities Crurse](https://hdf.benschmidt.org/R/) [Ben Schmidt Book Humanities](http://benschmidt.org/HDA/texts-as-data.html)

[tidyTuesday cast on tidytext](https://github.com/dgrtwo/data-screencasts/tree/master/screencast-annotations)

  1. ✔️ [MEDIUM articles: common words, pairwise correlations - 2018-12-04](https://www.youtube.com/watch?v=C69QyycHsgE)
  2. [TidyTuesday Tweets -  2019-01-07](https://www.youtube.com/watch?v=KE9ItC3doEU)
  3. [Wine Ratings - 2019-05-31](https://www.youtube.com/watch?v=AQzZNIyjyWM) Lasso regression | sentiment lexicon,
  4. [Simpsons Guest Stars 	2019-08-30](https://www.youtube.com/watch?v=EYuuAGDeGrQ) geom_histogram
  5. [Horror Movies 	2019-10-22](https://www.youtube.com/watch?v=yFRSTlk3kRQ) explaining glmnet package | Lasso regression
  6. [The Office 	2020-03-16](https://www.youtube.com/watch?v=_IvAubTDQME) geom_text_repel from ggrepel | glmnet package to run a cross-validated LASSO regression
  7. [Animal Crossing 	2020-05-05](https://www.youtube.com/watch?v=Xt7ACiedRRI) Using geom_line and geom_point to graph ratings over time | geom_text to visualize what words are associated with positive/negative reviews |topic modelling

# Connections

Following the example of [David Robinson on HN titles](http://varianceexplained.org/r/hn-trends/)


