---
title: "WB Project PDO features classification"
subtitle: "Supervised ML for text classification"
author: "Luisa M. Mimmi"
# date: "Last run: `r format(Sys.time(), '%F')`"
lang: en
editor: source
engine: knitr
## ------  general Output Options
execute:     
  eval: true    # actually run? 
  echo: true     #  include source code in output
  warning: false  #  include warning code in output
  error: false    #  include error code in output
  output: false   # include output code in output (CHG in BLOCKS)
  # include: false   # R still runs but code and results DON"T appear in output  
  cache: false # normalmnte false
toc: true
fig-cap-location: top
tbl-cap-location: top
format:
  html:
    # theme: flatly #spacelab
    code-fold: false # redundant bc echo false 
    toc-depth: 3
    toc_float: true
    toc-location: left
    toc-title: Outline
    embed-resources: true # external dependencies embedded (Not in ..._files/)
  # pdf:
  #   toc-depth: 2
  #   toc-title: Indice
  #   highlight-style: github
  #   #lang: it
  #   embed-resources: true # external dependencies embedded (Not in ..._files/)
format-links: false
bibliography: ../bib/slogan.bib
---

::: {.callout-warning collapse="false" style="background-color: #fffcf9;"}
WORK IN PROGRESS!
(Please expect unfinished sections, and unpolished code. Feedback is welcome!)
:::

<!-- In this file I address the **research question 1.2**, which is also to learn ML, IS IT POSSIBLE TO IMPROVE THE QUALITY OF THE DATA (E.G. MISSING FEATURES IN METADATA) BY USING TOPIC MODELING?  -->

<!-- I experiment predicting the `environmental risk category` (e.g., `A`, `B`, `C`, `D`) TURNED INTO BINARY OUTCOME `env_cat_f2` based on the available features in the dataset. -->

# Set up

```{r}
# Pckgs -------------------------------------
library(fs) # Cross-Platform File System Operations Based on 'libuv'
library(janitor) # Simple Tools for Examining and Cleaning Dirty Data
library(skimr) # Compact and Flexible Summaries of Data
library(here) # A Simpler Way to Find Your Files
library(paint) # paint data.frames summaries in colour
library(readxl) # Read Excel Files
library(kableExtra) # Construct Complex Table with 'kable' and Pipe Syntax)
library(glue) # Interpreted String Literals
#library(tidyverse) # Easily Install and Load the 'Tidyverse'
library(dplyr) # A Grammar of Data Manipulation
library(tidyr) # Tidy Messy Data
library(tibble) # Tibbles: A Modern Version of Data Frames
library(purrr) # Functional Programming Tools
library(lubridate) # Make Dealing with Dates a Little Easier
library(ggplot2) # Create Elegant Data Visualisations Using the Grammar of Graphics
library(stringr) # Simple, Consistent Wrappers for Common String Operations
library(forcats) # Tools for Working with Categorical Variables (Factors)
# ML & Text Mining -------------------------------------
library(tidymodels) # Easily Install and Load the 'Tidymodels' Packages 
library(textrecipes) # Extra 'Recipes' for Text Processing  
library(discrim) # Model Wrappers for Discriminant Analysis
library(tidytext) # Text Mining using 'dplyr', 'ggplot2', and Other Tidy Tools 
library(SnowballC) # Snowball Stemmers Based on the C 'libstemmer' UTF-8 Library
library(rvest) # Easily Harvest (Scrape) Web Pages
library(cleanNLP) # A Tidy Data Model for Natural Language Processing
library(themis) # Extra Recipes for Dealing with Unbalanced Classes

set.seed(123) # for reproducibility

```

```{r}
#| echo: TRUE
#| eval: TRUE

# 1) --- Set the font as the default for ggplot2
# Who else? https://datavizf24.classes.andrewheiss.com/example/05-example.html 
lulas_theme <- theme_minimal(base_size = 12) +
  theme(panel.grid.minor = element_blank(),
        # Bold, bigger title
        plot.title = element_text(face = "bold", size = rel(1.6)),
        # Plain, slightly bigger subtitle that is grey
        plot.subtitle = element_text(face = "plain", size = rel(1.4), color = "#A6A6A6"),
        # Italic, smaller, grey caption that is left-aligned
        plot.caption = element_text(face = "italic", size = rel(0.7), 
                                    color = "#A6A6A6", hjust = 0),
        # Bold legend titles
        legend.title = element_text(face = "bold"),
        # Bold, slightly larger facet titles that are left-aligned for the sake of repetition
        strip.text = element_text(face = "bold", size = rel(1.1), hjust = 0),
        # Bold axis titles
        axis.title = element_text(face = "bold"),
        # Change X-axis label size
        axis.text.x = element_text(size = rel(1.4)),   
        # Change Y-axis label size
        axis.text.y = element_text(size = 14),   
        # Add some space above the x-axis title and make it left-aligned
        axis.title.x = element_text(margin = margin(t = 10), hjust = 0),
        # Add some space to the right of the y-axis title and make it top-aligned
        axis.title.y = element_text(margin = margin(r = 10), hjust = 1),
        # Add a light grey background to the facet titles, with no borders
        strip.background = element_rect(fill = "grey90", color = NA),
        # Add a thin grey border around all the plots to tie in the facet titles
        panel.border = element_rect(color = "grey90", fill = NA))

# 2) --- use 
# ggplot + lulas_theme
```


# Load data
```{r}
# Load Proj train dataset `projs_train_t`
projs_train2 <- readRDS(here("data","derived_data", "projs_train2.rds"))  
custom_stop_words_df  <-  readRDS(here("data","derived_data", "custom_stop_words_df.rds")) 
# Load function
source(here("R","f_recap_values.R")) 
```

# SEE [https://bookdown.org/f_lennert/text-mining-book/supervisedml.html](https://bookdown.org/f_lennert/text-mining-book/supervisedml.html)

# PREDICT MISSING FEATUREs

## What ML models work with text?

> Remember that text data is SPARSE!

To predict a missing feature (e.g., sector) based on available features from text data, several supervised machine learning algorithms can be applied. Given that you have a mixture of text and structured data, here are some suitable algorithms:

-   **Logistic Regression / Multinomial Logistic Regression**: If you're predicting a categorical variable like "sector", logistic regression can work well, especially with appropriate feature engineering for text (e.g., converting text data into numeric features using TF-IDF or word embeddings).
   + **Lasso regression/classification** learns `how much of a penalty` to put on some features (sometimes penalizing all the way down to zero) so that we can select only some features out of the high-dimensional space of original possible variables (tokens) for the final model.
-   **k-Nearest Neighbors (k-NN)**: k-NN can be useful for text data, especially when you have a mix of structured and unstructured data. It's a simple algorithm that can work well with text data, but it can be computationally expensive.
-   **Decision Trees / Random Forests**: These algorithms handle both numeric and categorical data efficiently and can manage missing values quite well. You can input text-based features as well, though you might need to preprocess the text into numeric form (e.g., using embeddings).
-   **Naive Bayes**: Naive Bayes is a simple and efficient algorithm for text classification. It assumes feature independence, which may not always hold, but it's often a good baseline, particularly with short texts.
-   **Support Vector Machines (SVMs)**: SVMs are useful when you have high-dimensional data, which is common with text after feature extraction (like TF-IDF). They can perform well with a mix of structured and unstructured data. <!-- + **Gradient Boosting Machines (GBM)**: Models like XGBoost or LightGBM can be highly effective, especially when you have structured data alongside text features. These models can also handle missing data naturally in the training process. --> <!-- + **Neural Networks (MLP or LSTM)**: If you have a larger dataset, neural networks could be useful. A multilayer perceptron (MLP) is good for mixed feature types, while LSTM (Long Short-Term Memory) networks can work for sequential text data, especially if you want to capture contextual information from the abstracts. -->

<!-- All available models are listed at [parsnip](https://tidymodels.org/find/parsnip) -->

Some model parameters can be learned from data during fitting/training. Some CANNOT 😱. These are **hyperparameters**, and we estimate them by training lots of models with different hyperparameters and comparing them

### Check missing feature

```{r}
names (projs_train2)

tot <- sum(!is.na(projs_train2$pdo)) # 4425
sum(!is.na(projs_train2$regionname)) / tot  # 100%
sum(!is.na(projs_train2$countryname)) / tot  # 100%
sum(!is.na(projs_train2$status)) / tot  # 100%
sum(!is.na(projs_train2$lendinginstr)) / tot  # 98% 
sum(!is.na(projs_train2$curr_total_commitment)) / tot  # 100% 

sum(!is.na(projs_train2$ESrisk)) / tot  # 0.092  !!!!!
projs_train2 |> count(ESrisk) # 4 levels+ NA

sum(!is.na(projs_train2$env_cat)) / tot  # 72%
table(projs_train2$env_cat, useNA = "ifany") # 5 levels+ NA
projs_train2 |> count(env_cat) # 5 levels+ NA

sum(!is.na(projs_train2$sector1)) /tot# 99%
projs_train2 |> count(sector1) # 76levels

sum(!is.na(projs_train2$theme1)) /tot # 71%  --> 99%
projs_train2 |> count(theme1, useNA = "ifany") # 81 levels
```


```{r}
#| output: true
#| tbl-caption: Missing values in the dataset

# source function
source(here("R","f_recap_values.R")) 

# check candidate lables for classification 
f_recap_values(projs_train2, c("sector1", "theme1","env_cat","ESrisk")) %>% 
   kable()
```

### Identify features for classification

These could be:

-   *features derived from raw text* (e.g. characters, words, ngrams, etc.),
-   *feature vectors* (e.g. word embeddings), or
-   *meta-linguistic features* (e.g. part-of-speech tags, syntactic parses, or semantic features)

How do we use them?

-   Do we use raw token counts?\
-   Do we use normalized frequencies?
-   Do we use some type of weighting scheme? ✅
    -   yes, we use `tf-idf` (a weighting scheme, which will downweight words that are common across all documents and upweight words that are unique to a document)
-   Do we use some type of dimensionality reduction? ✅
# TEXT CLASSIFICATION for Environmental Assessment Category

[Francom](chap%209.2.1%20Text%20classification,%20p.%202018-) [text Classification (in R)](https://burtmonroe.github.io/TextAsDataCourse/Tutorials/TADA-ClassificationV2.nb.html) [Stanford slide](https://web.stanford.edu/class/cs124/lec/naivebayes2021.pdf)

# ___ BINARY OUTCOME ___

One candidate variable that could be predicted is the `env_cat` variable (*"Environmental Assessment Category"*). This is a categorical variable with 7 levels (A, B, C, F, H, M, U ) , but, to simplify, I converted it into a binary outcome defined as in @tbl-env_cat_f_f2. 

# ____ MODEL A) `lasso regr ONLY pdo` ____
Lasso regression or classification *learns how much of a `penalty` to put on some features* (sometimes penalizing all the way down to zero) so that we can select only some features out of the high-dimensional space of original possible variables (tokens) for the final model.

## 0) Prep for data split based on outcome [`projs_train2`]

### Recode `env_cat` variable 

```{r}
projs_train2 <- projs_train2 %>% 
   # useful for later 
   # rename(., proj_id = "id") %>%
   # recode as factors 
   dplyr::mutate (across (c(status, FY_appr, regionname, countryname, sector1, 
                            theme1, lendinginstr), as.factor))  %>% 
   # env risk category 7 levels
   dplyr::mutate(env_cat_f = forcats::fct_na_value_to_level(
      env_cat, level = "Missing")) %>% 
   dplyr::mutate(env_cat_f = forcats::fct_recode(
      env_cat_f, 
      "A_high risk" = "A", 
      "B_med risk" = "B", 
      "C_low risk" = "C", 
      "F_fin expos" = "F", 
      # "Other" = "H", 
      # "Other" = "M", 
      "Other" = "U", 
      "Missing" = "Missing" )) %>% 
   dplyr::relocate(env_cat_f , .after = env_cat) %>% 
   # collaapse env_cat_f into 2 levels
   dplyr::mutate(env_cat_f2 = forcats::fct_collapse(
      env_cat_f, 
      "High-Med-risk" = c("A_high risk", "B_med risk"),
      "Low-risk_Othr" = c("C_low risk", "F_fin expos", "Other", "Missing")
   )) %>% 
   dplyr::relocate(env_cat_f2, .after = env_cat_f)

# Recap
tabyl(projs_train2, env_cat, show_na = TRUE) # 7 levels
tabyl(projs_train2, env_cat_f, show_na = TRUE) # 2 levels
tabyl(projs_train2, env_cat_f2, show_na = TRUE) # 7levels
```

```{r}
#| label: tbl-env_cat_f_f2
#| output: true
#| tbl-cap: Recoded Environmental Assessment Category

# Show as contingency table env_cat_f and env_cat_f2
env_cat_f_f2_k <- table(projs_train2$env_cat_f, projs_train2$env_cat_f2) %>% 
   kable() %>% 
   kable_styling("striped", full_width = F)

env_cat_f_f2_k

saveRDS(env_cat_f_f2_k, here("analysis", "output", "tables", "env_cat_f_f2_k.rds"))
```

## Recode `sector1` variable 
(this I use later as MULTI-CLASS outcome)

```{r}
# !!!!! `sector_f` e' diverso da `tok_sector_broad` XCHE si basa su `sector1` !!!! 
projs_train2 <- projs_train2 %>% # sector1 99 levels
   mutate(sector_f = case_when(
      str_detect(sector1, regex("water|wastewater|sanitat|Sewer|Irrigat|Drainag", 
                                ignore_case = T)) ~ "WAT & SAN",
      str_detect(sector1, regex("transport|railway|road|airport|waterway|bus|metropolitan|inter-urban|aviation",
                                ignore_case = T)) ~ "TRANSPORT",
      sector1 == "port" ~ "TRANSPORT",
      str_detect(sector1, regex("urban|housing|inter-urban|peri-urban|waste",
                                ignore_case = T)) ~ "URBAN",
      str_detect(sector1, regex("energ|electri|hydroele|hydropow|renewable|transmis",
                                ignore_case = T)) ~ "ENERGY",  # Matches either "energy" or "power"
      str_detect(sector1, regex("health|hospital|medicine|drugs|epidem|pandem|covid-19|vaccin",
                                ignore_case = T)) ~ "HEALTH",
      str_detect(sector1, regex("educat|school|vocat|teach|univers|student|literacy|training|curricul",
                                ignore_case = T)) ~ "EDUCATION",
      
      str_detect(sector1, regex("Agricultural|Agro|Fish|Forest|Crop|livestock|agri-business",
                                ignore_case = T)) ~ "AGR FOR FISH",
      str_detect(sector1, regex("Minin|oil|gas|mineral|Extract",
                                ignore_case = T)) ~ "MINING OIL&GAS",
      str_detect(sector1, regex("Social Protec",
                                ignore_case = T)) ~ "SOCIAL PROT.",
      
      str_detect(sector1, regex("Bank|finan|Investment",
                                ignore_case = T)) ~ "FINANCIAL",
      str_detect(sector1, regex("Information|Communication|ICT|Internet|Technologies",
                                ignore_case = T)) ~ "ICT",
      str_detect(sector1, regex("Tourism|Trade and Services|Manuf|Other Industry|Trade and Services",
                                ignore_case = T)) ~ "IND TRADE SERV",
      str_detect(sector1, regex("Government|Public Admin|Institution|Central Agenc|Sub-national Gov|law|justice|governance",
                                ignore_case = T)) ~ "INSTIT. SUPP.",
      TRUE ~ "Missing")) %>% 
   relocate(sector_f, .after = sector1)  

# Recap
projs_train2 |> count(sector1) # 76 levels
projs_train2 |> count(sector_f) # 14 levels
```


## 1) Split data in train/test [based on `env_cat`]

<!-- ![Sampling strategy](/images/Sample_EnvCat.pdf){fig-align="center"} -->
![Sampling strategy](../images/Sample_EnvCat2.png){fig-align="center"} 

###  🟧 Proportional sub-set `rsample::initial_split`

Based on availability of `env_cat_f`.

We will use the `strata` argument to stratify the data by the outcome variable (`env_cat_f`). This will ensure that the *training* and *validation* sets have the same proportion.

```{r}
# Create a stratified split based on missing vs non-missing env_cat
projs_train2 %>% tabyl(env_cat_f) # 7 levels

# Split BUT only "Not Missing" `env_cat_f` 
## --- 0) THIS WILL BE 4 TRAINING & VALIDATION 
env_cat_use <- projs_train2 %>% 
   filter(env_cat_f != "Missing") # 3236 proj 

# SPLIT INTO TRAINING, VALIDATION 
set.seed(123)  # Ensure reproducibility
env_split <- initial_split(env_cat_use, prop = 0.7, # 70% training, 30% testing
                       strata = env_cat_f) # stratify by OUTCOME 

## -- 1) for training (labelled `env_cat_f`)
env_cat_train <- training(env_split)   # 2265 proj
    
## -- 2) for validation (labelled `env_cat_f`)
env_cat_test <- testing(env_split)  # 971 proj
   
# # UNLABELLED PORTION 
## -- 3) for actual test (UNlabelled `env_cat_f`)
env_cat_missing <- projs_train2 %>% 
  filter(env_cat_f == "Missing") # 1167 proj 

# check ditribution of `env_cat_f` in training and validation
tabyl(env_cat_train, env_cat_f) |> adorn_totals("row") |> adorn_pct_formatting(digits = 1)# 
tabyl(env_cat_test, env_cat_f)|> adorn_totals("row") |> adorn_pct_formatting(digits = 1)# 
```

```{r}
rm( env_cat_use, env_cat_missing)
# env_cat_train |> count(env_cat_f)
```


## 2) Pre-processing and featurization (`textrecipes`)

::: {.callout-note collapse="true" style="background-color: #eef2fb;"}
{`textrecipes`} provides a number of step functions for pre-processing text data. These include functions to tokenize (e.g. `step_tokenize()`), remove stop words (e.g. step_stopwords()), and to derive meta-features (e.g. `step_lemma()`, `step_stem()`, etc.) Furthermore, there are functions to engineer features in ways that are particularly relevant to text data, such as feature frequencies and weights (e.g. `step_tf()`, `step_tfidf()`, etc.) and token filtering (e.g. `step_tokenfilter()`).

-   `step_tokenize()`
-   `step_tfidf()`
-   `smooth_idf = FALSE` (terms that appear in many (or all) documents will not be down weighted as much as they would be if the smoothing term was not added)

see [here](https://bookdown.org/f_lennert/text-mining-book/supervisedml.html#pre-processing-and-featurization)
:::

### Null ecipe [`env_recipe_zero`]

```{r}
# Create a recipe that only uses the `pdo` text variable
env_recipe_zero <- recipe (formula = env_cat_f2 ~ pdo,
                            data = env_cat_train) %>%
   # tokenize
   step_tokenize(pdo) %>% 
   # tf-idf matrix of term frequencies weighted by inverse document frequency 
   step_tfidf(pdo, smooth_idf = FALSE) 

# Review the recipe   
env_recipe_zero
```

`recipes::bake()` takes a trained recipe and applies its operations to a data set to create a **design matrix**.
```{r}
# Run the recipe 
env_recipe_zero_desmatrix <-  env_recipe_zero %>% 
   # chooses the parameters for the recipe based on the data
   prep(training = NULL) %>% 
   # applies the recipe to the data already specified in the recipe
   bake(new_data = NULL)

# preview the DESIGN Matrix baked recipe -- TOO SPARSE 
dim(env_recipe_zero_desmatrix)
#[1] 2260 7763
```

The resulting engineered features data frame has `2260` observations and `7763` variables!!! I.e. for each writing sample, only a small subset of them will actually appear, most of our cells will be filled with zeros. This is what is known as a **sparse matrix**. Furthermore, the more features we have, the more chance these features will capture the nuances of these particular writing samples increasing the likelihood we **overfit the model**.

### Improved recipe [`env_recipe`]

Basically, we added `step_tokenfilter(pdo, max_tokens = 100)` to use just raw the 100 most common words (and reduce the number of features).
```{r}
# -- Rebuild recipe with tokenfilter step
env_recipe <- recipe (formula = env_cat_f2 ~ pdo,
                      data = env_cat_train) %>%
   # tokenize
   step_tokenize(pdo) %>%   
   # !!! filter by frequency of occurrence !!!
   step_tokenfilter(pdo, max_tokens = 100) %>%  
   # tf-idf  creates matrix of weighted term frequencies  
   step_tfidf(pdo, smooth_idf = FALSE)
```

Re-check the design matrix
```{r}
# -- Run the recipe 
env_recipe_bake <-  env_recipe %>% 
   # chooses the parameters for the recipe based on the data
   prep(training = NULL) %>% 
   # applies the recipe to the data
   bake(new_data = NULL)

# -- preview the baked recipe
dim(env_recipe_bake)
#[1] 2260 7763--> #[1] 2260  101

# subset check
env_recipe_bake[1:5, 1:10 ]
```

## 3) Model specification

Now that the data is ready, the model can be specified. The `parsnip` package is used for this:

   + model `specification`, 
   + model type `mode`, 
   + the `engine`, i.e. the software that fits the model (given the type)

Let's start with a simple logistic regression model to see how well we can classify the texts in the training set with the features we have engineered. We will use the `parsnip::logistic_reg()` function to specify the logistic regression model. We then select the implementation engine (`glmnet` General Linear Model) and the mode of the model (`classification`).

   + `tune()` is a placeholder for a range of values for the penalty hyperparameter.
```{r}
# Create a model specification
env_spec <-
   # lasso regularized model
   parsnip::logistic_reg(
      # non-negative number ~ the total amount of regularization
      penalty = tune(),  # 0 = no penalty, 1 = max
      # number between zero and one (inclusive) 
      mixture = 1 # specifies a pure lasso model,
   ) %>%
   # set the mode of the model
   parsnip::set_mode("classification") %>%
   # set the engine of the model
   parsnip::set_engine("glmnet")

# Preview 
env_spec
```

## 4) Model training (`workflows`)
The resulting workflow (*a container object that aggregates information required to fit and predict from a model*) by adding the recipe and model to it.

```{r}
# Create a workflow
env_wf <- workflows::workflow() %>%
   # preprocessing recipe
   add_recipe(env_recipe) %>%
   # model specifications
   add_model(env_spec)
```

> I haven't chosen the penaly yet in `env_spec`!

It can then be fit using `fit()`, but I need to specify hyperparameters (e.g. `pennalty` first).

```{r}
#env_wf %>% workflows::fit(data = env_cat_train)
```


## 5) Model evaluation / tweaking
Different algorithms will have different parameters that can be adjusted which can affect the performance of the model (**hyperparameters**) ≠ **parameters** (features) --> **hyperparameters tuning** which is tipically done during fitting the model to the training set and evaluating its performance

### --- Penalty tuning [`env_grid`]

In logistic regression, the `penalty` hyperparameter is like a control that helps prevent the model from becoming too complex and overfitting to the training data. There are two common types of penalties:

- **L1 (Lasso)**: Encourages the model to *use fewer features* by making some of the coefficients exactly zero. This can simplify the model.

- **L2 (Ridge)**: Tries to keep all the coefficients small but not exactly zero, which can help stabilize the model and avoid overfitting.

- the logistic regression model using `glmnet` can be tuned to prevent *overfitting* by adjusting the `penalty` and `mixture` (combination of L1 and L2) hyperparameters

    - In our `env_spec` model, `tune()` was a **placeholder for a range of values** for the penalty hyperparameter.
    - To tune the *penalty hyperparameter*, we use the`grid_regular()` function from {`dials`} to specify a grid of values to try.

-   The package `dials` contains infrastructure to create and manage values of tuning parameters for the `tidymodels` packages.

```{r}
# Create a grid of values for the penalty hyperparameter (random set of 10 values)
env_grid <- dials::grid_regular(
   penalty(), 
   levels = 10)

# Preview
env_grid 
# 0 no penalty
# ... 
# 1 max penalty
```

### --- K-fold cross-validation (for penalty optimal) [ `env_fold`, `env_tune`]

Now to perform the tuning and choose an optimal value for penalty we need to create a **tuning workflow**. We use the strategy of resampling (splitting `env_cat_train` in multiple training/testing sets) called **k-fold cross-validation** to arrive at the *optimal value for the penalty hyperparameter*.

```{r}
set.seed(123)

# Create a resampling object
env_vfold <- rsample::vfold_cv(env_cat_train, v = 10)

# Create a tuning workflow
env_tune <- tune::tune_grid(
  object = env_wf,
  resamples = env_vfold,
  grid = env_grid,
  control = control_grid(save_pred = TRUE),
  metrics = metric_set(roc_auc, accuracy, sensitivity, specificity)
  
)

# preview
env_tune
```

The `env_tune` object contains the results of the tuning for each fold. We can see the results of the tuning for each fold by calling the `collect_metrics()` function on the `env_tune` object

```{r}
# Collect the results of the tuning
env_tune_metrics <- tune::collect_metrics(env_tune)
env_tune_pred  <- tune::collect_predictions(env_tune)

# visualize the results
autoplot(env_tune) +
  labs(
    title = "Lasso model performance across regularization penalties",
    subtitle = "Performance metrics can be used to identify the best penalty"
  )

# in roc_auc: many of the penalty values performed similarly, with a drop-off in performance at the higher values
```

The most common metrics for model performance in classification are: 

1. `accuracy` (the proportion of correct predictions)
   + here it drops sharply at the higher values of penalty (1e-02), meaning dropping too many features will hurt predictive performance
2. `ROC-AUC` (area under the *receiver operating characteristic area under the curve*, i.e. a single score for how well the model can distinguish between classes) The closer to 1 the more discriminating power the model has.
   + similar, i.e song penalties will damage the model's ability to distinguish between classes. 
3. `sensitivity` (the proportion of TP that are correctly identified)
   + here it peaks at 1.0 beyond 1e-02, meaning the model is very good at identifying the positive class (High-Med-risk) BUT...
4. `specificity` (the proportion of TN that are correctly identified)
   + ... at the cost of specificity). In fact here, performance is very poor, suggesting no true negatives are correctly identified.

Conveniently, the `show_best()` function from {`tune`} takes a `tune_grid` object and returns the *best performing hyperparameter values*.

```{r}
# Show the best hyperparameter values
tune::show_best(env_tune, metric = "roc_auc") # 0.00599

# Make selection of penalty programmatically
env_best <- select_best(env_tune, metric ="roc_auc")           # 0.00599
env_best_lambda <- env_best$penalty                            # 1e-10

env_best_acc <- select_best(env_tune, metric ="accuracy")      # 0.00599
env_best_sens <- select_best(env_tune, metric ="sensitivity")  # 0.0774

```


## 6) Hyperparameter tuning

### Update workflow [`env_wf_lasso`]

Now we can update the model specification and workflow with the best performing hyperparameter value using the previous `cls_wf_tune` workflow and the `finalize_workflow()` function.

> Instead of `penalty = tune()` like before, now our workflow has finalized values for all arguments.


```{r}
# Update the workflow/model specification with the best penalty value
env_wf_lasso <- env_wf %>% 
   tune::finalize_workflow(env_best)

# Preview updated workflow object (with defined penalty paramv  0.00599)
env_wf_lasso
```

## 7) Final fit on training set

### Fit the final model
```{r}
# Fit the model to the training data
env_lasso_fit <- env_wf_lasso %>% 
   # fit the model to the training data
   workflows::fit(data = env_cat_train)
```

Here I see: 

+ `Df` = Number of non-zero coefficients (i.e., selected features) at a given lambda.
+ `%Dev` = Percentage of null deviance explained (i.e., a measure of model fit – how well the model explains the variation in the response).
+ `Lambda` = The regularization parameter – higher values imply more shrinkage (simpler models).
   + (here) `lambda = 1e-10` means no penalty (i.e., no regularization), i.e. the model is almost unpenalized, so it is similar to a standard logistic regression model.

This table shows the model performance across different levels of regularization (lambda). As lambda decreases:

+ The number of selected features (Df) increases.
+ Model fit improves (%Dev goes up), but complexity increases.
- At the smallest lambda shown (0.001950), 93 features are included, explaining 23.73% of deviance.

> You’ll typically select a lambda using cross-validation (e.g., tune_grid() or select_best() in tidymodels). The goal is to balance simplicity and predictive performance – too small a lambda overfits; too large underfits.


### Coefficients for the lambda with hte best performance

+ using the `workflows::extract_fit_parsnip()` function, I see only the model coefficients for the best performing lambda value. This function extracts the fitted model object from the workflow, and 
+ then I can use the `tidy()` function to get the coefficients.

```{r}
# Get the coefficients of the model
env_lasso_fit %>% 
   workflows::extract_fit_parsnip() %>% 
   broom::tidy() %>% 
   dplyr::filter(term != "(Intercept)") %>% 
   dplyr::arrange(desc(abs(estimate))) %>% 
   dplyr::slice(1:20) # top 10
```

I can see stopwords like among the most important features!

### --- Get predicted class and probabilities (train)

Here I got the predicted *probabilities* from `env_lasso_fit` logistic regression model on the dataset `env_cat_train`
   + with `type = "prob"` argument in the `predict()` function, the model returns **class probabilities** for each of the 2260 observations
      1. `.pred_High-Med-risk` = probability of being in the `High-Med-risk` category
      2. `.pred_Low-risk_Othr` = probability of being in the `Low-risk_Othr` category
```{r}
# Prep data frame containing actual and predicted values
pred_train  <-  stats::predict(env_lasso_fit, 
                               new_data = env_cat_train, 
                               type = "prob") |> # prob **high** and prob **low** for each observ based on model 
   # combine with the original training data (labeled)
   bind_cols(env_cat_train) %>% 
   select(env_cat_f2,  # ACTUAL CLASS of the 1obs
          pred_high_risk = '.pred_High-Med-risk', # PROB 1obs is PREDICTED HIGH-MED-RISK 
          pred_low_risk = '.pred_Low-risk_Othr'   # PROB 1obs is PREDICTED LOW-RISK
   )  # 2260 rows


# convert to long format for plotting
pred_train_long <- pred_train |>
   pivot_longer(cols = c(pred_high_risk, pred_low_risk),
                names_to = "pred_risk_type", values_to = "pred_risk_value") # 4,520 rows
```

### --- [FIG] Assessing performance on training set

```{r}
#| output: TRUE

# Plot the predictions with boxplots and jittered points without duplicate legends
ggplot(pred_train_long, aes(x = env_cat_f2, y = pred_risk_value, fill = pred_risk_type)) +
  geom_boxplot(alpha = 0.4, position = position_dodge(width = 0.8)) +
  #geom_jitter(alpha = 0.6, position = position_dodge(width = 0.8)) +  # Remove width and use position_dodge
   labs(title = "PREDICTED class distribution (y axis) v. ACTUAL class (x axis)",
        subtitle = "Model: Lasso Regression fitted on training data",
       x = "ACTUAL env. class",
       y = "Probabiity of PREDICTED env. class",
       fill = "Risk Type") +  # Set label for fill legend
  theme_minimal() +
  guides(color = "none")  # Suppress the color legend
```

### Calculate performance metrics
```{r}
#| output: TRUE
#| tbl-cap: Model performance metrics
#| tbl-label: tbl-modelA-train-perf

train_pred_probs <- predict(env_lasso_fit, new_data = env_cat_train, type = "prob")
train_pred_class <- predict(env_lasso_fit, new_data = env_cat_train, type = "class")

train_results <- bind_cols(env_cat_train, train_pred_probs, train_pred_class) |> 
   select(env_cat_f2,  # ACTUAL CLASS of the 1obs
          pred_high_risk = '.pred_High-Med-risk', # PROB 1obs is PREDICTED HIGH-MED-RISK 
          pred_low_risk = '.pred_Low-risk_Othr',   # PROB 1obs is PREDICTED LOW-RISK
          pred_class = '.pred_class')

# Confusion matrix
accuracy(train_results, truth = env_cat_f2, estimate = pred_class)
# ROC AUC (needs probabilities + positive class specified)
roc_auc(train_results, truth = env_cat_f2, pred_high_risk)
# Sensitivity and Specificity
sens(train_results, truth = env_cat_f2, estimate = pred_class)
spec(train_results, truth = env_cat_f2, estimate = pred_class)

# confusion matrix
conf_mat(train_results, truth = env_cat_f2, estimate = pred_class) |> 
   autoplot(type = "heatmap") +
   labs(title = "Confusion matrix for Lasso model",
        subtitle = "Training set")

# roc curve
roc_curve(train_results, truth = env_cat_f2, pred_high_risk) |> 
   autoplot() +
   labs(title = "ROC curve for Lasso model (only x = pdo)",
        subtitle = "Training set")
```



## 8) Evaluation on test set
### --- Get predicted class and probabilities (test)

```{r}
# Predict on the test set
pred_test  <-  stats::predict(env_lasso_fit, 
                      new_data = env_cat_test, 
                      type = "prob") |> # prob **high** and prob **low** for each observ based on model 
   # combine with the original training data (labeled)
   bind_cols(env_cat_test) %>% 
   select(env_cat_f2,  # ACTUAL CLASS of the 1obs
          pred_high_risk = '.pred_High-Med-risk', # PROB 1obs is PREDICTED HIGH-MED-RISK 
          pred_low_risk = '.pred_Low-risk_Othr'   # PROB 1obs is PREDICTED LOW-RISK
          )  # 970 rows

# convert to long format for plotting
pred_test_long <- pred_test |> 
   pivot_longer(cols = c(pred_high_risk, pred_low_risk),
                names_to = "pred_risk_type", values_to = "pred_risk_value") # 1,940 rows
```

### --- [FIG] Assessing performance on test set

```{r}
#| output: TRUE

# Plot the predictions with boxplots and jittered points without duplicate legends
ggplot(pred_test_long, aes(x = env_cat_f2, y = pred_risk_value, fill = pred_risk_type)) +
  geom_boxplot(alpha = 0.4, position = position_dodge(width = 0.8)) +
  #geom_jitter(alpha = 0.6, position = position_dodge(width = 0.8)) +  # Remove width and use position_dodge
   labs(title = "PREDICTED class distribution (y axis) v. ACTUAL class (x axis)",
        subtitle = "Model: Lasso Regression fitted on test data",
       x = "ACTUAL env. class",
       y = "Probabiity of PREDICTED env. class",
       fill = "Risk Type") +  # Set label for fill legend
  theme_minimal() +
  guides(color = "none")  # Suppress the color legend
```

### Calculate performance metrics
Using `yardstick` package, I can calculate the performance metrics for the model on the test set. The `roc_auc()` function calculates the area under the ROC curve, which is a measure of how well the model can distinguish between the two classes.

### Calculate performance metrics
```{r}
#| output: TRUE
#| tbl-cap: Model performance metrics
#| tbl-label: tbl-modelA-test-perf

test_pred_probs <- predict(env_lasso_fit, new_data = env_cat_test, type = "prob")
test_pred_class <- predict(env_lasso_fit, new_data = env_cat_test, type = "class")

test_results <- bind_cols(env_cat_test, test_pred_probs, test_pred_class) |> 
   select(env_cat_f2,  # ACTUAL CLASS of the 1obs
          pred_high_risk = '.pred_High-Med-risk', # PROB 1obs is PREDICTED HIGH-MED-RISK 
          pred_low_risk = '.pred_Low-risk_Othr',   # PROB 1obs is PREDICTED LOW-RISK
          pred_class = '.pred_class')

# Confusion matrix
accuracy(test_results, truth = env_cat_f2, estimate = pred_class)
# ROC AUC (needs probabilities + positive class specified)
roc_auc(test_results, truth = env_cat_f2, pred_high_risk)
# Sensitivity and Specificity
sens(test_results, truth = env_cat_f2, estimate = pred_class)
spec(test_results, truth = env_cat_f2, estimate = pred_class)

# confusion matrix
conf_mat(test_results, truth = env_cat_f2, estimate = pred_class) |> 
   autoplot(type = "heatmap") +
   labs(title = "Confusion matrix for Lasso model",
        subtitle = "testing set")

# roc curve
roc_curve(test_results, truth = env_cat_f2, pred_high_risk) |> 
   autoplot() +
   labs(title = "ROC curve for Lasso model (only x = pdo)",
        subtitle = "testing set")
```


### Recap 

So, the results of the 'best model' in the training and testing set are:

```{r}
#| output: TRUE

#| tbl-cap: Model performance metrics

#| tbl-label: tbl-model-metrics
#| output: true

# table with accuracy, roc_auc, sensitivity, specificity
model_metrics_recap <- tibble(
   metric = c("Accuracy", "ROC AUC", "Sensitivity", "Specificity"),
   train = c(0.761, 0.818, 0.883, 0.515), # OK
   test = c(0.745, 0.770, 0.871, 0.5)
   ) |> 
   kable() |> 
   kable_styling("striped", full_width = F) |> 
   # header merged column 
   kableExtra::add_header_above(c("A) Lasso Logistic model", "datasets" = 2), 
                                bold = TRUE, color = "black", background = "#D9EAD3")  

model_metrics_recap
```

#  ----- 🔴🟢  QUI  🟠🟡  ----
 

# ____ MODEL B) `lasso w pdo + tf-idf + stopwords` ____

## 0) Prep for split based on outcome [using `projs_train2`]
## 1) Split data in train/test [based on `env_cat`]
## 2) Pre-processing and featurization (`recipes`)
## 3) Model specification
## 4) Model training
## 5) Model evaluation / tweaking
## 6) Hyperparameter tuning
## 7) Final fit on training set
## 8) Evaluation on test set

To improve supervised learning models, consider:

1.  **Engineering the features differently**
    -   we set a token filter to limit the number of features to 100, which we could adjust (`max_tokens`)
2.  Selecting different (or additional) features
3.  Changing the algorithm
4.  Tuning the hyperparameters differently

## [🤞🏻] Add stopwords exclusion to the recipe

### 3) Define preprocessing steps (NEW!)

```{r}
# Create a custom stopword list
stop_vector <- custom_stop_words_df %>%  pull(word)
```

### --- Improve recipe [`env_stop_recipe`]

```{r}
# ---  Create a recipe with a token filter step that excludes stopwords
# Rebuild recipe with tokenfilter step
env_stop_recipe <- recipes::recipe (
   formula = env_cat_f2 ~ pdo,
   data = env_cat_train) %>%
   step_tokenize(pdo) %>%   # tokenize
   # remove CUSTOM stopwords
   step_stopwords(pdo, custom_stopword_source = stop_vector) %>%  
   step_tokenfilter(pdo, max_tokens = 100) %>%  # filter by frequency of occurrence
   step_tfidf(pdo, smooth_idf = FALSE)      # tf-idf  creates matrix of weighted term frequencies  

# prep and bake the recipe
env_stop_recipe_bake <-  env_stop_recipe %>% 
  prep() %>% 
   bake(new_data = NULL)

# preview the baked recipe
dim(env_stop_recipe_bake)
#[1] 2264 101
env_recipe_bake[1:5, 1:10]
env_stop_recipe_bake[1:5, 1:10]
```

### 4) Select a classification algorithm (same)

### --- Model specification

```{r}
# Create a model specification
env_spec <-
   # generalized linear model for binary outcomes
   parsnip::logistic_reg(
      # A non-negative number representing the total amount of regularization
      penalty = tune(),  # 0 = no penalty, 1 = max
      #A number between zero and one (inclusive)
      mixture = 1 # pecifies a pure lasso model,
   ) %>%
   set_engine("glmnet")
                           ##### tune() IS A PLACEHOLDER
# Preview
env_spec
```

### --- Create workflow [`env_stop_wf`]

`env_stop_recipe` is actually the part that changed in this workflow adding `step_stopwords()`.

```{r}
# Create a workflow
env_stop_wf <- workflows::workflow() %>%
   add_recipe(env_stop_recipe) %>%  # NEW RECIPE
   add_model(env_spec)
# Preview
env_stop_wf
```

### 5) Tuning hyperparameters (same)

### --- Penalty tuning [`env_grid`] (same)

```{r}
# Create a grid of values for the penalty hyperparameter (random set of 10 values)
env_grid <- dials::grid_regular(
  penalty(), levels = 10
  )
# Preview
env_grid 
```

### --- K-fold cross-val [`env_fold`, `env_FEAT_tune`] (same/NEW)

```{r}
# Create a resampling object
env_vfold <- rsample::vfold_cv(env_cat_train, v = 10)

# Create a tuning workflow
env_stop_tune <- tune::tune_grid(
  object = env_stop_wf, # changed ! 
  resamples = env_vfold,
  grid = env_grid,
  control = control_grid(save_pred = TRUE)
)
# preview
env_stop_tune
```

The `env_stop_tune` object contains the results of the tuning for each fold. We can see the results of the tuning for each fold by calling the `collect_metrics()` function on the `env_stop_tune` object

```{r}
# Collect the results of the tuning
env_stop_tune_metrics <- tune::collect_metrics(env_stop_tune)

# visualize the results
autoplot(env_stop_tune) +
  labs(
    title = "Lasso model performance across regularization penalties",
    subtitle = "Performance metrics can be used to identify the best penalty"
  )

# in roc_auc: many many of the penalty values performed similarly, with a drop-off in performance at the higher val- ues
```

Conveniently, the `tune::show_best()` function takes a `tune_grid` object and returns the *best performing hyperparameter values*.

```{r}
# Show the best hyperparameter values
show_best(env_stop_tune, metric = "roc_auc")

# Make selection programmatically
env_stop_best <- select_best(env_stop_tune, metric ="roc_auc")
env_stop_best_acc <- select_best(env_stop_tune, metric ="accuracy")
env_stop_best_brier <- select_best(env_stop_tune, metric ="brier_class")
```

### 6) Update model specification and workflow with best HP (NEW)

### Update workflow [`env_stop_wf_lasso`]

Now we can update the model specification and workflow with the best performing hyperparameter value using the previous `cls_wf_tune` workflow and the `finalize_workflow()` function.

```{r}
# Update the model specification
env_stop_wf_lasso <- env_stop_wf %>% 
   tune::finalize_workflow(env_stop_best)

# Preview updated workflow object (with defined penalty paramv  0.00599)
env_stop_wf_lasso
```

### 7) Assess the model performance on training set

### --- See the results  [`env_lasso_fit`] 
Here we access the model coefficients to see which features are most important in the model
+ We see here, for the penalty we chose, what terms contribute the most to a en cat NOT being high risk . 
```{r}
# Fit the model to the training data
env_stop_lasso_fit <- fit (env_stop_wf_lasso, data = env_cat_train)
```

### --- Coefficients  [`enf_fitted_coeff`] 
```{r}
env_stop_fitted_coeff <- env_stop_lasso_fit %>% extract_fit_parsnip() %>% 
   tidy() %>%
   arrange(-estimate)
```
"and" is not top coefficient anymore!!!

### --- [FIG] Assessing performance [`env_stop_lasso_fit`] on training set

```{r}
#| output: TRUE

# Example of a data frame containing actual and predicted values
pred_stop_long  <- predict(env_stop_lasso_fit, new_data = env_cat_train , type = "prob")|>
   bind_cols(env_cat_train) %>% 
   select(env_cat_f2,  pred_high_risk = '.pred_High-Med-risk', pred_low_risk = '.pred_Low-risk_Othr')   %>%
   pivot_longer(cols = c(pred_high_risk, pred_low_risk),
                names_to = "risk_type", values_to = "risk_value")

# Plot the predictions with boxplots and jittered points without duplicate legends
ggplot(pred_stop_long, aes(x = env_cat_f2, y = risk_value, fill = risk_type)) +
  geom_boxplot(alpha = 0.4, position = position_dodge(width = 0.8)) +
  #geom_jitter(alpha = 0.6, position = position_dodge(width = 0.8)) +  # Remove width and use position_dodge
   labs(title = "Predicted High and Low Risk Distribution by Env Category",
        subtitle = "Model: Lasso Regression fitted on training data (stopwords)",
       x = "ACTUAL",
       y = "PREDICTED",
       fill = "Risk Type") +  # Set label for fill legend
  theme_minimal() +
  guides(color = "none")  # Suppress the color legend
```

This model did not improve much (especially on the LOW-risk-Other level prediction!


### --- Assess the model w cross-valid [`env_stop_lasso_cv`]

```{r}
# (this is similar to env_tune) 

# Cross validate the (optimized) workflow
env_stop_lasso_cv <- env_stop_wf_lasso %>%
   tune::fit_resamples(
      # 10 fold cross validation splits
      resamples = env_vfold,
      # save predictions for confusion matrix
      control = control_resamples(save_pred = TRUE)
   )
```

We want to aggregate the metrics across the folds to get a sense of the variability of the model. The `collect_metrics()` function takes the results of a cross-validation and returns a data frame with the metrics.

```{r}
env_stop_lasso_cv[[3]][[1]] # 1st split 
env_stop_lasso_cv[[3]][[3]] # 3rd split

# Collect the results of the cross-validation (this is the average from the 10 splits!)
collect_metrics(env_stop_lasso_cv)
# 1 accuracy    binary     0.750    10 0.00745 Preprocessor1_Model1
# 2 brier_class binary     0.172    10 0.00322 Preprocessor1_Model1
# 3 roc_auc     binary     0.784    10 0.00881 Preprocessor1_Model1

# OLD 
#collect_metrics(env_lasso_cv)
# 1 accuracy    binary     0.750    10 0.0102  Preprocessor1_Model1
# 2 brier_class binary     0.174    10 0.00352 Preprocessor1_Model1
# 3 roc_auc     binary     0.777    10 0.00771 Preprocessor1_Model1
```

Same `accuracy` but improve in `roc_auc`  and `brier_class` compared to the previous model!

### --- [FIG] Visualize the confusion matrix

```{r}
# BTW 
collect_predictions(env_stop_lasso_cv)
```

This is done on the cross-valid results `env_stop_lasso_cv` (?!)

```{r}
#| output: TRUE
# Plot the confusion matrix
env_stop_lasso_cv %>%
   tune::conf_mat_resampled(tidy = FALSE) %>%
   autoplot(type = "heatmap")

# 137.2 = true positives  --> 136.4 
# 33.6 = true negatives  --> 33.1

# 40.5 = false positives  --> 40.8
# 15.1 = false negatives  --> 15.3
```

There are more false positives (*low risk predicted to be high risk*) than false negatives. (This is a common issue in imbalanced datasets and can be addressed by adjusting the decision threshold of the model.)


### 8) Evaluate the best model on the validation/test set

<!-- FRAMCOM p [264 -271]  -->

To do this we need to fit the tuned workflow to the training set, which is the actual training phase. We will use the `last_fit()` function from {`workflows`}

Le's use the updated workflow `env_stop_wf_lasso`

### --- Fit the best model(training) and evaluate on the test set

```{r}
# fit the model to the training set and evaluate on the validation set
env_stop_lasso_final_fit <- last_fit(
   env_stop_wf_lasso, 
   split = env_split)

# Evaluate the model on the validation set (in my case)
collect_metrics(env_stop_lasso_final_fit)

# 1 accuracy    binary         0.762 Preprocessor1_Model1
# 2 roc_auc     binary         0.807 Preprocessor1_Model1
# 3 brier_class binary         0.163 Preprocessor1_Model1
```

The performance metrics are very close to those we achieved on the training set --> *good sign* that the model is robust as it performs well on both training and ~~test~~ (validation) sets.

### --- [FIG] Visualize the confusion matrix

```{r}
#| output: TRUE
 
# Plot the confusion matrix
env_stop_lasso_final_fit %>%
   collect_predictions() %>%
   conf_mat(truth = env_cat_f2, estimate = .pred_class) %>%
   autoplot(type = "heatmap")
```

Still (on the test set) imbalanced false positives (well) and false negatives (poor).


### --- Visualize the `ROC AUC` curve

Take the output of `last_fit()` and use it (`env_stop_lasso_final_fit`) to plot the ROC curve.

```{r}
colnames(env_stop_lasso_final_fit)
# Extract predictions from the final fit object
# Extract the tibble from the list
env_stop_lassofinal_fit_pred <-  env_stop_lasso_final_fit$.predictions[[1]]
str(env_stop_lassofinal_fit_pred)

# Visualize the ROC curve 
env_stop_lassofinal_fit_pred %>% 
   roc_curve(truth = env_cat_f2, '.pred_High-Med-risk') %>% 
   autoplot() +
   labs(
      title = "ROC Curve for High-Med Risk Prediction",
      x = "1 - Specificity (False Positive Rate)",
      y = "Sensitivity (True Positive Rate)",
      caption = "logistic regression model on text (stopwords)"
   )
```

#__________

<!-- .... follow the grid in `analysis/_zz_old_files/classific_steps.R` -->
# ____ MODEL C) [👍🏻] `lasso w pdo + tf-idf + stopwords + 3 fct` ____

## 0) Prep for split based on outcome [using `projs_train2`]
## 1) Split data in train/test [based on `env_cat`]
## 2) Pre-processing and featurization (`recipes`)
## 3) Model specification
## 4) Model training
## 5) Model evaluation / tweaking
## 6) Hyperparameter tuning
## 7) Final fit on training set
## 8) Evaluation on test set

 
<!-- -   can I add non text features? [smltar 7.7](https://smltar.com/mlclassification#case-study-including-non-text-data) -->

### 3) Define preprocessing steps [`env_FEAT_recipe`] (same)

### --- Improve recipe [`env_FEAT_recipe`] (NEW!)
+ using `sector_f` to include the sector tag but with less dimensions
+ `step_dummy` because logistic regression, especially when using certain tuning functions in `tidymodels`, requires numeric or dummy variables. 
```{r}
# ---  Create a recipe with a token filter step that excludes stopwords
# Rebuild recipe with tokenfilter step
env_FEAT_recipe <- recipe (env_cat_f2 ~ pdo + sector_f + regionname + FY_appr,
                           data = training(env_split)) %>%
   # tokenize the text
   step_tokenize(pdo) %>%  
   # remove CUSTOM stopwords
   step_stopwords(pdo, custom_stopword_source = stop_vector) %>%  
   # filter by frequency of occurrence
   step_tokenfilter(pdo, max_tokens = 100) %>%  
   # creates tf-idf matrix of weighted term frequencies
   step_tfidf(pdo, smooth_idf = FALSE) %>%
   # add NA as special factor level
   step_unknown(sector_f ,new_level = "Unknown sect" ) %>%
   step_unknown(regionname ,new_level = "Unknown reg" ) %>%
   step_unknown(FY_appr ,new_level = "Unknown FY" ) %>%
   # convert to dummy variables
   step_dummy(sector_f, regionname, FY_appr, one_hot = TRUE) 
```
check what changed... 
```{r}
# prep and bake the recipe
env_FEAT_recipe_bake <-  env_FEAT_recipe %>% 
  prep() %>% 
   bake(new_data = NULL)

# preview the baked recipe
dim(env_FEAT_recipe_bake)
#[1] 2264 101 --> 2264  150
env_FEAT_recipe_bake[1:5, 1:10]
```

### 4) Select algorithm + workflow (same)

### --- Model specification [`env_spec`]
```{r}
# Create a model specification
env_spec <-
   # generalized linear model for binary outcomes
   parsnip::logistic_reg(
      mode = "classification",
      # A non-negative number representing the total amount of regularization
      penalty = tune(),  # 0 = no penalty, 1 = max
      #A number between zero and one (inclusive)
      mixture = 1 # pecifies a pure lasso model,
   ) %>%
   set_engine("glmnet")
                           ##### tune() IS A PLACEHOLDER
# Preview
env_spec
```

### --- Create workflow [`env_FEAT_wf`] (NEW!)

`env_FEAT_recipe` is actually the part that changed in this workflow adding `step_stopwords()`.

```{r}
# Create a workflow
env_FEAT_wf <- workflows::workflow() %>%
   add_recipe(env_FEAT_recipe) %>%  # NEW RECIPE
   add_model(env_spec) # same model
# Preview
env_FEAT_wf
```

### 5) Tuning hyperparameters (same)

### --- Penalty tuning + folds [`env_grid`, `env_fold`] (same)
```{r}
# Create a grid of values for the penalty hyperparameter (random set of 10 values)
env_grid <- dials::grid_regular(
  penalty(), levels = 10
  )
# Create a resampling object
env_vfold <- rsample::vfold_cv(env_cat_train, v = 10)
```

### --- K-fold cross-val tuning [`env_FEAT_tune`] (NEW)
```{r}
# Create a tuning workflow
env_FEAT_tune <- tune::tune_grid(
  object = env_FEAT_wf, # changed ! 
  resamples = env_vfold,
  grid = env_grid,
  control = control_grid(save_pred = TRUE)
)
# preview
env_FEAT_tune
```

The `env_FEAT_tune` object contains the results of the tuning for each fold. We can see the results of the tuning for each fold by calling the `collect_metrics()` function on the `env_FEAT_tune` object

```{r}
# Collect the results of the tuning
env_FEAT_tune_metrics <- tune::collect_metrics(env_FEAT_tune)

# visualize the results
autoplot(env_FEAT_tune)

# in roc_auc: many many of the penalty values performed similarly, with a drop-off in performance at the higher val- ues
```

Conveniently, the `tune::show_best()` function takes a `tune_grid` object and returns the *best performing hyperparameter values*.

```{r}
# Show the best hyperparameter values
show_best(env_FEAT_tune, metric = "roc_auc")

# Make selection programmatically
env_FEAT_best <- select_best(env_FEAT_tune, metric ="roc_auc")
env_FEAT_best_acc <- select_best(env_FEAT_tune, metric ="accuracy")
env_FEAT_best_brier <- select_best(env_FEAT_tune, metric ="brier_class")
```

### 6) Update model specification and workflow with best HP (NEW)

### Update workflow [`env_FEAT_wf2`]

Now we can update the model specification and workflow with the best performing hyperparameter value using the previous `env_FEAT_tune` workflow and the `finalize_workflow()` function.

```{r}
# Update the model specification
env_FEAT_wf2 <- env_FEAT_wf %>% 
   tune::finalize_workflow(env_FEAT_best)

# Preview updated workflow object (with defined penalty paramv  0.00599)
env_FEAT_wf2
```

### 7) Assess the model performance on training set

### --- See the results  [`env_lasso_fit`] 
Here we access the model coefficients to see which features are most important in the model
+ We see here, for the penalty we chose, what terms contribute the most to a en cat NOT being high risk . 
```{r}
# Fit the model to the training data
env_FEAT_fit <- fit (env_FEAT_wf2, data = env_cat_train)
```

### --- Coefficients  [`enf_fitted_coeff`] 
```{r}
env_FEAT_fitted_coeff <- env_FEAT_fit %>% 
   extract_fit_parsnip() %>% 
   tidy() %>%
   arrange(-estimate)
```
+ "sector_f... " appear among the top coefficients!!!

### --- [FIG] Assessing performance [`env_FEAT_fit`] on training set
Now is `env_split_train`
```{r}
#| output: TRUE

# Fit the model on the training set
env_FEAT_fit <- env_FEAT_wf2 %>% # NEW
   fit(data = training(env_split))

# Example of a data frame containing actual and predicted values
pred_FEAT_long  <- predict(env_FEAT_fit, new_data = training(env_split), type = "prob")|>
   bind_cols(training(env_split)) %>% 
   select(env_cat_f2,  pred_high_risk = '.pred_High-Med-risk', pred_low_risk = '.pred_Low-risk_Othr')   %>%
   pivot_longer(cols = c(pred_high_risk, pred_low_risk),
                names_to = "risk_type", values_to = "risk_value")

# Plot the predictions with boxplots and jittered points without duplicate legends
ggplot(pred_FEAT_long, aes(x = env_cat_f2, y = risk_value, fill = risk_type)) +
  geom_boxplot(alpha = 0.4, position = position_dodge(width = 0.8)) +
  #geom_jitter(alpha = 0.6, position = position_dodge(width = 0.8)) +  # Remove width and use position_dodge
   labs(title = "Predicted High and Low Risk Distribution by Env Category",
        subtitle = "Model: Lasso Regression fitted on training data (stop and feat)",
       x = "ACTUAL",
       y = "PREDICTED",
       fill = "Risk Type") +  # Set label for fill legend
  theme_minimal() +
  guides(color = "none")  # Suppress the color legend
```
Seems improved also LOW risk prediction (at least o average although more dispersion)

```{r}
#| output: TRUE

# Fit the model on the testing set
env_FEAT_fit <- env_FEAT_wf2 %>% # NEW
   fit(data = testing(env_split))

# Example of a data frame containing actual and predicted values
pred_FEAT_long  <- predict(env_FEAT_fit, new_data = testing(env_split), type = "prob")|>
   bind_cols(testing(env_split)) %>% 
   select(env_cat_f2,  pred_high_risk = '.pred_High-Med-risk', pred_low_risk = '.pred_Low-risk_Othr')   %>%
   pivot_longer(cols = c(pred_high_risk, pred_low_risk),
                names_to = "risk_type", values_to = "risk_value")

# Plot the predictions with boxplots and jittered points without duplicate legends
ggplot(pred_FEAT_long, aes(x = env_cat_f2, y = risk_value, fill = risk_type)) +
  geom_boxplot(alpha = 0.4, position = position_dodge(width = 0.8)) +
  #geom_jitter(alpha = 0.6, position = position_dodge(width = 0.8)) +  # Remove width and use position_dodge
   labs(title = "Predicted High and Low Risk Distribution by Env Category",
        subtitle = "Model: Lasso Regression fitted on testing data (stop and feat)",
       x = "ACTUAL",
       y = "PREDICTED",
       fill = "Risk Type") +  # Set label for fill legend
  theme_minimal() +
  guides(color = "none")  # Suppress the color legend
```
 
### --- Assess the model w cross-valid [`env_FEAT_cv`] (NEW)

```{r}
# (this is similar to env_tune) 
set.seed(123)
# Cross validate the (optimized) workflow
env_FEAT_cv <- env_FEAT_wf2 %>%
   tune::fit_resamples(
      # 10 fold cross validation splits
      resamples = env_vfold,
      # save predictions for confusion matrix
      control = control_resamples(save_pred = TRUE)
   )
```

We want to aggregate the metrics across the folds to get a sense of the variability of the model. The `collect_metrics()` function takes the results of a cross-validation and returns a data frame with the metrics.

```{r}
env_FEAT_cv[[3]][[1]] # 1st split 
env_FEAT_cv[[3]][[3]] # 3rd split

# Collect the results of the cross-validation (this is the average from the 10 splits!)
collect_metrics(env_FEAT_cv)
# 1 accuracy    binary     0.777    10 0.0118  Preprocessor1_Model1
# 2 brier_class binary     0.155    10 0.00503 Preprocessor1_Model1
# 3 roc_auc     binary     0.820    10 0.0101  Preprocessor1_Model1

# OLD 
collect_metrics(env_stop_lasso_cv)
#1 accuracy    binary     0.754    10 0.00997 Preprocessor1_Model1
#2 brier_class binary     0.170    10 0.00371 Preprocessor1_Model1
#3 roc_auc     binary     0.785    10 0.00813 Preprocessor1_Model1
```

Improved a bit `roc_auc` & `accuracy` and `brier_class` lower compared to the previous model!

### --- [FIG] Visualize the confusion matrix
This is done on the cross-valid results `env_FEAT_cv` (?!)
OKKIO: the number of obs were less because of the dropped missing factors 
```{r}
#| output: TRUE
set.seed(123)
# Plot the confusion matrix
env_FEAT_cv %>%
   tune::conf_mat_resampled(tidy = FALSE) %>%
   autoplot(type = "heatmap")

# env_stop_lasso_cv    -->   env_FEAT_cv
# 137.2 = true positives  -->  135 
# 33.6 = true negatives  -->    33.7

# 40.5 = false positives  -->   33.8 
# 15.1 = false negatives  -->   16.7 
```

Compared to previous model, there is just a little improvement in false positive (*low risk predicted to be high risk*) as they are less than before. 
This model did improve (especially in the low risk category): in fact the probability of being classified as HIGH RISK is less than 50% and of being classified LOW RISK above 50%.   

### 8) Evaluate the best model on the validation/test set

<!-- FRAMCOM p [264 -271]  -->

To do this we need to fit the tuned workflow to the training set, which is the actual training phase. We will use the `last_fit()` function from {`workflows`}

Le's use the updated workflow `env_FEAT_wf2`

RECALL I DID

1.  for training (labelled `env_cat_f`) `env_cat_train <- training(env_split)` \# 2265 proj

2.  for validation (labelled `env_cat_f`) `env_cat_test <- testing(env_split)` \# 971 proj

### --- Fit the best model(training) and evaluate on the test set

After determining the best model, the final fit on the entire training set is needed and is then evaluated on the `test set`.

```{r}
# fit the model to the training set and evaluate on the validation set
env_FEAT_final_fit <- last_fit(
   env_FEAT_wf2, 
   split = env_split)

# Evaluate the model on the validation set (in my case)
collect_metrics(env_FEAT_final_fit)

# 1 accuracy     0.793 Preprocessor1_Model1 --> 0.79 
# 2 roc_auc      0.851 Preprocessor1_Model1 --> 0.85
# 3 brier_class  0.143 Preprocessor1_Model1 --> 0.144
```

The performance metrics are very close to those we achieved on the training set (actually better!!) --> good sign that the model is robust as it performs well on both training and ~~test~~ (validation) sets.

### --- [FIG] Visualize the confusion matrix

```{r}
#| output: TRUE
# Plot the confusion matrix
env_FEAT_final_fit %>%
   collect_predictions() %>%
   conf_mat(truth = env_cat_f2, estimate = .pred_class) %>%
   autoplot(type = "heatmap")
```

```{r}
#| label: fig-confusion_matrix_final
#| output: TRUE
#| fig-cap: Confusion matrix for the final model on the validation set
 
library(tidymodels)
library(ggplot2)

library(tidymodels)
library(ggplot2)

# Create a table for the confusion matrix counts
ML_final_fit_cm_p <- env_FEAT_final_fit %>%
  collect_predictions() %>%
  conf_mat(truth = env_cat_f2, estimate = .pred_class) %>%
  autoplot(type = "heatmap") +
  labs(
    title = "Confusion Matrix for Lasso Logistic Regression Model",
    x = "Predicted Class",
    y = "True Class",
    fill = "Count"
  ) +
  scale_fill_gradient(low = "#f2e8ea", high = "#964957") +  # Adjust color gradient for better contrast
  theme_minimal(base_size = 14) +                              # Set a clean theme with larger base text size
  theme(
    plot.title = element_text(hjust = 0.5, face = "bold"),     # Center and bold the title
    #axis.text.x = element_text(angle = 45, hjust = 1),         # Angle x-axis text for readability
    legend.position = "right"                                  # Place the legend on the right
  )

ML_final_fit_cm_p
f_save_plot_obj <- function(plot_object, plot_obj_name) {
   # Save the plot object
   saveRDS(plot_object, here("analysis", "output", "figures", paste0(plot_obj_name, ".rds")))
}

f_save_plot_obj(ML_final_fit_cm_p, "ML_final_fit_cm_p")

```


Still (on the validation set) imbalanced false positives (138) and false negatives (63).

### --- [FIG] Visualize the `ROC AUC` curve

Take the output of `last_fit()` and use it (`env_FEAT_final_fit`) to plot the ROC curve.

```{r}
colnames(env_FEAT_final_fit)
# Extract predictions from the final fit object
# Extract the tibble from the list
env_FEAT_final_fit_pred <-  env_FEAT_final_fit$.predictions[[1]]
str(env_FEAT_final_fit_pred)

# Visualize the ROC curve 
env_FEAT_final_fit_pred %>% 
   roc_curve(truth = env_cat_f2, '.pred_High-Med-risk') %>% 
   autoplot() +
   labs(
      title = "ROC Curve for High-Med Risk Prediction",
      x = "1 - Specificity (False Positive Rate)",
      y = "Sensitivity (True Positive Rate)",
      caption = "logistic regression model on text (stopwords) + features"
   )
```


### 9) Interpret the model
<!-- EXAMPLE 9.32  ( 268 - 271)  -->

### --- Inspecting what levels of the outcome are most difficult to estimate 
```{r}
# collect the predictions from the final model
env_FEAT_final_fit_feat <- env_FEAT_final_fit %>%
   collect_predictions() %>%
   bind_cols(env_cat_test)  %>%
   rename(env_cat_f2 = 'env_cat_f2...6') %>% 
   select ( -'env_cat_f2...32')

#preview the predictions
glimpse(env_FEAT_final_fit_feat)
```

I will then select the columns with the actual outcome (`env_cat_f2`), the predicted outcome, the `env_cat_f` level, and the `pdo` text and separate the predicted outcome to inspect them separately

```{r}
env_FEAT_final_fit_feat %>%
   filter(env_cat_f2 != .pred_class ) %>%  
   select(env_cat_f2, .pred_class,  env_cat_f, pdo, proj_id) 
```

Inspect to see in which actual category (`env_cat_f`) are proj when they are actually `env_cat_f2 == 'High-Med-risk'` but falsely predicted to be `.pred_class == 'Low-risk_Othr'`: not surprisingly most of them are med risk level. (this makes sense) 

```{r}
env_FEAT_final_fit_feat %>%
   filter(env_cat_f2 == 'High-Med-risk' & .pred_class == 'Low-risk_Othr') %>%  
   select(env_cat_f2, .pred_class,  env_cat_f, pdo, proj_id) %>% 
   count(env_cat_f )

levels(env_FEAT_final_fit_feat$env_cat_f2)
#[1] "High-Med-risk" "Low-risk_Othr"

```

### --- Inspecting the most important features for predicting the outcome 

+ using the `extract_fit_parsnip()` function from the `workflows` package to extract the model object from the workflow. 
+ estimates are the *log odds* of the outcome for each feature (i.e. the probability of the outcome (High risk) divided by the probability of the opposite outcome (low risk)).
   + `Positive coefficient`: A positive coefficient indicate an increased likelihood of being in the *"Low-risk_Othr"* category compared to the "High-Med-risk" category.
   + `Negative coefficient`: A negative coefficient indicate an increased likelihood of being in the *"High-Med-risk"* category compared to the "Low-risk_Othr" category.

+ *odds ratio* (exponentiated coeff) means that the feature is associated with a lower probability of the outcome, while positive odds means that the feature is associated with a higher probability of the outcome.
   + `Odds ratio > 1`: Indicates that the predictor increases the likelihood of the outcome (e.g., *"Low-risk_Othr"*).
   + `Odds ratio < 1`: Indicates that the predictor decreases the likelihood of the outcome *"High-Med-risk"*.

```{r}
# Extract the estimate (log-odds)
env_FEAT_final_fit_features <- extract_fit_parsnip(env_FEAT_final_fit) %>% 
   tidy() %>% 
   # Calculate the exponentiated estimate
   mutate(odds = exp(estimate),
          probability = odds / (1 + odds))  

#  tibble: 206 × 5
# term                   estimate penalty  odds probability
# <chr>                     <dbl>   <dbl> <dbl>       <dbl>
# (Intercept)            -1.26    0.00599 0.284      0.221 
# tfidf_pdo_access       -0.00452 0.00599 0.995      0.499 
# tfidf_pdo_activities    0.786   0.00599 2.19       0.687 
```

+ `tfidf_pdo_activities` est 0.786  | odds 2.19  | prob 0.687 (associated with a LOW risk)
+ `tfidf_pdo_access`     est -0.004 | odds 0.995 | prob 0.499 (associated with a HIGH risk)


### --- Extract the most important features

A quick way to extract the most important features for predicting each out￾come is to use the `vi()` function from {`vip`}. 

+ The `vi()` function calculates the permutation **importance** of each feature in the model.

```{r}
library(vip)

# Extract the most important features
env_FEAT_var_importance <-  extract_fit_parsnip(env_FEAT_final_fit) %>% 
   vip::vi() %>%
   # it is kinda counterintuitive  
   mutate(note = case_when(
       Sign  ==  "POS" ~ "More likely to be in Low-risk_Othr",
       Sign  ==  "NEG" ~ "More likely to be in High-Med-risk",    
      TRUE ~ "NEU"
   )) 
```

### --- [FIG] Plot the most important features

```{r}
# Recode variable and sign 
var_importance_tbl <- env_FEAT_var_importance %>% 
   mutate(Feature =  str_remove(Variable, "tfidf_"),
          EnvRiskOutcome = case_when(
             Sign == "NEG" ~ "High-Med-risk",
             Sign == "POS" ~ "Low-risk_Othr") ) %>% 
   select(Feature, Importance,  EnvRiskOutcome)  

summary(var_importance_tbl$EnvRiskOutcome)
```

```{r}
#| output: TRUE

# Plot the most important features
ML_feature_importance_p <- var_importance_tbl %>%
   slice_max(Importance, n = 50) %>%
   ggplot(aes(x = reorder(Feature, Importance), y = Importance, color = EnvRiskOutcome)) +
   geom_point() +
   coord_flip() +
   facet_wrap(~EnvRiskOutcome, ncol = 2, scales = "free_y") +
   labs(
      title = glue("Most influential features for predicting Environmental Risk"),
      subtitle = "LASSO Logistic Regression model on text + metadata tags \n(Importance = absolute value of the logistic regression coefficients)",
      #caption = "(Importance of each feature calculated as the absolute value of the logistic regression coefficients)",
      x = "",
      y = "",
      fill = ""
   ) +
   lulas_theme + 
   theme(
     plot.title = element_text(hjust = 0.5, face = "bold")
   ) +
   guides(color = "none")

ML_feature_importance_p

```

The feature importance plot highlights the top 50 predictors of `environmental risk (binary) category`, ranked by their influence in a LASSO logistic regression model. For better readability the predictors are split according to the level of risk predicted. It should be no surprise that words in the PDO text (those variables starting with `pdo_*`) are the most important predictors given the data. Still some of the other predictors are also important, such as `sector_f_TRANSPORT` (left panel) or `regionname` and `sector_f_FINANCIAL` (right panel). 

 

Each facet groups features by environmental risk outcome, allowing a comparison of which factors contribute most to each category. Features with higher importance values, like [mention a few key features], play a significant role in predicting environmental risk, offering insights for targeted risk assessment and decision-making.

```{r}
# show plot
ML_feature_importance_p

# save as rds
f_save_plot_obj(ML_feature_importance_p, "ML_feature_importance_p")

```

# __________ 

# ____ MODEL D) [🫤] `NB w pdo + tf-idf + stopwords + 3 fct` ____
 
## 0) Prep for split based on outcome [using `projs_train2`]
## 1) Split data in train/test [based on `env_cat`]
## 2) Pre-processing and featurization (`recipes`)
## 3) Model specification
## 4) Model training
## 5) Model evaluation / tweaking
## 6) Hyperparameter tuning
## 7) Final fit on training set
## 8) Evaluation on test set



### 3) Define preprocessing steps (same)

### 4) Select algorithm + workflow (same)
### --- Model specification [`env_spec`]

<!-- https://smltar.com/mlclassification#classfirstattemptlookatdata -->
Let’s use a *naive Bayes model*, which is available in the `tidymodels` package `discrim.` One of the main advantages is its ability to handle a large number of features, such as those we deal with when using word count methods. Here we have only kept the 1000 most frequent tokens, but we could have kept more tokens and a naive Bayes model would still be able to handle such predictors well. For now, we will limit the model to a moderate number of tokens.

```{r}
# needed for naive Bayes
library(discrim)

# Create a model specification
env_NB_spec <-
   # generalized linear model for binary outcomes
   parsnip::naive_Bayes() %>%
   # Specify the mode of the model
   set_mode("classification") %>%
   # Specify the engine
   set_engine("naivebayes")
 
# Preview
env_NB_spec
```

### --- Create workflow [`env_NB_wf`] (NEW!)

`env_NB_spec` is actually the part that changed in this workflow adding `step_stopwords()`.

```{r}
# Create a workflow
env_NB_wf <- workflows::workflow() %>%
   add_recipe(env_FEAT_recipe) %>%  # same RECIPE
   add_model(env_NB_spec) # NEW MODEL
# Preview
env_NB_wf
```

### --- Fit the classificatoin model to the training set [`env_NB_fit`] (NEW!)
```{r}
# Fit the model to the training set
env_NB_fit <- env_NB_wf %>%
   #add_model(env_NB_spec) %>%
   fit(data = env_cat_train)
```

### 5) Tuning hyperparameters (same)
Let’s use resampling to estimate the performance of the naive Bayes classification model we just fit. We can do this using resampled data sets built from the training set. Let’s create 10-fold cross-validation sets, and use these resampled sets for performance estimates.

<!-- ### --- Penalty tuning + folds + K-fold cross-val [`env_grid`, `env_fold`, `env_NB_tune`] (same, same, new) -->
<!-- ```{r} -->
<!-- set.seed(123) -->

<!-- # Create a grid of values for the penalty hyperparameter (random set of 10 values) -->
<!-- env_grid <- dials::grid_regular( -->
<!--   penalty(), levels = 10 -->
<!--   ) -->
<!-- # Create a resampling object -->
<!-- env_vfold <- rsample::vfold_cv(env_cat_train, v = 10) -->

<!-- # Create a tuning workflow -->
<!-- env_NB_tune <- tune::tune_grid( -->
<!--   object = env_NB_wf, # changed !  -->
<!--   resamples = env_vfold, -->
<!--   grid = env_grid, -->
<!--   control = control_grid(save_pred = TRUE) -->
<!-- ) -->
<!-- # preview -->
<!-- env_NB_tune -->
<!-- # Collect the results of the tuning -->
<!-- env_NB_tune_metrics <- tune::collect_metrics(env_NB_tune) -->

<!-- # visualize the results -->
<!-- autoplot(env_NB_tune) -->
<!-- ``` -->

The `env_FEAT_tune` object contains the results of the tuning for each fold. We can see the results of the tuning for each fold by calling the `collect_metrics()` function on the `env_FEAT_tune` object

```{r}
set.seed(123)
env_vfold <- rsample::vfold_cv(env_cat_train, v = 10) 

# Fit the model to the resampled folds
env_NB_rs <- fit_resamples(
   env_NB_wf,
   resamples = env_vfold,
   control = control_resamples(save_pred = TRUE)
)
```

We can extract the relevant information using `collect_metrics()` and `collect_predictions()`.
```{r}
env_NB_rs_metrics <- collect_metrics(env_NB_rs)
#1 accuracy    binary     0.684    10 0.00843 Preprocessor1_Model1
#2 brier_class binary     0.314    10 0.00884 Preprocessor1_Model1
#3 roc_auc     binary     0.762    10 0.00715 Preprocessor1_Model1

env_NB_rs_predictions <- collect_predictions(env_NB_rs)
```
Worse than logistic!

+ `Accuracy` is the proportion of the data that is predicted correctly. Be aware that accuracy can be misleading in some situations, such as for imbalanced data sets. 
+ `ROC AUC` measures how well a classifier performs at different thresholds. The ROC curve plots the *true positive rate against the false positive rate*; AUC closer to 1 indicates a better-performing model, while AUC closer to 0.5 indicates a model that does no better than random guessing. 
+ `Brier score` is a measure of the mean squared difference between the predicted probabilities and the actual outcomes.

### --- Visualize the `ROC AUC` curve
```{r}
# Visualize the ROC curve 
env_NB_rs_predictions %>% 
   group_by(id) %>% 
   # roc_curve(truth = env_cat_f2, .pred_class) %>% 
   # Not work with factor, use "positive" class 
   roc_curve(truth = env_cat_f2, '.pred_High-Med-risk') %>% 
   autoplot() 
```

The area under each of these curves is the `roc_auc` metric we have computed. (If the curve was close to the diagonal line, then the model’s predictions would be no better than random guessing.)

### --- [FIG] Visualize the confusion matrix

```{r}
#| output: TRUE

# Plot the confusion matrix
conf_mat_resampled(env_NB_rs, tidy = FALSE) %>% 
   autoplot(type = "heatmap")
```

+ different from logistic 
+ Very well with true positive (high risk), but very bad with true negative (low risk).


### 8) Evaluate the best model on the validation/test set

<!-- FRAMCOM p [264 -271]  -->


### --- Fit the best model(training) and evaluate on the test set

After determining the best model, the final fit on the entire training set is needed and is then evaluated on the `test set`.

```{r}
# fit the model to the training set and evaluate on the validation set
env_NB_rs_final_fit <- last_fit(
   env_NB_wf, 
   split = env_split)

# Evaluate the model on the validation set (in my case)
collect_metrics(env_NB_rs_final_fit)
# 1 accuracy    binary         0.691 Preprocessor1_Model1
# 2 roc_auc     binary         0.784 Preprocessor1_Model1
# 3 brier_class binary         0.307 Preprocessor1_Model1
```

# SET OF MODELS      

What if I want to see them all together? 

[Spiega `workflowsets`](https://workflowsets.tidymodels.org/articles/tuning-and-comparing-models.html)


#
# RENDER this

```{bash}
#| eval: false
quarto render analysis/01c_WB_project_pdo_feat_class.qmd --to html
open ./docs/analysis/01c_WB_project_pdo_feat_class.html
```


