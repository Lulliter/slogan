---
title: "WB Project PDO unsupervised ML: topic modeling"
author: "Luisa M. Mimmi"
# date: "Last run: `r format(Sys.time(), '%F')`"
lang: en
editor: source
engine: knitr
## ------  general Output Options
execute:     
  eval: true    # actually run? 
  echo: true     #  include source code in output
  warning: false  #  include warning code in output
  error: false    #  include error code in output
  output: false   # include output code in output (CHG in BLOCKS)
  # include: false   # R still runs but code and results DON"T appear in output  
  cache: false # normalmnte false
toc: true
fig-cap-location: top
tbl-cap-location: top
format:
  html:
    # theme: flatly #spacelab
    code-fold: false # redundant bc echo false 
    toc-depth: 3
    toc_float: true
    toc-location: left
    toc-title: Outline
    embed-resources: true # external dependencies embedded (Not in ..._files/)
  # pdf:
  #   toc-depth: 2
  #   toc-title: Indice
  #   highlight-style: github
  #   #lang: it
  #   embed-resources: true # external dependencies embedded (Not in ..._files/)
format-links: false
bibliography: ../bib/slogan.bib
---

::: {.callout-warning collapse="false" style="background-color: #fffcf9;"}
WORK IN PROGRESS!
(Please expect unfinished sections, and unpolished code. Feedback is welcome!)
:::

<!-- In this file I address the **research question 1.2**, which is also to learn ML, IS IT POSSIBLE TO IMPROVE THE QUALITY OF THE DATA (E.G. MISSING FEATURES IN METADATA) BY USING TOPIC MODELING?  -->

<!-- I experiment predicting the `environmental risk category` (e.g., `A`, `B`, `C`, `D`) TURNED INTO BINARY OUTCOME `env_cat_f2` based on the available features in the dataset. -->

# Set up

```{r}
# Pckgs -------------------------------------
library(fs) # Cross-Platform File System Operations Based on 'libuv'
library(janitor) # Simple Tools for Examining and Cleaning Dirty Data
library(skimr) # Compact and Flexible Summaries of Data
library(here) # A Simpler Way to Find Your Files
library(paint) # paint data.frames summaries in colour
library(readxl) # Read Excel Files
library(kableExtra) # Construct Complex Table with 'kable' and Pipe Syntax)
library(glue) # Interpreted String Literals
#library(tidyverse) # Easily Install and Load the 'Tidyverse'
library(dplyr) # A Grammar of Data Manipulation
library(tidyr) # Tidy Messy Data
library(tibble) # Tibbles: A Modern Version of Data Frames
library(purrr) # Functional Programming Tools
library(lubridate) # Make Dealing with Dates a Little Easier
library(ggplot2) # Create Elegant Data Visualisations Using the Grammar of Graphics
library(stringr) # Simple, Consistent Wrappers for Common String Operations
# ML & Text Mining -------------------------------------
library(tidymodels) # Easily Install and Load the 'Tidymodels' Packages 
library(textrecipes) # Extra 'Recipes' for Text Processing  
library(tidytext) # Text Mining using 'dplyr', 'ggplot2', and Other Tidy Tools 
library(SnowballC) # Snowball Stemmers Based on the C 'libstemmer' UTF-8 Library
library(rvest) # Easily Harvest (Scrape) Web Pages
library(cleanNLP) # A Tidy Data Model for Natural Language Processing
library(themis) # Extra Recipes for Dealing with Unbalanced Classes
library(discrim) # Model Wrappers for Discriminant Analysis

set.seed(123) # for reproducibility
```

```{r}
#| echo: TRUE
#| eval: TRUE

# 1) --- Set the font as the default for ggplot2
# Who else? https://datavizf24.classes.andrewheiss.com/example/05-example.html 
lulas_theme <- theme_minimal(base_size = 12) +
  theme(panel.grid.minor = element_blank(),
        # Bold, bigger title
        plot.title = element_text(face = "bold", size = rel(1.6)),
        # Plain, slightly bigger subtitle that is grey
        plot.subtitle = element_text(face = "plain", size = rel(1.4), color = "#A6A6A6"),
        # Italic, smaller, grey caption that is left-aligned
        plot.caption = element_text(face = "italic", size = rel(0.7), 
                                    color = "#A6A6A6", hjust = 0),
        # Bold legend titles
        legend.title = element_text(face = "bold"),
        # Bold, slightly larger facet titles that are left-aligned for the sake of repetition
        strip.text = element_text(face = "bold", size = rel(1.1), hjust = 0),
        # Bold axis titles
        axis.title = element_text(face = "bold"),
        # Change X-axis label size
        axis.text.x = element_text(size = rel(1.4)),   
        # Change Y-axis label size
        axis.text.y = element_text(size = 14),   
        # Add some space above the x-axis title and make it left-aligned
        axis.title.x = element_text(margin = margin(t = 10), hjust = 0),
        # Add some space to the right of the y-axis title and make it top-aligned
        axis.title.y = element_text(margin = margin(r = 10), hjust = 1),
        # Add a light grey background to the facet titles, with no borders
        strip.background = element_rect(fill = "grey90", color = NA),
        # Add a thin grey border around all the plots to tie in the facet titles
        panel.border = element_rect(color = "grey90", fill = NA))

# 2) --- use 
# ggplot + lulas_theme
```


# __________
# TOPIC MODELING w ML
# __________

Topic modeling is an **unsupervised machine learning** technique used to hat exploratively identifies latent topics based on frequently co-occurring words.

It can identify topics or themes that occur in a collection of documents, allowing hidden patterns and relationships within text data to be discovered. It is widely applied in fields such as social sciences and humanities.

https://bookdown.org/valerie_hase/TextasData_HS2021/tutorial-13-topic-modeling.html

https://m-clark.github.io/text-analysis-with-R/topic-modeling.html

https://sicss.io/2020/materials/day3-text-analysis/topic-modeling/rmarkdown/Topic_Modeling.html

## Document-Term Matrix

...

## /include independent variables in my topic model?

https://bookdown.org/valerie_hase/TextasData_HS2021/tutorial-13-topic-modeling.html#how-do-i-include-independent-variables-in-my-topic-model

### Compare PDO text v. project METADATA [CMPL ðŸŸ ]

Using NLP models trained on document metadata and structure can be combined with text analysis to improve classification accuracy.

STEPS

1.  Use document text (abstracts) as features to train a supervised machine learning model. The labeled data (documents with sector tags) will serve as training data, and the model can predict the missing sector tags for unlabeled documents.
2.  TEXT preprocessing (e.g. tokenization, lemmatization, stopword removal, *TF-IDF*)
    -   Convert the processed text into a numerical format using Term Frequency-Inverse Document Frequency (TF-IDF), which gives more weight to terms that are unique to a document but less frequent across the entire corpus.
3.  Define data features, e.g.
    -   Document Length: Public sector documents might be longer, more formal.
    -   Presence of Certain Keywords: Use specific keywords that correlate with either the public or private sector.
    -   Sector Tags: In documents where the "sector tag" is present, you can use it as a feature for training.
4.  Predicting Missing Sector Tags (Classification):
    -   Use models like: Logistic Regression: For a binary classification (e.g., public vs. private). Random Forest or XGBoost: If you have a more complex tagging scheme (e.g., multiple sector categories).
    -   Cross-validation: Ensure the model generalizes well by validating with the documents that already have the sector tag filled in.
    -   Evaluate the model: Use metrics like accuracy, precision, recall, and F1 score to evaluate the model's performance.

```{r}
# #| eval: FALSE
# #| echo: FALSE
# library(tidytext) # Text Mining using 'dplyr', 'ggplot2', and Other Tidy Tools
# library(dplyr) # A Grammar of Data Manipulation
# library(tidyr) # Tidy Messy Data
# library(caret) # Classification and Regression Training # Classification and Regression Training
# 
# # ----- 1. Tokenization and stopwords removal using tidytext.
# # Assuming df is your dataframe with "abstract" and "sector_tag"
# # Tokenize the text and remove stopwords
# tidy_abstracts <- df %>%
#   unnest_tokens(word, abstract) %>%
#   anti_join(stop_words)  # Remove stopwords
# 
# # Optional: Apply stemming (you can also use `SnowballC` if you prefer)
# tidy_abstracts <- tidy_abstracts %>%
#   mutate(word = SnowballC::wordStem(word))
# 
# # ----- 2. Document-term matrix or TF-IDF calculation using bind_tf_idf().
# # Create a term-frequency matrix
# abstract_dtm <- tidy_abstracts %>%
#   count(document_id = row_number(), word) %>%  # Assuming each row is a document
#   cast_dtm(document_id, word, n)
# 
# # Alternatively, use TF-IDF weighting
# abstract_tfidf <- tidy_abstracts %>%
#   count(document_id = row_number(), word) %>%
#   bind_tf_idf(word, document_id, n)
# 
# # ----- 3. Model training using caret (Random Forest, Logistic Regression, etc.).
# # Split data into training (with sector tags) and testing (missing sector tags)
# train_data <- df[!is.na(df$sector_tag), ]
# test_data  <- df[is.na(df$sector_tag), ]
# 
# # Combine the DFM or TF-IDF with the training dataset
# train_tfidf <- abstract_tfidf %>%
#   filter(document_id %in% train_data$row_number()) %>%
#   spread(word, tf_idf, fill = 0)
# 
# # Merge with sector tags
# train_tfidf <- left_join(train_tfidf, train_data, by = c("document_id" = "row_number"))
# 
# # Prepare for machine learning by ensuring you have sector tags in the final dataset
# 
# # ----- 4. Prediction of missing sector tags based on the trained model.
# # Random Forest Model
# model <- train(sector_tag ~ ., data = train_tfidf, method = "rf")
# 
# # Predict missing sector tags for the test data
# test_tfidf <- abstract_tfidf %>%
#   filter(document_id %in% test_data$row_number()) %>%
#   spread(word, tf_idf, fill = 0)
# 
# # Predict sector tags for the missing observations
# predicted_tags <- predict(model, newdata = test_tfidf)
# 
# # Add the predicted sector tags to the original dataset
# df$sector_tag[is.na(df$sector_tag)] <- predicted_tags
# 
# # ----- 5. Evaluate and Refine the Model
# confusionMatrix(predicted_tags, test_data$sector_tag)
```

### --- I could see if corresponds to sector flags in the project metadata

more missing but more objective!

# Topic modeling with LDA (Latent Dirichlet Allocation)

<!-- https://www.nlpdemystified.org/course/topic-modelling -->

Topic modeling algorithms like Latent Dirichlet Allocation (LDA) can be applied to automatically uncover underlying themes within a corpus. The detected topics may highlight key terms or subject areas that are strongly associated with either the public or private sector.

# Named Entity Recognition using CleanNLP and spaCy

NER is especially useful for analyzing unstructured text.

NER can identify key entities (organizations, people, locations) mentioned in the text. By tracking which entities appear frequently (e.g., government agencies vs. corporations), it's possible to categorize a document as more focused on the public or private sector.

### --- Summarise the tokens by parts of speech

```{r}
#| eval: false
#| output: false
#| echo: true

# Initialize the spacy backend
cnlp_init_spacy() 
```

# __________
# STRUCTURAL TOPIC MODELING (STM)
# __________

The Structural Topic Model is a general framework for topic modeling with document-level covariate information. The covariates can improve inference and qualitative interpretability and are allowed to affect topical prevalence, topical content or both.

MAIN REFERENCE `stm` R package [Ref. on STM 1](http://www.structuraltopicmodel.com/) EXAMPLE UN corpus 
[Ref. on STM 2](https://content-analysis-with-r.com/6-topic_models.html STM 1/2) 
[Ref. on STM 3](https://jovantrajceski.medium.com/structural-topic-modeling-with-r-part-i-2da2b353d362 STM 2/2) 
[Ref. on STM 4](https://jovantrajceski.medium.com/structural-topic-modeling-with-r-part-ii-462e6e07328)

# BERTopic

Developed by Maarten Grootendorst, BERTopic enhances the process of discovering topics by using document embeddings and a class-based variation of Term Frequency-Inverse Document Frequency (TF-IDF).

[Ref. on STM 5](https://medium.com/@supunicgn/a-beginners-guide-to-bertopic-5c8d3af281e8)

# __________

# (dYnamic) TOPIC MODELING OVER TIME

# __________

Example: [An analysis of Peter Pan using the R package koRpus](https://irhuru.github.io/blog/korpus-peterpan/) https://ladal.edu.au/topicmodels.html#Topic_proportions_over_time

# RENDER this

```{bash}
#| eval: false
quarto render analysis/01c_WB_project_pdo_feat_class.qmd --to html
open ./docs/analysis/01c_WB_project_pdo_feat_class.html
```

