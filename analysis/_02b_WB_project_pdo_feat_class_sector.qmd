---
title: "WB Project PDO features classification"
subtitle: "Supervised ML for text classification"
author: "Luisa M. Mimmi"
# date: "Last run: `r format(Sys.time(), '%F')`"
lang: en
editor: source
engine: knitr
## ------  general Output Options
execute:     
  eval: true    # actually run? 
  echo: true     #  include source code in output
  warning: false  #  include warning code in output
  error: false    #  include error code in output
  output: false   # include output code in output (CHG in BLOCKS)
  # include: false   # R still runs but code and results DON"T appear in output  
  cache: false # normalmnte false
toc: true
fig-cap-location: top
tbl-cap-location: top
format:
  html:
    # theme: flatly #spacelab
    code-fold: false # redundant bc echo false 
    toc-depth: 3
    toc_float: true
    toc-location: left
    toc-title: Outline
    embed-resources: true # external dependencies embedded (Not in ..._files/)
  # pdf:
  #   toc-depth: 2
  #   toc-title: Indice
  #   highlight-style: github
  #   #lang: it
  #   embed-resources: true # external dependencies embedded (Not in ..._files/)
format-links: false
bibliography: ../bib/slogan.bib
---

::: {.callout-warning collapse="false" style="background-color: #fffcf9;"}
WORK IN PROGRESS!
(Please expect unfinished sections, and unpolished code. Feedback is welcome!)
:::

<!-- In this file I address the **research question 1.2**, which is also to learn ML, IS IT POSSIBLE TO IMPROVE THE QUALITY OF THE DATA (E.G. MISSING FEATURES IN METADATA) BY USING TOPIC MODELING?  -->

<!-- I experiment predicting the `environmental risk category` (e.g., `A`, `B`, `C`, `D`) TURNED INTO BINARY OUTCOME `env_cat_f2` based on the available features in the dataset. -->

# Set up

```{r}
# Pckgs -------------------------------------
library(fs) # Cross-Platform File System Operations Based on 'libuv'
library(janitor) # Simple Tools for Examining and Cleaning Dirty Data
library(skimr) # Compact and Flexible Summaries of Data
library(here) # A Simpler Way to Find Your Files
library(paint) # paint data.frames summaries in colour
library(readxl) # Read Excel Files
library(kableExtra) # Construct Complex Table with 'kable' and Pipe Syntax)
library(glue) # Interpreted String Literals
#library(tidyverse) # Easily Install and Load the 'Tidyverse'
library(dplyr) # A Grammar of Data Manipulation
library(tidyr) # Tidy Messy Data
library(tibble) # Tibbles: A Modern Version of Data Frames
library(purrr) # Functional Programming Tools
library(lubridate) # Make Dealing with Dates a Little Easier
library(ggplot2) # Create Elegant Data Visualisations Using the Grammar of Graphics
library(stringr) # Simple, Consistent Wrappers for Common String Operations
library(forcats) # Tools for Working with Categorical Variables (Factors)
# ML & Text Mining -------------------------------------
library(tidymodels) # Easily Install and Load the 'Tidymodels' Packages 
library(textrecipes) # Extra 'Recipes' for Text Processing  
library(discrim) # Model Wrappers for Discriminant Analysis
library(tidytext) # Text Mining using 'dplyr', 'ggplot2', and Other Tidy Tools 
library(SnowballC) # Snowball Stemmers Based on the C 'libstemmer' UTF-8 Library
library(rvest) # Easily Harvest (Scrape) Web Pages
library(cleanNLP) # A Tidy Data Model for Natural Language Processing
library(themis) # Extra Recipes for Dealing with Unbalanced Classes

set.seed(123) # for reproducibility

```

```{r}
#| echo: TRUE
#| eval: TRUE

# 1) --- Set the font as the default for ggplot2
# Who else? https://datavizf24.classes.andrewheiss.com/example/05-example.html 
lulas_theme <- theme_minimal(base_size = 12) +
  theme(panel.grid.minor = element_blank(),
        # Bold, bigger title
        plot.title = element_text(face = "bold", size = rel(1.6)),
        # Plain, slightly bigger subtitle that is grey
        plot.subtitle = element_text(face = "plain", size = rel(1.4), color = "#A6A6A6"),
        # Italic, smaller, grey caption that is left-aligned
        plot.caption = element_text(face = "italic", size = rel(0.7), 
                                    color = "#A6A6A6", hjust = 0),
        # Bold legend titles
        legend.title = element_text(face = "bold"),
        # Bold, slightly larger facet titles that are left-aligned for the sake of repetition
        strip.text = element_text(face = "bold", size = rel(1.1), hjust = 0),
        # Bold axis titles
        axis.title = element_text(face = "bold"),
        # Change X-axis label size
        axis.text.x = element_text(size = rel(1.4)),   
        # Change Y-axis label size
        axis.text.y = element_text(size = 14),   
        # Add some space above the x-axis title and make it left-aligned
        axis.title.x = element_text(margin = margin(t = 10), hjust = 0),
        # Add some space to the right of the y-axis title and make it top-aligned
        axis.title.y = element_text(margin = margin(r = 10), hjust = 1),
        # Add a light grey background to the facet titles, with no borders
        strip.background = element_rect(fill = "grey90", color = NA),
        # Add a thin grey border around all the plots to tie in the facet titles
        panel.border = element_rect(color = "grey90", fill = NA))

# 2) --- use 
# ggplot + lulas_theme
```


# Load data
```{r}
# Load Proj train dataset `projs_train_t`
projs_train2 <- readRDS(here("data","derived_data", "projs_train2.rds"))  
custom_stop_words_df  <-  readRDS(here("data","derived_data", "custom_stop_words_df.rds")) 
# Load function
source(here("R","f_recap_values.R")) 
```

# __________
# PREDICT MISSING FEATUREs
# __________

## What ML models work with text?

> Remember that text data is SPARSE!

To predict a missing feature (e.g., sector) based on available features from text data, several supervised machine learning algorithms can be applied. Given that you have a mixture of text and structured data, here are some suitable algorithms:

-   **Logistic Regression / Multinomial Logistic Regression**: If you're predicting a categorical variable like "sector", logistic regression can work well, especially with appropriate feature engineering for text (e.g., converting text data into numeric features using TF-IDF or word embeddings).
   + **Lasso regression/classification** learns `how much of a penalty` to put on some features (sometimes penalizing all the way down to zero) so that we can select only some features out of the high-dimensional space of original possible variables (tokens) for the final model.
-   **k-Nearest Neighbors (k-NN)**: k-NN can be useful for text data, especially when you have a mix of structured and unstructured data. It's a simple algorithm that can work well with text data, but it can be computationally expensive.
-   **Decision Trees / Random Forests**: These algorithms handle both numeric and categorical data efficiently and can manage missing values quite well. You can input text-based features as well, though you might need to preprocess the text into numeric form (e.g., using embeddings).
-   **Naive Bayes**: Naive Bayes is a simple and efficient algorithm for text classification. It assumes feature independence, which may not always hold, but it's often a good baseline, particularly with short texts.
-   **Support Vector Machines (SVMs)**: SVMs are useful when you have high-dimensional data, which is common with text after feature extraction (like TF-IDF). They can perform well with a mix of structured and unstructured data. <!-- + **Gradient Boosting Machines (GBM)**: Models like XGBoost or LightGBM can be highly effective, especially when you have structured data alongside text features. These models can also handle missing data naturally in the training process. --> <!-- + **Neural Networks (MLP or LSTM)**: If you have a larger dataset, neural networks could be useful. A multilayer perceptron (MLP) is good for mixed feature types, while LSTM (Long Short-Term Memory) networks can work for sequential text data, especially if you want to capture contextual information from the abstracts. -->

All available models are listed at [parsnip](https://tidymodels.org/find/parsnip)

Some model parameters can be learned from data during fitting/training. Some CANNOT 😱. These are **hyperparameters of a model**, and we estimate them by training lots of models with different hyperparameters and comparing them

### --- Check missing feature

```{r}
names (projs_train2)

tot <- sum(!is.na(projs_train2$pdo)) # 4425
sum(!is.na(projs_train2$sector1)) /tot# 99%
sum(!is.na(projs_train2$regionname)) / tot  # 100%
sum(!is.na(projs_train2$countryname)) / tot  # 100%
sum(!is.na(projs_train2$status)) / tot  # 100%
sum(!is.na(projs_train2$lendinginstr)) / tot  # 98% 

sum(!is.na(projs_train2$env_cat)) / tot  # 72%
table(projs_train2$env_cat , useNA = "ifany") # 7 levels --> 5 
sum(!is.na(projs_train2$ESrisk)) / tot  # 0.092  !!!!!
sum(!is.na(projs_train2$curr_total_commitment)) / tot  # 100% 

sum(!is.na(projs_train2$theme1)) /tot # 71% 
table(projs_train2$theme1, useNA = "ifany") # 71 levels
```

```{r}
# sum(!is.na(projs_train_old$pdo)) # 5637
# sum(!is.na(projs_train_old$sector1)) /5637# 99%
# sum(!is.na(projs_train_old$regionname)) / 5637  # 100%
# sum(!is.na(projs_train_old$countryname)) / 5637  # 100%
# sum(!is.na(projs_train_old$status)) / 5637  # 100%
# sum(!is.na(projs_train_old$lendinginstr)) / 5637  # 98% 
# sum(!is.na(projs_train_old$ESrisk)) / 5637  # 78% 
# sum(!is.na(projs_train_old$curr_total_commitment)) / 5637  # 100% 
# 
# sum(!is.na(projs_train_old$theme1)) /5637 # 71% 
# table(projs_train_old$theme1, useNA = "ifany") # 71 levels
# sum(!is.na(projs_train_old$env_cat)) / 5637  # 73%  ---- 0.0729 ?????
# table(projs_train_old$env_cat , useNA = "ifany") # 7 levels
```


```{r}
#| output: true
#| tbl-caption: Missing values in the dataset

# source function
source(here("R","f_recap_values.R")) 

# check candidate lables for classification 
f_recap_values(projs_train2, c("sector1", "theme1","env_cat","ESrisk" ) ) %>% kable()
```

### --- Identify features for classification

These could be:

-   *features derived from raw text* (e.g. characters, words, ngrams, etc.),
-   *feature vectors* (e.g. word embeddings), or
-   *meta-linguistic features* (e.g. part-of-speech tags, syntactic parses, or semantic features)

How do we use them?

-   Do we use raw token counts?\
-   Do we use normalized frequencies?
-   Do we use some type of weighting scheme? ✅
    -   yes, we use `tf-idf` (a weighting scheme, which will downweight words that are common across all documents and upweight words that are unique to a document)
-   Do we use some type of dimensionality reduction? ✅
# TEXT CLASSIFICATION for Environmental Assessment Category

[Francom](chap%209.2.1%20Text%20classification,%20p.%202018-) [text Classification (in R)](https://burtmonroe.github.io/TextAsDataCourse/Tutorials/TADA-ClassificationV2.nb.html) [Stanford slide](https://web.stanford.edu/class/cs124/lec/naivebayes2021.pdf)


# STEPS 
## 0) Prep for split based on outcome [using `projs_train2`]
## 1) Split data in train/test [based on `env_cat`]
## 2) Pre-processing and featurization (`recipes`)
## 3) Model specification
## 4) Model training
## 5) Model evaluation
## 6) Hyperparameter tuning
## 7) Final fit on training set
## 8) Evaluation on test set


# ____________________________________________________________ #
#      ---------------------- MULTICLASS OUTCOME ------------------- 
# ____________________________________________________________ #

## Recode `sector1` variable 
(this I use later as MULTI-CLASS outcome)

```{r}
# !!!!! `sector_f` e' diverso da `tok_sector_broad` XCHE si basa su `sector1` !!!! 
projs_train2 <- projs_train2 %>% # sector1 99 levels
   mutate(sector_f = case_when(
      str_detect(sector1, regex("water|wastewater|sanitat|Sewer|Irrigat|Drainag", ignore_case = T)) ~ "WAT & SAN",
      str_detect(sector1, regex("transport|railway|road|airport|waterway|bus|metropolitan|inter-urban|aviation", ignore_case = T)) ~ "TRANSPORT",
      sector1 == "port" ~ "TRANSPORT",
      str_detect(sector1, regex("urban|housing|inter-urban|peri-urban|waste", ignore_case = T)) ~ "URBAN",
      str_detect(sector1, regex("energ|electri|hydroele|hydropow|renewable|transmis", ignore_case = T)) ~ "ENERGY",  # Matches either "energy" or "power"
      str_detect(sector1, regex("health|hospital|medicine|drugs|epidem|pandem|covid-19|vaccin", ignore_case = T)) ~ "HEALTH",
      str_detect(sector1, regex("educat|school|vocat|teach|univers|student|literacy|training|curricul", ignore_case = T)) ~ "EDUCATION",
      
      str_detect(sector1, regex("Agricultural|Agro|Fish|Forest|Crop|livestock|agri-business", ignore_case = T)) ~ "AGR FOR FISH",
      str_detect(sector1, regex("Minin|oil|gas|mineral|Extract", ignore_case = T)) ~ "MINING OIL&GAS",
      str_detect(sector1, regex("Social Protec", ignore_case = T)) ~ "SOCIAL PROT.",
      
      str_detect(sector1, regex("Bank|finan|Investment", ignore_case = T)) ~ "FINANCIAL",
      str_detect(sector1, regex("Information|Communication|ICT|Internet|Technologies", ignore_case = T)) ~ "ICT",
      str_detect(sector1, regex("Tourism|Trade and Services|Manuf|Other Industry|Trade and Services", ignore_case = T)) ~ "IND TRADE SERV",
      str_detect(sector1, regex("Government|Public Admin|Institution|Central Agenc|Sub-national Gov|law|justice|governance", ignore_case = T)) ~ "INSTIT. SUPP.",
      TRUE ~ "Missing")) %>% 
   relocate(sector_f, .after = sector1)  

tabyl(projs_train2, sector1, show_na = TRUE) # 99 levels
tabyl(projs_train2, sector_f, show_na = TRUE) # 7 levels
#tabyl(projs_train2, tok_sector_broad, show_na = TRUE) # 7 levels
```



# 4. & 5.  model - Multiclass outcome `sector_f`

## [🙃] Multiclass outcome   

<!-- [smltar 7.6](https://smltar.com/mlclassification#mlmulticlass)  -->
<!-- [ANdrew Couch](https://www.google.com/url?q=https://youtu.be/ntk_1xLQBwo?si%3DKLyDqum-02TvQFt-&source=gmail-imap&ust=1729273003000000&usg=AOvVaw2U7DIV1lwHfclSEeNVc0AM) -->

### 2) Split sample (based on `sector_f`)
This time I need to create a new split of the data using `initial_split()` based on levels of `sector_f` (here I collapsed the original 99 levels into 7 macro levels.

We will use the `strata` argument to stratify the data by the outcome variable (`sector_f`). This will ensure that the *training* and *validation* sets have the same proportion.

```{r}
# Create a stratified split based on missing vs non-missing env_cat
projs_train2 %>% tabyl(sector_f) # 7 levels

# Split BUT only "Not Missing" `env_cat_f` 
## --- 0) THIS WILL BE 4 TRAINING & VALIDATION 
sec_use <- projs_train2 %>% 
   filter(sector_f != "Missing") # 4316 proj 

# SPLIT INTO TRAINING, VALIDATION 
set.seed(123)  # Ensure reproducibility
sec_split <- initial_split(sec_use, prop = 0.7, # 70% training, 30% testing
                       strata = sector_f) # stratify by OUTCOME 

## -- 1) for training (labelled `sector_f`)
sec_train <- training(sec_split)   # 3019 proj
    
## -- 2) for validation (labelled `sector_f`)
sec_test <- testing(sec_split)  # 1297 proj
   
# # UNLABELLED PORTION 
## -- 3) for actual test (UNlabelled `sector_f`)
sec_missing <- projs_train2 %>% 
  filter(sector_f == "Missing") # 87 proj 

# check ditribution of `sector_f` in training and validation
tabyl(sec_train, sector_f) |> adorn_totals("row") |> adorn_pct_formatting(digits = 1)# 
tabyl(sec_test, sector_f)|> adorn_totals("row") |> adorn_pct_formatting(digits = 1)# 
```

There is no terrible imbalance between the levels of `sector_f` in the training and validation sets. However, Compared to binary classification, there are several additional issues to keep in mind when working with multiclass classification:

+ Many machine learning algorithms do not handle imbalanced data well and are likely to have a hard time predicting minority classes.
+ Not all machine learning algorithms are built for multiclass classification at all.
+ Many evaluation metrics need to be reformulated to describe multiclass predictions.

### 3) Preprocessing steps [`multi_rec`] (same)
we have added `step_downsample(sector_f)` to the end of the recipe specification to downsample after all the text preprocessing. 
+ We want to *downsample last* so that we still generate features on the full training data set. 
+ The downsampling will then only affect the modeling step, not the preprocessing steps, with hopefully better results.

```{r}
multi_rec <- recipe(
   #sector_f ~ pdo, data = sec_train) %>%
   sector_f ~ pdo + regionname + FY_appr + env_cat_f, data = sec_train) %>%
   step_tokenize(pdo) %>%  
   step_stopwords(pdo, custom_stopword_source = stop_vector) %>%  
   step_tokenfilter(pdo, max_tokens = 100) %>%  
   step_tfidf(pdo, smooth_idf = FALSE) %>%
   # add NA as special factor level
   step_unknown(regionname ,new_level = "Unknown reg" ) %>%
   step_unknown(FY_appr ,new_level = "Unknown FY" ) %>%
   step_unknown(env_cat_f ,new_level = "Unknown env cat" ) %>%
   # convert to dummy variables
   step_dummy(regionname, FY_appr,env_cat_f, one_hot = TRUE) %>% 
   # resolve imnbalance
   step_downsample(sector_f) 
```


### --- COULD do `step_word_embeddings` to see a diffrernent 


check what changed... 
```{r}
# prep and juice the recipe
multi_juice <- multi_rec %>% 
   prep() %>% 
   #bake(new_data = NULL)
   juice()

# preview the baked recipe
dim(multi_juice)
#[1] 559 142
slice_head(multi_juice, n = 3)
```


### 4.a) Specify models (NEW!)

### --- i) Multin. Lasso Logistic Regress [`logistic_model`]
Some model algorithms and computational engines (examples are most random forests and SVMs) automatically detect when we perform multiclass classification from the number of classes in the outcome variable and do not require any changes to our model specification. For lasso regularization, we need to create a new special model specification just for the multiclass class using `multinom_reg()`.
```{r}
# MULTINOMIAL LASSO REGRESSION MODEL 
# For lasso regularization, we need to create a new special model specification just for the multiclass class
logistic_model <- multinom_reg(penalty = tune(), mixture = 1) %>%
  set_mode("classification") %>%
  set_engine("glmnet")

logistic_model
```


### --- ii) KNN [`knn_model`]
```{r}
knn_model <- nearest_neighbor(neighbors = tune()) %>%
  set_engine("kknn") %>%
  set_mode("classification")
```

### 4.b) Specify workflows (NEW!)

### --- i) Multin. Lasso Logistic Regress [`logistic_wf`]
To keep our text data sparse throughout modeling and use the sparse capabilities of set_engine("glmnet"), we need to explicitly set a non-default preprocessing blueprint, using the package `hardhat`. This *“blueprint”* lets us specify during modeling how we want our data passed around from the preprocessing into the model. The composition "dgCMatrix" is the most common sparse matrix type, from the `Matrix` package, used in R for modeling. We can use this blueprint argument when we add our recipe to our modeling workflow, to define how the data should be passed into the model.

```{r}
library(hardhat)
sparse_bp <- default_recipe_blueprint(composition = "dgCMatrix")

# workflow
logistic_wf <- workflow() %>%
  add_recipe(multi_rec, blueprint = sparse_bp) %>%
  add_model(logistic_model)

logistic_wf
```

### --- ii) KNN [`knn_wf`] 

```{r}
library(hardhat)
#sparse_bp <- default_recipe_blueprint(composition = "dgCMatrix")

# workflow
knn_wf <- workflow() %>%
  add_recipe(multi_rec, blueprint = sparse_bp) %>%
  add_model(knn_model)

knn_wf
```

### 5) Hyperparameters tuning
We used the same arguments for *penalty* and *mixture* as before, as well as the same mode and engine, but this model specification is set up to handle **more than just two classes**.

### --- folds for cross-validation
We also need a new `cross-validation object` since we are using a *different data set*.
```{r}
set.seed(123)
# random splits ("folds") of the data for cross-validation
multi_folds <- vfold_cv(sec_train)
```

### --- Define Grids
The last time we tuned a lasso model, we used the defaults for the penalty parameter and 30 levels. Let’s *restrict the values* this time using the range argument, so we don’t test out as small values for regularization, and only try 20 levels.

+ `grid_regular()` chooses sensible values to try for the penalty parameter, based on the range we provide - we ask for 20 different possible values.

### --- i) Multin. Lasso Logistic Regress [`logistic_grid`]

<!-- ```{r} -->
<!-- smaller_grid <- grid_regular( -->
<!--    penalty(range = c(-5, 0)), levels = 20) -->
<!-- ``` -->

```{r}
logistic_grid <- grid_regular(hardhat::extract_parameter_set_dials(logistic_model), levels = 10)
```

### --- ii) KNN [`knn_grid`] 
```{r}
knn_grid <- grid_regular(hardhat::extract_parameter_set_dials(knn_model), levels = 10, 
                         filter = c( neighbors >1))
```

### --- Define tuning process
Multiclass support for the parameters
```{r}
model_control <- control_grid(save_pred = TRUE)
model_metrics <- metric_set( accuracy, sens, spec,  mn_log_loss, roc_auc)
```


Now we have everything we need to tune the *regularization penalty* and find an appropriate value. 

+ `tune_grid()` can fit a model at each of the values for the regularization penalty in our regular grid.
+ Note that we specify `save_pred = TRUE`so we can create ROC curves and a confusion matrix later.

### --- i) Multin. Lasso Logistic Regress [`logistic_model`]

```{r}
# Tune model 
multi_lasso_rs <- tune_grid(
   # A) unfitted workflow
   # logistic_wf,
   # B) model specification
   logistic_model,
   # B) recipe
   multi_rec,
   grid = logistic_grid,
   metrics = model_metrics, # pre defined metrics
   control = model_control, # pre defined control
   # folds for cross-validation
   resamples = multi_folds)

multi_lasso_rs
```

### --- ii) KNN [`knn_grid`] 
```{r}
# Tune model 
multi_knn_rs <- tune_grid(
   # A) unfitted workflow
   # logistic_wf,
   # B) model specification
    knn_model,
   # B) recipe
   multi_rec,
   grid = knn_grid,
   metrics = model_metrics, # pre defined metrics
   control = model_control, # pre defined control
   # folds for cross-validation
   resamples = multi_folds)

multi_knn_rs
```

### 6) Evaluate the model performance & upd WF
### --- Accuracy metric
What do we see, in terms of performance metrics?
```{r}
multi_lasso_rs %>%  collect_metrics()  
multi_knn_rs %>%  collect_metrics()  

multi_lasso_rs %>%  show_best(metric = "roc_auc")   # better 
multi_knn_rs %>%  show_best(metric ="roc_auc")  

multi_lasso_rs %>%  show_best(metric = "accuracy")   # better 
multi_knn_rs %>%  show_best(metric ="accuracy")  
```

Even the very best *"accuracy"* value here is quite low (0.44), *significantly lower* than the binary classification model. This is expected because multiclass classification is more difficult than binary classification.

### --- [FIG] performance metrics

### --- i) Multin. Lasso Logistic Regress [`logistic_model`]
```{r}
autoplot(multi_lasso_rs) +
  labs(
    color = "Number of tokens",
    title = "Multiclass Lasso Logistic Regression performance across regularization penalties and tokens"
  )
```

### --- ii) KNN [`knn_grid`]  
```{r}
autoplot(multi_knn_rs) +
  labs(
    color = "Number of tokens",
    title = "Knn performance across regularization penalties and tokens"
  )
```

### --- choose final hyperparameters
### --- i) Multin. Lasso Logistic Regress [`log_final_param`]
 
### --- TIDY conf matrix
```{r}
log_final_param <- multi_lasso_rs %>% 
   show_best(metric = "accuracy") %>% 
   slice(1) %>% 
   select(-.config) 
```

```{r}
multi_lasso_rs %>% 
   collect_predictions() %>%
   inner_join(log_final_param) %>%
   conf_mat(truth = sector_f, estimate = .pred_class)  

```

### --- [FIG] conf matrix (1st fold)

To get a more detailed view of how our classifier is performing, let us look at one of the `confusion matrices` in 

<!-- ```{r} -->
<!-- choose_acc <- multi_lasso_rs %>% -->
<!--    select_by_pct_loss(metric = "accuracy", -penalty) -->

<!-- choose_acc -->
<!-- ``` -->

```{r}
final_penalty <- multi_lasso_rs %>% 
   show_best(metric = "accuracy") %>% 
   slice(1) %>% 
   pull(penalty)


multi_lasso_rs %>% 
   collect_predictions() %>%
   filter(penalty == final_penalty) %>%
   filter(id == "Fold01") %>%
   conf_mat(truth = sector_f, estimate = .pred_class) %>%
   autoplot(type = "heatmap") +
   scale_y_discrete(labels = function(x) str_wrap(x, 20)) +
   scale_x_discrete(labels = function(x) str_wrap(x, 20))
```

+ The *diagonal* is fairly well populated, which is a good sign. This means that the model generally predicted the right class. 
+ The **off-diagonal numbers** are all the failures and where we should direct our focus. It is a little hard to see these cases well since the majority class affects the scale. A trick to deal with this problem is to remove all the correctly predicted observations.

### --- [FIG] conf matrix (1st fold) only wrong pred
```{r}
multi_lasso_rs %>% 
   collect_predictions() %>%
   # final param 
   filter(penalty == final_penalty) %>%
   filter(id == "Fold01") %>%
   # exclude correct 
   filter(.pred_class != sector_f) %>%
   conf_mat(truth = sector_f, estimate = .pred_class) %>%
   autoplot(type = "heatmap", values = "prop") +
   scale_y_discrete(labels = function(x) str_wrap(x, 20)) +
   scale_x_discrete(labels = function(x) str_wrap(x, 20))

```
Here it is more clear where the model breaks down: **"Institutional Supp."** which makes sense because it was often an agency created FOR a sector!

### --- ii) KNN [`knn_final_param`] 


### --- TIDY conf matrix

```{r}
knn_final_param <- multi_knn_rs %>% 
   show_best(metric = "accuracy") %>% 
   slice(1)  
 
multi_knn_rs %>% 
   collect_predictions() %>%
   inner_join(knn_final_param) %>%
   conf_mat(truth = sector_f, estimate = .pred_class)  
```

### --- [FIG] conf matrix (1st fold)
To get a more detailed view of how our classifier is performing, let us look at one of the `confusion matrices` in 
```{r}
neighbors_final <- multi_knn_rs %>% 
   show_best(metric = "accuracy") %>% 
   slice(1) %>% 
   pull(neighbors)

multi_knn_rs %>% 
   collect_predictions() %>%
   # final param 
   filter(neighbors == neighbors_final) %>%
   filter(id == "Fold01") %>%
   conf_mat(truth = sector_f, estimate = .pred_class) %>%
   autoplot(type = "heatmap") +
   scale_y_discrete(labels = function(x) str_wrap(x, 20)) +
   scale_x_discrete(labels = function(x) str_wrap(x, 20))
```

To get a more detailed view of how our classifier is performing, let us look at one of the `confusion matrices` in 
```{r}
multi_knn_rs %>% 
   collect_predictions() %>%
   # final param 
   filter(neighbors == neighbors_final) %>%
   filter(id == "Fold01") %>%
   # exclude correct 
   filter(.pred_class != sector_f) %>%
    conf_mat(truth = sector_f, estimate = .pred_class) %>%
   autoplot(type = "heatmap") +
   scale_y_discrete(labels = function(x) str_wrap(x, 20)) +
   scale_x_discrete(labels = function(x) str_wrap(x, 20))
```

### 6.b) Finalize the workflow
This I do only for the best model 

### --- Finalize workflow 
After we have those parameters (`final_penalty`), penalty {and max_tokens}, we can finalize our earlier tunable workflow, by updating it with this value.

```{r}
logistic_wf_final <- logistic_wf %>%
   # here it wants a tibble 
   finalize_workflow(parameters =  log_final_param )
```

### 7) Assess the model performance on training set

### --- See the results  [`multi_fit`] 
Here we access the model coefficients to see which features are most important in the model
+ We see here, for the penalty we chose, what terms contribute the most to a en cat NOT being high risk. 
```{r}
# Fit the model to the training data
multi_fit <- fit (logistic_wf_final, data = sec_train)
```

### --- Coefficients  [`multi_fitted_coeff`] 
```{r}
multi_fitted_coeff <- multi_fit %>% 
   extract_fit_parsnip() %>% 
   tidy( penalty = final_penalty) %>% 
   arrange(-estimate)

multi_fitted_coeff

```

In this model, it seems that words contained in PDO are the most influential in predicting the sector of a project.

+ "tfidf_pdo_education" appear among the top coefficients!!!
+ "tfidf_pdo_health " appear among the top coefficients!!!


### 8) Evaluate the best model on the test set [`last_fit()`]
 
We can now fit this finalized workflow on training data and finally return to our *testing data*.

+ `last_fit()` emulates the process where, after determining the best model, the final fit on the entire training set is needed and is then evaluated on the test set

### --- metrics
```{r}
multi_final_fitted <- last_fit(logistic_wf_final, sec_split)

collect_metrics(multi_final_fitted)
#1 accuracy    multiclass     0.448 Preprocessor1_Model1
#2 roc_auc     hand_till      0.800Preprocessor1_Model1
#3 brier_class multiclass     0.367 Preprocessor1_Model1
```

### --- predictions
```{r}
multi_final_fitted %>%
   collect_predictions() %>%
   conf_mat(truth = sector_f, estimate = .pred_class)
```

### --- [FIG] confusion matrix
```{r}
multi_final_fitted %>%
   collect_predictions() %>%
   conf_mat(truth = sector_f, estimate = .pred_class) %>%
   autoplot(type = "heatmap") +
   scale_y_discrete(labels = function(x) str_wrap(x, 20)) +
   scale_x_discrete(labels = function(x) str_wrap(x, 20)) +
   theme(axis.text.x = element_text(angle = 30, hjust = 1))
```

### --- [FIG] ROC curve
```{r}
pred_tibble <- multi_final_fitted %>%
   collect_predictions()

# Assuming you have your results (with columns for true classes and predicted probabilities)
results <- pred_tibble # e.g., tibble with true and predicted columns
paint(results)

# Compute ROC curve (micro-average)
roc_curve(results, truth = sector_f, 
          '.pred_AGR FOR FISH',
          '.pred_EDUCATION'           ,
          '.pred_ENERGY'              ,
          '.pred_FINANCIAL'           ,
          '.pred_HEALTH'              ,
          '.pred_ICT'                 ,
          '.pred_IND TRADE SERV'      ,
          '.pred_INSTIT. SUPP.' ,
          '.pred_MINING OIL&GAS'      ,
          '.pred_SOCIAL PROT.'   ,
          '.pred_TRANSPORT'           ,
          '.pred_URBAN'               ,
          '.pred_WAT & SAN'           
) %>%
   autoplot()

```

It makes sense that the sectors' levels in which I have collapsed more things and/or have a more "blurred" definitions are the ones that are harder to predict: 

+ "URBAN" 
+ "IND TRADE SERV"
+ "INSTIT. SUPP." 
+ "ICT" (?) 

### 9) Interpret the model
The output of `last_fit()` (`multi_final_fitted`) also contains a fitted model (a workflow, to be more specific) that has been trained on the training data. We can use the vip package to understand what the most important variables are in the predictions

```{r}
multi_final_fitted$.workflow
multi_final_fitted$.metrics
multi_final_fitted$.predictions 
# N of observation is the same as the `sec_test` set 1267 
nrow(multi_final_fitted$.predictions[[1]])
```

### --- Inspecting what levels of the outcome are most difficult to estimate 
```{r}
# collect the predictions from the final model
multi_final_fit_feat <- multi_final_fitted %>%
   collect_predictions() %>%
   bind_cols(sec_test)  %>%
   rename(sector_f = 'sector_f...17') %>% 
   select ( -'sector_f...46')

#preview the predictions
glimpse(multi_final_fit_feat)
```

### --- Inspecting the most important features for predicting the outcome 
We can use the vip package to understand what the most important variables are in the predictions

+ using the `extract_fit_parsnip()` function from the `workflows` package to extract the model object from the workflow. 

A quick way to extract the most important features for predicting each out￾come is to use the `vi()` function from {`vip`}. 

+ The `vi()` function calculates the permutation **importance** of each feature in the model.

```{r}
library(vip)
sector_feat_imp <- extract_fit_parsnip(multi_final_fitted$.workflow[[1]]) %>%
  vi(lambda = final_penalty) %>% 
  mutate(
    Sign = case_when(Sign == "POS" ~ "More important for predicting a given sector", 
                     Sign == "NEG" ~ "Less important for predicting a given sector",
                     TRUE ~ "Neutral"),
    Importance = abs(Importance))   %>% 
   #filter(!str_detect(Variable, "FY_appr"))  %>%
   filter (Importance != 0.00000000)
```

### --- [FIG] Variable importance for predicting the sector category
```{r}
#| output: TRUE
sector_feat_imp %>% group_by(Sign) %>%
   top_n(30, Importance) %>%
   mutate(Variable =  str_replace(Variable, "tfidf_pdo_", "PDO_"),
          Variable =  str_replace(Variable, "env_cat_f", "ENVCAT_"),
          Variable =  str_replace(Variable, "regionname_", "REG_"),
          Variable =  str_replace(Variable, pattern = "boardapprovalFY_X" , replacement = "APPR_FY_")
          ) %>% 
   ungroup %>%
   ggplot(aes(x = Importance,
              y = fct_reorder(Variable, Importance),
              fill = Sign
   )) +
   geom_col(show.legend = FALSE) +
   scale_x_continuous(expand = c(0, 0)) +
   facet_wrap(~Sign, scales = "free", ncol = 2) +
   labs(
      y = NULL,
      title = "Variable importance for predicting the sector category",
      #subtitle = paste0("These features are the most important in predicting a given class" )
   )

```

### --- Misclassified sector 
We can gain some final insight into our model by looking at observations from the test set that it misclassified. 
I will then select the columns with the actual outcome (`sector_f`), and compare with the predicted sector: What was predicted wrong? 

```{r}
# solo sbagliate 
compare_true_pred <- multi_final_fit_feat %>%
   filter(sector_f != .pred_class ) %>%  
   select(.pred_class, sector_f,sector1,  proj_id , env_cat_f, FY_appr , regionname, pdo )
```

Let's see for example cases labelled "AGR FOR FISH" but misclassified
```{r}
check_AGR <- compare_true_pred %>% 
   filter(.pred_class == "AGR FOR FISH") %>%
   select(.pred_class, sector_f, sector1,  proj_id , env_cat_f, FY_appr , regionname, pdo ) %>%    slice_sample(n = 5)

 
```

> Interesting: they all have very brief PDO ! 

```{r}
check_FIN <- compare_true_pred %>% 
   filter(.pred_class == "FINANCIAL") %>%
   select(.pred_class, sector_f, sector1,  proj_id , env_cat_f, FY_appr , regionname, pdo ) %>%
   slice_sample(n = 5)

```

> same !!! 

# ____________________
# SET OF MODELS      
# ____________________

What if I want to see them all together? 

[Spiega `workflowsets`](https://workflowsets.tidymodels.org/articles/tuning-and-comparing-models.html)


# RENDER this

```{bash}
#| eval: false
quarto render analysis/01c_WB_project_pdo_feat_class.qmd --to html
open ./docs/analysis/01c_WB_project_pdo_feat_class.html
```

