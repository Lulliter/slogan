---
title: "WB Project Data Preprocessing"
author: "Luisa M. Mimmi"
date: "Last run: `r format(Sys.time(), '%F')`"
lang: en
editor: source
engine: knitr
## ------  general Output Options
execute:     
  eval: false    # actually run? 
  echo: true     #  include source code in output
  warning: false  #  include warning code in output
  error: false    #  include error code in output
  output: false   # include output code in output (CHG in BLOCKS)
  # include: false   # R still runs but code and results DON"T appear in output  
  cache: false # normalmnte false
toc: true
fig-cap-location: top
tbl-cap-location: top
format:
  html:
    # theme: flatly #spacelab
    code-fold: false # redundant bc echo false 
    toc-depth: 2
    toc_float: true
    toc-location: left
    toc-title: Outline
    embed-resources: true # external dependencies embedded (Not in ..._files/)
  # pdf:
  #   toc-depth: 2
  #   toc-title: Indice
  #   highlight-style: github
  #   #lang: it
  #   embed-resources: true # external dependencies embedded (Not in ..._files/)
format-links: false
bibliography: ../bib/slogan.bib
---

<i class="fa fa-refresh" style="color: firebrick"></i> Work in progress


```{r setup_inp}
# Pckgs -------------------------------------
if (!require ("pacman")) (install.packages("pacman"))
p_load(tidyverse, 
       janitor, skimr,
       here, paint,
       readxl,
       repurrrsive, # examples of recursive lists
       listviewer, # provides an interactive method for viewing the structure of a list.
       httr, jsonlite, XML,xml2, 
       oai, # R client to work with OAI-PMH 
       citr,
       fs)
```

### ----------------------------------------------------------------------------

# Data sources

# WB Projects & Operations
**World Bank Projects & Operations** can be explored at: 

1. [Data Catalog](https://datacatalog.worldbank.org/search/dataset/0037800> <https://datacatalog.worldbank.org/search/dataset/0037800/World-Bank-Projects---Operations). From which 
  + [API for projects](https://search.worldbank.org/api/v3/projects) 

2. [Advanced Search](https://projects.worldbank.org/en/projects-operations/project-search) 

-   Accessibility Classification: **public** under [Creative Commons Attribution 4.0](https://datacatalog.worldbank.org/public-licenses?fragment=cc)

- esempio <https://datacatalog.worldbank.org/search/dataset/0037800> <https://datacatalog.worldbank.org/search/dataset/0037800/World-Bank-Projects---Operations>

 
## Raw data


<!-- EXE  -->
<!-- https://projects.worldbank.org/en/projects-operations/projects-list?str_fiscalyear=1979&end_fiscalyear=1979&os=0 -->
<!-- https://projects.worldbank.org/en/projects-operations/projects-list?str_fiscalyear=2023&end_fiscalyear=2023&os=0 -->

### ---------------------------------------------------------------------------

## Attempt # 1 {Ingest Projects data (via API)}
 
> DOESN'T WORK! 

```{r}
#| eval: false
#| echo: false

# DONE WITH CHAT GPT 
# Load necessary libraries
library(httr)
library(jsonlite)
library(dplyr)

# Define API parameters
base_url <- "https://search.worldbank.org/api/v3/projects"
start_year <- 2001
end_year <- 2025
rows_per_page <- 500
output_dir <- "data/raw_data/project2"  # Output directory for the files

# Ensure the directory exists
if (!dir.exists(output_dir)) {
  dir.create(output_dir, recursive = TRUE)
}

# Function to fetch and save data for a specific year
fetch_and_save_data_year <- function(year) {
  cat("Fetching data for fiscal year:", year, "\n")
  
  # Simulate data fetching (replace with actual data fetching logic)
  data <- tryCatch({
    # Replace this with the actual data fetching logic
    # Example: data <- fetch_data_from_api(year)
    data <- list(id = 1:2, theme_list = 1:3) # Simulated incorrect data
    if (length(data$id) != length(data$theme_list)) {
      stop("Data size mismatch")
    }
    data
  }, error = function(e) {
    cat("Failed to retrieve data for year", year, ". Error:", e$message, "\n")
    return(NULL)
  })
  
  if (is.null(data)) {
    return(NULL)
  }
  
  year_df <- bind_rows(lapply(data, bind_rows))
  
  # Save the data frame to a CSV file
  output_file_path <- file.path(output_dir, paste0("world_bank_projects_", year, ".csv"))
  write.csv(year_df, output_file_path, row.names = FALSE)
  cat("Data for year", year, "saved to", output_file_path, "\n")
}

# Loop through each year and process data
lapply(start_year:end_year, function(year) {
  cat("Processing data for fiscal year:", year, "\n")
  fetch_and_save_data_year(year)
})


```
 


## Attempt # 2.a {Ingest Projects data (manually split)}

I retrieved manually ALL WB projects approved *between FY 1973 and 2023 (last FY incomplete)* on 09/22/2022 (WDRs go from 1978-2022)
using this [example url](https://projects.worldbank.org/en/projects-operations/projects-list?str_fiscalyear=1979&end_fiscalyear=1979&os=0) and saved individual `.xlsx` files in `data/raw_data/project`

+ note the manual download is limited to # = 500

### --- Load all `.xlsx` files separately 
<!-- Following https://martinctc.github.io/blog/vignette-write-and-read-multiple-excel-files-with-purrr/ -->

```{r}
#| eval: false
#| echo: false

# --- define directory path
file_path <- here("data","raw_data", "projects/")

# --- get a character vector of the names of files
file_path %>% list.files(. , 
                         pattern = ".xlsx",
                         full.names = FALSE) -> xlsx_file_names
# # or 
# file_path %>% list.files() %>% .[str_detect(., ".xlsx")] -> csv_file_names

# check
class(xlsx_file_names)

# ---- Load everything into the Global Environment
xlsx_file_names %>%
  purrr::map(function(file_name){ # iterate through each file name
  assign(x = str_remove(file_name, "_downloaded_9_22_2022.xlsx"), # Remove file extension 
         value = readxl::read_excel(paste0(file_path, file_name), skip = 1),
         envir = .GlobalEnv)
})
```

### --- Save objs in folder as `.Rds` files separately 

```{r}
#| eval: false
#| echo: false
# [inputs] 2 write as .Rds 
list_objects  <-  ls(pattern = "*WB_Projects*" ) # list newly created objects 
list_objects
rdsFilesFolder <- fs::path("./data/raw_data/projects")
saveRDSobjects <- paste0(rdsFilesFolder, "/", list_objects, ".Rds")

# [function] write as .Rds 
for (i in seq_along(list_objects)) {
  base::saveRDS(get(list_objects[i]), file = saveRDSobjects[i])
}
```

```{r}
#| eval: false
#| echo: false
str(object = '1973_WB_Projects_1_500')
```


## Attempt # 2.b Ingest Projects data (manually all together)!

+ I retrieved manually ALL WB projects approved *between FY 1947 and 2026* as of *31/08/2024* using simply the `Excel button` on this page [WBG Projects](https://projects.worldbank.org/en/projects-operations/projects-list?str_fiscalyear=1979&end_fiscalyear=1979&os=0) 

+ then saved HUUUGE `.xls` files in `data/raw_data/project2/all_projects_as_of29ago2024.xls`
  + (plus a `Rdata` copy of the original file )

```{r}
all_projects_as_of29ago2024   <- read_excel("data/raw_data/project2/all_projects_as_of29ago2024.xls", 
                         col_names = FALSE,
                         skip = 1) 
# Nomi delle colonne
cnames <- read_excel("data/raw_data/project2/all_projects_as_of29ago2024.xls", 
                         col_names = FALSE,
                         skip = 1,
                     n_max = 2) 
# file completo
all_proj <- read_excel("data/raw_data/project2/all_projects_as_of29ago2024.xls", 
                         col_names = TRUE,
                         skip = 2) 

save(all_proj, file = "data/raw_data/project2/all_projects_as_of29ago2024.Rdata")  

rm(all_projects_as_of29ago2024)
```
 

# Explore Project mega file 

```{r}
paint::paint(cnames)
paint::paint(all_proj)
 
skimr::skim(all_proj$pdo)  # complete_rate 0.503
skimr::skim(all_proj$boardapprovaldate) # complete_rate 0.779
skimr::skim(all_proj$closingdate) # complete_rate 0.707 
```

# Clean all_proj

This data set has a lot of blank values, probably also bc some information was not collected way back in 1947... (e.g. PDO) 

```{r}
# Mess of data format weird in different ways in 2 cols: 
    # 1947-12-31 12:00:00 # closingdate
    # 8/3/1948 12:00:00 AM # closingdate
    # 
    # 1955-03-15T00:00:00Z # boardapprovaldate
 
# Mutate the date columns to parse the dates, handling different formats and blanks
all_proj_t <- all_proj %>%
   # 1) Parsed using parse_date_time() with mdy HMS and mdy HMSp to handle "MM/DD/YYYY HH:MM AM/PM" formats.
  mutate(across("closingdate", ~ if_else(
    . == "", 
    NA_POSIXct_,  # Return NA for blank entries
    parse_date_time(., orders = c("mdy HMS", "mdy HMSp"))
  )),
    # 2) Parsed using ymd_hms() because it follows the ISO 8601 format (e.g., "1952-04-29T00:00:00Z").
  across("boardapprovaldate", ~ if_else(
    . == "", 
    NA_POSIXct_,  # Return NA for blank entries
    ymd_hms(., tz = "UTC")  # Handle ISO 8601 format (e.g., "1952-04-29T00:00:00Z")
  ))) %>% 
   mutate(boardapproval_year = year(boardapprovaldate),
          boardapproval_month = month(boardapprovaldate)) %>% 
   mutate(boardapprovalFY = case_when( 
             boardapproval_month >= 1 & boardapproval_month < 7 ~ boardapproval_year,
             boardapproval_month >= 7 & boardapproval_month <= 12 ~ boardapproval_year +1)) %>% 
   relocate(boardapprovalFY, .after = boardapprovaldate ) %>% 
   mutate(closingdate_year = year(closingdate),
          closingdate_month = month(closingdate)) %>% 
   mutate(closingdateFY = case_when( 
             closingdate_month >= 1 & closingdate_month < 7 ~ closingdate_year,
             closingdate_month >= 7 & closingdate_month <= 12 ~ closingdate_year +1)) %>% 
   relocate(closingdateFY, .after = closingdate ) 

tabyl(all_proj$closingdate)   
tabyl(all_proj_t$closingdateFY)

tabyl(all_proj$boardapprovaldate)   
tabyl(all_proj_t$boardapprovalFY)

```

# Explore who are the ones with no PDO 
```{r}
# Function to count missing values in a subset of columns
count_missing_values <- function(data, columns) {
  # Select the subset of columns
  df_subset <- data %>% select(all_of(columns))
  
  # Use skimr to skim the data
  skimmed <- skim(df_subset)
  
  # Extract the relevant columns for column names and missing values
  missing_table <- skimmed %>%
    select(skim_variable, n_missing)
  
  # Return the table
  return(missing_table)
}

# Use the function on a subset of columns
count_missing_values(all_proj_t, c("pdo", "projectstatusdisplay", "boardapprovalFY", "sector1", "theme1"))

missing_pdo <- all_proj_t %>% 
   #select(id, pdo, countryname, projectstatusdisplay, lendinginstr, boardapprovalFY, projectfinancialtype) %>% 
   filter(is.na(pdo))

# Now I compare to get a sense of distribution in all_proj_t v. missing_pdo... 
tabyl(all_proj_t$projectstatusdisplay) %>%  adorn_pct_formatting()
tabyl(missing_pdo$projectstatusdisplay) %>%  adorn_pct_formatting()

tabyl(all_proj_t$regionname)  %>% adorn_pct_formatting() 
tabyl(missing_pdo$regionname)  %>% adorn_pct_formatting() 

tabyl(all_proj_t$boardapprovalFY) %>%  adorn_pct_formatting()
tabyl(missing_pdo$boardapprovalFY) %>%  adorn_pct_formatting()

tabyl(all_proj_t$projectfinancialtype)  %>% adorn_pct_formatting() 
tabyl(missing_pdo$projectfinancialtype)  %>% adorn_pct_formatting() 

tabyl(all_proj_t$sector1)  %>% adorn_pct_formatting() 
tabyl(missing_pdo$sector1)  %>% adorn_pct_formatting() 

tabyl(all_proj_t$theme1)  %>% adorn_pct_formatting() 
tabyl(missing_pdo$theme1)  %>% adorn_pct_formatting() # most NA

#Environmental Assessment Category
tabyl(all_proj_t$envassesmentcategorycode)  %>% adorn_pct_formatting() # most NA
tabyl(missing_pdo$envassesmentcategorycode)  %>% adorn_pct_formatting() 
# Environmental and Social Risk
tabyl(all_proj_t$esrc_ovrl_risk_rate)  %>% adorn_pct_formatting() # most NA
tabyl(missing_pdo$esrc_ovrl_risk_rate)  %>% adorn_pct_formatting() 

tabyl(all_proj_t$lendinginstr)  %>% adorn_pct_formatting()  # Specific Investment Loan 4928   43.9%
tabyl(missing_pdo$lendinginstr)  %>% adorn_pct_formatting()  # Specific Investment Loan 4928   43.9%
```

Based on some "critical" category, I would say that even if many projects are missing PDO the incidence seems to happen at random, except maybe for `lendinginstr` **specific Investment Loan** are missing PDO in 4928 pr (43.9%). Why? 

https://stackoverflow.com/questions/71608612/how-to-add-keybindings-in-visual-studio-code-for-the-r-terminal


# REDUCED DATASET of PROJECTS 
For my purposes it is safe to drop all the projects with missing PDO !

+ it turns out there are no **Development objectives** spelled out until FY2001

```{r}
projs <- all_proj_t %>% 
   filter(!is.na(pdo)) %>% 
   filter(!is.na(projectstatusdisplay)) %>% 
   filter(boardapprovalFY < 2024 & boardapprovalFY >1972)  %>% 
   select(id, pr_name = project_name, pdo, boardapprovalFY, closingdateFY,status = projectstatusdisplay, regionname, countryname, sector1, theme1 ,
   lendinginstr,env_cat = envassesmentcategorycode, ESrisk = esrc_ovrl_risk_rate ,curr_total_commitment  )   


tabyl(projs$boardapprovalFY)  %>% adorn_pct_formatting() # most NA

nrow(projs) # 8836 
paint(cnames) 
paint(projs) 
```
 

# _______ 
# >>>>>> QUI <<<<<<<<<<<<<<<<<< 
rivedere cos'avevo fatto x pulire in `analysis//03_WDR_abstracs_explor.qmd` 


# _______ 

# World Development Reports (WRDs)

-   DATA <https://datacatalog.worldbank.org/search/dataset/0037800>
-   INSTRUCTIONS <https://documents.worldbank.org/en/publication/documents-reports/api>
-   Following: [@kaye_ella_2019; @robinson_words_2017; @robinson_1_2022]


```{r}
# Function(s) -------------------------------------
source(here::here ("R", "f_scrape_WB-OKR.R") )
source(here::here ("R", "turn_list_to_tibble.R") )


```


## Raw data

## OAI-PMH  

**OAI-PMH** (Open Archives Initiative Protocol for Metadata Harvest-ing) services, a protocol developed by the Open Archives Initiative (https://en.wikipedia.org/wiki/Open_Archives_Initiative). OAI-PMH uses XML data format transported over HTTP.

+ packg: Package [`oai`](https://docs.ropensci.org/oai/) is built on xml2 and httr. 
+ paging: OAI-PMH uses (optionally) `resumptionTokens`, with an optional expiration date. These tokens can be used to continue on to the next chunk of data, if the first request did not get to the end.

## 1. World Bank list of World Development Reports
Genreal OKR link [https://openknowledge.worldbank.org/search?spc.page=1&query=%20&scope=3d9bbbf6-c007-5043-b655-04d8a1cfbfb2](https://openknowledge.worldbank.org/search?spc.page=1&query=%20&scope=3d9bbbf6-c007-5043-b655-04d8a1cfbfb2)

https://openknowledge.worldbank.org/entities/publication/5e5ac9f1-71ee-4734-825e-60966658395f
2023 | key takeaways | https://openknowledge.worldbank.org/server/api/core/bitstreams/54de9b54-dc23-43da-9a88-fe94dd5a3c24/content

https://openknowledge.worldbank.org/server/api/core/bitstreams/e1e22749-80c3-50ea-b7e1-8bc332d0c2ff/content

 + **World Development Report (WDR)**;  
   + (in 2022) https://openknowledge.worldbank.org/handle/10986/2124
   + (in 2024) https://openknowledge.worldbank.org/collections/3d9bbbf6-c007-5043-b655-04d8a1cfbfb2?spc.sf=dc.date.issued&spc.sd=DESC 
   + (in 2024) https://openknowledge.worldbank.org/collections/3d9bbbf6-c007-5043-b655-04d8a1cfbfb2?spc.sf=dc.date.issued&spc.sd=DESC&f.supportedlanguage=en,equals&spc.page=1&spc.rpp=100




### ---------------------------------------------------------------------------

 
```{r}
install.packages("rvest")
library(rvest)
```
 
```{r}
link2023 <- "https://openknowledge.worldbank.org/entities/publication/5e5ac9f1-71ee-4734-825e-60966658395f/full"
WDR2023 <- read_html(link2023)

WDR2023  %>%
  html_elements(xpath = "")
```

 

## 1. World Bank list of World Development Reports

Here I want to extract documents metadata from WBG OKR repository. 

Which metadata:
  +  collections: "Books"                    https://openknowledge.worldbank.org/handle/10986/4
    + sub-collections: "Corporate Flagships" https://openknowledge.worldbank.org/handle/10986/2123
      + **World Development Report (WDR)**;  https://openknowledge.worldbank.org/handle/10986/2124
      https://openknowledge.worldbank.org/collections/3d9bbbf6-c007-5043-b655-04d8a1cfbfb2?spc.sf=dc.date.issued&spc.sd=DESC 
      
      + Global Economic Prospects (GEP), 
      + Doing Business (DB), and 
      + Poverty and Shared Prosperity (PSP).  
      + ...

I can search adding a keyword:           
      + World Development Report (WDR);
        + Keyword = _"economic development"_    
... the url would become: `https://openknowledge.worldbank.org/handle/10986/2124/discover?filtertype=subject&filter_relational_operator=equals&filter=economic+development`
 
# >>>>>> QUI <<<<<<<<<<<<<<<<<< 
### API response  


```{r API-get}
# DATA https://datacatalog.worldbank.org/search/dataset/0037800 
# INSTRUCTIONS https://documents.worldbank.org/en/publication/documents-reports/api

# VARIABLES ----------------------------------------------
# base url for query
base <- "https://openknowledge.worldbank.org/oai/request?verb=ListRecords&metadataPrefix=oai_dc&set=col_10986_"
# base url with resumption token
base0 <- "https://openknowledge.worldbank.org/oai/request?verb=ListRecords&resumptionToken=oai_dc///col_10986_"
 
n <- 45 
subid <-   2124 # WRD 

temp_url <- paste0(base, subid)

# response <- httr::GET(temp_url) # xml_document
# response <- httr::content(response, as="parsed")
response <- xml2::read_xml(x = temp_url, encoding = "",
                           options = "NOBLANKS") # remove blank nodes
class(response)
 
# Parse a url into its component pieces.
xml2::xml_name(response) # [1] "OAI-PMH"
xml2::xml_name(xml_children(response)) # [1] "responseDate" "request"  "ListRecords" 

# print the full path directory:
response %>% xml_find_all( '//*') %>% xml_path()

# check length of id
#======# DON'T KNOW HOW SHE NEW THE NAME #======# 
# "/*/*[3]/*[1]/*[2]/oai_dc:dc/dc:identifier[1]"  
r_id <-  xml2::xml_find_all(response, ".//d1:identifier") %>% xml_text()
l <- length(r_id)
print(paste("length of total list:", l))

# #======#  check length of subject ???  #======# 
# "/*/*[3]/*[1]/*[2]/oai_dc:dc/dc:subject[216]" 
r_subject <-  xml2::xml_find_all(response, ".//d1:subject") %>% xml_text() # empty 
r_subject <-  xml2::xml_find_all(response, ".//subject") %>% xml_text() # empty 


# START NAVIGATION
r_root <- xml2::xml_root(response )   # Returns root node
r_root
r_cdr <- xml2::xml_children(response )   # Access the children nodes
r_cdr
str(r_cdr)

# all records as a list of lists 
r_cdr_l <- xml2::as_list(r_cdr)
str(r_cdr_l)
# only the info in record (l1-of - l45)
r_cdr_l_records <-  r_cdr_l[3]

# only records as a list of 45 
records <-  r_cdr_l_records[[1]] 
records[[1]][["metadata"]][["dc"]][["subject[1]"]]


# name the records 
names(records) <- paste0 ("WRD_", 1:length(records))
records_names <- c(names(records))
class(records_names)
```

### Parsing xml ONE INDICATOR AT A TIME (header)

#### ... (form header) identifier
```{r pars_one }
# # # identifier of record 1 
# # records[["WRD_1"]][["header"]][["identifier"]]
# # # identifier of record 2 
# # records[["WRD_2"]][["header"]][["identifier"]]
# 
# # all identifier 
# 
# # 1) Prepare empty vectors 
# id_l <- vector(mode = "character", length = length(records))
# id_v <- vector(mode = "character", length = length(records))
# # 2) 
# for(i in seq_along(records)) {
#  # extract ALL identifier LIST LIST 
#   id_l[[i]] <- records[[i]][["header"]][["identifier"]]
#   # transform list in vector
#    id_v[[i]] <- unlist(id_l[[i]])
# }
# 
# # 3) 
# id_v_t <- tibble::as_tibble_col(id_v, column_name ="WDR_hd_id")
# df_header_identifier <- id_v_t 
```

#### ... (form header) setSpec
```{r EXTRAPOLATE}
# # 0) set variables 
# meta_item <- "setSpec"
# l <- records
# 
# # 1) Prepare empty output 
# item_l <- list(mode = "character", length = length(l))
# item_v <- vector(mode = "character", length = length(l))
# 
# # 2) ET loop 
# for(i in seq_along(l)) {
#  # extract ALL setSpec LIST LIST 
#   item_l[[i]] <- l[[i]][["header"]][[meta_item]]
#   # transform list in vector
#    item_v[[i]] <- unlist(item_l[[i]])
# }
# # 3) set column name 
# column_name <- paste0("WDR_hd_", meta_item)
# 
# # 4) make it a tibble   
# item_v_t <- as_tibble_col(item_v, column_name =column_name)
# 
# # 5) rename result df 
# # assign("new.name",old.name)
# assign(paste0("df_header_", meta_item),item_v_t)
# 
# df_header_setSpec <- item_v_t # non so se serve a qlc 
```

### Parsing xml ONE INDICATOR AT A TIME (metadata)  
 
+ here I am going to list one level up `records[["WRD_1"]][["metadata"]][["dc"]][["item]]`
+ Problem: multiple repetition of some "items"   

#### ... (form metadata) title
```{r}
# # EG all title(s)
# # records[["WRD_1"]][["metadata"]][["dc"]][["title"]]
# 
# # 1) Prepare empty ouppt 
# item_l <- list(mode = "character", length = length(records))
# item_v <- vector(mode = "character", length = length(records))
# # 2) 
# for(i in seq_along(records)) {
#  # extract ALL identifier LIST LIST 
#   item_l[[i]] <- records[[i]][["metadata"]][["dc"]][["title"]]
#   # transform list in vector
#    item_v[[i]] <- unlist(item_l[[i]])
# }
# column_name <- paste0("WRD_mt_", "title")  
# 
# # 3) 
# item_v_t <- as_tibble_col(item_v, column_name =column_name)
# df_meta_title <- item_v_t 
```

#### ... (form metadata) creator
```{r}
# # EG all creator(s)
# # records[["WRD_1"]][["metadata"]][["dc"]][["creator"]]
# 
# # 0) choose metat_item
# meta_item <- "creator"
# 
# # 1) Prepare empty ouppt 
# item_l <- list(mode = "character", length = length(records))
# item_v <- vector(mode = "character", length = length(records))
# # 2) 
# for(i in seq_along(records)) {
#  # extract ALL identifier LIST LIST 
#   item_l[[i]] <- records[[i]][["metadata"]][["dc"]][[meta_item]]
#   # transform list in vector
#    item_v[[i]] <- unlist(item_l[[i]])
# }
# # 3) set column name 
# column_name <- paste0("WRD_mt_", meta_item)
# 
# # 3) 
# item_v_t <- as_tibble_col(item_v, column_name =column_name)
# df_meta_creator<- item_v_t 
```

#### ... (form metadata) identifier
```{r}
#  # 0) set variables 
# meta_item <- "identifier"
# l <- records
# 
# # 1) Prepare empty output 
# item_l <- list(mode = "character", length = length(l))
# item_l
# item_v <- vector(mode = "character", length = length(l))
# item_v
# # 2) ET loop 
# for(i in seq_along(l)) {
#  # extract ALL identifier LIST LIST 
#   item_l[[i]] <- l[[i]][["metadata"]][["dc"]][[meta_item]]
#   # transform list in vector
#    item_v[[i]] <- unlist(item_l[[i]])
# }
# # 3) set column name 
# column_name <- paste0("WRD_mt_", meta_item)
# 
# # 4) make it a tibble   
# item_v_t <- as_tibble_col(item_v, column_name =column_name)
# 
# # 5) rename result df 
# # assign("new.name",old.name)
# # assign(paste0("df_meta_", meta_item),item_v_t)
# df_meta_identifier <- item_v_t
```

#### ... (form metadata) description 
```{r}
#  # 0) set variables 
# meta_item <- "description"
# l <- records
# 
# # 1) Prepare empty output 
# item_l <- list(mode = "character", length = length(l))
# item_l
# item_v <- vector(mode = "character", length = length(l))
# item_v
# # 2) ET loop 
# for(i in seq_along(l)) {
#  # extract ALL description LIST LIST 
#   item_l[[i]] <- l[[i]][["metadata"]][["dc"]][[meta_item]]
#   # transform list in vector
#    item_v[[i]] <- unlist(item_l[[i]])
# }
# # 3) set column name 
# column_name <- paste0("WRD_mt_", meta_item)
# 
# # 4) make it a tibble   
# item_v_t <- as_tibble_col(item_v, column_name =column_name)
# 
# # 5) rename result df 
# # assign("new.name",old.name)
# assign(paste0("df_meta_", meta_item),item_v_t)
```

#### ... (form metadata) date 

```{r}
#  # 0) set variables 
# meta_item <- "date"
# l <- records
# 
# # 1) Prepare empty output 
# item_l <- list(mode = "character", length = length(l))
# item_l
# item_v <- vector(mode = "character", length = length(l))
# item_v
# # 2) ET loop 
# for(i in seq_along(l)) {
#  # extract ALL date LIST LIST 
#   item_l[[i]] <- l[[i]][["metadata"]][["dc"]][[meta_item]]
#   # transform list in vector
#    item_v[[i]] <- unlist(item_l[[i]])
# }
# # 3) set column name 
# column_name <- paste0("WRD_mt_", meta_item)
# 
# # 4) make it a tibble   
# item_v_t <- as_tibble_col(item_v, column_name =column_name)
# 
# # 5) rename result df 
# # assign("new.name",old.name)
# assign(paste0("df_meta_", meta_item),item_v_t)
```

#### ... (form metadata) subject 

```{r}
#  # 0) set variables 
# meta_item <- "subject"
# l <- records
# 
# # 1) Prepare empty output 
# item_l <- list(mode = "character", length = length(l))
# item_l
# item_v <- vector(mode = "character", length = length(l))
# item_v
# # 2) ET loop 
# for(i in seq_along(l)) {
#  # extract ALL subject LIST LIST 
#   item_l[[i]] <- l[[i]][["metadata"]][["dc"]][[meta_item]]
#   # transform list in vector
#    item_v[[i]] <- unlist(item_l[[i]])
# }
# # 3) set column name 
# column_name <- paste0("WRD_mt_", meta_item)
# 
# # 4) make it a tibble   
# item_v_t <- as_tibble_col(item_v, column_name =column_name)
# 
# # 5) rename result df 
# # assign("new.name",old.name)
# assign(paste0("df_meta_", meta_item),item_v_t)
```

### same but as FUNCTION -> YEP!!!!!!!  

####... ALL from header (func)
```{r callfun_head}
# ---------- FUNCTION to create a column of Titles --> DOES NOT Work! 
# source(here::here ("R", "turn_list_to_tibble.R") )

# ------------- USE FUNCTION  (called INSIDE THE `assign`)
## 1) input: header --> identifier
list <- records
item_name <- "identifier"
where <- "header" # or "header"

df_name <- paste("col", item_name, sep = "_")

## function call (that changes the name of output on the fly )
assign(df_name, value = turn_list_to_tibble(list, where, item_name ))

## 2) input: header --> setSpec
list <- records
item_name <- "setSpec"
where <- "header" # or "header"

df_name <- paste("col", item_name, sep = "_")

## function call (that changes the name of output on the fly )
df<- turn_list_to_tibble(list, where, item_name )
# rename ouptut
df_name <- paste("col", item_name, sep = "_")
assign(df_name, value = df)

```

####... ALL from meta (func)
```{r callfun_meta}
## ------- --------- 1) input: meta --> identifier
list <- records
item_name <- "identifier"
where <- "meta" # or "meta"
## function call (that changes the name of output on the fly )
df<- turn_list_to_tibble(list, where, item_name )
# rename ouptut
df_name <- paste("col_m", item_name, sep = "_")
assign(df_name, value = df)

## --------- 2) input: meta --> title
list <- records
item_name <- "title"
where <- "meta" # or "meta"
## function call (that changes the name of output on the fly )
df<- turn_list_to_tibble(list, where, item_name )
# rename ouptut
df_name <- paste("col_m", item_name, sep = "_")
assign(df_name, value = df)

## --------- 3) input: meta --> date
list <- records
item_name <- "date"
where <- "meta" # or "meta"
## function call (that changes the name of output on the fly )
df<- turn_list_to_tibble(list, where, item_name )
# rename ouptut
df_name <- paste("col_m", item_name, sep = "_")
assign(df_name, value = df)

## --------- 4) input: meta --> creator
list <- records
item_name <- "creator"
where <- "meta" # or "meta"
## function call (that changes the name of output on the fly )
df<- turn_list_to_tibble(list, where, item_name )
# rename ouptut
df_name <- paste("col_m", item_name, sep = "_")
assign(df_name, value = df)

## --------- 5) input: meta --> subject
list <- records
item_name <- "subject"
where <- "meta" # or "meta"
## function call (that changes the name of output on the fly )
df<- turn_list_to_tibble(list, where, item_name )
# rename ouptut
df_name <- paste("col_m", item_name, sep = "_")
assign(df_name, value = df)

## --------- 6)  input: meta --> description
list <- records
item_name <- "description"
where <- "meta" # or "meta"
## function call (that changes the name of output on the fly )
df<- turn_list_to_tibble(list, where, item_name )
# rename ouptut
df_name <- paste("col_m", item_name, sep = "_")
assign(df_name, value = df)
```


#### ... bind ALL - ALL cols into 1 tibble 
```{r}
list_col  <-  ls(pattern =  '^col_', all.names = TRUE)
list_col
 cat(noquote(list_col) )
df_metadata <- bind_cols(col_identifier,
                          col_m_identifier,
                          col_m_title,
                          col_m_date,
                          col_m_creator,
                          col_m_subject,
                          col_m_description,
                          col_setSpec, .name_repair = "unique"
                          ) %>% 
  janitor::clean_names()
```

# save in data/derived_data

```{r}
names(df_metadata)
dataDir <- fs::path_abs(here::here("data","raw_data"))
fileName <- "/WDR.rds"
Dir_File <- paste0(dataDir, fileName)
write_rds(x = df_metadata, file = Dir_File)
```


# Subject / keywords problem 

Following [SO Rsponse](https://stackoverflow.com/questions/48476663/extract-text-from-xml-but-file-has-duplicated-node-names/48477966#48477966)

---- DONT RUN ----- 
```{r NONVIENE, eval=FALSE}
xml_find_all(doc, ".//ArchivedIncident") %>% # iterate over each incident
  map_df(~{
    set_names(
      xml_find_all(.x, ".//value/value") %>% xml_text(), # get entry values
      xml_find_all(.x, ".//key") %>% xml_text()          # get entry keys (column names)
    ) %>% 
      as.list() %>%                                      # turn named vector to list
      flatten_df() %>%                                   # and list to df
      mutate(ID = xml_attr(.x, "ID"))                    # add id
  }) %>%
  type_convert() %>% # let R convert the values for you
  select(ID, everything()) # get it in the order you likely want
## # A tibble: 2 x 5
##      ID TEST1 TEST2 TEST3 TEST4
##   <int> <chr> <int> <chr> <chr>
## 1   100  <NA>    12     A  <NA>
## 2   101  BLAH    NA  <NA>  <NA>

# ---- my try 
child <-  xml_child(response, 3) 

xml_find_all(child, ".//record") %>% # iterate over each incident
  map_df(~{
    set_names(
      xml_find_all(.x, ".//subject/subject") %>% xml_text(), # get entry values
      xml_find_all(.x, ".//metadata") %>% xml_text()          # get entry keys (column names)
    ) %>% 
      as.list() %>%                                      # turn named vector to list
      flatten_df() %>%                                   # and list to df
      mutate(ID = xml_attr(.x, "ID"))                    # add id
  }) %>%
  type_convert() %>% # let R convert the values for you
  select(ID, everything()) # get it in the order you likely want

```

### Here I see keywords 

2022:  https://openknowledge.worldbank.org/handle/10986/36883?show=full

oai:openknowledge.worldbank.org:10986/2586

```{r}
# read WRD metadata
WDR <- readr::read_rds(here::here("data", "raw_data", "WDR.rds" ))


WDR <- WDR  %>% 
   # Extract only the portion of string AFTER the backslash {/}
   mutate(id =  stringr::str_extract(doc_mt_identifier_1, "[^/]+$") ) %>% 
   dplyr::relocate(id, .before = doc_mt_identifier_1) %>% 
   mutate(url_keys = paste0("https://openknowledge.worldbank.org/handle/10986/", id , "?show=full") )%>% 
   dplyr::relocate(url_keys, .before =  doc_mt_identifier_1)

WDR$id[1] 
WDR$url_keys[1] 
  
WDR$id[45] 
   
for (i in 1:45){
     print (WDR$url_keys[i] )
   }
```

# REVIEW --- 
### Go back and try to extract dates & keywords 

# REVIEW --- 

## 1. World Bank Projects & Operations:

---- DONT RUN ----- 
```{r API2-get, eval=FALSE, include=FALSE}
# GET call to API --> raw Unicode in the res list, which ultimately needs to be converted into JSON format.
# I added `?format=json&rows=150` to ask for 50 (if not the defauls is 10 obs)
response <- httr::GET(url = "https://search.worldbank.org/api/v2/projects?format=json&rows=50") 
httr::http_type(response)  

# convert the raw Unicode into a character vector that resembles the JSON format 
rawToChar(response$content)

 
# =============
# we can convert it into list data structure
data <- jsonlite::fromJSON(rawToChar(response$content))
head(data, n = 2)
typeof(data)

#make lists of list into df
projects <- data$projects 

projects$P178245$project_name

## Compute maximum length
max.length <- max(sapply(projects, length))
## Add NA values to list elements
projects <- lapply(projects, function(v) { c(v, rep(NA, max.length-length(v)))})
## Rbind
projects_df <- do.call(rbind, projects) %>% as_tibble()
```
  

# Wrangling the text 
Following: [@kaye_ella_2019; @robinson_words_2017]


## Tokenization 

* A _token_ is a meaningful unit of text, such as a word, that we are interested in using for analysis

* bigrams 

# connections 

Following the example of [David Robinson on HN titles](http://varianceexplained.org/r/hn-trends/)
  
# Data sources: 

1. World Bank Projects & Operations: https://datacatalog.worldbank.org/search/dataset/0037800 
 https://datacatalog.worldbank.org/search/dataset/0037800/World-Bank-Projects---Operations

  + Accessibility Classification: **public** under [Creative Commons Attribution 4.0](https://datacatalog.worldbank.org/public-licenses?fragment=cc)

2. World Bank - World Development Reports

  + Accessibility Classification:

# Acknowledgements 

+ [Computing for Social Science Course](https://cfss.uchicago.edu/syllabus/getting-data-from-the-web-api-access/)
+ [Stephanie Tran](https://github.com/transteph/world-bank-document-scraping) project who created the function `R/f_scrape_WB-OKR.R` 
+ [Renu Khandelwal](https://medium.com/geekculture/reading-xml-files-in-r-3122c3a2a8d9) tutorial 







### ----------------------------------------------------------------------------

# Reference Tutorials

[@robinson_1_2022]
[@ldal_tutorials_2022]
[@edureka!2019]

 
[David Robinson on HN titles](http://varianceexplained.org/r/hn-trends/)

[Benjamin Soltoff: Computing 4 Social Sciences - API](https://cfss.uchicago.edu/syllabus/getting-data-from-the-web-api-access/)

[Benjamin Soltoff: Computing 4 Social Sciences - text analysis](https://cfss.uchicago.edu/syllabus/text-analysis-fundamentals-and-sentiment-analysis/)

[Ben Schmidt Book Humanities Crurse](https://hdf.benschmidt.org/R/) [Ben Schmidt Book Humanities](http://benschmidt.org/HDA/texts-as-data.html)

[tidyTuesday cast on tidytext](https://github.com/dgrtwo/data-screencasts/tree/master/screencast-annotations)

1.  ✔️ [MEDIUM articles: common words, pairwise correlations - 2018-12-04](https://www.youtube.com/watch?v=C69QyycHsgE)
2.  [TidyTuesday Tweets - 2019-01-07](https://www.youtube.com/watch?v=KE9ItC3doEU)
3.  [Wine Ratings - 2019-05-31](https://www.youtube.com/watch?v=AQzZNIyjyWM) Lasso regression \| sentiment lexicon,
4.  [Simpsons Guest Stars 2019-08-30](https://www.youtube.com/watch?v=EYuuAGDeGrQ) geom_histogram
5.  [Horror Movies 2019-10-22](https://www.youtube.com/watch?v=yFRSTlk3kRQ) explaining glmnet package \| Lasso regression
6.  [The Office 2020-03-16](https://www.youtube.com/watch?v=_IvAubTDQME) geom_text_repel from ggrepel \| glmnet package to run a cross-validated LASSO regression
7.  [Animal Crossing 2020-05-05](https://www.youtube.com/watch?v=Xt7ACiedRRI) Using geom_line and geom_point to graph ratings over time \| geom_text to visualize what words are associated with positive/negative reviews \|topic modelling
