# _______ 

# >>>>>> QUI <<<<<<<<<<<<<<<<<< 
Main ref https://www.nlpdemystified.org/course/advanced-preprocessing 
rivedere cos'avevo fatto x pulire in `analysis//03_WDR_pdotracs_explor.qmd` 
https://cengel.github.io/R-text-analysis/textprep.html#detecting-patterns
https://guides.library.upenn.edu/penntdm/r
https://smltar.com/stemming#how-to-stem-text-in-r `BOOK STEMMING`

START FROM `## III.i) Tokenization`

# _______ 


## iii) Stopwords

### --- Default `tidytext` packg `stop_words`
```{r stop}
#| eval: FALSE
# default stopwords that come with the tidytext package t
sw <- tidytext::stop_words 
paint(stop_words)

```

### --- My own `custom_stop_words` |
Remove stop words, which are the most common words in a language. 

+ but I don't want to remove any *meaningful* word for now

```{r}
# Custom list of articles, prepositions, and pronouns
custom_stop_words <- c(
   # Articles
   "the", "a", "an",   
   "and", "but", "or", "yet", "so", "for", "nor", "as", "at", "by", "per",  
   # Prepositions
   "of", "in", "on", "at", "by", "with", "about", "against", "between", "into", "through", 
   "during", "before", "after", "above", "below", "to", "from", "up", "down", "under",
   "over", "again", "further", "then", "once",  
   # Pronouns
   "i", "me", "my", "myself", "we", "our", "ours", "ourselves", "you", "your",
   "yours", "yourself", "yourselves", "he", "him", "his", "himself", "she", "her", 
   "hers", "herself", "it", "its", "itself", "they", "them", "their", "theirs", "themselves" ,
   "this", "that", "these", "those", "which", "who", "whom", "whose", "what", "where",
   "when", "why", "how", "all", "any", "both", "each", "few", "more", "most", "other",
   # "some", "such", "no",  "not", 
   # "too", "very",   
   # verbs
   "is", "are", "would", "could", "will", "be"
)

# Convert to a data frame if needed for consistency with tidytext
custom_stop_words_df <- tibble(word = custom_stop_words)
```

### --- Remove `custom_stop_words` from `train` PDOs
 
```{r}
# with stopwords
pdo_train_token <- pdo_train_token %>%  # 218,191
   # without stopwords 
   # get rid of stop words (from defualt list)   
   anti_join(custom_stop_words_df, by = join_by(word)) # 138,210

# Count words
count_train <- pdo_train_token %>%
  count(word, sort = TRUE) # 11,179
```

### --- Other unwanted tokens  

You may want to use your own curated list 

+ no numbers (bc not needed in this context)
+ no saxon genitive (`text's` TURNED TO `text` )
+ no units of measurement ("km", "MW", etc)

First, I check which they are...
```{r}
# further restriction on the words
pdo_train_tok <- pdo_train_token %>%  # 142,195
   mutate(word_original = word) %>% 
   relocate(word_original, .after = pdo) %>% 
#### ------  NO `numbers` 
   # The regex "\\d" detects any digit (0-9) 
   # The regex "^\\d*\\.?\\d+$" any string that consists only of digits (with an optional decimal point)
   mutate (word_num = str_detect(word_original, "^\\d*\\.?\\d+$")) %>% 
   # The regex "^\\d*\\.?\\d+$" match numbers with no letters in the cell, allowing for both decimal points and thousands separators
   mutate (word_num2 = str_detect(word_original, "^\\d{1,3}(,\\d{3})*(\\.\\d+)?$"))  %>% 
#### ------  NO punctuation signs (except for hyphens)       
   # The regex ""[[:punct:]]"" match numbers that any string that contains at least one punctuation symbol or sign.
   mutate (word_punct2 = str_detect(word_original, "[[:punct:]]") & !str_detect(word_original, "^[[:alpha:]]+-[[:alpha:]]+$")) %>% 
#### ------  NO hyphen with nothing else (redundant for above )
   mutate (word_hyp = str_detect(word_original, "^-$")) %>%
#### ------  NO units      
   mutate(word_units = str_detect(word_original, "\\b(usd|mw|gw|kwh|1,2,3)\\b")) 
```

Second I get rid of the unwanted tokens
```{r}
pdo_train_t <- pdo_train_tok %>% # 138,210
# get rid of numbers and other non meaningful words.... 
   filter (word_num == FALSE)  %>%    # ... >  136,875
   filter (word_num2 == FALSE)  %>%   # ... >  139,769
   filter (word_punct2 == FALSE)  %>% # ... >  135,162
   # filter (word_hyp == FALSE)  %>%  # ...    (redudant with above)
   filter (word_units == FALSE)  %>%  # ... >  135,129
   filter (word != "s")  %>%      # ... >  134,858
   # DROP temporary cols
   select (-word_num, -word_num2, -word_punct2, -word_hyp, -word_units, -word_s)

# Count words
count_train <- pdo_train_t  %>%
  count(word, sort = TRUE) # 11,201--> 10,344
```


## ii) PoS Tagging
Classifying noun, verb, adjective, etc. can help discover *intent* or *action* in a sentence, or scanning "verb-noun" *patterns.*




## ii) Word stemming

to reduce them to their word stem or root form

```{r}
pdo_train_t <- pdo_train_t %>% 
   mutate(stem = wordStem(word))  # 134,858
```


# _______ 
# TEXT ANALYSIS/SUMMARY
# _______ 

NOTE: Among `word` / `stems` encountered in PDOs, there are a lot of acronyms which may refer to World Bank lingo, or local agencies, etc... Especially when looked at in low case form they don't make much sense... 

see https://cengel.github.io/R-text-analysis/textanalysis.html 

## Frequencies of documents/words/stems
```{r}
# Count words
counts_pdo <- pdo_train_t %>%
     count(pdo, sort = TRUE)  # 4,069

counts_words <- pdo_train_t %>%
     count(word, sort = TRUE)  # 10,344

counts_stems <- pdo_train_t %>%
  count(stem, sort = TRUE)   # 7,826
```

We are looking at `pdo_train_t` which has 134,858 rows and 7 columns.

+ PDOs = `r nrow(counts_pdo)` in projects  
   + ranging from `r min(pdo_train_t$boardapprovalFY)`  to `r max(pdo_train_t$boardapprovalFY)`
+ Words = `r nrow(counts_words)`
+ Stems = `r nrow(counts_stems)`

### [FIG] Overall word freq ggplot
```{r}
#| output = TRUE

# Evaluate the title with glue first
title_text <- glue::glue("Most frequent word in {n_distinct(pdo_train_t$id)} PDOs from projects approved between FY {min(pdo_train_t$boardapprovalFY)} and {max(pdo_train_t$boardapprovalFY)}") 

proj_wrd_freq <- pdo_train_t %>%
   filter (!(word %in% c("pdo","project", "development", "objective", "i","ii", "iii"))) %>%
   count(word) %>% 
   filter(n > 500) %>% 
   mutate(word = reorder(word, n)) %>%  # reorder values by frequency
   # plot 
   ggplot(aes(word, n)) +
   geom_col(fill = "gray") +
   coord_flip() + # flip x and y coordinates so we can read the words better
   labs(title = title_text,
        subtitle = "[word count > 500]", y = "", x = "")

proj_wrd_freq
```

#### [FUNC] save plots 
```{r}
f_save_plot <- function(plot_name, plot_object) {
  # Print the plot, save as PDF and PNG
  plot_object %T>%
    print() %T>%
    ggsave(., filename = here("analysis", "output", "figures", paste0(plot_name, ".pdf")),
           # width = 4, height = 2.25, units = "in",
           device = cairo_pdf) %>%
    ggsave(., filename = here("analysis", "output", "figures", paste0(plot_name, ".png")),
           # width = 4, height = 2.25, units = "in",
           type = "cairo", dpi = 300)
}

# Example of using the function
# f_save_plot("proj_wrd_freq", proj_wrd_freq)

```


```{r}
f_save_plot("proj_wrd_freq", proj_wrd_freq)
```


### [FIG] Overall stem freq ggplot
```{r}
#| output = TRUE
# Evaluate the title with glue first
title_text <- glue::glue("Most frequent *stem* in {n_distinct(pdo_train_t$id)} PDOs from projects approved between FY {min(pdo_train_t$boardapprovalFY)} and {max(pdo_train_t$boardapprovalFY)}") 

proj_stem_freq <- pdo_train_t %>%
   filter (!(stem %in% c("pdo","project", "develop", "object", "i","ii", "iii"))) %>%
   count(stem) %>% 
   filter(n > 500) %>% 
   mutate(stem = reorder(stem, n)) %>%  # reorder values by frequency
   # plot 
   ggplot(aes(stem, n)) +
   geom_col(fill = "gray") +
   coord_flip() + # flip x and y coordinates so we can read the words better
   labs(title = title_text,
        subtitle = "[stem count > 500]", y = "", x = "")

proj_stem_freq
```

```{r}
f_save_plot("proj_stem_freq", proj_stem_freq)
```
Evidently, after stemming, more words (or stems) reach the threshold frequency count of 500.


### Isolate SECTOR words and see frequency over years

```{r}
#| output = TRUE
df <- pdo_train_t %>%
   filter (stem %in% c("water", "transport", "urban", "energi", "health")) %>%
   mutate (FY = boardapprovalFY) %>%
   # group_by(FY) %>% 
   #summarize (n_rep = length(stem)) %>%
   count(FY,  stem) 

#df$FY

proj_sect_stem_fr <-  ggplot(data = df, aes(x = FY, y = n, group = stem, color = stem)) +
   geom_line() +
   geom_point() +
   scale_x_continuous(breaks =  seq(2001, 2023, by=  2)) +
   scale_color_viridis_d(option = "magma", end = 0.9) + 
   facet_wrap(~stem, ncol = 2, scales = "free")+   guides(color = FALSE) +
   theme_bw()+
   theme(# Adjust angle and alignment of x labels
      axis.text.x = element_text(angle = 45, hjust = 1)) + 
   labs(title = "Sector words frequency in PDO over Fiscal Years",x =   "Board approval FY", y = "Counts of 'sector' word (stem)") + 
   geom_vline(data = subset(df, stem == "health"), aes(xintercept = 2020), 
              linetype = "dashed", color = "#9b6723") +
   geom_text(data = subset(df, stem == "health"), aes(x = 2020, y = max(df$n)*0.85, label = "Covid"), 
             angle = 90, vjust = -0.5, color = "#9b6723")


proj_sect_stem_fr
```

```{r}
f_save_plot("proj_sect_stem_fr", proj_sect_stem_fr)
```

### Isolate INSTITUTIONAL words and see frequency over years


```{r}
#| output = TRUE
df <- pdo_train_t %>%
   mutate (stem = if_else (stem == "sme" | stem ==  "msme" , "sme-msme", stem)) %>%
   filter (stem %in% c( "public","privat", "govern","ngo", "enterpris", "sme-msme")) %>%
   mutate (FY = boardapprovalFY) %>%
   # group_by(FY) %>% 
   #summarize (n_rep = length(stem)) %>%
   count(FY,  stem)  
   

#df$FY

proj_inst_stem_fr <-  ggplot(data = df, aes(x = FY, y = n, group = stem, color = stem)) +
   geom_line() +
   geom_point() +
   scale_x_continuous(breaks =  seq(2001, 2023, by=  2)) +
   scale_color_viridis_d(option = "magma", end = 0.9) + 
   # Reorders the stem variable based on the total count (n), with .desc = TRUE to order from highest to lowest
   facet_wrap(~ fct_reorder(stem, n, .fun = sum, .desc = TRUE), ncol = 2, scales = "free_y") +
   guides(color = FALSE) +
   theme_bw()+
   theme(# Adjust angle and alignment of x labels
      axis.text.x = element_text(angle = 45, hjust = 1)) + 
   labs(title = "Sector words frequency in PDO over Fiscal Years",x =   "Board approval FY", y = "Counts of 'sector' word (stem)")  # + 
# geom_vline(data = subset(df, stem == "health"), aes(xintercept = 2020), 
#            linetype = "dashed", color = "#9b6723") +
# geom_text(data = subset(df, stem == "health"), aes(x = 2020, y = max(df$n)*0.85, label = "Covid"), 
#           angle = 90, vjust = -0.5, color = "#9b6723")


proj_inst_stem_fr
```

```{r}
f_save_plot("proj_inst_stem_fr", proj_inst_stem_fr)
```





## Term frequency




 


## Word and document frequency: Tf-idf

The goal is to quantify what a document is about. What is the document about?

-   **term frequency (tf)** = how frequently a word occurs in a document... but there are words that occur many time and are not important
-   term's **inverse document frequency (idf)** = decreases the weight for commonly used words and increases the weight for words that are not used very much in a collection of documents.
-   **statistic tf-idf (= tf*-*idf)** = an alternative to using stopwords is *the frequency of a term adjusted for how rarely it is used*. [It measures how important a word is to a document in a collection (or corpus) of documents, but it is still a rule-of-thumb or heuristic quantity]

> The tf-idf is the product of the term frequency and the inverse document frequency::

## N-Grams
...

## Co-occurrence
...


# _______ 
# TOPIC MODELING  
# _______ 

Topic modeling is an **unsupervised machine learning** technique  used to hat exploratively identifies latent topics based on frequently co-occurring words. 

It can identify topics or themes that occur in a collection of documents, allowing hidden patterns and relationships within text data to be discovered. It is widely applied in fields such as social sciences and humanities.

https://bookdown.org/valerie_hase/TextasData_HS2021/tutorial-13-topic-modeling.html

https://m-clark.github.io/text-analysis-with-R/topic-modeling.html

https://sicss.io/2020/materials/day3-text-analysis/topic-modeling/rmarkdown/Topic_Modeling.html

## Document-Term Matrix
...

## Latent Dirichlet Allocation (LDA)
...
SPIEGA https://towardsdatascience.com/latent-dirichlet-allocation-lda-9d1cd064ffa2



## /include independent variables in my topic model?
https://bookdown.org/valerie_hase/TextasData_HS2021/tutorial-13-topic-modeling.html#how-do-i-include-independent-variables-in-my-topic-model

# _______ 
# STRUCTURAL TOPIC MODELING (STM)
# _______ 

The Structural Topic Model is a general framework for topic modeling with document-level covariate information. The covariates can improve inference and qualitative interpretability and are allowed to affect topical prevalence, topical content or both. 


MAIN REFERENCE `stm` R package  http://www.structuraltopicmodel.com/
EXAMPLE UN corpus https://content-analysis-with-r.com/6-topic_models.html 
STM 1/2  https://jovantrajceski.medium.com/structural-topic-modeling-with-r-part-i-2da2b353d362
STM 2/2 https://jovantrajceski.medium.com/structural-topic-modeling-with-r-part-ii-462e6e07328


# BERTopic
Developed by Maarten Grootendorst, BERTopic enhances the process of discovering topics by using document embeddings and a class-based variation of Term Frequency-Inverse Document Frequency (TF-IDF).


https://medium.com/@supunicgn/a-beginners-guide-to-bertopic-5c8d3af281e8 


# _______ 
# (dYnamic) TOPIC MODELING OVER TIME 
# _______ 

Example: [An analysis of Peter Pan using the R package koRpus](https://irhuru.github.io/blog/korpus-peterpan/)
https://ladal.edu.au/topicmodels.html#Topic_proportions_over_time 


